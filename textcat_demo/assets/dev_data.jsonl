{"text": "Determining Gains Acquired from Word Embedding Quantitatively using Discrete Distribution Clustering\n1 000\n013\n014\n015\n016\n017\n018\n019\n020\n021\n022\n023\n024\n025\n026\n027\n028\n029\n030\n031\n032\n033\n034\n035\n036\n037\n038\n039\n040\n041\n042\n043\n044\n045\n046\n047\n048\n049\n063\n064\n065\n066\n067\n068\n069\n070\n071\n072\n073\n074\n075\n076\n077\n078\n079\n080\n081\n082\n083\n084\n085\n086\n087\n088\n089\n090\n091\n092\n093\n094\n095\n096\n097\n098\n099\n1 Introduction\nWord embeddings, or word vectors, have been broadly adopted for document analysis (Mikolov et al., 2013b,a). A key appeal of word embedding methods is that they can be obtained from external large-scale corpus and then be easily utilized for different data. Before choosing word embeddings for data analysis, researchers must first consider how much extra gain can be brought from the \u201cembedded\u201d knowledge of words in comparison with that achieved by existing bag-of-words based approaches. Moreover, they must also consider how to quantify that gain. Such a preliminary evaluation is often necessary before any further decisions can be made about the data.\nAnswering such questions is important: almost every model used in practice exploits some basic\nrepresentations \u2014 bag-of-words and word embeddings \u2014 for the sake of its computational tractability. Based on word embeddings, high-level models are designed for various tasks. Examples include entity representations, similarity measures, data manifolds, hierarchical structures, language models, and neural architectures. Therefore, it is important to investigate whether the gain or loss found in practice should be credited to the extra assumptions associated with those high-level models or to the use of basic word embeddings. As our experiments demonstrate, introducing these extra assumptions will make individual methods effective only if certain constraints are met. We will address this issue from an unsupervised perspective.\nOur proposed clustering framework has several advantages. Instead of suppressing a document into a fixed-length vector feeding post-analysis, our framework uses the Wasserstein distance (or the Earth Mover\u2019s Distance, EMD) as a metadistance to quantify the dissimilarity between two empirical nonparametric measures (or discrete distributions) over word embedding space (Wan, 2007; Kusner et al., 2015). Hence, it excludes any vector representation of the documents and sidesteps extra high-level assumptions, which is crucial and beneficial to evaluating the gain from basic word embeddings.\nOur approach is intuitive and robust. the Wasserstein distance considers the cross-term relationship between different words in a principled fashion. As defined, the distance between two documents, say A and B, are the minimum cumulative cost that words from document A need to \u201ctravel\u201d to match exactly the point cloud of document B. Here, the travel cost of a path between two words is their (squared) Euclidean distance in the word embedding space. Therefore, how much\n2\n101\n102\n103\n104\n105\n106\n107\n108\n109\n110\n111\n112\n113\n114\n115\n116\n117\n118\n119\n120\n121\n122\n123\n124\n125\n126\n127\n128\n129\n130\n131\n132\n133\n134\n135\n136\n137\n138\n139\n140\n141\n142\n143\n144\n145\n146\n147\n148\n149\n150\n151\n152\n153\n154\n155\n156\n157\n158\n159\n160\n161\n162\n163\n164\n165\n166\n167\n168\n169\n170\n171\n172\n173\n174\n175\n176\n177\n178\n179\n180\n181\n182\n183\n184\n185\n186\n187\n188\n189\n190\n191\n192\n193\n194\n195\n196\n197\n198\n199\nbenefit Wasserstein distance brings also depends on how the utilized word embedding space flattens document data as distributions, disentangling independent factors into different regions, subject to the interest of task.\nWhile Wasserstein distance is well suited for document analysis, a major obstacle of EMDbased approaches is the high-magnitude computation involved, especially for the original D2clustering method (Li and Wang, 2008) \u2014 an EMD-based clustering framework. The main technical hurdle is to compute the Wasserstein barycenter, which is a discrete distribution, efficiently for a given set of discrete distributions. Thanks to the recent advance of algorithms for efficient solving of Wasserstein barycenters, one can now perform document clustering in a nonparametric way by directly treating them as empirical measures over word embedding space (Cuturi and Doucet, 2014; Ye and Li, 2014; Benamou et al., 2015; Ye et al., 2017). In short, the intuition behind such a nonparametric framework is: By keeping D2-clustering to a minimal assumption set, we can achieve results of the highest possible clustering quality regardless the size and patterns of data. Obtaining highest quality clustering of unstructured text data merits consuming extra computational resources. Because it is a crucial step in many techniques and applications of natural language processing, such as cross-document co-reference resolution (Singh et al., 2011), document summarization (Radev et al., 2004; Wang and Li, 2010), retrospective events detection (Yang et al., 1998), and opinion mining (Zhai et al., 2011).\nOur contributions. Our work has two main contributions. First, we create a basic tool of document clustering with mere hyper-parameters at scale. Our tool leverages the state-of-the-art numerical toolbox developed for optimal transport to achieve computational feasibility. Meanwhile, it gives state-of-the-art clustering performances across heterogeneous text data \u2014 an advantage over other methods in the literature. Second, with our tool, one can quantitatively inspect how well a word-embedding model can fit the data and how much gain or loss will be obtained compared to traditional bag-of-words models. Acquiring insights on these questions is valuable for document analysis beyond clustering. The exploration of D2-clustering for documents will provide a win-\ndow for investigating these questions.\n2 Related Work\nIn the original D2-clustering framework (Li and Wang, 2008), calculating Wasserstein barycenter involves solving a large-scale LP problem at each inner iteration, severely limiting the scalability and robustness of the framework. Such high magnitude of computations had prohibited it from many real-world applications until recently. To accelerate the computation of Wasserstein barycenter, and ultimately to improve D2clustering, multiple numerical algorithmic efforts have been made in the recent few years (Cuturi and Doucet, 2014; Ye and Li, 2014; Benamou et al., 2015; Ye et al., 2017).\nAlthough the effectiveness of Wasserstein distance has been well recognized in the computer vision and multimedia literature, the property of Wasserstein barycenter has not been well understood. To our knowledge, there still lacks systematic study of applying Wasserstein barycenter and D2-clustering in document analysis with word embeddings.\nA closely related work to ours was proposed by authors of (Kusner et al., 2015) who recently connected the Wasserstein distance to the word embeddings for comparing documents. Our work differs from theirs in the methodology. We directly pursue a scalable clustering setting rather than construct a nearest neighbor graph based on calculated distances, because the calculation of the Wasserstein distances of all pairs is too expensive to be practical. The authors of (Kusner et al., 2015) used a lower bound that was cheaper to compute in order to prune unnecessary full distance calculation, but the scalability of this modified approach is still considered very limited, an issue to be discussed in Section 4.3. On the other hand, our approach adopts the framework similar to K-means which is of complexity O(n) per iteration, and usually converges within tens of iterations. The computation of D2-clustering, though in its original form was magnitudes heavier than other document clustering methods, can now be done with efficient parallelization and proper implementations (Ye et al., 2017).\n3 Our Approach\nThis section introduces the distance, the D2clustering technique, the fast computation frame-\n3\n201\n202\n203\n204\n205\n206\n207\n208\n209\n210\n211\n212\n213\n214\n215\n216\n217\n218\n219\n220\n221\n222\n223\n224\n225\n226\n227\n228\n229\n230\n231\n232\n233\n234\n235\n236\n237\n238\n239\n240\n241\n242\n243\n244\n245\n246\n247\n248\n249\n250\n251\n252\n253\n254\n255\n256\n257\n258\n259\n260\n261\n262\n263\n264\n265\n266\n267\n268\n269\n270\n271\n272\n273\n274\n275\n276\n277\n278\n279\n280\n281\n282\n283\n284\n285\n286\n287\n288\n289\n290\n291\n292\n293\n294\n295\n296\n297\n298\n299\nwork, and how they are used in the proposed document clustering method.\n3.1 Wasserstein Distance\nSuppose we represent each document d k consisting m\nk unique words by a discrete measure or a discrete distribution, where k = 1, . . . , N with N being the sample size:\nd k =\nX mk\ni=1 w(k) i x (k) i . (1)\nHere x denotes the Dirac measure with support x, and w(k)\ni 0 is the \u201cimportance weight\u201d for the i-th word in the k-th document, with P mk i=1w (k) i = 1. And x(k) i\n2 Rd, called a support point, is the semantic embedding vector of the i-th word. The 2nd-order Wasserstein distance between two documents d1 and d2 (and likewise for any document pairs) is defined by the following LP problem: W 2(d1, d2) :=\nmin\n\u21e7\nP i,j \u21e1 i,j kx(1) i x(2) j\nk22 s.t. P m2 j=1 \u21e1i,j = wi, 8i, P m1 i=1 \u21e1i,j = wj , 8j\n\u21e1 i,j 0, 8i, j , (2)\nwhere \u21e7 = {\u21e1 i,j } is a m1 \u21e5 m2 coupling matrix, and let {C\ni,j := kx(1) i x(2) j k22} be transportation costs between words. Wasserstein distance is a true metric (Villani, 2003) for measures, and its best exact algorithm has a complexity of O(m3 logm) (Pele and Werman, 2009; Cuturi, 2013), if m1 = m2 = m.\n3.2 Discrete Distribution (D2-) Clustering\nD2-clustering (Li and Wang, 2008) iterates between the assignment step and centroids updating step in a similar way as the Lloyd\u2019s K-means. Suppose we are to find K clusters. The assignment step finds each member distribution its nearest mean from K candidates. The mean of each cluster is again a discrete distribution with m support points, denoted by c\ni , i = 1, . . . ,K. Each mean is iteratively updated to minimize its total within cluster variation. We can write the D2clustering problem as follows: given sample data {d\nk }N k=1, support size of means m, and desired\nnumber of clusters K, D2-clustering solves\nmin\nc1,...,cK\nX N\nk=1 min 1iK W 2(d k , c i ) , (3)\nwhere c1, . . . , cK are Wasserstein barycenters. At the core of solving the above formulation is an optimization method that searches the Wasserstein barycenters of varying partitions. Therefore, we concentrate on the following problem. For each cluster, we reorganize the index of member distributions from 1, . . . , n. The Wasserstein barycenter (Agueh and Carlier, 2011; Cuturi and Doucet, 2014) is by definition the solution of\nmin\nc\nX n\nk=1 W 2(d k , c) , (4)\nwhere c = P m\ni=1wi xi . The above Wasserstein barycenter formulation involves two levels of optimization: the outer level finding the minimizer of total variations, and the inner level solving Wasserstein distances. We remark that in D2clustering, we need to solve multiple Wasserstein barycenters rather than a single one. This constitutes the third level of optimization.\n3.3 Modified Bregman ADMM for Computing Wasserstein Barycenter\nThe recent modified Bregman alternating direction method of multiplier (B-ADMM) algorithm (Ye et al., 2017), motivated by (Wang and Banerjee, 2014), is a practical choice for computing Wasserstein barycenters. We briefly sketch their algorithmic procedure of this optimization method here for the sake of completeness. To solve for Wasserstein barycenter defined in Eq. (4), the key procedure of the modified Bregman ADMM involves iterative updates of four block of primal variables: the support points of c \u2014 {x\ni }m i=1 (with trans-\nportation costs {C i,j }(k) for k = 1, . . . , n), the importance weights of c \u2014 {w\ni }m i=1, and two sets of\nsplit matching variables \u2014 {\u21e1(k,1) i,j } and {\u21e1(k,2) i,j }, for k = 1, . . . , n, as well as Lagrangian variables { (k)\ni,j } for k = 1, . . . , n. In the end, both {\u21e1(k,1) i,j } and {\u21e1(k,2)\ni,j } converge to the matching weight in Eq. (2) with respect to d(c, d\nk ). The iterative algorithm proceeds as follows until c converges or a maximum number of iterations are reached: given constant \u2327 10, \u21e2 / P i,j,k C(k) i,jP\nn k=1mkm and round-off\ntolerance \u270f = 10 10, those variables are updated in the following order. Update {x\ni }m i=1 and {C (k) i,j } in every \u2327 iterations:\nx i :=\n1\nnw i\nX n\nk=1\nX mk\nj=1 \u21e1(k,1) i,j x(k) j , 8i, (5)\nC(k) i,j := kx i x(k) j k22, 8i, j and k, (6)\n4\n301\n302\n303\n304\n305\n306\n307\n308\n309\n310\n311\n312\n313\n314\n315\n316\n317\n318\n319\n320\n321\n322\n323\n324\n325\n326\n327\n328\n329\n330\n331\n332\n333\n334\n335\n336\n337\n338\n339\n340\n341\n342\n343\n344\n345\n346\n347\n348\n349\n350\n351\n352\n353\n354\n355\n356\n357\n358\n359\n360\n361\n362\n363\n364\n365\n366\n367\n368\n369\n370\n371\n372\n373\n374\n375\n376\n377\n378\n379\n380\n381\n382\n383\n384\n385\n386\n387\n388\n389\n390\n391\n392\n393\n394\n395\n396\n397\n398\n399\nUpdate {\u21e1(k,1) i,j } and {\u21e1(k,2) i,j }. For each i, j and k,\n\u21e1(k,2) i,j := \u21e1(k,2) i,j exp\nC(k)\ni,j\n(k) i,j\n\u21e2\n! + \u270f (7)\n\u21e1(k,1) i,j := w(k) j \u21e1(k,2) i,j\n.\u21e3X m\nl=1 \u21e1(k,2) l,j\n\u2318 (8)\n\u21e1(k,1) i,j := \u21e1(k,1) i,j exp \u21e3 (k) i,j /\u21e2 \u2318 + \u270f (9)\nUpdate {w i }m i=1. For i = 1, . . . ,m ,\nw i :=\nnX\nk=1\nP mk j=1 \u21e1 (k,1) i,j\nP i,j \u21e1(k,1) i,j\n(10)\nw i := w i\n.\u21e3X m\ni=1 w i\n\u2318 (11)\nUpdate {\u21e1(k,2) i,j } and { (k) i,j }. For each i, j and k,\n\u21e1(k,2) i,j := w i \u21e1(k,1) i,j\n.\u21e3X mk\nl=1 \u21e1(k,1) i,l\n\u2318 (12)\n(k) i,j := (k) i,j + \u21e2 \u21e3 \u21e1(k,1) i,l \u21e1(k,2) i,l \u2318 . (13)\nEq. (5)-(13) can all be vectorized as very efficient numerical routines. In a data parallel implementation, only Eq. (5) and Eq. (10) (involving P n\nk=1) needs to be synchronized. The software package detailed in (Ye et al., 2017) was used to generate relevant experiments. We make available our codes and pre-processed datasets for reproducing all experiments of our approach.\n4 Experiments\n4.1 Datasets and Evaluation Metrics\nWe prepare six datasets to conduct a set of experiments. Two short-text datasets are created as follows. (1) BBCNews abstract: we concatenate the title and the first sentence of news posts from BBCNews dataset1 to create an abstract version. (2) Wiki events: each cluster/class contains a set of news abstracts on the same story such as \u201c2014 Crimean Crisis\u201d crawled from Wikipedia current events following (Wu et al., 2015); this dataset offers more challenges because it has more finegrained classes and fewer documents (with shorter length) per class than the others have. It also shows more realistic nature of real-world applications such as news event clustering.\nWe also experiment with two long-text datasets and two domain-specific text datasets.\n1BBCNews and BBCSport are downloaded from http://mlg.ucd.ie/datasets/bbc.html\n(3) Reuters-21578: we obtain the original Reuters21578 text dataset and process as follows: remove documents with multiple categories, remove documents with empty body, remove duplicates, and select documents from the largest 10 categories. Reuters dataset is a highly unbalanced dataset (the top category has more than 3000 documents while the 10-th category has fewer than 100), this imbalance induces some extra randomness in comparing the results. (4) 20Newsgroups \u201cbydate\u201d version: we obtain the raw \u201cbydate\u201d versionand process them as follows: remove headers and footers, remove URLs and Email addresses, delete documents with less than 10 words. 20Newsgroups have roughly comparable sizes of categories. (5) BBCSports. (6) Ohsumed and Ohsumed-full: documents are medical abstracts from the MeSH categories of the year 1991. Specifically, there are 23 cardiovascular diseases categories.\nEvaluating clustering results is known to be nontrivial. We use the following three sets of quantitative metrics to assess the quality of clusters by knowing the groundtruth categorical labels of documents: (1) Homogeneity, Completeness, and V-measure (Rosenberg and Hirschberg, 2007); (2) Adjusted Mutual Information (AMI) (Vinh et al., 2010); (3) Adjusted Rand Index (ARI) (Rand, 1971). For sensitivity analysis, we use the homogeneity score (Rosenberg and Hirschberg, 2007) as a projection dimension of other metrics, creating a 2D plot to visualize the metrics of a method along different homogeneity levels. Generally speaking, more clusters leads to higher homogeneity by chance.\n4.2 Methods in Comparison\nWe examine four categories of methods that place a vector-space model over documents, and compare them to our D2-clustering framework. When needed, we use K-means++ to obtain clusters from dimension reduced vectors. To diminish the randomness brought by K-mean initialization, we ensemble the clustering results of 50 repeated runs (Strehl and Ghosh, 2003), and report the metrics for the ensembled one. The largest possible vocabulary used, excluding word embedding based approaches, is composed of words appearing in at least two documents. On each dataset, we select the same set of Ks, the number of clusters, for all methods. Typically, Ks are chosen around\n5\n401\n402\n403\n404\n405\n406\n407\n408\n409\n410\n411\n412\n413\n414\n415\n416\n417\n418\n419\n420\n421\n422\n423\n424\n425\n426\n427\n428\n429\n430\n431\n432\n433\n434\n435\n436\n437\n438\n439\n440\n441\n442\n443\n444\n445\n446\n447\n448\n449\n456\n457\n458\n459\n460\n461\n462\n463\n464\n465\n466\n467\n468\n469\n470\n471\n472\n473\n483\n484\n485\n486\n487\n488\n489\n490\n491\n492\n493\n494\n495\n496\n497\n498\n499\nthe number of groundtruth categories in logarithmic scale.\nWe prepare two versions of the TF-IDF vectors as the unigram model. The ensembled K-means methods are used to obtain clusters. (1) TF-IDF vector (Sparck Jones, 1972). (2) TF-IDF-N vector is found by choosing the most frequent N words in a corpus, where N 2 {500, 1000, 1500, 2000}. The difference between the two methods highlights the sensitivity issue brought by the size of chosen vocabulary.\nWe also compare our approach with the following seven additional baselines. They are (3) Spectral Clustering (Laplacian) (4) Latent Semantic Indexing (LSI) (Deerwester et al., 1990). (5) Locality Preserving Projection (LPP) (He and Niyogi, 2004; Cai et al., 2005). (6) Nonnegative Matrix Factorization (NMF) (Lee and Seung, 1999; Xu et al., 2003). (7) Latent Dirichlet Allocation (LDA) (Blei et al., 2003; Hoffman et al., 2010; Pritchard et al., 2000). (8) Average of word vectors (AvgDoc) (9) Paragraph Vectors (PV) (Le and Mikolov, 2014). Details on their experimental setups and hyper-parameter search strategies can be found in the Appendix.\n4.3 Runtime\nWe report the runtime for our approach upon two largest datasets. The experiments regarding other smaller datasets all finish within minutes in a single machine, which we omit due to page constraints. Like K-means, the running time spent by our approach depends on the number of actual iterations before a termination criterion is met. In the Newsgroups dataset, with m = 100 and K = 45, the time per iteration is 121 seconds on 48 processors. In Reuters dataset, with m = 100 and K = 20, the time per iteration is 190 seconds on 24 processors. Each run finishes in around tens of iterations typically, upon which the percentage of label changes is less than 0.1%.\nOur approach adopts the Elkan\u2019s algorithm pruning unnecessary computations of Wasserstein distance in assignment steps of K-means (Elkan, 2003). For the Newsgroups data (with m = 100 and K = 45), our approach terminates in 36 iterations, and totally computes 12, 162, 717 (\u21e1 3.5% \u21e5 186122) distance pairs in assignment steps, saving 60% (\u21e1 1 12,162,71736\u21e545\u21e518612 ) distance pairs to calculate in the standard D2-clustering. In comparison, the clustering approaches based on\nK nearest neighbor graph with the prefetch-andprune method of (Kusner et al., 2015) needs substantially more pairs to compute Wasserstein distance, meanwhile the speed-ups also suffer from the curse of dimensionality. Their detailed statistics are reported in Table 1. Based on the results, our approach is much more practical as a basic document clustering tool.\n4.4 Results\nWe summarize our numerical results in this section.\nRegular text datasets. The first four datasets in Table 2 cover quite general and broad topics. We consider them to be regular and representative datasets encountered more frequently in applications. We report the clustering performances of the ten methods in Fig. 1, where three different metrics are plotted against the clustering homogeneity. The higher result at the same level of homogeneity is better, and the ability to achieve higher homogeneity is also welcomed. Clearly, D2-clustering is the only method that shows ro-\n6\n501\n502\n503\n504\n505\n506\n507\n508\n509\n510\n511\n512\n513\n514\n515\n516\n517\n518\n519\n520\n521\n522\n525\n526\n527\n528\n529\n530\n531\n532\n533\n534\n535\n536\n537\n538\n539\n540\n541\n542\n543\n544\n545\n546\n547\n548\n549\n550\n551\n552\n553\n554\n555\n556\n557\n558\n559\n560\n561\n562\n563\n564\n565\n566\n567\n568\n569\n570\n571\n572\n575\n576\n577\n578\n579\n580\n581\n582\n583\n584\n585\n586\n587\n588\n589\n590\n591\n592\n593\n594\n595\n596\n597\n598\n599\nbustly superior performances among all ten methods. Specifically, it ranks first in three datasets, and second in the other one. In comparison, LDA performs competitively on the \u201cReuters\u201d dataset, but is substantially unsuccessful on others. Meanwhile, LPP performs competitively on the \u201cWiki events\u201d and \u201cNewsgroups\u201d datasets, but it underperforms on other two. Laplacian, LSI, and TfidfN can achieve comparably performance if their reduced dimensions are fine tuned, which unfortunately is not realistic in practice. NMF is a simple and effective method which always gives stable, though subpar, performance.\nShort texts vs. long texts. D2-clustering performs much more impressive on short texts (\u201cBBC abstract\u201d and \u201cWiki events\u201d) than it does on long texts (\u201cReuters\u201d and \u201cNewsgroups\u201d). This outcome is somewhat expected, because the bag-ofwords method suffers from high sparsity for short texts, and word-embedding based methods in theory should have an edge here. As shown in Fig. 1, D2-clustering has indeed outperformed other nonembedding approaches by a large margin on short texts (improved by about 40% and 20% respectively). Nevertheless, we find lifting from word embedding to document clustering is not a free\nlunch. Neither AvgDoc nor PV can perform as competitively as D2-clustering performs on both. Domain-specific text datasets. We are also interested in how word embedding can help group domain-specific texts into clusters. In particular, does the semantic knowledge \u201cembedded\u201d in words provides enough clues to discriminate fine-grained concepts? We report the best AMI achieved by each method in Table 3. Our preliminary result indicates state-of-the-art word embeddings do not provide enough gain here to exceed the performance of existing methodologies. On the easy one, aka \u201cBBCSport\u201d dataset, basic bagof-words approach (Tfidf and Tfidf-N) already suffices to discriminate different sport categories; and on the hard one, aka \u201cOhsumed\u201d dataset, D2clustering only slightly improves over Tf-idf and others, ranking behind LPP. Meanwhile, we feel the overall quality of clustering \u201cOhsumed\u201d texts is quite far from useful in practice, no matter which method to use. (See next section for more discussions.)\n4.5 Sensitivity to Word Embeddings.\nWe validate the robustness of D2 clustering with different word embedding models, and we also\n7\n612\n613\n627\n628\n629\n630\n631\n632\n633\n634\n635\n636\n637\n638\n639\n640\n641\n642\n643\n644\n645\n646\n647\n648\n649\n662\n663\n677\n678\n679\n680\n681\n682\n683\n684\n685\n686\n687\n688\n689\n690\n691\n692\n693\n694\n695\n696\n697\n698\n699\nshow all their results in Fig. 2. As we mentioned, the effectiveness of Wasserstein document clustering depends on how relevant the utilized word embeddings are with the tasks. In those general document clustering tasks, however, word embedding models trained on general corpus perform robustly well with acceptably small variations. This outcome reveals our framework as generally effective and not dependent on a specific word embedding model. In addition, we also conduct experiments with word embeddings with a smaller dimension, aka 50 and 100. Their results are not as good as those we reported (therefore detailed numbers are not included due to space limit).\nInadequate embeddings may not be disastrous. In addition to our standard running set, we also try D2-clustering with purely random word embeddings, meaning each word vector is independently sampled from spherical Gaussian at 300 dimen-\nARI AMI V-measure BBCNews .146 .187 .190\nabstract .792 +442% .759+306% .762+301%\nWiki events .194 .369 .463 .277 +43% .545+48% .611+32%\nReuters .498 .524 .588 .515 +3% .534+2% .594+1%\nNewsgroups .194 .358 .390 .305 +57% .493+38% .499+28%\nBBCSport .755 .740 .760 .801 +6% .812+10% .817+8%\nOhsumed .080 .204 .292 .116 +45% .260+27% .349+20%\nTable 4: Comparison between random word embeddings (upper row) and meaningful pre-trained word embeddings (lower row), based on their best ARI, AMI, and V-measures. The improvements by percentiles are also shown in the subscripts.\n8\n704\n705\n706\n707\n708\n709\n710\n711\n712\n713\n714\n715\n716\n717\n718\n719\n720\n721\n722\n723\n724\n725\n726\n727\n728\n729\n730\n731\n732\n733\n734\n735\n736\n737\n738\n739\n740\n741\n742\n743\n744\n745\n746\n747\n748\n749\n750\n751\n752\n753\n754\n755\n756\n757\n758\n759\n760\n761\n762\n763\n764\n765\n766\n767\n768\n769\n770\n771\n772\n773\n774\n775\n776\n777\n778\n779\n780\n781\n782\n783\n784\n785\n786\n787\n788\n789\n790\n791\n792\n793\n794\n795\n796\n797\n798\n799\nsion, to see how deficient it can be. From our experimental results, random word embeddings degrade the performance of D2-clustering, but it still performs much better than purely random clustering does, and is even consistently better than LDA. Its performances across different datasets is highly correlated with the bag-of-words (Tfidf and TfidfN). By comparing a pre-trained word embedding model to a randomly generated one, we find that the extra gain is significant (> 10%) in clustering four of the six datasets. Their detailed statistics are in Table 4 and Fig. 3.\n5 Discussions\nPerformance advantage. There has been one immediate observation from these studies, D2clustering always outperforms two of its degenerated cases, namely Tf-idf and AvgDoc, and three other popular methods: LDA, NMF, and PV, on all tasks. Therefore, for document clustering, users can expect to gain performance improvements by using our approach. Clustering sensitivity. From the four 2D plots in Fig. 1, we notice that the results of Laplacian, LSI and Tfidf-N are rather sensitive to their extra hyper-parameters. Once the vocabulary set, weight scheme and embeddings of words are fixed, our framework involves only two additional hyper-parameters: the number of intended clusters, K, and the selected support size of centroid distributions, m. We have chosen more than one m in all related experiments (m = {64, 100} for long documents, and m = 10, 20 for short documents). Our empirical experiments show that the effect of m on different metrics is less sensitive than the change of K. As shown in Fig. 1, results at different K are plotted for each method. The gray dots denote results of multiple runs of D2clustering. They are always contracted around the top-right region of the whole population, revealing\nthe predictive and robustly supreme performance of our approach. When bag-of-words suffices. Among the results of \u201cBBCSport\u201d dataset, Tfidf-N shows that by restricting the vocabulary set into a smaller one (which may be more relevant to the interest of tasks), it already can achieve highest clustering AMI without any other techniques. Other unsupervised regularization over data is likely unnecessary, or even degrades the performance slightly. Toward better word embeddings. Our experiments on the Ohsumed dataset have been limited. The result shows that it could be highly desirable to incorporate certain domain knowledge to derive more effective vector embeddings of words and phrases to encode their domainspecific knowledge, such as jargons that have knowledge dependencies and hierarchies in educational data mining, and signal words that capture multi-dimensional aspects of emotions in sentiment analysis.\nFinally, we report the best AMIs of all methods on all datasets in Table 3. By looking at each method and the average of best AMIs over six datasets, we find our proposed clustering framework often performs competitively and robustly, which is the only method reaching more than 90% of the best AMI on each dataset. Furthermore, this observation holds for varying lengths of documents and varying difficulty levels of clustering tasks. Our nonparametric framework benefits from both bag-of-words and word embeddings.\n6 Conclusions\nThis paper introduces a nonparametric clustering framework for document analysis. Its computational tractability, robustness and supreme performance, as a fundamental tool, are empirically validated. Its ease of use enables data scientists to use it for the pre-screening purpose of examining word embeddings in a specific task. Finally, the gains acquired from word embeddings are quantitatively measured from a nonparametric unsupervised perspective.\n9\n801\n802\n803\n804\n805\n806\n807\n808\n809\n810\n811\n812\n813\n814\n815\n816\n817\n818\n819\n820\n821\n822\n823\n824\n825\n826\n827\n828\n829\n830\n831\n832\n833\n834\n835\n836\n837\n838\n839\n840\n841\n842\n843\n844\n845\n846\n847\n848\n849\n850\n851\n852\n853\n854\n855\n856\n857\n858\n859\n860\n861\n862\n863\n864\n865\n866\n867\n868\n869\n870\n871\n872\n873\n874\n875\n876\n877\n878\n879\n880\n881\n882\n883\n884\n885\n886\n887\n888\n889\n890\n891\n892\n893\n894\n895\n896\n897\n898\n899\n", "cats": {"READ": 1.0, "DONTREAD": 0.0}, "reviews": [{"IMPACT": "3", "SUBSTANCE": "2", "APPROPRIATENESS": "4", "MEANINGFUL_COMPARISON": "3", "PRESENTATION_FORMAT": "Poster", "comments": "- Strengths:\n\n- Weaknesses:\nMany grammar errors, such as the abstract\n\n- General Discussion:", "SOUNDNESS_CORRECTNESS": "4", "ORIGINALITY": "3", "is_meta_review": null, "RECOMMENDATION": "4", "CLARITY": "3", "REVIEWER_CONFIDENCE": "4"}, {"IMPACT": "3", "SUBSTANCE": "4", "APPROPRIATENESS": "5", "MEANINGFUL_COMPARISON": "3", "PRESENTATION_FORMAT": "Oral Presentation", "comments": "- Strengths: Introduces  a new document clustering approach and compares it to\nseveral established methods, showing that it improves results in most cases.\nThe analysis is very detailed and thorough--quite dense in many places and\nrequires careful reading.\n\nThe presentation is organized and clear, and I am impressed by the range of\ncomparisons and influential factors that were considered. Argument is\nconvincing and the work should influence future approaches.\n\n- Weaknesses:\n\n The paper does not provide any information on the availability of the software\ndescribed.\n\n- General Discussion:\n\nNeeds some (minor) editing for English and typos--here are just a few:\n\nLine 124: regardless the size > regardless of the size\nLine 126: resources. Because > resources, because\nLine 205: consist- ing mk > consisting of mk\nLine 360: versionand > version and", "SOUNDNESS_CORRECTNESS": "4", "ORIGINALITY": "3", "is_meta_review": null, "RECOMMENDATION": "4", "CLARITY": "4", "REVIEWER_CONFIDENCE": "4"}]}
{"text": "Automatically Generating Rhythmic Verse with Neural Networks\n1 000\n011\n012\n013\n014\n015\n016\n017\n018\n019\n020\n021\n022\n023\n024\n025\n026\n027\n028\n029\n030\n031\n032\n033\n034\n035\n036\n037\n038\n039\n040\n041\n042\n043\n044\n045\n046\n047\n048\n049\n061\n062\n063\n064\n065\n066\n067\n068\n069\n070\n071\n072\n073\n074\n075\n076\n077\n078\n079\n080\n081\n082\n083\n084\n085\n086\n087\n088\n089\n090\n091\n092\n093\n094\n095\n096\n097\n098\n099\nWe propose two novel methodologies for the automatic generation of rhythmic poetry in a variety of forms. The first approach uses a neural language model trained on a phonetic encoding to learn an implicit representation of both the form and content of English poetry. This model can effectively learn common poetic devices such as rhyme, rhythm and alliteration. The second approach considers poetry generation as a constraint satisfaction problem where a generative neural language model is tasked with learning a representation of content, and a discriminative weighted finite state machine constrains it on the basis of form. By manipulating the constraints of the latter model, we can generate coherent poetry with arbitrary forms and themes. A large-scale extrinsic evaluation demonstrated that participants consider machine-generated poems to be written by humans 54% of the time. In addition, participants rated a machinegenerated poem to be the best amongst all evaluated.\n1 Introduction\nPoetry is an advanced form of linguistic communication, in which a message is conveyed that satisfies both aesthetic and semantic constraints. As poetry is one of the most expressive forms of language, the automatic creation of texts recognisable as poetry is difficult. In addition to requiring an understanding of many aspects of language including phonetic patterns such as rhyme, rhythm and alliteration, poetry composition also requires a deep understanding of the meaning of language.\nPoetry generation can be divided into two subtasks, namely the problem of content, which is concerned with a poem\u2019s semantics, and the problem of form, which is concerned with the aesthetic rules that a poem follows. These rules may describe aspects of the literary devices used, and are usually highly prescriptive. Examples of different forms of poetry are limericks, ballads and sonnets. Limericks, for example, are characterised by their strict rhyme scheme (AABBA), their rhythm (two unstressed syllables followed by one stressed syllable) and their shorter third and fourth lines. Creating such poetry requires not only an understanding of the language itself, but also of how it sounds when spoken aloud. Statistical text generation usually requires the construction of a generative language model that explicitly learns the probability of any given word given previous context. Neural language models (Schwenk and Gauvain, 2005; Bengio et al., 2006) have garnered signficant research interest for their ability to learn complex syntactic and semantic representations of natural language (Mikolov et al., 2010; Sutskever et al., 2014; Cho et al., 2014; Kim et al., 2015). Poetry generation is an interesting application, since performing this task automatically requires the creation of models that not only focus on what is being written (content), but also on how it is being written (form). We experiment with two novel methodologies for solving this task. The first involves training a model to learn an implicit representation of content and form through the use of a phonological encoding. The second involves training a generative language model to represent content, which is then constrained by a discriminative pronunciation model, representing form. This second model is of particular interest because poetry with arbitrary rhyme, rhythm, repetition and themes can be generated by tuning the pronunciation model.\n2\n101\n102\n103\n104\n105\n106\n107\n108\n109\n110\n111\n112\n113\n114\n115\n116\n117\n118\n119\n120\n121\n122\n123\n124\n125\n126\n127\n128\n129\n130\n131\n132\n133\n134\n135\n136\n137\n138\n139\n140\n141\n142\n143\n144\n145\n146\n147\n148\n149\n150\n151\n152\n153\n154\n155\n156\n157\n158\n159\n160\n161\n162\n163\n164\n165\n166\n167\n168\n169\n170\n171\n172\n173\n174\n175\n176\n177\n178\n179\n180\n181\n182\n183\n184\n185\n186\n187\n188\n189\n190\n191\n192\n193\n194\n195\n196\n197\n198\n199\n2 Related Work\nAutomatic poetry generation is an important task due to the significant challenges involved. Most systems that have been proposed can loosely be categorised as rule-based expert systems, or statistical approaches. Rule-based poetry generation attempts include case-based reasoning (Gerva\u0301s, 2000), templatebased generation (Colton et al., 2012), constraint satisfaction (Toivanen et al., 2013) and text mining (Netzer et al., 2009). These approaches are often inspired by how humans might generate poetry. Statistical approaches, conversely, make no assumptions about the creative process. Instead, they attempt to extract statistical patterns from existing poetry corpora in order to construct a language model, which can then be used generate new poetic variants (Yi et al., 2016; Greene et al., 2010). The work of Zhang and Lapata (2014) is similar to ours, in that they make use of neural language models. For the task of automatic generation of classical Chinese poetry, they were able to outperform all other Chinese poetry generation systems with both manual and automatic evaluation.\n3 Phonetic-level Model\nOur first model is a pure neural language model, trained on a phonetic encoding of poetry in order to represent both form and content. Phonetic encodings of language represent information as sequences of around 40 basic acoustic symbols. Training on phonetic symbols allows the model to learn effective representations of pronunciation, including rhyme and rhythm. However, just training on a large corpus of poetry data is not enough. Specifically, two problems need to be overcome. 1) Phonetic encoding results in information loss: words that have the same pronunciation (homophones) cannot be perfectly reconstructed from the corresponding phonemes. This means that we require an additional probabilistic model in order to determine the most likely word given a sequence of phonemes. 2) The variety of poetry and poetic devices one can use\u2014 e.g., rhyme, rhythm, repetition\u2014means that poems sampled from a model trained on all poetry would be unlikely to maintain internal consistency of meter and rhyme. It is therefore important to train the model on poetry which has its own internal consistency.\nThus, the model comprises three steps: transliterating an ortographic sequence to its phonetic representation, training a neural language model on the phonetic encoding, and decoding the generated sequence back from phonemes to orthographic symbols.\nPhonetic encoding To solve the first step, we apply a combination of word lookups from the CMU pronunciation dictionary (Weide, 2005) with letter-to-sound rules for handling out-ofvocabulary words. These rules are based on the CART techniques described by Black et al. (1998), and are represented with a simple Finite State Transducer1. The number of letters and number of phones in a word are rarely a one-to-one match: letters may match with up to three phones. In addition, virtually all letters can, in some contexts, map to zero phones, which is known as \u2018wild\u2019 or epsilon. Expectation Maximisation is used to compute the probability of a single letter matching a single phone, which is maximised through the application of Dynamic Time Warping (Myers et al., 1980) to determine the most likely position of epsilon characters. Although this approach offers full coverage over the training corpus\u2014even for abbreviated words like ask\u2019d and archaic words like renewest\u2014it has several limitations. Irregularities in the English language result in difficulty determining general letter-to-sound rules that can manage words with unusual pronunciations such as \u201ccolonel\u201d and \u201creceipt\u201d 2.\nIn addition to transliterating words into phoneme sequences, we also represent word break characters as a specific symbol. This makes decipherment, when converting back into an orthographic representation, much easier. Phonetic transliteration allows us to construct a phonetic poetry corpus comprising 1,046,536 phonemes.\nNeural language model We train a Long-Short Term Memory Network (Hochreiter and Schmidhuber, 1997) on the phonetic representation of our poetry corpus. The model is trained using stochastic gradient descent to predict the next phoneme given a sequence of phonemes. Specifically, we\n1Implemented using FreeTTS (Walker et al., 2010) 2An evaluation of models in American English, British English, German and French was undertaken by Black et al. (1998), who reported an externally validated per token accuracy on British English as low as 67%. Although no experiments were carried out on corpora of early-modern English, it is likely that this accuracy would be significantly lower.\n3\n201\n202\n203\n204\n205\n206\n207\n208\n209\n210\n211\n212\n213\n214\n215\n216\n217\n218\n219\n220\n221\n222\n223\n224\n225\n226\n227\n228\n229\n230\n231\n232\n233\n234\n235\n236\n237\n238\n239\n240\n241\n242\n243\n244\n245\n246\n247\n248\n249\n250\n251\n252\n253\n254\n255\n256\n257\n258\n259\n260\n261\n262\n263\n264\n265\n266\n267\n268\n269\n270\n271\n272\n273\n274\n275\n276\n277\n278\n279\n280\n281\n282\n283\n284\n285\n286\n287\n288\n289\n290\n291\n292\n293\n294\n295\n296\n297\n298\n299\nmaximize a multinomial logistic regression objective over the final softmax prediction. Each phoneme is represented as a 256-dimensional embedding, and the model consists of two hidden layers of size 256. We apply backpropagationthrough-time (Werbos, 1990) for 150 timesteps, which roughly equates to four lines of poetry in sonnet form. This allows the network to learn features like rhyme even when spread over multiple lines. Training is preemptively stopped at 25 epochs to prevent overfitting.\nOrthographic decoding When decoding from phonemes back to orthographic symbols, the goal is to compute the most likely word corresponding to a sequence of phonemes. That is, we compute the most probable hypothesis word W given a phoneme sequence \u03c1:\nargmaxi P (Wi | \u03c1 ) (1)\nWe can consider the phonetic encoding of plaintext to be a homophonic cipher; that is, a cipher in which each symbol can correspond to one or more possible decodings. The problem of homophonic decipherment has received significant research attention in the past; with approaches utilising Expectation Maximisation (Knight et al., 2006), Integer Programming (Ravi and Knight, 2009) and A* search (Corlett and Penn, 2010). Transliteration from phonetic to an orthographic representation is done by constructing a Hidden Markov Model using the CMU pronunciation dictionary (Weide, 2005) and an n-gram language model. We calculate the transition probabilities (using the n-gram model) and the emission matrix (using the CMU pronunciation dictionary) to determine pronunciations that correspond to a single word. All pronunciations are naively considered equiprobable. We perform Viterbi decoding to find the most likely sequence of words. This means finding the most likely word wt+1 given a previous word sequence (wt\u2212n, ..., wt).\nargmaxwt+1 P ( wt+1 | w1, ... , wt ) (2)\nIf a phonetic sequence does not map to any word, we apply the heuristic of artificially breaking the sequence up into two subsequences at index n, such that nmaximises the n-gram frequency of the subsequences.\nAnd humble and their fit flees are wits size but that one made and made thy step me lies\n\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014 Cool light the golden dark in any way the birds a shade a laughter turn away\n\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014 Then adding wastes retreating white as thine\nShe watched what eyes are breathing awe what shine \u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\nBut sometimes shines so covered how the beak Alone in pleasant skies no more to seek\nFigure 1: Example output of the phonetic-level model trained on Iambic Pentameter poetry (grammatical errors are emphasised).\nOutput A popular form of poetry with strict internal structure is the sonnet. Popularised in English by Shakespeare, the sonnet is characterised by a strict rhyme scheme and exactly fourteen lines of Iambic Pentameter (Greene et al., 2010). Since the 17,134 word tokens in Shakespeare\u2019s 153 sonnets are insufficient to train an effective model, we augment this corpus with poetry taken from the website sonnets.org, yielding a training set of 288,326 words and 1,563,457 characters. An example of the output when training on this sonnets corpus is provided in Figure 1. Not only is it mostly in strict Iambic Pentameter, but the grammar of the output is mostly correct and the poetry contains rhyme.\n4 Constrained Character-level Model\nAs the example shows, phonetic-level language models are effective at learning poetic form, despite small training sets and relatively few parameters. However, the fact that they require training data with internal poetic consistency implies that they do not generalise to other forms of poetry. That is, in order to generate poetry in Dactylic Hexameter (for example), a phonetic model must be trained on a corpus of Dactylic poetry. Not only is this impractical, but in many cases no corpus of adequate size even exists. Even when such poetic corpora are available, a new model must be trained for each type of poetry. This precludes tweaking the form of the output, which is important when generating poetry automatically. We now explore an alternative approach. Instead of attempting to represent both form and content in a single model, we construct a pipeline containing a generative language model represent-\n4\n301\n302\n303\n304\n305\n306\n307\n308\n309\n310\n311\n312\n313\n314\n315\n316\n317\n318\n319\n320\n321\n322\n323\n324\n325\n326\n327\n328\n329\n330\n331\n332\n333\n334\n335\n336\n337\n338\n339\n340\n341\n342\n343\n344\n345\n346\n347\n348\n349\n350\n351\n352\n353\n354\n355\n356\n357\n358\n359\n360\n361\n362\n363\n364\n365\n366\n367\n368\n369\n370\n371\n372\n373\n374\n375\n376\n377\n378\n379\n380\n381\n382\n383\n384\n385\n386\n387\n388\n389\n390\n391\n392\n393\n394\n395\n396\n397\n398\n399\ning content, and a discriminative model representing form. This allows us to represent the problem of creating poetry as a constraint satisfaction problem, where we can modify constraints to restrict the types of poetry we generate.\nCharacter Language Model Rather than train a model on data representing features of both content and form, we now use a simple character-level model (Sutskever et al., 2011) focused solely on content. This approach offers several benefits over the word-level models that are prevalent in the literature. Namely, their more compact vocabulary allows for more efficient training; they can learn common prefixes and suffixes to allow us to sample words that are not present in the training corpus and can learn effective language representations from relatively small corpora; and they can handle archaic and incorrect spellings of words. As we no longer need the model to explicitly represent the form of generated poetry, we can loosen our constraints when choosing a training corpus. Instead of relying on poetry only in sonnet form, we can instead construct a generic corpus of poetry taken from online sources. This corpus is composed of 7.56 million words and 34.34 million characters, taken largely from 20th Century poetry books found online. The increase in corpus size facilitates a corresponding increase in the number of permissible model parameters. This allows us to train a 3-layer LSTM model with 2048- dimensional hidden layers. The model was trained to predict the next character given a sequence of characters, using stochastic gradient descent. We attenuate the learning rate over time, and by 20 epochs the model converges.\nRhythm Modeling Although a character-level language model trained on a corpus of generic poetry allows us to generate interesting text, internal irregularities and noise in the training data prevent the model from learning important features such as rhythm. Hence, we require an additional classifier to constrain our model by either accepting or rejecting sampled lines based on the presence or absence of these features. As the presence of meter (rhythm) is the most characteristic feature of poetry, it therefore must be our primary focus. Pronunciation dictionaries have often been used to determine the syllabic stresses of words (Colton et al., 2012; Manurung et al., 2000; Misztal and Indurkhya, 2014), but suffer from some limitations\nfor constructing a classifier. All word pronunciations are considered equiprobable, including archaic and uncommon pronunciations, and pronunciations are provided context free, despite the importance of context for pronunciation3. Furthermore, they are constructed from American English, meaning that British English may be misclassified. These issues are circumvented by applying lightly supervised learning to determine the contextual stress pattern of any word. That is, we exploit the latent structure in our corpus of sonnet poetry, namely, the fact that sonnets are composed of lines in rigid Iambic Pentameter, and are therefore exactly ten syllables long with alternating syllabic stress. This allows us to derive a syllablestress distribution. Although we use the sonnets corpus for this, it is important to note that any corpus with such a latent structure could be used. By representing a line as a cascade of Weighted Finite State Transducers (WFST), we can perform Expectation Maximisation over the poetry corpus to obtain a probabilistic classifier which enables us to determine the most likely stress patterns for each word. Every word is represented by a single transducer. Since weights can be assigned to state transitions, we can model the probability that a given input string maps to a particular output. In each cascade, a sequence of input words is mapped onto a sequence of stress patterns \u27e8\u00d7, /\u27e9 where each pattern is between 1 and 5 syllables in length4. We initially set all transition probabilities equally, as we make no assumptions about the stress distributions in our training set. We then iterate over each line of the sonnet corpus, using Expectation Maximisation to train the cascades. In practice, there are several de facto variations of Iambic meter which are permissible, as shown in Figure 2. We train the rhythm classifier by converging the cascades to whatever output is the most likely given the line.\nConstraining the model To generate poetry using this model, we sample sequences of characters from the character-level language model. To impose rhythm constrains on the language model, we first represent these sampled characters at the word level and pool sampled characters into word\n3For example, the independent probability of stressing the single syllable word at is 40%, but this increases to 91%when the following word is the (Greene et al., 2010)\n4Words of more than 5 syllables comprise less than 0.1% of the lexicon (Aoyama and Constable, 1998).\n5\n401\n402\n403\n404\n405\n406\n407\n408\n409\n410\n411\n412\n413\n414\n415\n416\n417\n418\n419\n420\n421\n422\n423\n424\n425\n426\n427\n428\n429\n430\n431\n432\n433\n434\n435\n436\n437\n438\n439\n440\n441\n442\n443\n444\n445\n446\n447\n448\n449\n450\n451\n452\n453\n454\n455\n456\n457\n458\n459\n460\n461\n462\n463\n464\n465\n466\n467\n468\n469\n470\n471\n472\n473\n474\n475\n476\n477\n478\n479\n480\n481\n482\n483\n484\n485\n486\n487\n488\n489\n490\n491\n492\n493\n494\n495\n496\n497\n498\n499\n\u00d7 / \u00d7 / \u00d7 / \u00d7 / \u00d7 / / \u00d7 \u00d7 / \u00d7 / \u00d7 / \u00d7 / \u00d7 / \u00d7 / \u00d7 / \u00d7 / \u00d7 / \u00d7 / \u00d7 \u00d7 / \u00d7 / \u00d7 / \u00d7 / \u00d7\nFigure 2: Permissible variations of Iambic Pentameter in Shakespeare\u2019s sonnets.\ntokens in an intermediary buffer. We then apply the separately trained word-level WFSTs to construct a cascade of this buffer and perform Viterbi decoding over the cascade. This defines the distribution of stress-patterns over our word tokens. We can represent this cascade as a probabilistic classifier, and accept or reject the buffered output based on how closely it conforms to the desired meter. While sampling sequences of words from this model, the entire generated sequence is passed to the classifier each time a new word is sampled. The pronunciation model then returns the probability that the entire line is within the specified meter. If a new word is rejected by the classifier, the state of the network is rolled back to the state of the last formulaically acceptable line, removing the rejected word from memory. The constraint on rhythm can be controlled by adjusting the acceptability threshold of the classifier. By increasing the threshold, output focuses on form over content. Conversely, decreasing the criterion puts greater emphasis on content.\nGeneric poetry\nSonnet poetry\nLSTM\nWFST\nRhythmic Output\nTrained\nTrained\nBuffer\n4.1 Themes and Poetic devices\nIt is important for any generative poetry model to include themes and poetic devices. One way to achieve this would be by constructing a corpus that exhibits the desired themes and devices. To create a themed corpus about \u2018love\u2019, for instance,\nThemed Training Set\nPoetry LSTM\nThemed Output\nTraining Set\nPoetry LSTM\nThemed Output\nThematic Boosting\nImplicit Explicit\nFigure 3: Two approaches for generating themed poetry.\nwe would aggregate love poetry to train the model, which would thus learn an implicit representation of love. However, this forces us to generate poetry according to discrete themes and styles from pretrained models, requiring a new training corpus for each model. In other words, we would suffer from similar limitations as with the phonetic-level model, in that we require a dedicated corpus. Alternatively, we can manipulate the language model by boosting character probabilities at sample time to increase the probability of sampling thematic words like \u2018love\u2019. This approach is more robust, and provides us with more control over the final output, including the capacity to vary the inclusion of poetic devices in the output.\nThemes In order to introduce thematic content, we heuristically boost the probability of sampling words that are semantically related to a theme word from the language model. First, we compile a list of similar words to a key theme word by retrieving its semantic neighbours from a distributional semantic model (Mikolov et al., 2013). For example, the theme winter might include thematic words frozen, cold, snow and frosty. We represent these semantic neighbours at the character level, and heuristically boost their probability by multiplying the sampling probability of such character strings by a function of their cosine similarity to the key word. Thus, the likelihood of sampling a thematically related word is artificially increased, while still constraining the model rhythmically.\nPoetic devices A similar method may be used for poetic devices such as assonance, consonance and alliteration. Since these devices can be orthographically described by the repetition of identical sequences of characters, we can apply the\n6\n501\n502\n503\n504\n505\n506\n507\n508\n509\n510\n511\n512\n513\n514\n515\n516\n517\n518\n519\n520\n521\n522\n523\n524\n525\n526\n527\n528\n529\n530\n531\n532\n533\n534\n535\n536\n537\n538\n539\n540\n541\n542\n543\n544\n545\n546\n547\n548\n549\n550\n551\n552\n553\n554\n555\n556\n557\n558\n559\n560\n561\n562\n563\n564\n565\n566\n567\n568\n569\n570\n571\n572\n573\n574\n575\n576\n577\n578\n579\n580\n581\n582\n583\n584\n585\n586\n587\n588\n589\n590\n591\n592\n593\n594\n595\n596\n597\n598\n599\nErrors per line 1 2 3 4 Total\nPhonetic Model 11 2 3 1 28 Character Model + WFST 6 5 1 1 23 Character Model 3 8 7 7 68\nTable 1: Number of lines with n errors from a set of 50 lines generated by each of the three models.\nWord Line Coverage\nWikipedia 64.84% 83.35% 97.53% Sonnets 85.95% 80.32% 99.36%\nTable 2: Error when transliterating text into phonemes and reconstructing back into text.\nsame heuristic to boost the probability of sampling character strings that have previously been sampled. That is, to sample a line with many instances of alliteration (multiple words with the same initial sound) we record the historical frequencies of characters sampled at the beginning of each previous word. After a word break character, we boost the probability that those characters will be sampled again in the softmax. We only keep track of frequencies for a fixed number of time steps. By increasing or decreasing the size of this window, we can manipulate the prevalence of alliteration. Variations of this approach are applied to invoke consonance (by boosting intra-word consonants) and assonance (by boosting intra-word vowels). An example of two sampled lines with high degrees of alliteration, assonance and consonance is given in Figure 4c.\n5 Evaluation\nIn order to examine how effective our methodologies for generating poetry are, we evaluate the proposed models in two ways. First, we perform an intrinsic evaluation where we examine the quality of the models and the generated poetry. Second, we perform an extrinsic evaluation where we evaluate the generated output using human annotators, and compare it to human-generated poetry.\n5.1 Intrinsic evaluation\nTo evaluate the ability of both models to generate formulaic poetry that adheres to rhythmic rules, we compared sets of fifty sampled lines from each model. The first set was sampled from the phonetic-level model trained on Iambic poetry.\nThe second set was sampled from the characterlevel model, constrained to Iambic form. For comparison, and to act as a baseline, we also sampled from the unconstrained character model. We created gold-standard syllabic classifications by recording each line spoken-aloud, and marking each syllable as either stressed or unstressed. We then compared these observations to loose Iambic Pentameter (containing all four variants), to determine how many syllabic misclassifications existed on each line. This was done by speaking each line aloud, and noting where the speaker put stresses. As Table 1 shows, the constrained character level model generated the most formulaic poetry. Results from this model show that 70% of lines had zero mistakes, with frequency obeying an inverse power-law relationship with the number of errors. We can see that the phonetic model performed similarly, but produced more subtle mistakes than the constrained character model: many of the errors were single mistakes in an otherwise correct line of poetry. In order to investigate this further, we examined to what extent these errors are due to transliteration (i.e., the phonetic encoding and orthographic decoding steps). Table 2 shows the reconstruction accuracy per word and per line when transliterating either Wikipedia or Sonnets to phonemes using the CMU pronunciation dictionary and subsequently reconstructing English text using the ngram model5. Word accuracy reflects the frequency of perfect reconstruction, whereas per line tri-gram similarity (Kondrak, 2005) reflects the overall reconstruction. Coverage captures the percentage of in-vocabulary items. The relatively low per-word accuracy achieved on the Wikipedia corpus is likely due to the high frequency of out-ofvocabulary words. The results show that a significant number of errors in the phonetic-level model are likely to be caused by transliteration mistakes.\n5.2 Extrinsic evaluation\nWe conducted an indistinguishability study with a selection of automatically generated poetry and human poetry. As extrinsic evaluations are expensive and the phonetic model was unlikely to do well (as illustrated in Figure 4e: the model generates good Iambic form, but not very good English),\n5Obviously, calculating this value for the character-level model makes no sense, since no transliteration occurs in that case.\n7\n601\n602\n603\n604\n605\n606\n607\n608\n609\n610\n611\n612\n613\n614\n615\n616\n617\n618\n619\n620\n621\n622\n623\n624\n625\n626\n627\n628\n629\n630\n631\n632\n633\n634\n635\n636\n637\n638\n639\n640\n641\n642\n643\n644\n645\n646\n647\n648\n649\n670\n671\n672\n673\n674\n675\n676\n677\n678\n679\n680\n681\n682\n683\n684\n685\n686\n687\n688\n689\n690\n691\n692\n693\n694\n695\n696\n697\n698\n699\n(a) The crow crooked on more beautiful and free, He journeyed off into the quarter sea. his radiant ribs girdled empty and very - least beautiful as dignified to see.\n(c) Man with the broken blood blue glass and gold. Cheap chatter chants to be a lover do.\n(e) The son still streams and strength and spirit. The ridden souls of which the fills of.\n(b) Is that people like things (are the way we to figure it out) and I thought of you reading and then is your show or you know we will finish along will you play.\n(d) How dreary to be somebody, How public like a frog To tell one\u2019s name the livelong day To an admiring bog.\nFigure 5: The experimental environment for asking participants to distinguish between automatically generated and human poetry.\nwe only evaluate on the constrained characterlevel model.\nThe aim of the study was to determine whether participants could distinguish between human and generated poetry, and if so to what extent. A set of 70 participants (of whom 61 were English native speakers) were each shown a selection of randomly chosen poetry segments, and were invited to classify them as either human or generated. Participants were recruited from friends and people within poetry communities, with an age range of 17 to 80, and a mean age of 29. Our participants were not financially incentivised, perceiving the evaluation as an intellectual challenge.\nIn addition to the classification task, each partic-\nipant was also invited to rate each poem on a 1-5 scale with respect to three criteria, namely readability, form and evocation (how much emotion did a poem elicit). We naively consider the overall quality of a poem to be the mean of these three measures. We used a custom web-based environment, built specifically for this evaluation6, which is illustrated in Figure 5. Based on human judgments, we can determine whether the models presented in this work can produce poetry of a similar quality to humans. To select appropriate human poetry that could be meaningfully compared with the machinegenerated poetry, we performed a comprehension test on all poems used in the evaluation, using the Dale-Chall readability formula (Dale and Chall, 1948). This formula represents readability as a function of the complexity of the input words. We selected nine machine-generated poems with a high readability score. The generated poems produced an average score of 7.11, indicating that readers over 15 years of age should easily be able to comprehend them. For our human poems, we focused explicitly on poetry where greater consideration is placed on prosodic elements like rhythm and rhyme than semantic content (known as \u201cnonsense verse\u201d). We randomly selected 30 poems belonging to that category from the website poetrysoup.com, of which\n6[URL-ANONYMIZED]\n8\n701\n702\n703\n704\n705\n706\n707\n708\n709\n710\n711\n712\n713\n714\n715\n716\n717\n718\n719\n720\n721\n722\n723\n724\n725\n726\n727\n728\n729\n730\n731\n732\n733\n734\n735\n736\n737\n738\n739\n740\n741\n742\n743\n744\n745\n746\n747\n748\n749\n750\n751\n752\n753\n754\n755\n756\n757\n758\n759\n760\n761\n762\n763\n764\n765\n766\n767\n768\n769\n770\n771\n772\n773\n774\n775\n776\n777\n778\n779\n780\n781\n782\n783\n784\n785\n786\n787\n788\n789\n790\n791\n792\n793\n794\n795\n796\n797\n798\n799\nPoet Title Human Readability Emotion Form\nGenerated Best 0.66 0.60 -0.77 0.90 G. M. Hopkins Carrion Comfort 0.62 -1.09 1.39 -1.55\nJ. Thornton Delivery of Death 0.60 0.26 -1.38 -0.65\nGenerated All 0.54 -0.28 -0.30 0.23 M. Yvonne Intricate Weave 0.53 2.38 0.94 -1.67\nE. Dickinson I\u2019m Nobody 0.52 -0.46 0.92 0.44\nG. M. Hopkins The Silver Jubilee 0.52 0.71 -0.33 0.65\nR. Dryden Mac Flecknoe 0.51 -0.01 0.35 -0.78\nA. Tennyson Beautiful City 0.48 -1.05 0.97 -1.26\nW. Shakespeare A Fairy Song 0.45 0.65 1.30 1.18\nTable 3: Proportion of people classifying each poem as \u2019human\u2019, as well as the relative qualitative scores of each poem as deviations from the mean.\neight were selected for the final comparison based on their comparable readability score. The selected poems were segmented into passages of between four and six lines, to match the length of the generated poetry segments. An example of such a segment is shown in Figure 4d. The human poems had an average score of 7.52, requiring a similar level of English aptitude to the generated texts. The performance of each human poem, alongside the aggregated scores of the generated poems, is illustrated in Table 3. For the human poems, our group of participants guessed correctly that they were human 51.4% of the time. For the generated poems, our participants guessed correctly 46.2% of the time that they were machine generated. To determine whether our results were statistically significant, we performed a Chi2 test. This resulted in a p-value of 0.718. This indicates that our participants were unable to tell the difference between human and generated poetry in any significant way. Although our participants generally considered the human poems to be of marginally higher quality than our generated poetry, they were unable to effectively distinguish between them. Interestingly, our results seem to suggest that our participants consider the generated poems to be more \u2018human-like\u2019 than those actually written by humans. Furthermore, the poem with the highest overall quality rating is a machine generated one. This shows that our approach was effective at generating high-quality rhythmic verse. It should be noted that the poems that were most \u2018human-like\u2019, most aesthetic and most emotive re-\nspectively (though not the most readable) were all generated by the neural character model. Generally the set of poetry produced by the neural character model was slightly less readable and emotive than the human poetry, but had above average form. All generated poems included in this evaluation can be found in the supplementary material.\n6 Conclusions\nOur contributions are twofold. First, we developed a neural language model trained on a phonetic transliteration of poetic form and content. Although example output looked promising, this model was limited by its inability to generalise to novel forms of verse. We then proposed a more robust model trained on unformed poetic text, whose output form is constrained at sample time. This approach offers greater control over the style of the generated poetry than the earlier method, and facilitates themes and poetic devices. An indistinguishability test, where participants were asked to classify a randomly selected set of human \u201cnonsense verse\u201d and machine-generated poetry, showed generated poetry to be indistinguishable from that written by humans. In addition, the poems that were deemed most \u2018humanlike\u2019, most aesthetic and most emotive, respectively, were all machine-generated. In future work, it would be useful to investigate models based on morphemes, rather than characters, which offers potentially superior performance for complex and rare words (Luong et al., 2013), which are common in poetry.\n9\n801\n802\n803\n804\n805\n806\n807\n808\n809\n810\n811\n812\n813\n814\n815\n816\n817\n818\n819\n820\n821\n822\n823\n824\n825\n826\n827\n828\n829\n830\n831\n832\n833\n834\n835\n836\n837\n838\n839\n840\n841\n842\n843\n844\n845\n846\n847\n848\n849\n850\n851\n852\n853\n854\n855\n856\n857\n858\n859\n860\n861\n862\n863\n864\n865\n866\n867\n868\n869\n870\n871\n872\n873\n874\n875\n876\n877\n878\n879\n880\n881\n882\n883\n884\n885\n886\n887\n888\n889\n890\n891\n892\n893\n894\n895\n896\n897\n898\n899\n", "cats": {"READ": 0.0, "DONTREAD": 1.0}, "reviews": [{"IMPACT": "3", "SUBSTANCE": "5", "APPROPRIATENESS": "5", "MEANINGFUL_COMPARISON": "5", "PRESENTATION_FORMAT": "Oral Presentation", "comments": "The paper presents two approaches for generating English poetry. The first\napproach combine a neural phonetic encoder predicting the next phoneme with a\nphonetic-orthographic HMM decoder computing the most likely word corresponding\nto a sequence of phonemes. The second approach combines a character language\nmodel with a weigthed FST to impose rythm constraints on the output of the\nlanguage model. For the second approach, the authors also present a heuristic\napproach which permit constraining the generated poem according to theme (e.g;,\nlove) or poetic devices (e.g., alliteration). The generated poems are evaluated\nboth instrinsically by comparing the rythm of the generated lines with a gold\nstandard and extrinsically by asking 70 human evaluators to (i) determine\nwhether the poem was written by a human or a machine and (ii) rate poems wrt to\nreadability, form and evocation.  The results indicate that the second model\nperforms best and that human evaluators find it difficult to distinguish\nbetween human written and machine generated poems.\n\nThis is an interesting, clearly written article with novel ideas (two different\nmodels for poetry generation, one based on a phonetic language model the other\non a character LM) and convincing results.\n\n For the evaluation, more precision about the evaluators and the protocol would\nbe good. Did all evaluators evaluate all poems and if not how many judgments\nwere collected for each poem for each task ? You mention 9 non English native\nspeakers. Poems are notoriously hard to read. How fluent were these ? \n\nIn the second model (character based), perhaps I missed it, but do you have a\nmechanism to avoid generating non words ? If not, how frequent are non words in\nthe generated poems ?\n\nIn the first model, why use an HMM to transliterate from phonetic to an\norhographic representation rather than a CRF? \n\nSince overall, you rule out the first model as a good generic model for\ngenerating poetry, it might have been more interesting to spend less space on\nthat model and more on the evaluation of the second model. In particular, I\nwould have been interested in a more detailed discussion of the impact of the\nheuristic you use to constrain theme or poetic devices. How do these impact\nevaluation results ? Could they be combined to jointly constrain theme and\npoetic devices ? \n\nThe combination of a neural mode with a WFST is reminiscent of the following\npaper which combine character based neural model to generate from dialog acts\nwith an WFST to avoid generating non words. YOu should relate your work to\ntheirs and cite them. \n\nNatural Language Generation through Character-Based RNNs with Finite-State\nPrior Knowledge\nGoyal, Raghav and Dymetman, Marc and Gaussier, Eric and LIG, Uni\nCOLING 2016", "SOUNDNESS_CORRECTNESS": "3", "ORIGINALITY": "4", "is_meta_review": null, "RECOMMENDATION": "4", "CLARITY": "4", "REVIEWER_CONFIDENCE": "3"}, {"IMPACT": "3", "SUBSTANCE": "4", "APPROPRIATENESS": "5", "MEANINGFUL_COMPARISON": "5", "PRESENTATION_FORMAT": "Poster", "comments": "The paper describes two methodologies for the automatic generation of rhythmic\npoetry. Both rely on neural networks, but the second one allows for better\ncontrol of form.\n\n- Strengths:\n\nGood procedure for generating rhythmic poetry.\n\nProposals for adding control of theme and poetic devices (alliteration,\nconsonance, asonance).\n\nStrong results in evaluation of rhythm.\n\n- Weaknesses:\n\nPoor coverage of existing literature on poetry generation.\n\nNo comparison with existing approaches to poetry generation.\n\nNo evaluation of results on theme and poetic devices.\n\n- General Discussion:\n\nThe introduction describes the problem of poetry generation as divided into two\nsubtasks: the problem of content (the poem's semantics) and the problem of form\n(the \n\naesthetic rules the poem follows). The solutions proposed in the paper address\nboth of these subtasks in a limited fashion. They rely on neural networks\ntrained over corpora \n\nof poetry (represented at the phonetic or character level, depending on the\nsolution) to encode the linguistic continuity of the outputs. This does indeed\nensure that the \n\noutputs resemble meaningful text. To say that this is equivalent to having\nfound a way of providing the poem with appropriate semantics would be an\noverstatement. The \n\nproblem of form can be said to be addressed for the case of rhythm, and partial\nsolutions are proposed for some poetic devices. Aspects of form concerned with\nstructure at a \n\nlarger scale (stanzas and rhyme schemes) remain beyond the proposed solutions.\nNevertheless, the paper constitutes a valuable effort in the advancement of\npoetry generation.\n\nThe review of related work provided in section 2 is very poor. It does not even\ncover the set of previous efforts that the authors themselves consider worth\nmentioning in their paper (the work of Manurung et al 2000 and Misztal and\nIndurkhya 2014 is cited later in the paper - page 4 - but it is not placed in\nsection 2 with respect to the other authors mentioned there).\n\nA related research effort of particular relevance that the authors should\nconsider is:\n\n- Gabriele Barbieri, Fran\u00e7ois Pachet, Pierre Roy, and Mirko Degli Esposti.\n2012. Markov constraints for generating lyrics with style. In Proceedings of\nthe 20th European Conference on Artificial Intelligence (ECAI'12), Luc De\nRaedt, Christian Bessiere, Didier Dubois, Patrick Doherty, and Paolo Frasconi\n(Eds.). IOS Press, Amsterdam, The Netherlands, The Netherlands, 115-120. DOI:\nhttps://doi.org/10.3233/978-1-61499-098-7-115\n\nThis work addresses very similar problems to those discussed in the present\npaper (n-gram based generation and the problem of driving generation process\nwith additional constraints). The authors should include a review of this work\nand discuss the similarities and differences with their own.\n\nAnother research effort that is related to what the authors are attempting (and\nhas bearing on their evaluation process) is:\n\n- Stephen McGregor, Matthew Purver and Geraint Wiggins, Process Based\nEvaluation of Computer Generated Poetry,  in: Proceedings of the INLG 2016\nWorkshop on Computational Creativity and Natural Language Generation, pages\n51\u201360,Edinburgh, September 2016.c2016 Association for Computational\nLinguistics\n\nThis work is also similar to the current effort in that it models language\ninitially at a phonological level, but considers a word n-gram level\nsuperimposed on that, and also features a layer representint sentiment. Some of\nthe considerations McGregor et al make on evaluation of computer generated\npoetry are also relevant for the extrinsic evaluation described in the present\npaper.\n\nAnother work that I believe should be considered is:\n\n- \"Generating Topical Poetry\" (M. Ghazvininejad, X. Shi, Y. Choi, and K.\nKnight), Proc. EMNLP, 2016.\n\nThis work generates iambic pentameter by combining finite-state machinery with\ndeep learning. It would be interesting to see how the proposal in the current\npaper constrasts with this particular approach.\n\nAlthough less relevant to the present paper, the authors should consider\nextending their classification of poetry generation systems (they mention\nrule-based expert systems and statistical approaches) to include evolutionary\nsolutions. They already mention in their paper the work of Manurung, which is\nevolutionary in nature, operating over TAG grammars.\n\nIn any case, the paper as it stands holds little to no effort of comparison to\nprior approaches to poetry generation. The authors should make an effort to\ncontextualise their work with respect to previous efforts, specially in the\ncase were similar problems are being addressed (Barbieri et al, 2012) or\nsimilar methods are being applied (Ghazvininejad,  et al, 2016).", "SOUNDNESS_CORRECTNESS": "3", "ORIGINALITY": "4", "is_meta_review": null, "RECOMMENDATION": "3", "CLARITY": "3", "REVIEWER_CONFIDENCE": "4"}]}
{"text": "Sequential Matching Network: A New Architecture for Multi-turn Response Selection in Retrieval-based Chatbots\n1 000\n011\n012\n013\n014\n015\n016\n017\n018\n019\n020\n021\n022\n023\n024\n025\n026\n027\n028\n029\n030\n031\n032\n033\n034\n035\n036\n037\n038\n039\n040\n041\n042\n043\n044\n045\n046\n047\n048\n049\n071\n072\n073\n074\n075\n076\n077\n078\n079\n080\n081\n082\n083\n084\n085\n086\n087\n088\n089\n090\n091\n092\n093\n094\n095\n096\n097\n098\n099\n1 Introduction\nConversational agents include task-oriented dialog systems and non-task-oriented chatbots. Dialog systems focus on helping people complete specific tasks in vertical domains (Young et al., 2010), while chatbots aim to naturally and meaningfully converse with humans on open domain topics (Ritter et al., 2011). Existing work on building chatbots includes generation based methods and retrieval based methods. Retrieval based chatbots enjoy the advantage of informative and fluent responses, because they select a proper response for the current conversation from a repository with re-\nsponse selection algorithms. While most existing work on retrieval based chatbots studies response selection for single-turn conversation (Wang et al., 2013) which only considers the last input message, we consider the problem in a multi-turn scenario. In a chatbot, multi-turn response selection takes a message and utterances in its previous turns as input and selects a response that is natural and relevant to the whole context.\nThe key to response selection lies in inputresponse matching. Different from single-turn conversation, multi-turn conversation requires matching between a response and a conversation context in which one needs to consider not only the matching between the response and the input message but also the matching between the response and the utterances in previous turns. The challenges of the task include (1) how to identify important information (words, phrases, and sentences) in the context that is crucial to selecting a proper response and how to leverage the information in matching; and (2) how to model relationships among the utterances in the context. Table 1 illustrates the challenges with an example. First, \u201chold a drum class\u201d and \u201cdrum\u201d in the context are very important. Without them, one may find responses relevant to the message (i.e., the last turn of the context) but nonsense in the context (e.g., \u201cwhat lessons do you want?\u201d). Second,\n2\n101\n102\n103\n104\n105\n106\n107\n108\n109\n110\n111\n112\n113\n114\n115\n116\n117\n118\n119\n120\n121\n122\n123\n124\n125\n126\n127\n128\n129\n130\n131\n132\n133\n134\n135\n136\n137\n138\n139\n140\n141\n142\n143\n144\n145\n146\n147\n148\n149\n150\n151\n152\n153\n154\n155\n156\n157\n158\n159\n160\n161\n162\n163\n164\n165\n166\n167\n168\n169\n170\n171\n172\n173\n174\n175\n176\n177\n178\n179\n180\n181\n182\n183\n184\n185\n186\n187\n188\n189\n190\n191\n192\n193\n194\n195\n196\n197\n198\n199\nthe message highly depends on the second turn in the context, and the order of the utterances matters in response selection: exchanging the third turn and the last turn may lead to different responses. Existing work, however, either ignores relationships among utterances when concatenating them together (Lowe et al., 2015), or loses important information in context in the process of converting the whole context to a vector without enough supervision from responses (e.g., by a hierarchical RNN (Zhou et al., 2016)).\nWe propose a sequential matching network (SMN), a new context based matching model that can tackle both challenges in an end-to-end way. The reason that existing models lose important information in the context is that they first represent the whole context as a vector and then match the context vector with a response vector. Thus responses in these models cannot meet the context until the final step in matching. To avoid information loss, SMN matches a response with each utterance in the context at the beginning and encodes important information in each pair into a matching vector. The matching vectors are then accumulated in the temporal order of the utterances to model their relationships. The final matching degree is computed with the accumulation of the matching vectors. Specifically, for each utteranceresponse pair, the model constructs a word-word similarity matrix and a sequence-sequence similarity matrix by the embedding of words and the hidden states of a recurrent neural network with gated recurrent unites (GRU) (Chung et al., 2014) respectively. The two matrices capture important matching information in the pair on a word level and a segment level respectively, and the information is distilled and fused as a matching vector through an alternation of convolution and pooling operations on the matrices. By this means, important information from multiple levels of granularity in the context is recognized under sufficient supervision from the response and carried into matching with minimal loss. The matching vectors are then uploaded to another GRU to form a matching score for the context and the response. The GRU accumulates the pair matching in its hidden states in the chronological order of the utterances in the context. It models relationships and dependencies among the utterances in a matching fashion and has the utterance order supervise the accumulation of pair matching. The match-\ning degree of the context and the response is computed by a logit model with the hidden states of the GRU. SMN extends the powerful \u201c2D\u201d matching paradigm in text pair matching for single-turn conversation to context based matching for multi-turn conversation, and enjoys the advantage that both important information in utterance-response pairs and relationships among utterances are sufficiently preserved and leveraged in matching.\nWe test our model on the Ubuntu dialogue corpus (Lowe et al., 2015) which is a large scale public English data set for research in multi-turn conversation. The results show that our model can significantly outperform state-of-the-art methods, and improvement to the best baseline model on R10@1 is over 6%. In addition to the Ubuntu corpus, we create a human labeled Chinese data set, namely Douban Conversation Corpus, and test our model on it. Different from the Ubuntu corpus in which data is collected from a specific domain and negative candidates are randomly sampled, conversations in this data come from open domain, and response candidates in this data set are collected from a retrieval engine and labeled by three human judges. On this data, our model improves the best baseline model over 3% on R10@1 and 4% on P@1. As far as we know, Douban Conversation Corpus is the first human labeled data set for multi-turn response selection and could be a good complement to the Ubuntu corpus. We release Douban Conversation Corups and our source code at an anonymous url for blind review. We have uploaded code and data with this paper.\nOur contributions in this paper are three-folds: (1) proposal of a new context based matching model for multi-turn response selection in retrieval based chatbots; (2) publication of a large human labeled data set to research communities. (3) empirical verification of the effectiveness of the model on public data sets;\n2 Related Work\nRecently, building a chatbot with data driven approaches (Ritter et al., 2011; Higashinaka et al., 2014) has drawn a lot of attention. Existing work along this line includes retrieval based methods (Hu et al., 2014; Ji et al., 2014; Wang et al., 2015; Yan et al., 2016; Wu et al., 2016b; Zhou et al., 2016; Wu et al., 2016a) and generation based methods (Shang et al., 2015; Serban et al., 2015; Vinyals and Le, 2015; Li et al., 2015, 2016; Xing\n3\n201\n202\n203\n204\n205\n206\n207\n208\n209\n210\n211\n212\n213\n214\n215\n216\n217\n218\n219\n220\n221\n222\n223\n224\n225\n226\n227\n228\n229\n230\n231\n232\n233\n234\n235\n236\n237\n238\n239\n240\n241\n242\n243\n244\n245\n246\n247\n248\n249\n250\n251\n252\n253\n254\n255\n256\n257\n258\n259\n260\n261\n262\n263\n264\n265\n266\n267\n268\n269\n270\n271\n272\n273\n274\n275\n276\n277\n278\n279\n280\n281\n282\n283\n284\n285\n286\n287\n288\n289\n290\n291\n292\n293\n294\n295\n296\n297\n298\n299\n.... .... ....\nScore\n1 2,M M Convolution Pooling\n( )L\n.... .... ....\n1u\n1nu \nnu\nr\nWord Embedding GRU1\nGRU2\n....\n1v\n1nv \nnv\n1'nh \nUtterance-Response Matching Matching Accumulation\nSegment PairsWord Pairs\nMatching Prediction\n1'h\n'nh\nFigure 1: Architecture of SMN\net al., 2016; Serban et al., 2016a). Our work belongs to retrieval based methods, and we study context based response selection.\nEarly studies of retrieval based chatbots focus on response selection for single-turn conversation (Wang et al., 2013; Ji et al., 2014; Wang et al., 2015; Wu et al., 2016b). Recently, researchers begin to pay attention to multi-turn conversation. For example, Lowe et al. (2015) match a response with the literal concatenation of context utterances. Yan et al. (2016) concatenate context utterances with the input message as reformulated queries and perform matching with a deep neural network architecture. Zhou et al. (2016) improve multi-turn response selection with a multi-view model including an utterance view and a word view. Our model is different in that it matches a response with each utterance at first and accumulates matching information instead of sentences by a GRU, thus useful information for matching can be sufficiently retained.\n3 Sequential Matching Network\n3.1 Problem Formalization\nSuppose that we have a data set D = {(yi, si, ri)}Ni=1, where si = {ui,1, . . . , ui,ni} represents a conversation context with {ui,k}nik=1 as utterances. ri is a response candidate and yi \u2208 {0, 1} denotes a label. yi = 1 means ri is a proper response for si, otherwise yi = 0. Our goal is to learn a matching model g(\u00b7, \u00b7) with D. For any context-response pair (s, r), g(s, r) measures the matching degree between s and r.\n3.2 Model Overview\nWe propose a sequential matching network (SMN) to model g(\u00b7, \u00b7). Figure 1 gives the architecture.\nSMN first decomposes context-response matching into several utterance-response pair matching and then all pair matching is accumulated as a context based matching through a recurrent neural network. SMN consists of three layers. The first layer matches a response candidate with each utterance in the context on a word level and a segment level, and important matching information from the two levels is distilled by convolution and pooling and encoded in a matching vector. The matching vectors are then fed into the second layer where they are accumulated in the hidden states of a recurrent neural network with GRU following the chronological order of the utterances in the context. The third layer calculates the final matching score with the hidden states of the second layer.\nSMN enjoys several advantages over the existing models. First, a response candidate can meet each utterance in the context at the very beginning, thus matching information in every utteranceresponse pair can be sufficiently extracted and carried to the final matching score with minimal loss. Second, information extraction from each utterance is conducted on different levels of granularity and under sufficient supervision from the response, thus semantic structures that are useful to response selection in each utterance can be well identified and extracted. Third, matching and utterance relationships are coupled rather than separately modeled, thus utterance relationships (e.g., order), as a kind of knowledge, can supervise the formation of the matching score.\nBy taking utterance relationships into account, SMN extends the \u201c2D\u201d matching that has proven effective in text pair matching for single-turn response selection to sequential \u201c2D\u201d matching for context based matching in response selection for multi-turn conversation. In the following sections,\n4\n301\n302\n303\n304\n305\n306\n307\n308\n309\n310\n311\n312\n313\n314\n315\n316\n317\n318\n319\n320\n321\n322\n323\n324\n325\n326\n327\n328\n329\n330\n331\n332\n333\n334\n335\n336\n337\n338\n339\n340\n341\n342\n343\n344\n345\n346\n347\n348\n349\n350\n351\n352\n353\n354\n355\n356\n357\n358\n359\n360\n361\n362\n363\n364\n365\n366\n367\n368\n369\n370\n371\n372\n373\n374\n375\n376\n377\n378\n379\n380\n381\n382\n383\n384\n385\n386\n387\n388\n389\n390\n391\n392\n393\n394\n395\n396\n397\n398\n399\nwe will describe details of the three layers.\n3.3 Utterance-Response Matching\nGiven an utterance u in a context s and a response candidate r, the model looks up an embedding table and represents u and r as U = [eu,1, . . . , eu,nu ] and R = [er,1, . . . , er,nr ] respectively, where eu,i, er,i \u2208 Rd are the embeddings of the i-th word of u and r respectively. U \u2208 Rd\u00d7nu and R \u2208 Rd\u00d7nr are then used to construct a word-word similarity matrix M1 \u2208 Rnu\u00d7nr and a sequence-sequence similarity matrix M2 \u2208 Rnu\u00d7nr which are two input channels of a convolutional neural network (CNN). The CNN distills important matching information from the matrices and encodes the information into a matching vector v.\nSpecifically, \u2200i, j, the (i, j)-th element of M1 is defined by\ne1,i,j = e > u,i \u00b7 er,j . (1)\nM1 models the matching between u and r on a word level.\nTo construct M2, we first employ a GRU to transform U and R to hidden vectors. Suppose that Hu = [hu,1, . . . , hu,nu ] are the hidden vectors of U, then \u2200i, hu,i \u2208 Rm is defined by\nzi = \u03c3(Wzeu,i +Uzhu,i\u22121) ri = \u03c3(Wreu,i +Urhu,i\u22121)\nh\u0303u,i = tanh(Wheu,i +Uh(ri hu,i\u22121)) hu,i = zi h\u0303u,i + (1\u2212 zi) hu,i\u22121, (2)\nwhere hu,0 = 0, zi and ri are an update gate and a reset gate respectively, \u03c3(\u00b7) is a sigmoid function, and Wz, Wh, Wr, Uz, Ur,Uh are parameters. Similarly, we have Hr = [hr,1, . . . , hr,nr ] as the hidden vectors of R. Then, \u2200i, j, the (i, j)-th element of M2 is defined by\ne2,i,j = h > u,iAhr,j , (3)\nwhere A \u2208 Rm\u00d7m is a linear transformation. \u2200i, GRU models the sequential relationship and the dependency among words up to position i and encodes the text segment until the i-th word to a hidden vector. Therefore, M2 models the matching between u and r on a segment level. M1 and M2 are then processed by a CNN to form v. \u2200f = 1, 2, CNN regards Mf as an input channel, and alternates convolution and max-pooling operations. Suppose that z(l,f) =\n[ z (l,f) i,j ] I(l,f)\u00d7J(l,f) denotes the output of feature maps of type-f on layer-l, where z(0,f) = Mf , \u2200f = 1, 2. On the convolution layer, we employ a 2D convolution operation with a window size r (l,f) w \u00d7 r(l,f)h , and define z (l,f) i,j as\nz (l,f) i,j = \u03c3( Fl\u22121\u2211 f \u2032=0 r (l,f) w\u2211 s=0 r (l,f) h\u2211 t=0 W (l,f) s,t \u00b7 z (l\u22121,f \u2032) i+s,j+t + b l,k), (4)\nwhere \u03c3(\u00b7) is a ReLU, W(l,f) \u2208 Rr (l,f) w \u00d7r (l,f) h and bl,k are parameters, and Fl\u22121 is the number of feature maps on the (l \u2212 1)-th layer. A max pooling operation follows a convolution operation and can be formulated as\nz (l,f) i,j = max\np (l,f) w >s\u22650 max p (l,f) h >t\u22650 zi+s,j+t, (5)\nwhere p(l,f)w and p (l,f) h are the width and the height of the 2D pooling respectively. The output of the final feature maps are concatenated and mapped to a low dimensional space with a linear transformation as the matching vector v \u2208 Rq.\nFrom Equation (1), (3), (4), and (5), we can see that by learning word embedding and parameters of GRU from training data, words or segments in an utterance that are useful to recognize the appropriateness of a response may have high similarity with some words or segments in the response and result in high value areas in the similarity matrices. These areas will be transformed and selected by convolution and pooling operations and carry the important information in the utterance to the matching vector. This is how our model identifies important information in context and leverage it in matching under the supervision of the response. We consider multiple channels because we want to capture important matching information on multiple levels of granularity of text.\n3.4 Matching Accumulation\nSuppose that [v1, . . . , vn] is the output of the first layer (corresponding to n pairs), at the second layer, a GRU takes [v1, . . . , vn] as an input and encodes the matching sequence into its hidden states Hm = [h \u2032 1, . . . , h \u2032 n] \u2208 Rq\u00d7n with a detailed parameterization similar to Equation (2). This layer has two functions: (1) it models the dependency and the temporal relationship of utterances in the context; (2) it leverages the temporal relationship to supervise the accumulation of the pair matching as a context based matching. Moreover, from\n5\n401\n402\n403\n404\n405\n406\n407\n408\n409\n410\n411\n412\n413\n414\n415\n416\n417\n418\n419\n420\n421\n422\n423\n424\n425\n426\n427\n428\n429\n430\n431\n432\n433\n434\n435\n436\n437\n438\n439\n440\n441\n442\n443\n444\n445\n446\n447\n448\n449\n450\n451\n452\n453\n454\n455\n456\n457\n458\n459\n460\n461\n462\n463\n464\n465\n466\n467\n468\n469\n470\n471\n472\n473\n474\n475\n476\n477\n478\n479\n480\n481\n482\n483\n484\n485\n486\n487\n488\n489\n490\n491\n492\n493\n494\n495\n496\n497\n498\n499\nEquation (2), we can see that the reset gate (i.e., ri) and the update gate (i.e., zi) control how much information from the previous hidden state and the current input flows to the current hidden state, thus important matching vectors (corresponding to important utterances) can be accumulated while noise in the vectors can be filtered out.\n3.5 Matching Prediction and Learning\nWith [h\u20321, . . . , h \u2032 n], we define g(s, r) as\ng(s, r) = softmax(W2L[h \u2032 1, . . . , h \u2032 n] + b2), (6)\nwhere W2 and b2 are parameters. We consider three parameterizations for L[h\u20321, . . . , h \u2032 n]: (1) only the last hidden state is used. Then L[h\u20321, . . . , h \u2032 n] = h \u2032 n. (2) the hidden states\nare linearly combined. Then, L[h\u20321, . . . , h \u2032 n] =\u2211n\ni=1wih \u2032 i, where wi \u2208 R. (3) we follow (Yang et al., 2016) and employ an attention mechanism to combine the hidden states. Then, L[h\u20321, . . . , h \u2032 n] is defined as\nti = tanh(W1,1hui,nu +W1,2h \u2032 i + b1), \u03b1i = exp(t>i ts)\u2211 i(exp(t > i ts)) ,\nL[h\u20321, . . . , h \u2032 n] = n\u2211 i=1 \u03b1ih \u2032 i, (7)\nwhere W1,1 \u2208 Rq\u00d7m,W1,2 \u2208 Rq\u00d7q and b1 \u2208 Rq are parameters. h\u2032i and hui,nu are the i-th matching vector and the final hidden state of the i-th utterance respectively. ts \u2208 Rq is a virtual context vector which is randomly initialized and jointly learned in training.\nBoth (2) and (3) aim to learn weights for {h\u20321, . . . , h\u2032n} from training data and highlight the effect of important matching vectors in the final matching. The difference is that weights in (2) are static, because the weights are totally determined by the positions of utterances, while weights in (3) are dynamically computed by the matching vectors and utterance vectors. We denote our model with the three parameterizations of L[h\u20321, . . . , h \u2032 n] as SMNlast, SMNstatic, and SMNdynamic, and empirically compare them in experiments.\nWe learn g(\u00b7, \u00b7) by minimizing cross entropy withD. Let \u0398 denote the parameters of SMN, then the objective function L(D,\u0398) of learning can be formulated as\n\u2212 N\u2211 i=1 [yilog(g(si, ri)) + (1\u2212 yi)log(1\u2212 g(si, ri))] . (8)\n4 Response Candidate Retrieval\nIn practice of a retrieval based chatbot, to apply the matching approach to response selection, one needs to retrieve a bunch of response candidates from an index beforehand. While candidate retrieval is not the focus of the paper, it is an important step in a real system. In this work, we exploit a heuristic method to obtain response candidates from the index. Given a message un with {u1, . . . , un\u22121} utterances in its previous turns, we extract top 5 keywords from {u1, . . . , un\u22121} based on their tf-idf scores1 and expand un with the keywords. Then we send the expanded message to the index and retrieve response candidates using the inline retrieval algorithm of the index. Finally, we use g(s, r) to re-rank the candidates and return the top one as a response to the context.\n5 Experiments\nWe tested our model on a public English data set and a Chinese data set we publish with this paper.\n5.1 Ubuntu Corpus\nThe English data set is the Ubuntu Corpus (Lowe et al., 2015) which contains multi-turn dialogues collected from chat logs of Ubuntu Forum. The data set consists of 1 million context-response pairs for training, 0.5 million pairs for validation, and 0.5 million pairs for test. Positive responses are true responses from human, and negative ones are randomly sampled. The ratio of the positive and the negative is 1:1 in training, and 1:9 in validation and test. We used the copy shared by Xu et al. (2016) 2 in which numbers, urls, and paths are replaced by special placeholders. We followed (Lowe et al., 2015) and employed recall at position k in n candidates (Rn@k) as evaluation metrics.\n5.2 Douban Conversation Corpus\nUbuntu Corpus is a domain specific data set, and response candidates are obtained from negative sampling without human judgment. To further verify the efficacy of our model, we created a new data set with open domain conversations, namely Douban Conversation Corpus. Response candidates in the test set of Douban Conversation Corpus are collected following the procedure of a re-\n1Tf is word frequency in the context, while idf is calculated using the entire index.\n2https://www.dropbox.com/s/ 2fdn26rj6h9bpvl/ubuntudata.zip?dl=0\n6\n501\n502\n503\n504\n505\n506\n507\n508\n509\n510\n511\n512\n513\n514\n515\n516\n517\n518\n519\n520\n521\n522\n523\n524\n525\n526\n527\n528\n529\n530\n531\n532\n533\n534\n535\n536\n537\n538\n539\n540\n541\n542\n543\n544\n545\n546\n547\n548\n549\n557\n558\n559\n560\n561\n562\n563\n564\n565\n566\n567\n568\n569\n570\n571\n572\n573\n574\n575\n576\n577\n578\n579\n580\n581\n582\n583\n584\n585\n586\n587\n588\n589\n590\n591\n592\n593\n594\n595\n596\n597\n598\n599\ntrieval based chatbot and are labeled by human judges. Douban Conversation Corpus simulates the real scenario of a retrieval based chatbot, and we publish it to research communities to facilitate the research of multi-turn response selection.\nSpecifically, we crawled 1.1 million dyadic dialogues (conversation between two persons) longer than 2 turns from Douban group3 which is a popular social networking service in China. From the data, we randomly sampled 0.5 million dialogues for creating a training set, 25 thousand dialouges for creating a validation set, and 1, 000 dialogues for creating a test set, and made sure that there is no overlap among the three sets. For each dialogue in training and validation, we took the last turn as a positive response for the previous turns as a context and randomly sampled another response from the 1.1 million data as a negative response. There are 1 million context-response pairs in the training set and 50 thousand pairs in the validation set.\nTo create the test set, we first crawled 15 million post-reply pairs from Sina Weibo4 which is the largest microblogging service in China and indexed the pairs with Lucene5. We took the last turn of each Douban dyadic dialogue in the test set as a message, retrieved 10 response candidates from the index following the method in Section 4, and finally formed a test set with 10, 000 context-response pairs. We recruited three labelers to judge if a candidate is a proper response to the context. A proper response means the response can naturally reply to the message given the whole context. Each pair received three labels and the majority of the labels were taken as the final decision. Table 2 gives the statistics of the three sets. Note that the Fleiss\u2019 kappa (Fleiss, 1971) of the labeling is 0.41, which indicates that the three labelers reached a relatively high agreement.\nBesides Rn@ks, we also followed the convention of information retrieval and employed mean average precision (MAP) (Baeza-Yates et al., 1999), mean reciprocal rank (MRR) (Voorhees et al., 1999), and precision at position 1 (P@1) as evaluation metrics. We did not calculate R2@1 because in Douban corpus one context could have more than one correct responses, and we have to randomly sample one for R2@1, which may bring bias to evaluation. When using the labeled set,\n3https://www.douban.com/group 4http://weibo.com/ 5https://lucenenet.apache.org/\nwe removed conversations with all negative responses or all positive responses, as models make no difference on them. There are 6, 670 contextresponse pairs left in the test set.\n5.3 Baseline\nWe considered the following baselines:\nBasic models: models in (Lowe et al., 2015) and (Kadlec et al., 2015) including TF-IDF, RNN, CNN, LSTM and BiLSTM.\nMulti-view: the model proposed by Zhou et al. (2016) that utilizes a hierarchical recurrent neural network to model utterance relationships.\nDeep learning to respond (DL2R): the model proposed by Yan et al. (2016) that reformulates the message with other utterances in the context.\nAdvanced single-turn matching models: since BiLSTM does not represent the state-ofthe-art matching model, we concatenated the utterances in a context and matched the long text with a response candidate using more powerful models including MV-LSTM (Wan et al., 2016) (2D matching), Match-LSTM (Wang and Jiang, 2015), Attentive-LSTM (Tan et al., 2015) (two attention based models), and Multi-Channel which is described in Section 3.3. Multi-Channel is a simple version of our model without considering utterance relationships. We also appended the top 5 tf-idf words in context to the input message, and computed the score between the expanded message and a response with Multi-Channel, denoted as Multi-Channelexp.\n5.4 Parameter Tuning\nFor baseline models, if their results are available in the existing literatures (e.g., those on the Ubuntu corpus), we just copied the numbers, otherwise we implemented the models following the settings in the literatures. All models were implemented using Theano (Theano Development Team, 2016). Word embeddings were initialized by the results of word2vec (Mikolov et al., 2013) which ran on the training data, and the dimensionality of word vectors is 200. For Multi-Channel and layer one of\n7\n601\n602\n603\n604\n605\n606\n607\n608\n609\n610\n611\n612\n613\n614\n615\n616\n617\n618\n619\n620\n621\n622\n623\n624\n625\n626\n627\n628\n629\n630\n631\n632\n633\n634\n635\n636\n637\n638\n639\n640\n641\n642\n643\n644\n645\n646\n647\n648\n649\n650\n651\n652\n653\n654\n655\n656\n657\n658\n659\n660\n661\n662\n663\n664\n665\n666\n667\n668\n669\n670\n671\n672\n673\n674\n675\n676\n677\n678\n679\n680\n681\n682\n683\n684\n685\n686\n687\n688\n689\n690\n691\n692\n693\n694\n695\n696\n697\n698\n699\nUbuntu Corpus Douban Conversation Corpus R2@1 R10@1 R10@2 R10@5 MAP MRR P@1 R10@1 R10@2 R10@5\nTF-IDF 0.659 0.410 0.545 0.708 0.331 0.359 0.179 0.095 0.172 0.405 RNN 0.768 0.403 0.547 0.819 0.390 0.422 0.208 0.011 0.223 0.589 CNN 0.848 0.549 0.684 0.896 0.417 0.440 0.226 0.012 0.252 0.647 LSTM 0.901 0.638 0.784 0.949 0.485 0.527 0.320 0.187 0.343 0.720 BiLSTM 0.895 0.630 0.780 0.944 0.479 0.514 0.313 0.184 0.330 0.716 Multi-View 0.908 0.662 0.801 0.951 0.505 0.543 0.342 0.202 0.350 0.729 DL2R 0.899 0.626 0.783 0.944 0.488 0.527 0.330 0.193 0.342 0.705 MV-LSTM 0.906 0.653 0.804 0.946 0.498 0.538 0.348 0.202 0.351 0.710 Match-LSTM 0.904 0.653 0.799 0.944 0.500 0.537 0.345 0.202 0.348 0.720 Attentive-LSTM 0.903 0.633 0.789 0.943 0.495 0.523 0.331 0.192 0.328 0.718 Multi-Channel 0.904 0.656 0.809 0.942 0.506 0.543 0.349 0.203 0.351 0.709 Multi-Channelexp 0.714 0.368 0.497 0.745 0.476 0.515 0.317 0.179 0.335 0.691 SMNlast 0.923 0.723 0.842 0.956 0.526 0.571 0.392 0.236 0.387 0.729 SMNstatic 0.927 0.725 0.838 0.962 0.523 0.572 0.387 0.228 0.387 0.734 SMNdynamic 0.926 0.726 0.847 0.961 0.529 0.569 0.395 0.233 0.396 0.724\nTable 3: Evaluation results on the two data sets. Numbers in bold mean that the improvement is statistically significant compared with the best baseline.\nour model, we set the dimensionality of the hidden states of GRU as 200. We tuned the window size of convolution and pooling in {(2, 2), (3, 3)(4, 4)} and chose (3, 3) finally. The number of feature maps is 8. In layer two, we set the dimensionality of matching vectors and the hidden states of GRU as 50. The parameters were updated by stochastic gradient descent with Adam algorithm (Kingma and Ba, 2014) on a single Tesla K80 GPU. The initial learning rate is 0.001, and the parameters of Adam, \u03b21 and \u03b22 are 0.9 and 0.999 respectively. We employed early-stopping as a regularization strategy. Models were trained in minibatches with a batch size 200, and maximum utterance length is 50. We set the maximum context length (i.e., number of utterances) as 10, because performance of models does not get improved on contexts longer than 10 (details are shown in the supplementary material). We padded zeros if the number of utterances in a context is less than 10, otherwise we kept the last 10 utterances.\n5.5 Evaluation Results\nTable 3 shows the evaluation results on the two data sets. Our models outperform baselines greatly in terms of all metrics on both data sets, and the improvements are statistically significant (t-test with p-value \u2264 0.01, except R10@5 on Douban Corpus). Even the state-of-the-art singleturn matching models perform much worse than our models. The results demonstrate that one cannot neglects utterance relationships and simply perform multi-turn response selection by concatenating utterances together. Our models achieve significant improvements over Multi-View, which justified our \u201cmatching first\u201d strategy. DL2R is\nworse than our models, indicating that utterance reformulation with heuristic rules is not a good method to utilize context information. Rn@ks are low on Douban corpus as there are multiple correct candidates for a context (e.g., if there are 3 correct responses, then the maximumR10@1 is 0.33). SMNdynamic is only slightly better than SMNstatic and SMNlast. The reason might be that GRU can select useful signals from the matching sequence and accumulate them in the final state with its gate mechanism, thus the efficacy of attention mechanism is not obvious for the task.\n5.6 Further Analysis\nVisualization: we visualize the similarity matrices and the gates of GRU in layer two using an example from the Ubuntu corpus to further clarify how our model identifies important information in the context and how it selects important matching vectors with the gate mechanism of GRU as described in Section 3.3 and Section 3.4. The example is {u1: how can unzip many rar ( number for example ) files at once; u2: sure you can do that in bash; u3: okay how? u4: are the files all in the same directory? u5: yes they all are; r: then the command glebihan should extract them all from/to that directory}. It is from the test set and our model successfully ranked the correct response to the top position. Due to space limitation, we only visualized M1, M2 and the update gate (i.e. z) in Figure 2. Other pieces of our model are shown in the supplementary material. We can see that in u1 important words including \u201cunzip\u201d, \u201crar\u201d, \u201cfiles\u201d are recognized and carried to matching by \u201ccommand\u201d, \u201cextract\u201d, and \u201cdirectory\u201d in r, while u3 is almost useless and thus little infor-\n8\n701\n702\n703\n704\n705\n706\n707\n708\n709\n710\n711\n712\n713\n714\n715\n716\n717\n718\n719\n720\n721\n722\n723\n724\n725\n726\n727\n728\n729\n730\n731\n732\n733\n734\n735\n736\n737\n738\n739\n740\n741\n742\n743\n744\n745\n746\n747\n748\n749\n750\n751\n752\n753\n754\n755\n756\n757\n758\n759\n760\n761\n762\n763\n764\n765\n766\n767\n768\n769\n770\n771\n772\n773\n774\n775\n776\n777\n778\n779\n780\n781\n782\n783\n784\n785\n786\n787\n788\n789\n790\n791\n792\n793\n794\n795\n796\n797\n798\n799\nUbuntu Corpus Douban Conversation Corpus R2@1 R10@1 R10@2 R10@5 MAP MRR P@1 R10@1 R10@2 R10@5\nReplaceM 0.905 0.661 0.799 0.950 0.503 0.541 0.343 0.201 0.364 0.729 ReplaceA 0.918 0.716 0.832 0.954 0.522 0.565 0.376 0.220 0.385 0.727 Only M1 0.919 0.704 0.832 0.955 0.518 0.562 0.370 0.228 0.371 0.737 Only M2 0.921 0.715 0.836 0.956 0.521 0.565 0.382 0.232 0.380 0.734 SMNlast 0.923 0.723 0.842 0.956 0.526 0.571 0.392 0.236 0.387 0.729\nTable 4: Evaluation results of model ablation.\nth en th e\nco m\nm an\nd gl eb ih an sh ou ld ex tra ct th em a ll fro m /toth at di re ct or y\nhow can\nunzip many\nrar ( _number_ for\nexample )\nfiles at once\n0.00\n0.15\n0.30\n0.45\n0.60\n0.75\n0.90\n1.05\n1.20\n1.35\n1.50\nv a lu\ne\n(a) M1 of u1 and r\nth en th e\nco m\nm an\nd\ngl eb\nih an\nsh ou\nld ex tr ac\nt th em a ll\nfro m\n/toth at\ndi re\nct or\ny\nokay how 0.00 0.15 0.30 0.45 0.60 0.75 0.90 1.05 1.20 1.35 1.50 v a lu e\n(b) M1 of u3 and r\n0 10 20 30 40\nu_1\nu_2\nu_3\nu_4\nu_5\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nv a lu\ne\n(c) Update gate\nFigure 2: Model visualization. Darker areas mean larger value.\nmation is extracted from it. u1 is crucial to response selection and nearly all information from u1 and r flows to the hidden state of GRU, while other utterances are less informative and the corresponding gates are almost \u201cclosed\u201d to keep the information from u1 and r until the final state.\nModel ablation: we investigate the effect of different parts of SMN by removing them one by one from SMNlast, shown in Table 4. First, replacing the multi-channel \u201c2D\u201d matching with a neural tensor network (NTN) (Socher et al., 2013) (denoted as ReplaceM ) makes the performance drop dramatically. This is because NTN only matches a pair by an utterance vector and a response vector and loses important information in the pair. Together with the visualization, we can conclude that \u201c2D\u201d matching plays a key role in the \u201cmatching first\u201d strategy as it captures the important matching information in each pair with minimal loss. Second, the performance slightly drops when replacing the GRU for matching accumulation with a multi-layer perceptron (denoted as ReplaceA). This indicates that utterance relationships are useful. Finally, we left only one channel in matching and found that M2 is a little more powerful than M1 and we achieve the best results with both of them (except on R10@5 on Douban Corpus).\nContext length: we study how our model (SMNlast) performs across the length of contexts. Figure 3 shows the comparison on MAP in different length intervals on the Douban corpus. Our model consistently performs better than the baselines, and when contexts become longer, the gap becomes larger. The results demonstrate that our model can well capture the dependencies, espe-\ncially long dependencies, among utterances in contexts. We give the comparisons on other metrics in our supplementary material.\n(2,5] (5,10] (10,) context length\n40\n45\n50\n55 60 M A P\nLSTM MV-LSTM Multi-View SMN\nFigure 3: Comparison across context length Retrieval v.s. Generation: we compared SMN with a state-of-the-art response generation model VHERD (Serban et al., 2016b) which was trained using D on the Douban corpus. We conducted a side-by-side human comparison on the top one responses of the two models for each context in the test set. The result is that SMN wins on 238 examples, loses on 207 examples, and is comparable with VHRED on the remaining 555 examples. This indicates that a retrieval based chatbot with SMN can provide a better experience than the state-of-the-art generation model in practice.\n6 Conclusion and Future Work\nWe present a new context based model for multiturn response selection in retrieval-based chatbots. Experiment results on public data sets show that the model can significantly outperform the stateof-the-art methods. Besides, we publish the first human labeled multi-turn response selection data set to research communities. In the future, we are going to study how to model logical consistency of responses and improve candidate retrieval (see supplementary material).\n9\n801\n802\n803\n804\n805\n806\n807\n808\n809\n810\n811\n812\n813\n814\n815\n816\n817\n818\n819\n820\n821\n822\n823\n824\n825\n826\n827\n828\n829\n830\n831\n832\n833\n834\n835\n836\n837\n838\n839\n840\n841\n842\n843\n844\n845\n846\n847\n848\n849\n850\n851\n852\n853\n854\n855\n856\n857\n858\n859\n860\n861\n862\n863\n864\n865\n866\n867\n868\n869\n870\n871\n872\n873\n874\n875\n876\n877\n878\n879\n880\n881\n882\n883\n884\n885\n886\n887\n888\n889\n890\n891\n892\n893\n894\n895\n896\n897\n898\n899\n", "cats": {"READ": 1.0, "DONTREAD": 0.0}, "reviews": [{"IMPACT": "3", "SUBSTANCE": "4", "APPROPRIATENESS": "5", "MEANINGFUL_COMPARISON": "5", "PRESENTATION_FORMAT": "Poster", "comments": "- Strengths:\n\nRelatively clear description of context and structure of proposed approach.\nRelatively complete description of the math. Comparison to an extensive set of\nalternative systems.\n\n- Weaknesses:\n\nWeak results/summary of \"side-by-side human\" comparison in Section 5. Some\ndisfluency/agrammaticality.\n\n- General Discussion:\n\nThe article proposes a principled means of modeling utterance context,\nconsisting of a sequence of previous utterances. Some minor issues:\n\n1. Past turns in Table 1 could be numbered, making the text associated with\nthis table (lines 095-103) less difficult to ingest. Currently, readers need to\ncount turns from the top when identifying references in the authors'\ndescription, and may wonder whether \"second\", \"third\", and \"last\" imply a\nside-specific or global enumeration.\n\n2. Some reader confusion may be eliminated by explicitly defining what\n\"segment\" means in \"segment level\", as occurring on line 269. Previously, on\nline 129, this seemingly same thing was referred to as \"a sequence-sequence\n[similarity matrix]\". The two terms appear to be used interchangeably, but it\nis not clear what they actually mean, despite the text in section 3.3. It seems\nthe authors may mean \"word subsequence\" and \"word subsequence to word\nsubsequence\", where \"sub-\" implies \"not the whole utterance\", but not sure.\n\n3. Currently, the variable symbol \"n\" appears to be used to enumerate words in\nan utterance (line 306), as well as utterances in a dialogue (line 389). The\nauthors may choose two different letters for these two different purposes, to\navoid confusing readers going through their equations.\n\n4. The statement \"This indicates that a retrieval based chatbot with SMN can\nprovide a better experience than the state-of-the-art generation model in\npractice.\" at the end of section 5 appears to be unsupported. The two\napproaches referred to are deemed comparable in 555 out of 1000 cases, with the\nbaseline better than the proposed method in 238 our of the remaining 445 cases.\nThe authors are encouraged to assess and present the statistical significance\nof this comparison. If it is weak, their comparison permits to at best claim\nthat their proposed method is no worse (rather than \"better\") than the VHRED\nbaseline.\n\n5. The authors may choose to insert into Figure 1 the explicit \"first layer\",\n\"second layer\" and \"third layer\" labels they use in the accompanying text.\n\n6.  Their is a pervasive use of \"to meet\" as in \"a response candidate can meet\neach utterace\" on line 280 which is difficult to understand.\n\n7. Spelling: \"gated recurrent unites\"; \"respectively\" on line 133 should be\nremoved; punctuation on line 186 and 188 is exchanged; \"baseline model over\" ->\n\"baseline model by\"; \"one cannot neglects\".", "SOUNDNESS_CORRECTNESS": "5", "ORIGINALITY": "3", "is_meta_review": null, "RECOMMENDATION": "4", "CLARITY": "4", "REVIEWER_CONFIDENCE": "2"}]}
{"text": "A Full Non-Monotonic Transition System for Unrestricted Non-Projective Parsing\n1 000\n011\n012\n013\n014\n015\n016\n017\n018\n019\n020\n021\n022\n023\n024\n025\n026\n027\n028\n029\n030\n031\n032\n033\n034\n035\n036\n037\n038\n039\n040\n041\n042\n043\n044\n045\n046\n047\n048\n049\n061\n062\n063\n064\n065\n066\n067\n068\n069\n070\n071\n072\n073\n074\n075\n076\n077\n078\n079\n080\n081\n082\n083\n084\n085\n086\n087\n088\n089\n090\n091\n092\n093\n094\n095\n096\n097\n098\n099\n1 Introduction\nGreedy transition-based dependency parsers are widely used in different NLP tasks due to their speed and efficiency. They parse a sentence from left to right by greedily choosing the highestscoring transition to go from the current parser configuration or state to the next. The resulting sequence of transitions incrementally builds a parse for the input sentence. The scoring of the transitions is provided by a statistical model, previously trained to approximate an oracle, a function that selects the needed transitions to parse a gold tree.\nUnfortunately, the greedy nature that grants these parsers their efficiency also represents their main limitation. McDonald and Nivre (2007) show that greedy transition-based parsers lose accuracy to error propagation: a transition erroneously chosen by the greedy parser can place it\nin an incorrect and unknown configuration, causing more mistakes in the rest of the transition sequence. Training with a dynamic oracle (Goldberg and Nivre, 2012) improves robustness in these situations, but in a monotonic transition system, erroneous decisions made in the past are permanent, even when the availability of further information in later states might be useful to correct them.\nHonnibal et al. (2013) show that allowing some degree of non-monotonicity, by using a limited set of non-monotonic actions that can repair past mistakes and replace previously-built arcs, can increase the accuracy of a transition-based parser. In particular, they present a modified arc-eager transition system where the Left-Arc and Reduce transitions are non-monotonic: the former is used to repair invalid attachments made in previous states by replacing them with a leftward arc, and the latter allows the parser to link two words with a rightward arc that were previously left unattached due to an erroneous decision. Since the Right-Arc transition is still monotonic and leftward arcs can never be repaired because their dependent is removed from the stack by the arc-eager parser and rendered inaccessible, this approach can only repair certain kinds of mistakes: namely, it can fix erroneous rightward arcs by replacing them with a leftward arc, and connect a limited set of unattached words with rightward arcs. In addition, they argue that non-monotonicity in the training oracle can be harmful for the final accuracy and, therefore, they suggest to apply it only as a fallback component for a monotonic oracle, which is given priority over the non-monotonic one. Thus, this strategy will follow the path dictated by the monotonic oracle the majority of the time. Honnibal and Johnson (2015) present an extension of this transition system with an Unshift transition allowing it some extra flexibility to correct past errors. However, the restriction that only rightward\n2\n101\n102\n103\n104\n105\n106\n107\n108\n109\n110\n111\n112\n113\n114\n115\n116\n117\n118\n119\n120\n121\n122\n123\n124\n125\n126\n127\n128\n129\n130\n131\n132\n133\n134\n135\n136\n137\n138\n139\n140\n141\n142\n143\n144\n145\n146\n147\n148\n149\n150\n151\n152\n153\n154\n155\n156\n157\n158\n159\n160\n161\n162\n163\n164\n165\n166\n167\n168\n169\n170\n171\n172\n173\n174\n175\n176\n177\n178\n179\n180\n181\n182\n183\n184\n185\n186\n187\n188\n189\n190\n191\n192\n193\n194\n195\n196\n197\n198\n199\narcs can be deleted, and only by replacing them with leftward arcs, is still in place. Furthermore, both versions of the algorithm are limited to projective trees.\nIn this paper, we propose a non-monotonic transition system based on the non-projective Covington parser, together with a dynamic oracle to train it with erroneous examples that will need to be repaired. Unlike the system developed in (Honnibal et al., 2013; Honnibal and Johnson, 2015), we work with full non-monotonicity. This has a twofold meaning: (1) our approach can repair previous erroneous attachments regardless of their original direction, and it can replace them either with a rightward or leftward arc as both arc transitions are non-monotonic;1 and (2) we use exclusively a non-monotonic oracle, without the interferences of monotonic decisions. These modifications are feasible because the non-projective Covington transition system is less rigid than the arc-eager algorithm, as words are never deleted from the parser\u2019s data structures and can always be revisited, making it a better option to exploit the full potencial of non-monotonicity. To our knowledge, the presented system is the first nonmonotonic parser that can produce non-projective dependency analyses. Another novel aspect is that our dynamic oracle is approximate, i.e., based on efficiently-computable approximations of the loss due to the complexity of calculating its actual value in a non-monotonic and non-projective scenario. However, this is not a problem in practice: experimental results show how our parser and oracle can use non-monotonic actions to repair erroneous attachments, outperforming the monotonic version developed by Go\u0301mez-Rodr\u0131\u0301guez and Ferna\u0301ndez-Gonza\u0301lez (2015) in a large majority of the datasets tested.\n2 Preliminaries\n2.1 Non-Projective Covington Transition System\nThe non-projective Covington parser was originally defined by Covington (2001), and then recast by Nivre (2008) under the transition-based parsing framework.\n1The only restriction is that parsing must still proceed in left-to-right order. For this reason, a leftward arc cannot be repaired with a rightward arc, because this would imply going back in the sentence. The other three combinations (replacing leftward with leftward, rightward with leftward or rightward with rightward arcs) are possible.\nThe transition system that defines this parser is as follows: each parser configuration is of the form c = \u3008\u03bb1, \u03bb2, B,A\u3009, such that \u03bb1 and \u03bb2 are lists of partially processed words, B is another list (called the buffer) containing currently unprocessed words, and A is the set of dependencies that have been built so far. Suppose that our input is a string w1 \u00b7 \u00b7 \u00b7wn, whose word occurrences will be identified with their indices 1 \u00b7 \u00b7 \u00b7n for simplicity. Then, the parser will start at an initial configuration cs(w1 . . . wn) = \u3008[], [], [1 . . . n], \u2205\u3009, and execute transitions chosen from those in Figure 1 until a terminal configuration of the form {\u3008\u03bb1, \u03bb2, [], A\u3009 \u2208 C} is reached. At that point, the sentence\u2019s parse tree is obtained from A.2\nThese transitions implement the same logic as the double nested loop traversing word pairs in the original formulation by Covington (2001). When the parser\u2019s configuration is \u3008\u03bb1|i, \u03bb2, j|B,A\u3009, we say that it is considering the focus words i and j, located at the end of the first list and at the beginning of the buffer. At that point, the parser must decide whether these two words should be linked with a leftward arc i \u2190 j (Left-Arc transition), a rightward arc i \u2192 j (Right-Arc transition), or not linked at all (No-Arc transition). However, the two transitions that create arcs will be disallowed in configurations where this would cause a violation of the single-head constraint (a node can have at most one incoming arc) or the acyclicity constraint (the dependency graph cannot have cycles). After applying any of these three transitions, i is moved to the second list to make i \u2212 1 and j the focus words for the next step. As an alternative, we can instead choose to execute a Shift transition which lets the parser read a new input word, placing the focus on j and j + 1.\nThe resulting parser can generate any possible dependency tree for the input, including arbitrary non-projective trees. While it runs in quadratic worst-case time, in theory worse than lineartime transition-based parsers (e.g. (Nivre, 2003; Go\u0301mez-Rodr\u0131\u0301guez and Nivre, 2013)), it has been shown to outspeed linear algorithms in practice, thanks to feature extraction optimizations that cannot be implemented in other parsers (Volokh and Neumann, 2012). In fact, one of the fastest dependency parsers ever reported uses this algorithm\n2In general A is a forest, but it can be converted to a tree by linking headless nodes as dependents of an artificial root node at position 0. When we refer to parser outputs as trees, we assume that this transformation is being implicitly made.\n3\n201\n202\n203\n204\n205\n206\n207\n208\n209\n210\n211\n212\n213\n214\n215\n216\n217\n218\n219\n220\n221\n222\n223\n224\n225\n226\n227\n228\n229\n230\n231\n232\n233\n234\n235\n236\n237\n238\n239\n240\n241\n242\n243\n244\n245\n246\n247\n248\n249\n250\n251\n252\n253\n254\n255\n256\n257\n258\n259\n260\n261\n262\n263\n264\n265\n266\n267\n268\n269\n270\n271\n272\n273\n274\n275\n276\n277\n278\n279\n280\n281\n282\n283\n284\n285\n286\n287\n288\n289\n290\n291\n292\n293\n294\n295\n296\n297\n298\n299\nShift: \u3008\u03bb1, \u03bb2, j|B,A\u3009 \u21d2 \u3008\u03bb1 \u00b7 \u03bb2|j, [], B,A\u3009 No-Arc: \u3008\u03bb1|i, \u03bb2, B,A\u3009 \u21d2 \u3008\u03bb1, i|\u03bb2, B,A\u3009 Left-Arc: \u3008\u03bb1|i, \u03bb2, j|B,A\u3009 \u21d2 \u3008\u03bb1, i|\u03bb2, j|B,A \u222a {j \u2192 i}\u3009 only if @k | k \u2192 i \u2208 A (single-head) and i\u2192\u2217 j 6\u2208 A (acyclicity). Right-Arc: \u3008\u03bb1|i, \u03bb2, j|B,A\u3009 \u21d2 \u3008\u03bb1, i|\u03bb2, j|B,A \u222a {i\u2192 j}\u3009\nonly if @k | k \u2192 j \u2208 A (single-head) and j \u2192\u2217 i 6\u2208 A (acyclicity).\nFigure 1: Transitions of the monotonic Covington non-projective dependency parser. The notation i\u2192\u2217 j \u2208 A means that there is a (possibly empty) directed path from i to j in A.\n(Volokh, 2013).\n2.2 Monotonic Dynamic Oracle\nA dynamic oracle is a function that maps a configuration c and a gold tree tG to the set of transitions that can be applied in c and lead to some parse tree t minimizing the Hamming loss with respect to tG (the amount of nodes whose head is different in t and tG). Following Goldberg and Nivre (2013), we say that an arc set A is reachable from configuration c, and we write c A, if there is some (possibly empty) path of transitions from c to some configuration c\u2032 = \u3008\u03bb1, \u03bb2, B,A\u2032\u3009, with A \u2286 A\u2032. Then, we can define the loss of configuration c as\n`(c) = min t|c t\nL(t, tG),\nand therefore, a correct dynamic oracle will return the set of transitions\nod(c, tG) = {\u03c4 | `(c)\u2212 `(\u03c4(c)) = 0},\ni.e., the set of transitions that do not increase configuration loss, and thus lead to the best parse (in terms of loss) reachable from c. Hence, implementing a dynamic oracle reduces to computing the loss `(c) for each configuration c.\nGoldberg and Nivre (2013) show a straightforward method to calculate loss for parsers that are arc-decomposable, i.e., those where every arc set A that can be part of a well-formed parse verifies that if c (i \u2192 j) for every i \u2192 j \u2208 A (i.e., each of the individual arcs of A is reachable from a given configuration c), then c A (i.e., the set A as a whole is reachable from c). If this holds, then the loss of a configuration c equals the number of gold arcs that are not individually reachable from c, which is easy to compute in most parsers.\nGo\u0301mez-Rodr\u0131\u0301guez and Ferna\u0301ndez-Gonza\u0301lez (2015) show that the non-projective Covington parser is not arc-decomposable because sets of individually reachable arcs may form cycles together with already-built arcs, preventing them\nfrom being jointly reachable due to the acyclicity constraint. In spite of this, they prove that a dynamic oracle for the Covington parser can be efficiently built by counting individually unreachable arcs, and correcting for the presence of such cycles. Concretely, the loss is computed as:\n`(c) = |U(c, tG)|+ nc(A \u222a I(c, tG))\nwhere I(c, tG) = {x \u2192 y \u2208 tG | c (x \u2192 y)} is the set of individually reachable arcs of tG from configuration c; U(c, tG) is the set of individually unreachable arcs of tG from c, computed as tG\\I(c, tG); and nc(G) denotes the number of cycles in a graph G.\nTherefore, to calculate the loss of a configuration c, we only need to compute the two terms |U(c, tG)| and nc(A \u222a I(c, tG)). To calculate the first term, given a configuration cwith focus words i and j (i.e., c = \u3008\u03bb1|i, \u03bb2, j|B,A\u3009), an arc x\u2192 y will be in U(c, tG) if it is not in A, and at least one of the following holds:\n\u2022 j > max(x, y), (i.e., we have read too far in the string and can no longer get max(x, y) as right focus word), \u2022 j = max(x, y) \u2227 i < min(x, y), (i.e., we\nhave max(x, y) as the right focus word but the left focus word has already moved left past min(x, y), and we cannot go back), \u2022 there is some z 6= 0, z 6= x such that z \u2192 y \u2208 A, (i.e., we cannot create x\u2192 y because it would violate the single-head constraint), \u2022 x and y are on the same weakly connected\ncomponent of A (i.e., we cannot create x \u2192 y due to the acyclicity constraint).\nThe second term of the loss, nc(A \u222a I(c, tG)), can be computed by first obtaining I(c, tG) as tG \\ U(c, tG). Since the graph I(c, tG) has indegree 1, the algorithm by Tarjan (1972) can then be used to find and count the cycles in O(n) time.\n4\n301\n302\n303\n304\n305\n306\n307\n308\n309\n310\n311\n312\n313\n314\n315\n316\n317\n318\n319\n320\n321\n322\n323\n324\n325\n326\n327\n328\n329\n330\n331\n332\n333\n334\n335\n336\n337\n338\n339\n340\n341\n342\n343\n344\n345\n346\n347\n348\n349\n350\n351\n352\n353\n354\n355\n356\n357\n358\n359\n360\n361\n362\n363\n364\n365\n366\n367\n368\n369\n370\n371\n372\n373\n374\n375\n376\n377\n378\n379\n380\n381\n382\n383\n384\n385\n386\n387\n388\n389\n390\n391\n392\n393\n394\n395\n396\n397\n398\n399\nAlgorithm 1 Computation of the loss of a configuration in the monotonic oracle. 1: function LOSS(c = \u3008\u03bb1|i, \u03bb2, j|B,A\u3009, tG) 2: U \u2190 \u2205 . Variable U is for U(c, tG) 3: for each x\u2192 y \u2208 (tG \\A) do 4: left \u2190 min(x, y) 5: right \u2190 max(x, y) 6: if j > right \u2228 7: (j = right \u2227 i < left)\u2228 8: (\u2203z > 0, z 6= x : z \u2192 y \u2208 A)\u2228 9: WEAKLYCONNECTED(A, x, y) then 10: U \u2190 u \u222a {x\u2192 y} 11: I \u2190 tG \\U . Variable I is for I(c, tG) 12: return |U |+ COUNTCYCLES(A \u222a I )\nAlgorithm 1 shows the resulting loss calculation algorithm, where COUNTCYCLES is a function that counts the number of cycles in the given graph and WEAKLYCONNECTED returns whether two given nodes are weakly connected in A.\n3 Non-Monotonic Transition System for the Covington Non-Projective Parser\nWe now define a non-monotonic variant of the Covington non-projective parser. To do so, we allow the Right-Arc and Left-Arc transitions to create arcs between any pair of nodes without restriction. If the node attached as dependent already had a previous head, the existing attachment is discarded in favor of the new one. This allows the parser to correct erroneous attachments made in the past by assigning new heads, while still enforcing the single-head constraint, as only the most recent head assigned to each node is kept.\nTo enforce acyclicity, one possibility would be to keep the logic of the monotonic algorithm, forbidding the creation of arcs that would create cycles. However, this greatly complicates the definition of the set of individually unreachable arcs, which is needed to compute the loss bounds that will be used by the dynamic oracle. This is because a gold arc x \u2192 y may superficially seem unreachable due to forming a cycle together with arcs in A, but it might in fact be reachable if there is some transition sequence that first breaks the cycle using non-monotonic transitions to remove arcs from A, to then create x \u2192 y. We do not know of a way to characterize the conditions under which such a transition sequence exists, and thus cannot estimate the loss efficiently.\nInstead, we enforce the acyclicity constraint in a similar way to the single-head constraint: Right-Arc and Left-Arc transitions are always allowed, even if the prospective arc would create a\ncycle in A. However, if the creation of a new arc x\u2192 y generates a cycle in A, we immediately remove the arc of the form z \u2192 x from A (which trivially exists, and is unique due to the singlehead constraint). This not only enforces the acyclicity constraint while keeping the computation of U(c, tG) simple and efficient, but also produces a straightforward, coherent algorithm (arc transitions are always allowed, and both constraints are enforced by deleting a previous arc) and allows us to exploit non-monotonicity to the maximum (we can not only recover from assigning a node the wrong head, but also from situations where previous errors together with the acyclicity constraint prevent us from building a gold arc, keeping with the principle that later decisions override earlier ones).\nIn Figure 2, we can see the resulting nonmonotonic transition system for the non-projective Covington algorithm, where, unlike the monotonic version, all transitions are allowed at each configuration, and the single-head and acyclicity constraints are kept in A by removing offending arcs.\n4 Non-Monotonic Approximate Dynamic Oracle\nTo successfully train a non-monotonic system, we need a dynamic oracle with error exploration, so that the parser will be put in erroneous states and need to apply non-monotonic transitions in order to repair them. To achieve that, we modify the dynamic oracle defined by Go\u0301mez-Rodr\u0131\u0301guez and Ferna\u0301ndez-Gonza\u0301lez (2015) so that it can deal with non-monotonicity. Our modification is an approximate dynamic oracle: due to the extra flexibility added to the algorithm by non-monotonicity, we do not know of an efficient way of obtaining an exact calculation of the loss of a given configuration. Instead, we use upper or lower bounds on the loss, which we empirically show to be very tight (less that 1% relative error with respect to the real loss) and are sufficient for the algorithm to provide better accuracy than the exact monotonic oracle.\nFirst of all, we adapt the computation of the set of individually unreachable arcs U(c, tG) to the new algorithm. In particular, if c has focus words i and j (i.e., c = \u3008\u03bb1|i, \u03bb2, j|B,A\u3009), then an arc x \u2192 y is in U(c, tG) if it is not in A, and at least one of the following holds: \u2022 j > max(x, y), (i.e., we have read too far in\nthe string and can no longer get max(x, y) as\n5\n401\n402\n403\n404\n405\n406\n407\n408\n409\n410\n411\n412\n413\n414\n415\n416\n417\n418\n419\n420\n421\n422\n423\n424\n425\n426\n427\n428\n429\n430\n431\n432\n433\n434\n435\n436\n437\n438\n439\n440\n441\n442\n443\n444\n445\n446\n447\n448\n449\n450\n451\n452\n453\n454\n455\n456\n457\n458\n459\n460\n461\n462\n463\n464\n465\n466\n467\n468\n469\n470\n471\n472\n473\n474\n475\n476\n477\n478\n479\n480\n481\n482\n483\n484\n485\n486\n487\n488\n489\n490\n491\n492\n493\n494\n495\n496\n497\n498\n499\nShift: \u3008\u03bb1, \u03bb2, j|B,A\u3009 \u21d2 \u3008\u03bb1 \u00b7 \u03bb2|j, [], B,A\u3009 No-Arc: \u3008\u03bb1|i, \u03bb2, B,A\u3009 \u21d2 \u3008\u03bb1, i|\u03bb2, B,A\u3009 Left-Arc: \u3008\u03bb1|i, \u03bb2, j|B,A\u3009 \u21d2 \u3008\u03bb1, i|\u03bb2, j|B, (A \u222a {j \u2192 i}) \\{x\u2192 i \u2208 A} \\ {k \u2192 j \u2208 A | i\u2192\u2217 k \u2208 A}\u3009 Right-Arc: \u3008\u03bb1|i, \u03bb2, j|B,A\u3009 \u21d2 \u3008\u03bb1, i|\u03bb2, j|B,A \u222a {i\u2192 j}\n\\{x\u2192 j \u2208 A} \\ {k \u2192 i \u2208 A | j \u2192\u2217 k \u2208 A}\u3009\nFigure 2: Transitions of the non-monotonic Covington non-projective dependency parser. The notation i\u2192\u2217 j \u2208 A means that there is a (possibly empty) directed path from i to j in A.\nright focus word), \u2022 j = max(x, y) \u2227 i < min(x, y) (i.e., we\nhave max(x, y) as the right focus word but the left focus word has already moved left past min(x, y), and we cannot move it back).\nNote that, since the head of a node can change during the parsing process and arcs that produce cycles in A can be built, the two last conditions present in the monotonic scenario for computing U(c, tG) are not needed when we use nonmonotonicity and, as a consequence, the set of individually reachable arcs I(c, tG) is larger: due to the greater flexibility provided by nonmonotonicity, we can reach arcs that would be unreachable for the monotonic version.\nSince arcs that are in this new U(c, tG) are unreachable even by the non-monotonic parser, |U(c, tG)| is trivially a lower bound of the loss `(c). It is worth noting that there always exists at least one transition sequence that builds every arc in I(c, tG) at some point (although not all of them necessarily appear in the final tree, due to non-monotonicity). This can be easily shown based on the fact that the non-monotonic parser does not forbid transitions at any configuration. Thanks to this, we can can generate one such sequence by just applying the original Covington (2001) criteria (choose an arc transition whenever the focus words are linked in I(c, tG), and otherwise Shift or No-Arc depending on whether the left focus word is the first word in the sentence or not), although this sequence is not necessarily optimal in terms of loss. In such a transition sequence, the gold arcs that are missed are (1) those in U(c, tG), and (2) those that are removed by the cycle-breaking in Left-Arc and Right-Arc transitions. In practice configurations where (2) is needed are uncommon, so this lower bound is a very close approximation of the real loss, as will be seen empirically below.\nThis reasoning also helps us calculate an up-\nper bound of the loss: in a transition sequence as described, if we only build the arcs in I(c, tG) and none else, the amount of arcs removed by breaking cycles (2) cannot be larger than the number of cycles in A \u222a I(c, tG). This means that |U(c, tG)|+nc(A\u222aI(c, tG)) is an upper bound of the loss `(c). Note that, contrary to the monotonic case, this expression does not always give us the exact loss, for several reasons: firstly, A\u222aI(c, tG) can have non-disjoint cycles (a node may have different heads in A and I since attachments are not permanent, contrary to the monotonic version) and thus removing a single arc may break more than one cycle; secondly, the removed arc can be a non-gold arc of A and therefore not incur loss; and thirdly, there may exist alternative transition sequences where a cycle in A\u222aI(c, tG) is broken early by non-monotonic configurations that change the head of a wrongly-attached node in A to a different (and also wrong) head,3 removing the cycle before the cycle-breaking mechanism needs to come into play without incurring in extra errors. Characterizing the situations where such an alternative exists is the main difficulty for an exact calculation of the loss.\nHowever, it is possible to obtain a closer upper bound to the real loss if we consider the following: for each cycle in A \u222a I(c, tG) that will be broken by the transition sequence described above, we can determine exactly which is the arc removed by cycle-breaking (if x \u2192 y is the arc that will close the cycle according to the Covington arc-building order, then the affected arc is the one of the form z \u2192 x). The cycle can only cause the loss of a gold arc if that arc z \u2192 x is gold, which can be trivially checked. Hence, if we call cycles where that holds problematic cycles, then the expression\n3Note that, in this scenario, the new head must also be wrong because otherwise the newly created arc would be an arc of I(c, tG) (and therefore, would not be breaking a cycle in A \u222a I(c, tG)). However, replacing a wrong attachment with another wrong attachment need not increase loss.\n6\n501\n502\n503\n504\n505\n506\n507\n508\n509\n510\n511\n512\n513\n514\n515\n516\n517\n518\n519\n520\n521\n522\n523\n524\n525\n526\n527\n528\n529\n530\n531\n532\n533\n534\n535\n536\n537\n538\n539\n540\n541\n542\n543\n544\n545\n546\n547\n548\n549\n550\n551\n552\n553\n554\n555\n556\n557\n558\n559\n560\n561\n562\n563\n564\n565\n566\n567\n568\n569\n570\n571\n572\n573\n574\n575\n576\n577\n578\n579\n580\n581\n582\n583\n584\n585\n586\n587\n588\n589\n590\n591\n592\n593\n594\n595\n596\n597\n598\n599\naverage value relative difference to loss Language lower loss pc upper upper lower pc upper upper Arabic 0.66925 0.67257 0.67312 0.68143 0.00182 0.00029 0.00587 Basque 0.58260 0.58318 0.58389 0.62543 0.00035 0.00038 0.02732 Catalan 0.58009 0.58793 0.58931 0.60644 0.00424 0.00069 0.00961 Chinese 0.56515 0.56711 0.57156 0.62921 0.00121 0.00302 0.03984 Czech 0.57521 0.58357 0.59401 0.62883 0.00476 0.00685 0.02662 English 0.55267 0.56383 0.56884 0.59494 0.00633 0.00294 0.01767 Greek 0.56123 0.57443 0.57983 0.61256 0.00731 0.00296 0.02256 Hungarian 0.46495 0.46672 0.46873 0.48797 0.00097 0.00114 0.01165 Italian 0.62033 0.62612 0.62767 0.64356 0.00307 0.00082 0.00883 Turkish 0.60143 0.60215 0.60660 0.63560 0.00060 0.00329 0.02139 Bulgarian 0.61415 0.62257 0.62433 0.64497 0.00456 0.00086 0.01233 Danish 0.67350 0.67904 0.68119 0.69436 0.00291 0.00108 0.00916 Dutch 0.69201 0.70600 0.71105 0.74008 0.00709 0.00251 0.01862 German 0.54581 0.54755 0.55080 0.58182 0.00104 0.00208 0.02033 Japanese 0.60515 0.60515 0.60515 0.60654 0.00000 0.00000 0.00115 Portuguese 0.58880 0.60063 0.60185 0.61780 0.00651 0.00067 0.00867 Slovene 0.56155 0.56860 0.57135 0.60373 0.00396 0.00153 0.01979 Spanish 0.58247 0.59119 0.59277 0.61273 0.00487 0.00089 0.01197 Swedish 0.57543 0.58636 0.58933 0.61104 0.00585 0.00153 0.01383 Average 0.59009 0.59656 0.59954 0.62416 0.00355 0.00176 0.01513\nTable 1: Average value of the different bounds and the loss, and of the relative differences from each bound to the loss, on CoNLL-XI (first block) and CoNLL-X (second block) datasets during 100,000 transitions. For each language, we show in boldface the average value and relative difference of the bound that is closer to the loss.\n|U(c, tG)|+npc(A\u222aI(c, tG)), where \u201cpc\u201d stands for problematic cycles, is a closer upper bound to the loss `(c) and the following holds:\n|U(c, tG)| \u2264 `(c) \u2264 |U(c, tG)|+npc(A\u222aI(c, tG))\n\u2264 |U(c, tG)|+ nc(A \u222a I(c, tG))\nAs mentioned before, unlike the monotonic approach, a node can have a different head in A than in I(c, tG) and, as a consequence, the resulting graph A \u222a I(c, tG) has maximum in-degree 2 rather than 1, and there can be overlapping cycles. Therefore, the computation of the non-monotonic terms nc(A \u222a I(c, tG)) and npc(A \u222a I(c, tG)) requires an algorithm such as the one by Johnson (1975) to find all elementary cycles in a directed graph. This runs in O((n + e)(c + 1)), where n is the number of vertices, e is the number of edges and c is the number of elementary cycles in the graph. This implies that the calculation of the two non-monotonic upper bounds is less efficient than the linear loss computation in the monotonic scenario. However, a non-monotonic algorithm that uses the lower bound as loss expression is the fastest option (even faster than the monotonic approach) as the oracle does not need to compute cycles at all, speeding up the training process.\nAlgorithm 2 shows the non-monotonic variant of Algorithm 1, where COUNTRELEVANTCYCLES is a function that counts the number of cycles or problematic cycles in the given graph,\nAlgorithm 2 Computation of the approximate loss of a non-monotonic configuration. 1: function LOSS(c = \u3008\u03bb1|i, \u03bb2, j|B,A\u3009, tG) 2: U \u2190 \u2205 . Variable U is for U(c, tG) 3: for each x\u2192 y \u2208 (tG \\A) do 4: left \u2190 min(x, y) 5: right \u2190 max(x, y) 6: if j > right \u2228 7: (j = right \u2227 i < left) then 8: U \u2190 u \u222a {x\u2192 y} 9: I \u2190 tG \\U . Variable I is for I(c, tG) 10: return |U |+ COUNTRELEVANTCYCLES(A \u222a I )\ndepending on the upper bound implemented, and will return 0 in case we use the lower bound.\n5 Evaluation of the loss bounds\nTo determine how close the lower bound |U(c, tG)| and the upper bounds |U(c, tG)| + npc(A\u222aI(c, tG)) and |U(c, tG)|+nc(A\u222aI(c, tG)) are to the actual loss in practical scenarios, we use exhaustive search to calculate the real loss of a given configuration, to then compare it with the bounds. This is feasible because the lower and upper bounds allow us to prune the search space: if an upper and a lower bound coincide for a configuration we already know the loss and need not keep searching, and if we can branch to two configurations such that the lower bound of one is greater or equal than an upper bound of the other, we can discard the former as it will never lead to smaller loss than the latter. Therefore, this ex-\n7\n601\n602\n603\n604\n605\n606\n607\n608\n609\n610\n611\n612\n613\n614\n615\n616\n617\n618\n619\n620\n621\n622\n623\n624\n625\n626\n627\n628\n629\n630\n631\n632\n633\n634\n635\n636\n637\n638\n639\n640\n641\n642\n643\n644\n645\n646\n647\n648\n649\n650\n651\n652\n653\n654\n655\n656\n657\n658\n659\n660\n661\n662\n663\n664\n665\n666\n667\n668\n669\n670\n671\n672\n673\n674\n675\n676\n677\n678\n679\n680\n681\n682\n683\n684\n685\n686\n687\n688\n689\n690\n691\n692\n693\n694\n695\n696\n697\n698\n699\nUnigrams L0w; L0p; L0wp; L0l; L0hw; L0hp; L0hl; L0l\u2032w; L0l\u2032p; L0l\u2032 l; L0r\u2032w; L0r\u2032p; L0r\u2032 l; L0h2w; L0h2p; L0h2l; L0lw; L0lp; L0ll; L0rw; L0rp; L0rl; L0wd; L0pd; L0wvr ; L0pvr ; L0wvl; L0pvl; L0wsl; L0psl; L0wsr ; L0psr ; L1w; L1p; L1wp; R0w; R0p; R0wp; R0hw; R0hp;R0hl; R0h2w; R0h2p; R0l\u2032w; R0l\u2032p; R0l\u2032 l; R0lw; R0lp; R0ll; R0wd; R0pd; R0wvl; R0pvl; R0wsl; R0psl; R1w; R1p; R1wp; R2w; R2p; R2wp; CLw; CLp; CLwp; CRw; CRp; CRwp; Pairs L0wp+R0wp; L0wp+R0w; L0w+R0wp; L0wp+R0p; L0p+R0wp; L0w+R0w; L0p+R0p; R0p+R1p; L0w+R0wd; L0p+R0pd; Triples R0p+R1p+R2p; L0p+R0p+R1p; L0hp+L0p+R0p; L0p+L0l\u2032p+R0p; L0p+L0r\u2032p+R0p; L0p+R0p+R0l\u2032p; L0p+L0l\u2032p+L0lp; L0p+L0r\u2032p+L0rp; L0p+L0hp+L0h2p; R0p+R0l\u2032p+R0lp;\nTable 2: Feature templates. L0 and R0 denote the left and right focus words; L1, L2, . . . are the words to the left of L0 and R1, R2, . . . those to the right of R0. Xih means the head of Xi, Xih2 the grandparent, Xil and Xil\u2032 the farthest and closest left dependents, and Xir and Xir\u2032 the farthest and closest right dependents, respectively. CL and CR are the first and last words between L0 andR0 whose head is not in the interval [L0, R0]. Finally, w stands for word form; p for PoS tag; l for dependency label; d is the distance between L0 and R0; vl, vr are the left/right valencies (number of left/right dependents); and sl, sr the left/right label sets (dependency labels of left/right dependents).\nhaustive search with pruning guarantees to find the exact loss.\nDue to the time complexity of this process, we undertake the analysis of only the first 100,000 transitions on each dataset of the nineteen languages available from CoNLL-X and CoNLL-XI shared tasks (Buchholz and Marsi, 2006; Nivre et al., 2007). In Table 1, we present the average values for the lower bound, both upper bounds and the loss, as well as the relative differences from each bound to the real loss. After those experiments, we conclude that the lower and the closer upper bounds are a tight approximation of the loss, with both bounds incurring relative errors below 0.8% in all datasets. If we compare them, the real loss is closer to the upper bound |U(c, tG)| + npc(A \u222a I(c, tG)) in the majority of datasets (12 out of 18 languages, excluding Japanese where both bounds were exactly equal to the real loss in the whole sample of configurations). This means that the term npc(A\u222aI(c, tG)) provides a close approximation of the gold arcs missed by the presence of cycles in A. Regarding the upper bound |U(c, tG)|+nc(A\u222aI(c, tG)),\nit presents a more variable relative error, ranging from 0.1% to 4.0%.\nThus, although we do not know an algorithm to obtain the exact loss which is fast enough to be practical, any of the three studied loss bounds can be used to obtain a feasible approximate dynamic oracle with full non-monotonicity.\n6 Experiments\nTo prove the usefulness of our approach, we implement the static, dynamic monotonic and nonmonotonic oracles for the non-projective Covington algorithm and compare their accuracies on nine datasets from the CoNLL-X shared task (Buchholz and Marsi, 2006) and all datasets from the CoNLL-XI shared task (Nivre et al., 2007). For the non-monotonic algorithm, we test the three different loss expressions defined in the previous section. We train an averaged perceptron model for 15 iterations and use the same feature templates for all languages, which are listed in detail in Table 2.\nThe accuracies obtained by the non-projective Covington parser with the three available oracles are presented in Table 3. For the non-monotonic dynamic oracle, three variants are shown, one for each loss expression implemented. As we can see, the novel non-monotonic oracle improves over the accuracy of the monotonic version on 14 out of 19 languages (0.32 in UAS on average) with the best loss calculation |U(c, tG)|+nc(A\u222aI(c, tG)), where 6 of these improvements are statistically significant at the .05 level (Yeh, 2000). The other two loss calculation methods also achieve good results, outperforming the monotonic algorithm on 12 out of 19 datasets tested.\nThe loss expression |U(c, tG)| + nc(A \u222a I(c, tG)) obtains greater accuracy on average than the other two loss expressions, including the more adjusted upper bound that is provably closer to the real loss. This could be explained by the fact that identifying problematic cycles is a difficult task to learn for the parser, and for this reason a more straightforward approach, which tries to avoid all kinds of cycles (regardless of whether they will cost gold arcs or not), can perform better. This also leads us to hypothesize that, even if it were feasible to build an oracle with the exact loss, it would not provide practical improvements over these approximate oracles; as it appears difficult for a statistical model to learn the situations where repla-\n8\n701\n702\n703\n704\n705\n706\n707\n708\n709\n710\n711\n712\n713\n714\n715\n716\n717\n718\n719\n720\n721\n722\n723\n724\n725\n726\n727\n728\n729\n730\n731\n732\n733\n734\n735\n736\n737\n738\n739\n740\n741\n742\n743\n744\n745\n746\n747\n748\n749\n750\n751\n752\n753\n754\n755\n756\n757\n758\n759\n760\n761\n762\n763\n764\n765\n766\n767\n768\n769\n770\n771\n772\n773\n774\n775\n776\n777\n778\n779\n780\n781\n782\n783\n784\n785\n786\n787\n788\n789\n790\n791\n792\n793\n794\n795\n796\n797\n798\n799\ndynamic dynamic non-monotonic static monotonic lower pc upper upper\nLanguage UAS LAS UAS LAS UAS LAS UAS LAS UAS LAS Arabic 80.67 66.51 82.76\u2217 68.48\u2217 83.29\u2217 69.14\u2217 83.18\u2217 69.05\u2217 83.40\u2020 69.29\u2020 Basque 76.55 66.05 77.49\u2020 67.31\u2020 74.61 65.31 74.69 65.18 74.27 64.78 Catalan 90.52 85.09 91.37\u2217 85.98\u2217 90.51 85.35 90.40 85.30 90.44 85.35 Chinese 84.93 80.80 85.82 82.15 86.55\u2217 82.53\u2217 86.29\u2217 82.27\u2217 86.60\u2217 82.51\u2217 Czech 78.49 61.77 80.21\u2217 63.52\u2217 81.32\u2020 64.89\u2020 81.33\u2020 64.81\u2020 81.49\u2020 65.18\u2020 English 85.35 84.29 87.47\u2217 86.55\u2217 88.44\u2020 87.37\u2020 88.23\u2020 87.22\u2020 88.50\u2020 87.55\u2020 Greek 79.47 69.35 80.76 70.43 80.90 70.46 80.84 70.34 81.02\u2217 70.49\u2217 Hungarian 77.65 68.32 78.84\u2217 70.16\u2217 78.67\u2217 69.83\u2217 78.47\u2217 69.66\u2217 78.65\u2217 69.74\u2217 Italian 84.06 79.79 84.30 80.17 84.38 80.30 84.64 80.52 84.47 80.32 Turkish 81.28 70.97 81.14 71.38 80.65 71.15 80.80 71.29 80.60 71.07 Bulgarian 89.13 85.30 90.45\u2217 86.86\u2217 91.36\u2020 87.88\u2020 91.33\u2020 87.89\u2020 91.73\u2020 88.26\u2020 Danish 86.00 81.49 86.91\u2217 82.75\u2217 86.83\u2217 82.63\u2217 86.89\u2217 82.74\u2217 86.94\u2217 82.68\u2217 Dutch 81.54 78.46 82.07 79.26 82.78\u2217 79.64\u2217 82.80\u2217 79.68\u2217 83.02\u2020 79.92\u2020 German 86.97 83.91 87.95\u2217 85.17\u2217 87.31 84.37 87.18 84.22 87.48 84.54 Japanese 93.63 92.20 93.67 92.33 94.02 92.68 94.02 92.68 93.97 92.66 Portuguese 86.55 82.61 87.45\u2217 83.62\u2217 87.17\u2217 83.47\u2217 87.12\u2217 83.45\u2217 87.40\u2217 83.71\u2217 Slovene 76.76 63.53 77.86 64.43 80.39\u2020 67.04\u2020 80.56\u2020 67.10\u2020 80.47\u2020 67.10\u2020 Spanish 79.20 76.00 80.12\u2217 77.24\u2217 81.36\u2217 78.30\u2217 81.12\u2217 77.99\u2217 81.33\u2217 78.16\u2217 Swedish 87.43 81.77 88.05\u2217 82.77\u2217 88.20\u2217 83.02\u2217 88.09\u2217 82.87\u2217 88.36\u2217 83.16\u2217 Average 83.48 76.75 84.46 77.92 84.67 78.18 84.63 78.12 84.74 78.24\nTable 3: Parsing accuracy (UAS and LAS, including punctuation) of the Covington non-projective parser with static, and dynamic monotonic and non-monotonic oracles on CoNLL-XI (first block) and CoNLLX (second block) datasets. For the dynamic non-monotonic oracle, we show the performance with the three loss expressions, where lower stands for the lower bound |U(c, tG)|, pc upper for the upper bound |U(c, tG)| + npc(A \u222a I(c, tG)), and upper for the upper bound |U(c, tG)| + nc(A \u222a I(c, tG)). For each language, we run five experiments with the same setup but different seeds and report the averaged accuracy. Best results for each language are shown in boldface. Statistically significant improvements (\u03b1 = .05) of both dynamic oracles are marked with \u2217 if they are only over the static oracle, and with \u2020 if they are over the opposite dynamic oracle too.\ncing a wrong arc with another indirectly helps due to breaking prospective cycles.\nIt is also worth mentioning that the nonmonotonic dynamic oracle with the best loss expression accomplishes an average improvement over the static version (1.26 UAS) greater than that obtained by the monotonic oracle (0.98 UAS), resulting in 13 statistically significant improvements achieved by the non-monotonic variant over the static oracle in comparison to the 12 obtained by the monotonic system. Finally, note that, despite this remarkable performance, the non-monotonic version (regardless of the loss expression implemented) has an inexplicable drop in accuracy in Basque in comparison to the other two oracles.\n7 Conclusion\nWe presented a novel, fully non-monotonic variant of the well-known non-projective Covington parser, trained with a dynamic oracle. Due to the unpredictability of a non-monotonic scenario, the real loss of each configuration cannot be computed. To overcome this, we proposed three different loss expressions that closely bound the loss and enable us to implement a practical non-monotonic\ndynamic oracle.\nOn average, our non-monotonic algorithm obtains better perfomance than the monotonic version, regardless of the loss calculation used. In particular, one of the loss expressions developed proved very promising by providing the best average accuracy, in spite of being the farthest approximation from the actual loss. On the other hand, the proposed lower bound makes the nonmonotonic system the fastest one among all dynamic oracles developed for the non-projective Covington algorithm.\nTo our knowledge, this is the first implementation of non-monotonicity for a nonprojective parsing algorithm, and the first approximate dynamic oracle that uses close, efficientlycomputable approximations of the loss, showing this to be a feasible alternative when it is not practical to compute the actual loss.\nWhile we used a perceptron classifier for our experiments, our oracle could also be used in neuralnetwork implementations of greedy transitionbased parsing (Chen and Manning, 2014; Dyer et al., 2015), providing an interesting avenue for future work.\n9\n801\n802\n803\n804\n805\n806\n807\n808\n809\n810\n811\n812\n813\n814\n815\n816\n817\n818\n819\n820\n821\n822\n823\n824\n825\n826\n827\n828\n829\n830\n831\n832\n833\n834\n835\n836\n837\n838\n839\n840\n841\n842\n843\n844\n845\n846\n847\n848\n849\n850\n851\n852\n853\n854\n855\n856\n857\n858\n859\n860\n861\n862\n863\n864\n865\n866\n867\n868\n869\n870\n871\n872\n873\n874\n875\n876\n877\n878\n879\n880\n881\n882\n883\n884\n885\n886\n887\n888\n889\n890\n891\n892\n893\n894\n895\n896\n897\n898\n899\n", "cats": {"READ": 1.0, "DONTREAD": 0.0}, "reviews": [{"IMPACT": "3", "SUBSTANCE": "4", "APPROPRIATENESS": "5", "MEANINGFUL_COMPARISON": "4", "PRESENTATION_FORMAT": "Oral Presentation", "comments": "- Strengths:\n\nThe paper makes several novel contributions to (transition-based) dependency\nparsing by extending the notion of non-monotonic transition systems and dynamic\noracles to unrestricted non-projective dependency parsing. The theoretical and\nalgorithmic analysis is clear and insightful, and the paper is admirably clear.\n\n- Weaknesses:\n\nGiven that the main motivation for using Covington's algorithm is to be able to\nrecover non-projective arcs, an empirical error analysis focusing on\nnon-projective structures would have further strengthened the paper. And even\nthough the main contributions of the paper are on the theoretical side, it\nwould have been relevant to include a comparison to the state of the art on the\nCoNLL data sets and not only to the monotonic baseline version of the same\nparser.\n\n- General Discussion:\n\nThe paper extends the transition-based formulation of Covington's dependency\nparsing algorithm (for unrestricted non-projective structures) by allowing\nnon-monotonicity in the sense that later transitions can change structure built\nby earlier transitions. In addition, it shows how approximate dynamic oracles\ncan be formulated for the new system. Finally, it shows experimentally that the\noracles provide a tight approximation and that the non-monotonic system leads\nto improved parsing accuracy over its monotonic counterpart for the majority of\nthe languages included in the study.\n\nThe theoretical contributions are in my view significant enough to merit\npublication, but I also think the paper could be strengthened on the empirical\nside. In particular, it would be relevant to investigate, in an error analysis,\nwhether the non-monotonic system improves accuracy specifically on\nnon-projective structures. Such an analysis can be motivated on two grounds:\n(i) the ability to recover non-projective structures is the main motivation for\nusing Covington's algorithm in the first place; (ii) non-projective structures\noften involved long-distance dependencies that are hard to predict for a greedy\ntransition-based parser, so it is plausible that the new system would improve\nthe situation. \n\nAnother point worth discussion is how the empirical results relate to the state\nof the art in light of recent improvements thanks to word embeddings and neural\nnetwork techniques. For example, the non-monotonicity is claimed to mitigate\nthe error propagation typical of classical greedy transition-based parsers. But\nanother way of mitigating this problem is to use recurrent neural networks as\npreprocessors to the parser in order to capture more of the global sentence\ncontext in word representations. Are these two techniques competing or\ncomplementary? A full investigation of these issues is clearly outside the\nscope of the paper, but some discussion would be highly relevant.\n\nSpecific questions:\n\nWhy were only 9 out of the 13 data sets from the CoNLL-X shared task used? I am\nsure there is a legitimate reason and stating it explicitly may prevent readers\nfrom becoming suspicious. \n\nDo you have any hypothesis about why accuracy decreases for Basque with the\nnon-monotonic system? Similar (but weaker) trends can be seen also for Turkish,\nCatalan, Hungarian and (perhaps) German.\n\nHow do your results compare to the state of the art on these data sets? This is\nrelevant for contextualising your results and allowing readers to estimate the\nsignificance of your improvements.\n\nAuthor response:\n\nI am satisfied with the author's response and see no reason to change my\nprevious review.", "SOUNDNESS_CORRECTNESS": "4", "ORIGINALITY": "3", "is_meta_review": null, "RECOMMENDATION": "4", "CLARITY": "5", "REVIEWER_CONFIDENCE": "5"}]}
{"text": "Combining distributional and referential information for naming objects through cross-modal mapping and direct word prediction\n1 000\n011\n012\n013\n014\n015\n016\n017\n018\n019\n020\n021\n022\n023\n024\n025\n026\n027\n028\n029\n030\n031\n032\n033\n034\n035\n036\n037\n038\n039\n040\n041\n042\n043\n044\n045\n046\n047\n048\n049\n074\n075\n076\n077\n078\n079\n080\n081\n082\n083\n084\n085\n086\n087\n088\n089\n090\n091\n092\n093\n094\n095\n096\n097\n098\n099\n1 Introduction\nExpressions referring to objects in visual scenes typically include a word naming the type of the object: E.g., \u201chouse\u201d in Figure 1 (a), or, as a very general type, \u201cthingy\u201d in Figure 1 (d). Determining such a name is is a crucial step for referring expression generation (REG) systems, as many other decisions, concerning e.g. the selection of attributes, follow from it (Dale and Reiter,\n1995; Krahmer and Van Deemter, 2012). For a long time, however, research on REG mostly assumed the availability of symbolic representations of referent and scene, and sidestepped questions about how speakers actually choose these names, due to the lack of models capable of capturing what a word like house refers to in the real world.\nRecent advances in image processing promise to fill this gap, with state-of-the-art computer vision systems being able to classify images into thousands of different categories (eg. Szegedy et al. (2015)). However, classification is not naming (Ordonez et al., 2016). Classification schemes are typically designed to be \u201cflat\u201d, with labels being on the same ontological level and, ideally, having disjunct extensions. In contrast, humans seem to be more flexible as to the chosen level of generality. Depending on the prototypicality of the object to name, and possibly other visual properties, a general name might be more or less appropriate. For instance, a robin can be named \u201cbird\u201d, but a penguin is better referred to as \u201cpenguin\u201d (Rosch, 1978); along the same lines, the rather unusual building in Figure 1 that is not easy to otherwise\n2\n101\n102\n103\n104\n105\n106\n107\n108\n109\n110\n111\n112\n113\n114\n115\n116\n117\n118\n119\n120\n121\n122\n123\n124\n125\n126\n127\n128\n129\n130\n131\n132\n133\n134\n135\n136\n137\n138\n139\n140\n141\n142\n143\n144\n145\n146\n147\n148\n149\n150\n151\n152\n153\n154\n155\n156\n157\n158\n159\n160\n161\n162\n163\n164\n165\n166\n167\n168\n169\n170\n171\n172\n173\n174\n175\n176\n177\n178\n179\n180\n181\n182\n183\n184\n185\n186\n187\n188\n189\n190\n191\n192\n193\n194\n195\n196\n197\n198\n199\ncategorise was named \u201cstructure\u201d. Other work at the intersection of image and language processing has investigated models that learn to directly associate visual objects with a representation of word meaning, for example through cross-modal transfer into distributional vector spaces. Under the assumption that such semantic spaces represent, in some form at least, taxonomic knowledge, this makes labels on different levels of specificity available for a given object. Moreover, if the mapping is sufficiently general, it should be able to map objects to an appropriate label, even if during training of the mapping this label has not been seen (zero-shot learning). While indeed performing with some promise on this task (Lazaridou et al., 2014), this approach does not generally outperform standard object classification with known categories (Frome et al., 2013; Norouzi et al., 2013).\nThis paper pursues the hypothesis that an accurate model of referential word meaning does not need to fully integrate visual and lexical knowledge (e.g. as expressed in a distributional vector space), but at the same time, has to go beyond treating words as independent labels. We extend upon work on learning models of referential word use from corpora of images paired with referring expressions (Schlangen et al., 2016; Anonymous, in press) that treats words as individual predictors capturing referential appropriateness. We explore different ways of linking these predictors to distributional knowledge, during application and during training. We find that these improve over direct cross-modal mapping and direct visual classification in a standard and a zero-shot setup of an object naming task, as they allow for a more flexible combination of lexical and visual information when modeling referential meaning.\n2 Related Work\nGrounding and Reference An early example for work in REG that goes beyond Dale and Reiter (1995)\u2019s dominant symbolic paradigm is Deb Roy\u2019s work from the early 2000s (Roy et al., 2002; Roy, 2002, 2005). More recently, research on REG, which has traditionally been done on small toy data sets, is being scaled up to realworld images (Kazemzadeh et al., 2014; Gkatzia et al., 2015; Zarrie\u00df and Schlangen, 2016; Mao et al., 2015). In this paper, we focus on a particular problem posed by REG on real-world images,\nnamely generating the appropriate head noun for a given object. Similarly, Ordonez et al. (2016) have studied the problem of deriving appropriate object names, or so-called entry-level categories, from the output of an object recognizer. Their approach focusses links abstract object categories in ImageNet to actual words via various translation procedures. We are interested in learning referential appropriateness and extensional word meanings directly from actual human referring expressions (REs) paired with objects in images, using an existing object recognizer for feature extraction.\nMulti-modal and cross-modal distributional semantics Distributional semantic models are a well-known method for capturing lexical word meaning in a variety of tasks (Turney and Pantel, 2010; Mikolov et al., 2013; Erk, 2016). Recent work on multi-modal distributional vector spaces (Feng and Lapata, 2010; Silberer and Lapata, 2014; Kiela and Bottou, 2014; Lazaridou et al., 2015b; Kottur et al., 2016) has aimed at capturing semantic similarity even more accurately by integrating distributional and perceptual features associated with words (mostly taken from images) into a single representation. More related to our work are cross-modal mapping models (Socher et al., 2013; Frome et al., 2013; Norouzi et al., 2013; Lazaridou et al., 2014), that learn to transfer a representation of an object or image in the visual space to a vector in a distributional space. When tested on standard object recognition tasks, transfer, however, comes at a price. Frome et al. (2013) and Norouzi et al. (2013) both find that it slightly degrades performance as compared to a plain object classification using standard accuracy metrics (called flat \u201chit @k metric\u201d in their paper). Interestingly though, Frome et al. (2013) report better performance using \u201chierarchical precision\u201d, which essentially means that transfer predicts words that are ontologically closer to the gold label and makes \u201csemantically more reasonable errors\u201d. To the best of our knowledge, this pattern has not been systematically investigated any further. Another known problem with cross-modal transfer is that it seems to generalize less well than expected, i.e. tends to reproduce word vectors observed during training (Lazaridou et al., 2015a). In this work, we present a model that exploits distributional knowledge for learning referential word meaning as well, but explore and compare different ways of combining visual and lexical aspects of referential word meaning.\n3\n201\n202\n203\n204\n205\n206\n207\n208\n209\n210\n211\n212\n213\n214\n215\n216\n217\n218\n219\n220\n221\n222\n223\n224\n225\n226\n227\n228\n229\n230\n231\n232\n233\n234\n235\n236\n237\n238\n239\n240\n241\n242\n243\n244\n245\n246\n247\n248\n249\n250\n251\n252\n253\n254\n255\n256\n257\n258\n259\n260\n261\n262\n263\n264\n265\n266\n267\n268\n269\n270\n271\n272\n273\n274\n275\n276\n277\n278\n279\n280\n281\n282\n283\n284\n285\n286\n287\n288\n289\n290\n291\n292\n293\n294\n295\n296\n297\n298\n299\n3 Task and Data\nWe define object naming as follows: Given an object x in an image, the task is to predict a word w that could be used as the head noun of a realistic referring expression. (Cf. discussion above: \u201cbird\u201d when naming a robin, but \u201cpenguin\u201d when naming a penguin.) To get at this, we develop our approach using a corpus of referring expressions produced by human users under natural, interactive conditions (Kazemzadeh et al., 2014), and train and test on the corresponding head nouns in these REs. This is similar to picture naming setups used in psycholinguistic research (cf. Levelt et al. (1991)) and based on the simplifying assumption that the name used for referring to an object can be determined successfully without looking at other objects in the image.\nWe now summarise the details of our setup:\nCorpus We train and test on the REFERIT corpus (Kazemzadeh et al., 2014), which is based on the SAIAPR image collection (Grubinger et al., 2006) (99.5k image regions;120K REs). We follow (Schlangen et al., 2016) and select words with a minimum frequency of 40 in these two data sets, which gives us a vocabulary of 793 words.\nNames For most of our experiments, we only use a subset of this vocabulary, namely the set of object names. As the REs contain nouns that cannot be considered to be names (background, bottom, etc.), we extract from the semantically annotated portion of the REFERIT corpus a list of names which correspond to \u2018entry-level\u2019 nouns in terms of (Kazemzadeh et al., 2014). This gives us a list of 159 names. Thus, our experiments are on a smaller scale as compared to (Ordonez et al., 2016). Nevertheless, the data is challenging, as the corpus contains references to objects that fall outside of the object labeling scheme that available object recognition systems are typically optimized for, cf. Hu et al. (2015)\u2019s discussion on \u201cstuff\u201d entities such \u201csky\u201d or \u201cgrass\u201d in the REFERIT data. For testing, we remove relational REs (containing a relational preposition such as \u2018left of X\u2019), because here we cannot be sure that the head noun of the target is fully informative; we also remove REs with more than one head noun from our list (i.e. these are mostly relational expressions as well such as \u2018girl laughing at boy\u2019). We pair each image region from the test set with its corresponding names from the remaining REs.\nImage and Word Embeddings Following Schlangen et al. (2016), we derive representations of our visual inputs with a convolutional neural network, \u2018GoogleNet\u2019 (Szegedy et al., 2015), which was trained on the ImageNet corpus (Deng et al., 2009), and extract the final fully-connected layer before the classification layer, to give us a 1024 dimensional representation of the region. We add 7 features that encode information about the region relative to the image, thus representing each object as a vector of 1031 features. As distributional word vectors, we use the word2vec representations provided by Baroni et al. (2014) (trained with CBOW, 5-word context window, 10 negative samples, 400 dimensions).\n4 Three Models of Interfacing Visual and\nDistributional Information\n4.1 Direct Cross-Modal Mapping\nFollowing e.g. Lazaridou et al. (2014), referential meaning can be represented as a translation function that projects visual representations of objects to linguistic representations of words in a distributional vector space. Thus, in contrast to standard object recognition systems or the other models we will use here, cross-modal mapping does not treat words as individual labels or classifiers, but learns to directly predict continuous representations of words in a vector space, such as the space defined by the word2vec embeddings that we use in this work. This model will be called TRANSFER below.\nDuring training, we pair each object with the distributional embedding of its name, and use standard Ridge regression for learning the transformation. Lazaridou et al. (2014) and Lazaridou et al. (2015a) test a range of technical tweaks and different algorithms for cross-modal mapping. For ease of comparison with other models, we stick with simple Ridge Regression in this work.\nFor decoding, we map an object into the distributional space, and retrieve the nearest neighbors of the predicted vector using cosine similarity. In theory, the model should generalize easily to words that it has not observed in a pair with an object during training as it can map an object anywhere in the distributional space.\n4.2 Lexical Mapping Through Individual Word Classifiers\nAnother approach is to keep visual and distributional information separate, by training a separate visual classifier for each word w in the vocabu-\n4\n301\n302\n303\n304\n305\n306\n307\n308\n309\n310\n311\n312\n313\n314\n315\n316\n317\n318\n319\n320\n321\n322\n323\n324\n325\n326\n327\n328\n329\n330\n331\n332\n333\n334\n335\n336\n337\n338\n339\n340\n341\n342\n343\n344\n345\n346\n347\n348\n349\n350\n351\n352\n353\n354\n355\n356\n357\n358\n359\n360\n361\n362\n363\n364\n365\n366\n367\n368\n369\n370\n371\n372\n373\n374\n375\n376\n377\n378\n379\n380\n381\n382\n383\n384\n385\n386\n387\n388\n389\n390\n391\n392\n393\n394\n395\n396\n397\n398\n399\nlary. Predictions can then be mapped into distributional space during application time via the vectors of the predicted words. Here, we use Schlangen et al. (2016)\u2019s WAC model, building the training set for each word w as follows: all visual objects in a corpus that have been referred to as w are used as positive instances, the remaining objects as negative instances. Thus, the classifiers learn to predict referential appropriateness for individual words based on the visual features of the objects they refer to, in isolation of other words.\nDuring decoding, we apply all word classifiers from the model\u2019s vocabulary to the given object, and take the argmax over the individual word probabilities. The model can be used to predict names directly, without links into a distributional space.\nIn order to extend the model\u2019s vocabulary for zero-shot learning, we follow Norouzi et al. (2013) and associate the top n words with their corresponding distributional vector and compute the convex combination of these vectors. Then, in parallel to cross-modal mapping, we retrieve the nearest neighbors of the combined embedding from the distributional space. Thus, with this model, we use two different modes of decoding: one that projects into distributional space, one that only applies the available word classifiers.\n4.3 Word Prediction via Cross-Modal Similarity Mapping\nFinally, we implement an approach that combines ideas from cross-modal mapping with the WAC model: we train individual predictors for each word in the vocabulary, but, during training, we exploit lexical similarity relations encoded in a distributional space. Instead of treating a word as a binary classifier, we annotate its training instances with a fine-grained similarity signal according to their object names. When building the training set for such a word predictor w, instead of simply dividing objects into w and \u00acw instances, we label each object with a real-valued similarity obtained from cosine similarity between w and v in a distributional vector space, where v is the word that was used to refer to the object. Thus, we task the model with jointly learning similarities and referential appropriateness, by training it with Ridge regression on a continuous output space. Object instances where v = w (i.e., the positive instances in the binary setup) have maximal similarity; the\nremaining instances have a lower value which is more or less close to maximal similarity. This is the SIM-WAP model, recently proposed in (Anonymous).\nImportantly, and going beyond (Anonymous), this model allows for an innovative treatment of words that only exist in a distributional space (without being paired with visual referents in the image corpus): as the predictors are trained on a continuous output space, no genuine positive instances of a word\u2019s referent are needed. When training a predictor for such a word w, we use all available objects from our corpus and annotate them with the expected lexical similarity between w and the actual object names v, which for all objects will be below the maximal value that marks genuine positive instances. During decoding, this model does not need to project its predictions into a distributional space, but it simply applies all available predictors to the object, and takes the argmax over the predicted referential appropriateness scores.\n5 Experiment 1: Naming Objects\nThis Section reports on experiments in a standard setup of the object naming task where all object names are paired with visual instances of their referents during training. In a comparable task, i.e. object recognition with known object categories, cross-modal projection or transfer approaches have been reported to perform worse than standard object classification methods (Frome et al., 2013; Norouzi et al., 2013). This seems to suggest that lexical or at least distributional knowledge is detrimental when learning what a word refers to in the real world and that referential meaning should potentially be learned from visual object representation only.\n5.1 Model comparison\nSetup We use the train/test split of REFERIT data as in (Schlangen et al., 2016). We consider image regions with non-relational referring expressions that contain at least one of the 159 head nouns from the list of entry-level nouns (see section 3). This amounts to 6208 image regions for testing and 73K instances for training.\nResults Table 1 shows accuracies in the object naming task for the TRANSFER, WAC and SIMWAP models according to their accuracies in the top n, including two variants of WAC where its top\n5\n401\n402\n403\n404\n405\n406\n407\n408\n409\n410\n411\n412\n413\n414\n415\n416\n417\n418\n419\n420\n421\n422\n423\n424\n425\n426\n427\n428\n429\n430\n431\n432\n433\n434\n435\n436\n437\n438\n439\n440\n441\n442\n443\n444\n445\n446\n447\n448\n449\n450\n451\n452\n453\n454\n455\n456\n457\n458\n459\n460\n461\n462\n463\n464\n465\n466\n467\n468\n469\n470\n471\n472\n473\n474\n475\n476\n477\n478\n479\n480\n481\n482\n483\n484\n485\n486\n487\n488\n489\n490\n491\n492\n493\n494\n495\n496\n497\n498\n499\n5 and top 10 predictions are project into the distributional space. Overall, the differences in accuracy between the models are small, but the various models that link their predictions to word representations in the distributional space all perform slightly worse than the plain WAC model, i.e. individual word classifiers trained on visual features only. This suggests that referential meanings for a word are learned less accurately when mapping from visual to distributional space, which replicates results reported in the literature on standard object recognition benchmarks.\nhit @k(%) @1 @2 @5\ntransfer 48.34 60.49 74.89 wac 49.34 61.86 75.35 wac, project top5 48.73 61.10 74.07 wac, project top10 48.68 61.23 74.31 sim-wap 48.13 60.60 75.40\nTable 1: Accuracies in object naming\n5.2 Model combination\nIn order to get more insight into why the TRANSFER and SIM-WAP models produce slightly worse results than individual visual word classifiers, we now test to what extent the different models are complementary and combine them by aggregating over their naming predictions. If the models are complementary, their combination should lead to more confident and accurate naming decisions.\nSetup We combine TRANSFER, SIM-WAP and WAC by aggregating the scores they predict for different object names for a given object. During testing, we apply all models to an image region and consider words ranked among the top 10. We first normalize the referential appropriateness scores in each top-10 list and then compute their sum. This aggregation scheme will give more weight to words that appear in the top 10 list of different models, and less weight to words that only get top-ranked by a single model. We test on the same data as in Section 5.1.\nhit @k(%) 1 5 10\nsim-wap + transfer 49.10 61.78 75.81 sim-wap + wac 51.10 63.45 77.92 transfer + wac 51.13 63.76 77.84 wac + transfer + sim-wap 52.19 64.71 78.40\nTable 2: Object naming acc., combined models\nResults Table 2 shows that all model combinations improve over the results of their isolated models in Table 1, suggesting that WAC, TRANSFER and SIM-WAP indeed do capture complementary aspects of referential word meaning. On their own, the distributionally informed models are less tuned to specific word occurrences than the visual word classifiers in the WAC model, but they can add useful information which leads to a clear overall improvement. We take this as a promising finding, supporting our initial hypothesis that knowledge on lexical distributional meaning should and can be exploited when learning how to use words for reference.\nAv. cosine distance among top k gold - top k\n5 10 5 10\ntransfer 0.68 0.73 0.72 0.75 wac 0.82 0.80 0.82 0.84 sim-wap 0.68 0.74 0.72 0.75\nTable 3: Cosine distances between word2vec embeddings of nouns generated in the top k\n5.3 Analysis\nFigure 2 illustrates objects from our test set where the combination of TRANSFER, SIM-WAP and WAC predicts an accurate name, whereas the models in isolation do not. These examples give some interesting insight into why the models capture different aspects of referential word use and meaning.\nWord Similarities Many of the examples in Figure 2 suggest that the object names ranked among the top 3 by the TRANSFER and SIMWAP model are semantically similar to each other, whereas WAC generates object names on top that describe very different underlying object categories, such as seal / rock in Figure 2(a), animal / lamp in Figure 2(g) or chair / shirt in Figure 2(c). To quantify this general impression, Table 3 shows cosine distances among words in the top n generated by our models, using their word2vec embeddings. The average cosine distance between words in our vocabulary is 0.83. The transfer and sim-wap model rank words on top that are clearly more similar to each other than word pairs on average, whereas words ranked top by the wac model are more dissimilar. This parallels findings by Frome et al. (2013), discussed in Section 2. Additional evaluation metrics, such as success rates\n6\n501\n502\n503\n504\n505\n506\n507\n508\n509\n510\n511\n512\n513\n514\n515\n516\n517\n518\n519\n520\n521\n522\n523\n524\n525\n526\n527\n528\n529\n530\n531\n532\n533\n534\n535\n536\n537\n538\n539\n540\n541\n542\n543\n544\n545\n546\n547\n548\n549\n550\n551\n552\n553\n554\n555\n556\n557\n558\n559\n560\n561\n562\n563\n564\n565\n566\n567\n568\n569\n570\n571\n572\n573\n574\n575\n576\n577\n578\n579\n580\n581\n582\n583\n584\n585\n586\n587\n588\n589\n590\n591\n592\n593\n594\n595\n596\n597\n598\n599\nin a human evaluation (cf. Zarrie\u00df and Schlangen (2016)), would be an interesting direction for more detailed investigation here.\nWord Use But even though the WAC classifiers lack knowledge on lexical similarities, they seem to able to detect relatively specific instances of word use such as hut in Figure 2(b), shirt in 2(c) or lamp in 2(h). Here, the combination with TRANSFER and SIM-WAP is helpful to give more weight to the object name that is taxonomically correct (sometimes pushing up words below the top-3 and hence not shown in Figure 2). In Figure 1(e), SIMWAP and TRANSFER give more weight to typical names for persons, whereas WAC top-ranks more unusual names, reflecting that the person is difficult to identify visually. Another observation is that the mapping models have difficulties dealing with object names in singular and plural. As these words have very similar representations in the distributional space, they are often predicted as likely variants among the top 10 by SIM-WAP and TRANSFER, whereas the WAC model seems to predict inappropriate plural words less often among the top 3. Such specific phenomena at the intersection of visual and semantic similarity have found very little attention in the literature. We will investigate them further in our Experiments on zeroshot naming in the following Section.\n6 Zero-Shot Naming\nZero-shot learning is an attractive prospect for REG from images, as it promises to overcome dependence on pairings of visual instances and natural names being available for all names, if visual/referential data can be generalised from other types of information. Previous work has looked at the feasibility of zero-shot learning as a function of semantic similarity or ontological closeness between unknown and known categories, and confirmed the intuition that the task is harder the less close unknown categories are to known ones (Frome et al., 2013; Norouzi et al., 2013).\nOur experiments on object naming in Section 5 suggest that lexical similarities encoded in a distributional space might not always fully carry over to referential meaning. This could constitute an additional challenge for zero-shot learning, as distributional similarities might be misleading when the model has to fully rely on them for learning referential word meanings. Therefore, the following experiments investigate the performance of\nour models in zero-shot naming as a function of the lexical relation between unknown and known object names, i.e. namely hypernyms and singular/plurals. Both relations are typically captured by distributional models of word meaning in terms of closeness in the vector space, but their visual and referential relation is clearly different.\n6.1 Vocabulary Splits and Testsets\nRandom As in previous work on zero-shot learning, we consider zero-shot naming for words of varying degrees of similarity in our vocabulary. We randomly split our 159 names from Experiment 1 into 10 subsets. We train the models on 90% of the nouns (and all their visual instances in the image corpus) and test on the set of image regions that are named with words which the model did not observe during training. Results reported in Table 4 on the random test set correspond to averaged scores from cross-validation over the 10 splits.\nHypernyms We manually split the model\u2019s vocabulary into set of hypernyms (see Appendix A) and the remaining nouns. We train the models on those 84K image regions that where not named with a hypernym, and test on 8895 image regions that were named with a hypernym in the corpus. We checked that for each of these hypernyms, the vocabulary contains at least one or two names that can be considered as hyponyms, i.e. the model sees objects during training that are instances of vehicle for example, but never encounters actual uses of that name. This test set is particularly interesting from an REG perspective, as objects named with very general terms by human speakers are often difficult to describe with more common, but more specific terms, as is illustrated by the uses of structure and thingy in Figure 1.\nSingulars/Plurals We pick 68 words from our vocabulary that can be grouped into 34 singularplural noun pairs (see Appendix A). From each pair, we randomly include the singular or plural noun in the set of zero-shot nouns. Thus, we make sure that the model encounters singular and plural names during training, but it never encounters both variants of a name. This results in a more even training/test split, i.e. we train on 23K image regions and evaluate on 13825 instances.\n7\n601\n602\n603\n604\n605\n606\n607\n608\n609\n610\n611\n612\n613\n614\n615\n616\n617\n618\n619\n620\n621\n622\n623\n624\n625\n626\n627\n628\n629\n630\n631\n632\n633\n634\n635\n636\n637\n638\n639\n640\n641\n642\n643\n644\n645\n646\n647\n648\n649\n650\n651\n652\n653\n654\n655\n656\n657\n658\n659\n660\n661\n662\n663\n664\n665\n666\n667\n668\n669\n670\n671\n672\n673\n674\n675\n676\n677\n678\n(a) wac: seal, rock, water sim-wap: side, rock,rocks transfer: rocks, rock, water combination: rock\n(c) wac: chair, shirt, guy sim-wap: woman, man, girl transfer: door, woman, window combination: shirt\n(e) wac: chick, person, guy sim-wap: man, person, woman transfer: man, guy, girl combination: person\n(g) wac: animal, lamp, table sim-wap: man, girl, person transfer: man, clouds, cloud combination: person\n(b) wac: cactus, hut, mountain sim-wap: side, rock, mountain transfer: mountain, rocks, rock combination: hut\n(d) wac: roof, house, building sim-wap: building, house, trees transfer: building, house, trees combination: house\n(f) wac: bush, bushes, tree sim-wap: trees, tree, grass transfer: trees, tree, bushes combination: bushes\n(h) wac: post, light, lamp sim-wap: tree, sky, pole transfer: tree, sky, trees combination: lamp\nFigure 2: Examples from object naming experiment where model combination is accurate\nZero-shot Model full vocab disjoint vocab names @1 @2 @5 @10 @1 @2\nRandom\ntransfer 0.05 2.38 16.57 35.71 41.49 62.34 wac, project top10 0.00 4.42 21.16 39.17 38.03 58.07 wac, project top5 0.00 4.39 21.63 40.01 37.46 57.36 sim-wap 3.71 13.13 36.49 54.44 42.28 64.26\nHypernyms\ntransfer 0.07 1.25 7.75 29.93 59.88 73.88 wac, project top10 0.00 3.01 15.55 36.99 50.51 66.33 wac, project top5 0.00 2.78 16.75 38.13 47.73 64.38 sim-wap 3.16 10.33 31.14 49.62 57.55 70.15\nSingulars/Plurals transfer 0.01 22.84 44.30 72.85 34.56 51.79 wac, project top10 0.00 22.21 43.43 68.95 31.46 48.76 wac, project top5 0.00 22.18 43.93 69.33 31.46 48.88 sim-wap 15.39 34.73 56.62 77.32 37.24 54.02\n8\n701\n702\n703\n704\n705\n706\n707\n708\n709\n710\n711\n712\n713\n714\n715\n716\n717\n718\n719\n720\n721\n722\n723\n724\n725\n726\n727\n728\n729\n730\n731\n732\n733\n734\n735\n736\n737\n738\n739\n740\n741\n742\n743\n744\n745\n746\n747\n748\n749\n750\n751\n752\n753\n754\n755\n756\n757\n758\n759\n760\n761\n762\n763\n764\n765\n766\n767\n768\n769\n770\n771\n772\n773\n774\n775\n776\n777\n778\n779\n780\n781\n782\n783\n784\n785\n786\n787\n788\n789\n790\n791\n792\n793\n794\n795\n796\n797\n798\n799\n6.2 Evaluation\nSome previous work on zero-shot image labeling assumes additional components that first identify whether an image should be labelled by a known or unknown word (Frome et al., 2013). We follow Lazaridou et al. (2014) and let the model decide whether to refer to an object by a known or unknown name. Related to that, distinct evaluation procedures have been used in the literature on zero-shot learning:\nTesting on full vocabulary A realistic way to test zero-shot learning performance is to consider all words from a given vocabulary during testing, though the testset only contains instances of objects that have been named with a \u2018zero-shot word\u2019 (for which no visual instances were seen during training). Accuracies in this setup reflect how well the model is able to generalize, i.e. how often it decides to deviate from the words it was trained on, and (implicitly) predicts that the given object requires a \u201cnew\u201d name. In case of the (i) hypernym and (ii) singular/plural test set, this accuracy also reflects to what extent the model is able to detect cases where (i) a more general or vague term is needed, where (ii) an unknown singular/plural counterpart of a known object type occurs.\nTesting on disjoint vocabulary Alternatively, the model\u2019s vocabulary can be restricted during testing to zero-shot words only, such that names encountered during training and testing are disjoint, see e.g. (Lampert et al., 2009, 2013). This setup factors out the generalization problem, and assesses to what extent a model is able to capture the referential meaning of a word that does not have instances in the training data.\n6.3 Results\nAs compared to Experiment 1 where models achieved similar performance, differences are more pronounced in the zero-shot setup, as shown in Table 4. In particular, we find that the SIMWAP model which induces individual predictors for words that have not been observed in the training data is clearly more successful than TRANSFER or WAC that project predictions into the distributional space. When tested on the full vocabulary, we find that TRANSFER and WAC very rarely generate names whose referents were excluded from training, which is in line with observations made by Lazaridou et al. (2015a). The SIM-WAP\npredictors generalize much better, in particular on the singular/plural testset.\nAn interesting exception is the good performance of the TRANSFER model on the hypernym test set, when evaluated with a disjoint vocabulary. This corroborates evidence from Experiment 1, namely that the transfer model captures taxonomic aspects of object names better than the other models. Projection via individual word classifiers, on the other hand, seems to generalize better than TRANSFER, at least when looking at accuracies @2 ... @10. Thus, combining several vectors predicted by a model of referential word meaning can provide additional information, as compared to mapping an object to a single vector in distributional space. More work is needed to establish how these approaches can be integrated more effectively.\n7 Discussion and Conclusion\nIn this paper, we have investigated models of referential word meaning, using different ways of combining visual information about a word\u2019s referent and distributional knowledge about its lexical similarities. Previous cross-modal mapping models essentially force semantically similar objects to be mapped into the same area in the semantic space regardless of their actual visual similarity. We found that cross-modal mapping produces semantically appropriate and mutually highly similar object names in its top-n list, but does not preserve differences in referential word use (e.g. appropriatness of person vs. woman) especially within the same semantic field. We have shown that it is beneficial for performance in standard and zeroshot object naming to treat words as individual predictors that capture referential appropriateness and are only indirectly linked to a distributional space, either through lexical mapping during application or through cross-modal similarity mapping during training. As we have tested these approaches on a rather small vocabulary, which may limit generality of conclusions, future work will be devoted to scaling up these findings to larger test sets, as e.g. recently collected through conversational agents (Das et al., 2016) that circumvent the need for human-human interaction data. Also from an REG perspective, various extensions of this approach are possible, such as the inclusion of contextual information during object naming and its combination with attribute selection.\n9\n801\n802\n803\n804\n805\n806\n807\n808\n809\n810\n811\n812\n813\n814\n815\n816\n817\n818\n819\n820\n821\n822\n823\n824\n825\n826\n827\n828\n829\n830\n831\n832\n833\n834\n835\n836\n837\n838\n839\n840\n841\n842\n843\n844\n845\n846\n847\n848\n849\n850\n851\n852\n853\n854\n855\n856\n857\n858\n859\n860\n861\n862\n863\n864\n865\n866\n867\n868\n869\n870\n871\n872\n873\n874\n875\n876\n877\n878\n879\n880\n881\n882\n883\n884\n885\n886\n887\n888\n889\n890\n891\n892\n893\n894\n895\n896\n897\n898\n899\n", "cats": {"READ": 1.0, "DONTREAD": 0.0}, "reviews": [{"IMPACT": "3", "SUBSTANCE": "5", "APPROPRIATENESS": "5", "MEANINGFUL_COMPARISON": "3", "PRESENTATION_FORMAT": "Poster", "comments": "COMMENTS AFTER AUTHOR RESPONSE:\n\nThanks for your response, particularly for the clarification wrt the\nhypothesis. I agree with the comment wrt cross-modal mapping. What I don't\nshare is the kind of equation \"visual = referential\" that you seem to assume. A\nreferent can be visually presented, but visual information can be usefully\nadded to a word's representation in aggregate form to encode perceptual aspects\nof the words' meaning, the same way that it is done for textual information;\nfor instance, the fact that bananas are yellow\nwill not frequently be mentioned in text, and adding visual information\nextracted from images will account for this aspect of the semantic\nrepresentation of the word. This is kind of technical and specific to how we\nbuild distributional models, but it's also relevant if you think of human\ncognition (probably our representation for \"banana\" has some aggregate\ninformation about all the bananas we've seen --and touched, tasted, etc.). \nIt would be useful if you could discuss this issue explicitly, differentiating\nbetween multi-modal distributional semantics in general and the use of\ncross-modal mapping in particular.\n\nAlso, wrt the \"all models perform similarly\" comment: I really\nurge you, if the paper is accepted, to state it in this form, even if it\ndoesn't completely align with your hypotheses/goals (you have enough results\nthat do). It is a better description of the results, and more useful for the\ncommunity, than clinging to the\nn-th digit difference (and this is to a large extent independent of whether the\ndifference\nis actually statistical significant or not: If one bridge has 49% chances of\ncollapsing and another one 50%, the difference may be statistically\nsignificant, but that doesn't really make the first bridge a better bridge to\nwalk on).\n\nBtw, small quibble, could you find a kind of more compact and to the point\ntitle? (More geared towards either generally what you explore or to what you\nfind?)\n\n----------\n\nThe paper tackles an extremely interesting issue, that the authors label\n\"referential word meaning\", namely, the connection between a word's meaning and\nthe referents (objects in the external world) it is applied to. If I understood\nit correctly, they argue that\nthis is different from a typical word meaning representation as obtained e.g.\nwith distributional\nmethods, because one thing is the abstract \"lexical meaning\" of a word and the\nother which label is appropriate for a given referent with specific properties\n(in a specific context, although context is something they explicitly leave\naside in this paper). This hypothesis has been previously explored in work by\nSchlangen and colleagues (cited in the paper). The paper explores referential\nword meaning empirically on a specific version of the task of Referential\nExpression Generation (REG), namely, generating the appropriate noun for a\ngiven visually represented object.\n\n- Strengths:\n\n1) The problem they tackle I find extremely interesting; as they argue, REG is\na problem that had previously been addressed mainly using symbolic methods,\nthat did not easily allow for an exploration of how speakers choose the names\nof the objects. The scope of the research goes beyond REG as such, as it\naddresses the link between semantic representations and reference more broadly.\n\n2) I also like how they use current techniques and datasets (cross-modal\nmapping and word classifiers, the ReferIt dataset containing large amounts of\nimages with human-generated referring expressions) to address the problem at\nhand. \n\n3) There are a substantial number of experiments as well as analysis into the\nresults. \n\n- Weaknesses:\n\n1) The main weakness for me is the statement of the specific hypothesis, within\nthe general research line, that the paper is probing: I found it very\nconfusing.  As a result, it is also hard to make sense of the kind of feedback\nthat the results give to the initial hypothesis, especially because there are a\nlot of them and they don't all point in the same direction.\n\nThe paper says:\n\n\"This paper pursues the hypothesis that an accurate\nmodel of referential word meaning does not\nneed to fully integrate visual and lexical knowledge\n(e.g. as expressed in a distributional vector\nspace), but at the same time, has to go beyond\ntreating words as independent labels.\"\n\nThe first part of the hypothesis I don't understand: What is it to fully\nintegrate (or not to fully integrate) visual and lexical knowledge? Is the goal\nsimply to show that using generic distributional representation yields worse\nresults than using specific, word-adapted classifiers trained on the dataset?\nIf so, then the authors should explicitly discuss the bounds of what they are\nshowing: Specifically, word classifiers must be trained on the dataset itself\nand only word classifiers with a sufficient amount of items in the dataset can\nbe obtained, whereas word vectors are available for many other words and are\nobtained from an independent source (even if the cross-modal mapping itself is\ntrained on the dataset); moreover, they use the simplest Ridge Regression,\ninstead of the best method from Lazaridou et al. 2014, so any conclusion as to\nwhich method is better should be taken with a grain of salt. However, I'm\nhoping that the research goal is both more constructive and broader. Please\nclarify. \n\n2) The paper uses three previously developed methods on a previously available\ndataset. The problem itself has been defined before (in Schlangen et al.). In\nthis sense, the originality of the paper is not high. \n\n3) As the paper itself also points out, the authors select a very limited\nsubset of the ReferIt dataset, with quite a small vocabulary (159 words). I'm\nnot even sure why they limited it this way (see detailed comments below).\n\n4) Some aspects could have been clearer (see detailed comments).\n\n5) The paper contains many empirical results and analyses, and it makes a\nconcerted effort to put them together; but I still found it difficult to get\nthe whole picture: What is it exactly that the experiments in the paper tell us\nabout the underlying research question in general, and the specific hypothesis\ntested in particular? How do the different pieces of the puzzle that they\npresent fit together?\n\n- General Discussion: [Added after author response]\n\nDespite the weaknesses, I find the topic of the paper very relevant and also\nnovel enough, with an interesting use of current techniques to address an \"old\"\nproblem, REG and reference more generally, in a way that allows aspects to be\nexplored that have not received enough attention. The experiments and analyses\nare a substantial contribution, even though, as mentioned above, I'd like the\npaper to present a more coherent overall picture of how the many experiments\nand analyses fit together and address the question pursued.\n\n- Detailed comments:\n\nSection 2 is missing the following work in computational semantic approaches to\nreference:\n\nAbhijeet  Gupta,  Gemma  Boleda,  Marco  Baroni,  and Sebastian  Pado. 2015.  \nDistributional                                            vectors  encode \nreferential        \n\nattributes.\nProceedings of\nEMNLP,\n12-21\n\nAurelie Herbelot and Eva Maria Vecchi.                                           \n2015. \nBuilding\na\nshared\nworld:\nmapping\ndistributional to model-theoretic semantic spaces. Proceedings of EMNLP,\n22\u201332.\n\n142 how does Roy's work go beyond early REG work?\n\n155 focusses links\n\n184 flat \"hit @k metric\": \"flat\"?\n\nSection 3: please put the numbers related to the dataset in a table, specifying\nthe image regions, number of REs, overall number of words, and number of object\nnames in the original ReferIt dataset and in the version you use. By the way,\nwill you release your data? I put a \"3\" for data because in the reviewing form\nyou marked \"Yes\" for data, but I can't find the information in the paper.\n\n229 \"cannot be considered to be names\" ==> \"image object names\"\n\n230 what is \"the semantically annotated portion\" of ReferIt?\n\n247 why don't you just keep \"girl\" in this example, and more generally the head\nnouns of non-relational REs? More generally, could you motivate your choices a\nbit more so we understand why you ended up with such a restricted subset of\nReferIt?\n\n258 which 7 features? (list) How did you extract them?\n\n383 \"suggest that lexical or at least distributional knowledge is detrimental\nwhen learning what a word refers to in the world\": How does this follow from\nthe results of Frome et al. 2013 and Norouzi et al. 2013? Why should\ncross-modal projection give better results? It's a very different type of\ntask/setup than object labeling.\n\n394-395 these numbers belong in the data section\n\nTable 1: Are the differences between the methods statistically significant?\nThey are really numerically so small that any other conclusion to \"the methods\nperform similarly\" seems unwarranted to me. Especially the \"This suggests...\"\npart (407). \n\nTable 1: Also, the sim-wap method has the highest accuracy for hit @5 (almost\nidentical to wac); this is counter-intuitive given the @1 and @2 results. Any\nidea of what's going on?\n\nSection 5.2: Why did you define your ensemble classifier by hand instead of\nlearning it? Also, your method amounts to majority voting, right? \n\nTable 2: the order of the models is not the same as in the other tables + text.\n\nTable 3: you report cosine distances but discuss the results in terms of\nsimilarity. It would be clearer (and more in accordance with standard practice\nin CL imo) if you reported cosine similarities.\n\nTable 3: you don't comment on the results reported in the right columns. I\nfound it very curious that the gold-top k data similarities are higher for\ntransfer+sim-wap, whereas the results on the task are the same. I think that\nyou could squeeze more information wrt the phenomenon and the models out of\nthese results.\n\n496 format of \"wac\"\n\nSection 6 I like the idea of the task a lot, but I was very confused as to how\nyou did and why: I don't understand lines 550-553. What is the task exactly? An\nexample would help. \n\n558 \"Testsets\"\n\n574ff Why not mix in the train set examples with hypernyms and non-hypernyms?\n\n697 \"more even\": more wrt what?\n\n774ff \"Previous cross-modal mapping models ... force...\": I don't understand\nthis claim.\n\n792 \"larger test sets\": I think that you could even exploit ReferIt more (using\nmore of its data) before moving on to other datasets.", "SOUNDNESS_CORRECTNESS": "5", "ORIGINALITY": "5", "is_meta_review": null, "RECOMMENDATION": "4", "CLARITY": "3", "REVIEWER_CONFIDENCE": "4"}, {"IMPACT": "3", "SUBSTANCE": "4", "APPROPRIATENESS": "5", "MEANINGFUL_COMPARISON": "3", "PRESENTATION_FORMAT": "Poster", "comments": "AFTER AUTHOR RESPONSE\n\nI accept the response about emphasizing novelty of the task and comparison with\nprevious work. Also increase ratings for the dataset and software that are\npromised to become public before the article publishing.\n\n======================\n\nGENERAL \nThe paper presents an interesting empirical comparison of 3 referring\nexpression generation models. The main novelty lies in the comparison of a yet\nunpublished model called SIM-WAP (in press by Anonymous). The model is\ndescribed in SECTION 4.3 but it is not clear whether it is extended or modified\nanyhow in the current paper.  \n\nThe novelty of the paper may be considered as the comparison of the unpublished\nSIM-WAP model to existing 2 models. This complicates evaluation of the novelty\nbecause similar experiments were already performed for the other two models and\nit is unclear why this comparison was not performed in the paper where SIM-WAP\nmodel was presented. A significant novelty might be the combined model yet this\nis not stated clearly and the combination is not described with enough details.\n\nThe contribution of the paper may be considered the following: the side-by-side\ncomparison of the 3 methods for REG; analysis of zero-shot experiment results\nwhich mostly confirms similar observations in previous works; analysis of the\ncomplementarity of the combined model.                     \n\nWEAKNESSES\nUnclear novelty and significance of contributions. The work seems like an\nexperimental extension of the cited Anonymous paper where the main method was\nintroduced.    \n\nAnother weakness is the limited size of the vocabulary in the zero-shot\nexperiments that seem to be the most contributive part. \n\nAdditionally, the authors never presented significance scores for their\naccuracy results. This would have solidified the empirical contribution of the\nwork which its main value.   \n\nMy general feeling is that the paper is more appropriate for a conference on\nempirical methods such as EMNLP. \n\nLastly, I have not found any link to any usable software. Existing datasets\nhave been used for the work.  \n\nObservations by Sections: \n\nABSTRACT\n\"We compare three recent models\" -- Further in the abstract you write that you\nalso experiment with the combination of approaches. In Section 2 you write that\n\"we present a model that exploits distributional knowledge for learning\nreferential word meaning as well, but explore and compare different ways of\ncombining visual and lexical aspects of referential word meaning\" which\neventually might be a better summarization of the novelty introduced in the\npaper and give more credit to the value of your work. \n\nMy suggestion is to re-write the abstract (and eventually even some sections in\nthe paper) focusing on the novel model and results and not just stating that\nyou compare models of others.                  \n\nINTRODUCTION \n\"Determining such a name is is\" - typo \n\"concerning e.g.\" -> \"concerning, e.g.,\" \n\"having disjunct extensions.\" - specify or exemplify, please \n\"building in Figure 1\" -> \"building in Figure 1 (c)\"\n\nSECTION 4\n\"Following e.g. Lazaridou et al. (2014),\" - \"e.g.\" should be omitted  \n\nSECTION 4.2\n\"associate the top n words with their corresponding distributional vector\" -\nWhat are the values of N that you used? If there were any experiments for\nfinding the optimal values, please, describe because this is original work. The\nuse top N = K is not obvious and not obvious why it should be optimal (how\nabout finding similar vectors to each 5 in top 20?)    \n\nSECTION 4.3 \n\"we annotate its training instances with a fine-grained similarity signal\naccording to their object names.\" - please, exemplify. \n\nLANGUAGE   \nQuite a few typos in the draft. Generally, language should be cleaned up (\"as\nwell such as\"). \nAlso, I believe the use of American English spelling standard is preferable\n(e.g., \"summarise\" -> \"summarize\"). Please, double check with your conference\ntrack chairs.", "SOUNDNESS_CORRECTNESS": "5", "ORIGINALITY": "5", "is_meta_review": null, "RECOMMENDATION": "4", "CLARITY": "4", "REVIEWER_CONFIDENCE": "4"}]}
{"text": "1 000\n011\n012\n013\n014\n015\n016\n017\n018\n019\n020\n021\n022\n023\n024\n025\n026\n027\n028\n029\n030\n031\n032\n033\n034\n035\n036\n037\n038\n039\n040\n041\n042\n043\n044\n045\n046\n047\n048\n049\n061\n062\n063\n064\n065\n066\n067\n068\n069\n070\n071\n072\n073\n074\n075\n076\n077\n078\n079\n080\n081\n082\n083\n084\n085\n086\n087\n088\n089\n090\n091\n092\n093\n094\n095\n096\n097\n098\n099\n1 Introduction\nThere are latent nest structures beyond sequential surface words in natural language (Chomsky, 1957). In the last two decades, researchers incorporated more and more rich structural information into conventional language model (Jelinek and Lafferty, 1991; Chelba, 1997; Chelba and Jelinek, 2000; Emami and Jelinek, 2005) and\nmore recently on neural network-based language modeling (Dyer et al., 2016). Among the effort, one direction is to explore sub-word structures(Costa-Jussa\u0300 and Fonollosa, 2016; Chung et al., 2016; Luong and Manning, 2016; Sennrich et al., 2015), such as characters, mostly to handle out-of-vocabulary word problem. A somewhat opposite direction is to explore hyper-word structure. For example, (Eriguchi et al., 2016) adopts parsing tree in encoder phase in machine translation, (Stahlberg et al., 2016) proposes to use hierarchical phrase-based (HPB) model to guide the search in decoding. Both models, however, rely heavily on human labeled data on the language structures, which is extremely expensive and limited in scale.\nIn this paper, we propose phrasal recurrent neural networks (pRNNs; \u00a72), a general framework of RNNs (Elman, 1990) that explicitly models task-specific nested phrases from plain text. Here we use \u201cphrase\u201d as its definition in phrase-based statistical machine translation (PBSMT(Zens et al., 2002; Koehn et al., 2003)), which indicates any continues sequences of words. What different here are pRNNs permit phrases with arbitrary lengths instead of limiting them for the computational issue. The phrases in pRNNs are composed and selected in a way that is jointly learned in the language modeling, therefore requiring no human-labeled data or external model such as word alignment. In previous RNN-based language modeling, the hidden state of RNN before the word to predict summarizes the history of all previous words. Similarly, in pRNNs, we use the all state of all parallel RNNs (with the same parameters) to capture the history of all subsequence of words that precede the word to predict, with the starting word shifting from the first word the one right before the word to predict.\nThis set of RNNs applied parallelly to different choices of word sequences are called RNN pyra-\n2\n101\n102\n103\n104\n105\n106\n107\n108\n109\n110\n111\n112\n113\n114\n115\n116\n117\n118\n119\n120\n121\n122\n123\n124\n125\n126\n127\n128\n129\n130\n131\n132\n133\n134\n135\n136\n137\n138\n139\n140\n141\n142\n143\n144\n145\n146\n147\n148\n149\n166\n167\n168\n169\n170\n171\n172\n173\n174\n175\n176\n177\n178\n179\n180\n181\n182\n183\n184\n185\n186\n187\n188\n189\n190\n191\n192\n193\n194\n195\n196\n197\n198\n199\nmid. While most of those RNNs\u2019 status deal with incorrect word sequences: they could either start in the middle of a chunk, or in a place too early or too late for the prediction tasks, we left it to an attention mechanism to select and combine, therefore eliminate the need for external knowledge on chunking and composition. This mechanism will be trained jointly with the composition models in pRNNs in optimizing a designed objective function, e.g, perplexity or likelihood. With proper composition function in pRNNs, the RNN pyramid provides a \u201cphrase forest\u201d, which could potentially contain a fairly deep nested structure in some of its members.\nOur pRNN models have two merits:\n\u2022 They represent all phrases in the same vector space in an explicit and unsupervised way. Which shows the potential to discover and utilize hidden structures of surface word sequences.\n\u2022 They explore the possibility of network construction in another dimension: Parallel. Instead of stacking deeper and deeper layers of RNNs.\nExperiments show that pRNNs are effective for language modeling (\u00a74). Our model obtains significant better perplexities than state-of-the-art sequential Long-Short Term (LSTM) model on language modeling task, both on PTB and FBIS English data set. We also apply pRNNs to encode the source sentence of machine translation besides a conventional bi-direction encoder, which improves over the Moses (phrase-based statistical model) and a strong sequence-to-sequence baseline in the Chinese-English machine translation task.\n2 Phrasal RNNs\nWe assume that, in the task of language model and machine translation, selecting the appropriate hidden structures for one sentence is highly related to the performance of the task.\nFormally, a typical pRNN consist of three subnetworks: phrasal part P , attention part A and sequential part S. Each sub-network plays its own role and collaborates with others. P (\u00a72.1) constructs the neural structures (realvalued vectors) which corresponding to natural linguistic structures (phrases). It takes embed-\ndings (x?) of all words in a candidate phrase, as input, then output one fix-length real-valued vector p as the distributed representation (Hinton et al., 1986) of the phrase;\np = P (xji ) (1)\nWhere xji = [xi, \u00b7 \u00b7 \u00b7 , xj ] (2)\nA (\u00a72.2) compares the candidate phrase regards to the current situation, give a probabilistic distribution over them, then provides the weighted sum of their representations. It takes previous calculated candidate set {p} as input, then output a weighted sum of {p} as p\u0302 with the help of current hidden state (ht) at time step t given by (S).\np\u0302 = A(h, pk, \u00b7 \u00b7 \u00b7 , pl) (3)\nS (\u00a72.2) combine the weighted sum of candidate phrases into original RNN, forces RNN taking the structural history information into consideration. It is similar to original RNN except for one point: when predict next hidden state ht+1, besides ht and xt, it takes p\u0302 as input too.\nht = S(ht\u22121, xt, p\u0302t) (4)\n2.1 Represent Phrases\nThere are many types of neural networks which can transfer phrase into distributed representation. However, when we need to handle arbitrary length phrases, another way of saying, the entire history\n201\n202\n203\n204\n205\n206\n207\n208\n209\n210\n211\n212\n213\n214\n215\n216\n217\n218\n219\n220\n221\n222\n223\n224\n225\n226\n227\n228\n229\n230\n231\n232\n233\n234\n235\n236\n237\n238\n239\n240\n241\n242\n243\n244\n245\n246\n247\n248\n249\n250\n251\n252\n253\n254\n255\n256\n257\n258\n259\n260\n261\n262\n263\n264\n265\n266\n267\n268\n269\n270\n271\n272\n273\n274\n275\n276\n277\n278\n279\n280\n281\n282\n283\n284\n285\n286\n287\n288\n289\n290\n291\n292\n293\n294\n295\n296\n297\nof words, the choices are very limited to a few RNNs\u2019 variants.\nEven when we choose RNNs to construct structure vectors, we are still facing a big problem. Because the hidden state ht are considered to encode the entire history information from the beginning of the sentence xt1, they can be utilized as representations of phrases begin at the sentence head {xj1|1 \u2264 j \u2264 t}. But they do not provide the representations of phrases which do not begin with the first word of the sentence {xji |1 < i < j \u2264 t}.\nTo represent all candidate phrases in a sentence with n words, we build a RNN pyramid (RNNP (Fig. 3)), with n horizontal parallel RNNs {RNNn}Nn=1. RNNn indicates that it begins at the n-th word of the sentence. With all N(N + 1)/2 hidden status generated by RNN pyramid, we obtain distributed representation of all candidate phrases/structures of a sentence.\nTo keep consistent among these parallel RNNs in the pyramid and to limit the number of parameters for keeping the model simple, we let all parallel RNNs share the same network parameters (W,U, b).\nhnt = \u03c3(Wxt + Uh n t\u22121 + b) (5)\nWhere hnt of the RNNn indicates the hidden state of the t-th word in the sentence.\nThis method is kind of similar to the sharing parameters between filters of convolutional neural network (Cun et al., 1990), except for it working on the time axis, which recognizes the local invariant along each time steps.\nWith RNN pyramid built on a sentence, we can map all potential phrases with varying lengths into real-valued fix-length vectors. These vectors are representations of candidate structures we plan to compare at next stage.\n2.2 Utilize Phrases\nWith the candidate structures represented by a fixlength vector (Fig. 2), we can easily apply attention mechanism on these vectors, and soft combine them to output a weighted sum as the best structure selected:\ns\u0302t = \u2211 t,n \u03b1t,nh n t\u22121 (6)\n4\n301\n302\n303\n304\n305\n306\n307\n308\n309\n310\n311\n312\n313\n314\n315\n316\n317\n318\n319\n320\n321\n322\n323\n324\n325\n326\n327\n328\n329\n330\n331\n332\n333\n334\n335\n336\n337\n338\n339\n340\n341\n342\n343\n344\n345\n346\n347\n348\n349\n350\n351\n352\n353\n354\n355\n356\n357\n358\n359\n360\n361\n362\n363\n364\n365\n366\n367\n368\n369\n370\n371\n372\n373\n374\n375\n376\n377\n378\n379\n380\n381\n382\n383\n384\n385\n386\n387\n388\n389\n390\n391\n392\n393\n394\n395\n396\n397\n398\n399\nFigure 3: On a 4-word sentence, RNN pyramid (dashed line triangle) generated by 4 horizontal parallel RNNs, each begins at 1 of the 4 words in the sentence. Initial status are indicated by circles. Because hidden state is considered containing all history information. The set of all hidden status in the pyramid can be mapped one-to-one to the representation of all candidate phrases of the sentence.\nWhere the weight of hidden state (structure) \u03b1t,n can be represented by the following form:\n\u03b1t,n = exp(et,n)\u2211\nt,n exp(et,n)\n(7)\nIn which we define et,n as:\net,n = a(hk, ht,n) (8)\nHere we combine a(hk, ht,n) with one layer of feedforward neural network, where hk is the k-th word of sequential part S.\nWe adopt the attention mechanism from Bahdanau et. al. (2014). As we showed in Fig 4. We put s\u0302 into R part of the network, let the network to combine it with h and x to predict next hidden state. We also apply our model on machine translation task within successful EncoderDecoder framework as in Fig 5.\nFigure 4: Combine best structure s\u0302 given by attention partA in each predicting step of sequential part S. To reduce calculation, we limit the candidate phrase set at each step to the newly generated ones (the blue rectangles with solid boundaries), which means to ignore structures generated at previous steps (grey ones with dashed boundaries), therefore reduce the scale of candidate phrases set from O(N2) to O(N). The Pyramid and Seq part share the same embedding in experiments, we draw them separately in the diagram just for clearance.\n3 LM Experiment\n3.1 Data\nTo make experiment comparable with other methods, we apply our models on language model task, evaluate it in perplexity on the widely used English Penn Treebank (PTB) (Marcus et al., 1993), which pre-processing and splitting by Mikolov (2010). The data is utilized as following: sections (0-20) with 929k tokens are used for training, sections (21-22) with 73k tokens are held out as validation, and sections (23-24) with 82k tokens are used for testing. There are only top 10,000 highfrequency words are kept in the corpus. All rest low-frequency words are replaced with UNK tag. This version of data is widely used among the language modeling community. It is publicly avail-\n5\n401\n402\n403\n404\n405\n406\n407\n408\n409\n410\n411\n412\n413\n414\n415\n416\n417\n418\n419\n420\n421\n422\n423\n424\n425\n426\n427\n428\n429\n430\n431\n432\n433\n434\n435\n436\n437\n438\n439\n440\n441\n442\n443\n444\n445\n446\n447\n448\n449\n450\n451\n452\n453\n454\n455\n456\n457\n458\n459\n460\n461\n462\n463\n464\n465\n466\n467\n468\n469\n470\n471\n472\n473\n474\n475\n476\n477\n478\n479\n480\n481\n482\n483\n484\n485\n486\n487\n488\n489\n490\n491\n492\n493\n494\n495\n496\n497\n498\n499\nFigure 5: We use our pRNNs as the pyramid encoder to represent more structural information (all candidate phrases) of source sentences, just beside the original bi-direction encoder which only represents surface word sequence explictly. Then we join two context vectors from two encoders into a larger one. Then we allow the decoder to choose which portion of the larger context (which source words or candidate phrases) is more relevant to the next generated word of the target sentence. We adopt two settings of pyramid encoder, one takes only last status (the blue rectangles with solid boundaries) of each RNN as the input of attention part (src-pyr-last in Fig 4), the other takes all status (all rectangles including the grey ones with dashed boundaries) of all RNNs as the input of attention part (src-pyr-all in Fig 4).\nable.1\nBecause the scale of PTB corpus is relatively small, we also train our model on larger FBIS English(LDC2003E14). Accordingly, we only keep top 40,000 high-frequency words in the corpus, replace rest low-frequency word with UNK tag. We use NIST MT06 as the validation set, NIST MT08 as the test set.\ntrain valid test FBIS MT06 MT08\nSequences 219,280 6,560 5,424 Tokens 7,877,650 190,065 166,937 Types 49,210 8,476 9,576\nTable 1: FBIS and NIST MT Corpus statistics.\n3.2 Model Configuration\nBaseline In this paper, we utilize state-of-theart LSTM framework on language model proposed by Zaremba (2014) as the baseline model. Firstly, we stack 2 layers of LSTMs, to explore more abstracted patterns which are not supposed to be discovered by a single layer. Secondly, to increase the model\u2019s capability of noise tolerance and reduce the overfitting to training data, we introduce dropout (Hinton et al., 2012) before and after each recurrent layer. We choose dropout rate as 0.5 without tuning. For training, we used AdaDelta (Zeiler, 2012). To handle nan and inf which occasionally occur in gradient, we normalize the gradient of each batch to 1.0 and drop the parameter updates on such batches. To determine when to stop training, we set patience to 100. We set the dimension of both word embedding and hidden dimensions to 200. We initialized all parameters according to recommendations given in (Zaremba et al., 2014) and blocks(van Merrie\u0308nboer et al., 2015). For all models, we used word embedding, hidden dimensions of 200 and 2-layer LSTMs. For both models, we choose dropout rate as 0.5. For training, we used AdaDelta and normalize the gradient to 1.0, set patience to 100. We initialized all parameters according to recommendations given in Zaremba et al. (2014).\nPhrasal RNN We configure our model exactly as the baseline model, except adding an extra RNN\n1http://www.fit.vutbr.cz/\u02dcimikolov/ rnnlm/\n6\n501\n502\n503\n504\n505\n506\n507\n508\n509\n510\n511\n512\n513\n514\n515\n516\n517\n518\n519\n520\n521\n522\n523\n524\n525\n526\n527\n528\n529\n530\n531\n532\n533\n534\n535\n536\n537\n538\n539\n540\n541\n542\n543\n544\n545\n546\n547\n548\n549\n550\n551\n552\n553\n554\n555\n556\n557\n558\n559\n560\n561\n562\n563\n564\n565\n566\n567\n568\n569\n570\n571\n572\n573\n574\n575\n576\n577\n578\n579\n580\n581\n582\n583\n584\n585\n586\n587\n588\n589\n590\n591\n592\n593\n594\n595\n596\n597\n598\n599\npyramid layer above baseline\u2019s 2-layer LSTM. We also add dropout between 2nd LSTM layer and RNN pyramid layer. We set dimension of hidden state in RNN pyramid layer as 200 either. We utilize Gated Recurrent Unit (GRU) (Chung et al., 2014) to construct RNN pyramid layer. We also tried a simplified version of GRU to build the pyramid (pRNNv in table 2), which achieves the best result.\n3.3 Results\nModel Perplexity 5-gram, KN5 141.2 FFNN-LM 140.2 RNN 124.7 LSTM 126 genCNN 116.4 LSTM (baseline) 106.9 pRNN 97.6 pRNNv 94.5\nTable 2: Perplexity on PENN TREEBANK, where the top 5 rows of numbers are results reported in previous work, our baseline and new pRNN model are in last three rows.\nModel Perplexity 5-gram, KN5 278.6 FFNN-LM(5-gram) 248.3 FFNN-LM(20-gram) 228.2 RNN 223.4 LSTM 206.9 genCNN 181.2 LSTM (baseline) 171.8 pRNN 161.5\nTable 3: Perplexity on FBIS data set, where the top 5 rows of numbers are results reported in previous work, our baseline and new pRNN model are in last two rows.\nWe report our perplexities result of language model in table (2 and 3). We calculate perplexity over a sequence [w1, . . . , wn] with\nPPL = exp ( \u2212 log(Prob(w n 1 ))\nn\n) (9)\n(including the end of sentence (EOS) symbol). pRNN and its variant outperform over 10 points of\nppl over a strong baseline on both PTB and FBIS English data set.\n4 MT Experiment\n4.1 Data\nWe evaluate all three models, PBSMT, RNNsearch, pRNN on the same data set. We utilize 1.25M sentence pairs, which are extracted from LDC corpora as training data. There are 34.5 English words and 27.9M Chinese words in the training data. We select NIST 2002 (MT02) data set as our development set, the NIST 2003 (MT03), NIST 2004 (MT04), NIST 2005 (MT05), NIST 2006 (MT06) and NIST 2008 (MT08) as test sets. When training neural networks, we limit the size of vocabularies of both source and target side to the most frequent 16K words. All rest low-frequency words are replaced with UNK tag. Chinese vocabulary covers approximately 95.8% of the corpora. English vocabulary covers approximately 98.3% of the corpora.\n4.2 Model Configuration\nBaseline In this paper, we use an open-source implementation (Meng et al., 2015) of RNNsearch (Bahdanau et al., 2014) as baseline model. To increase the model\u2019s capability of noise tolerance and reduce the overfitting to training data, we introduce dropout (Hinton et al., 2012) before softmax layer, and set the dropout rate equal to 0.5. We choose dropout rate as 0.5 without tuning. For training, we used AdaDelta (Zeiler, 2012) and normalize the gradient to 1.0, . To handle nan and inf which occasionally occur in gradient, we normalize the gradient of each batch to 1.0 and drop the parameter updates on such batches. We set the dimension of both source and target word embedding as 620, and hidden dimensions to 1000. We initialized all parameters according to recommendations given in (Bahdanau et al., 2014). We also introduce the phrase-based model of Moses (Koehn et al., 2007) as a secondary baseline too.\nPhrase-based NMT We configure our model exactly as the baseline model, except adding an extra RNN pyramid as a secondary source encoder. As the limitation on the memory of GPU, we keep only phrases ended at eos into consideration. Thus we name it src-pyr-last in table 4. We set the dimension of hidden dimensions inside pyramid as 1000 too. We utilize Gated Recur-\n7\n601\n602\n603\n604\n605\n606\n607\n608\n609\n610\n611\n612\n613\n614\n615\n616\n617\n618\n619\n620\n621\n622\n623\n624\n625\n626\n627\n628\n629\n630\n631\n632\n633\n634\n635\n636\n637\n638\n639\n640\n641\n642\n643\n644\n645\n646\n647\n648\n649\n650\n651\n652\n653\n654\n655\n656\n657\n658\n659\n660\n661\n662\n663\n664\n665\n666\n667\n668\n669\n670\n671\n672\n673\n674\n675\n676\n677\n678\n679\n680\n681\n682\n683\n684\n685\n686\n687\n688\n689\n690\n691\n692\n693\n694\n695\n696\n697\n698\n699\nModels MT02 MT03 MT04 MT05 MT06 MT08 Test Avg. Diff moses 33.41 31.61 33.48 30.75 31.07 23.37 30.056 +0.754 RNNsearch (groundhog) 32.32 29.02 31.25 28.32 27.99 20.29 27.374 -1.928 RNNsearch (baseline) 34.28 30.61 33.24 30.66 29.83 22.17 29.302 +0.000 pRNN (src-pyr-last) 35.48 31.61 34.40 31.96 31.36 22.82 30.430 +1.128 pRNN (src-pyr-all) 35.49 32.08 34.51 31.81 30.91 22.86 30.434 +1.132\nTable 4: BLEU score on 1.25M training corpus with 16k dictionary on both source and target side. The above two lines of the table are results of open-source machine translation systems. Bold numbers indicate the best results on the data set (column). pRNNs are better than original RNNsearch model baseline (in-house reimplemented). We find it is interesting that results of src-pyr-all are only slightly better than src-pyr-last, we guess this is due to the limited discriminative power of simple attention mechanism when meeting large number of complex candidates.\nrent Unit (GRU) (Chung et al., 2014) to construct RNN pyramid layer.\nFor a fair comparison, we run baseline system (RNNsearch) many times (not epoch) and report only the best one. We only run PBNMT once.\n4.3 Results\nWe report our BLEU result of three models in table (4). We use the case-insensitive 4-gram NIST BLEU (Papineni et al., 2002) score given by mteval v11.pl pRNNs outperforms both PBSMT and Encoder-Decoder model.\n5 Related Work\n5.1 Relation to Previous Attempts on\nStructural Information\nIn the last two decades, to achieve better performance, researchers incorporated more and more rich structural information into conventional model (Jelinek and Lafferty, 1991; Chelba, 1997; Chelba and Jelinek, 2000; Emami and Jelinek, 2005). This trend is more clear in statistical machine translation (SMT) community, from wordbased SMT (Brown et al., 1993) to phrase-based SMT (Koehn et al., 2003), hierarchical phrasebased SMT (Chiang, 2005) and syntax-based SMT (Huang et al., 2006; Liu et al., 2006; Galley et al., 2006; Xie et al., 2011) (forest-based (Mi and Huang, 2008; Mi et al., 2008)). among them, phrase-based SMT (Zens et al., 2002; Koehn et al., 2003) was the most widely adopted translation model. The Same trend can be observed in neural network strand. There were many works which successfully modeled structure in neural network on parsing (Dyer et al., 2016; Emami and Jelinek, 2005; Henderson, 2004; Titov and Hender-\nson, 2007; Buys and Blunsom, 2015) or language modeling tasks (Dyer et al., 2016; Chelba and Jelinek, 2000; Emami and Jelinek, 2005; Chelba, 1997).\nIn neural machine translation (NMT) area, the situation is more complex. In recent years, the most popular and success NMT model is EncoderDecoder model (Bahdanau et al., 2014; Sutskever et al., 2014). Which has achieved competitive or better results in many translation tasks(Luong et al., 2015a,b). However, beyond sequential surface words, there are latent nest structures in natural language (Chomsky, 1957). One direction to explore is to introduce sub-word structures(CostaJussa\u0300 and Fonollosa, 2016; Chung et al., 2016; Luong and Manning, 2016; Sennrich et al., 2015), such as characters. The most important reason to dig into sub-word is to handle out-of-vocabulary word problem. This problem is rooted in the limited size of vocabulary, which utilized by NMT mapping symbols to real-valued dense vector.\nAnother direction is to explore hyper-word structure. Authors of (Eriguchi et al., 2016) adopted parsing tree in encoder phase. However, this method depends heavily on human-labeled data, which is always expensive and limited in scale. Authors of (Stahlberg et al., 2016) introduce hierarchical phrase-based (HPB) model as the guider of search space. However, the HPB model and NMT model are trained separately and combined only when decoding. Compare this to the previous method, (Eriguchi et al., 2016) can be categorized into introducing external data, (Stahlberg et al., 2016) can be categorized into introducing external model. In an ideal situation, all external model can be replaced by a neural net-\n8\n701\n702\n703\n704\n705\n706\n707\n708\n709\n710\n711\n712\n713\n714\n715\n716\n717\n718\n719\n720\n721\n722\n723\n724\n725\n726\n727\n728\n729\n730\n731\n732\n733\n734\n735\n736\n737\n738\n739\n740\n741\n742\n743\n744\n745\n746\n747\n748\n749\n750\n751\n752\n753\n754\n755\n756\n757\n758\n759\n760\n761\n762\n763\n764\n765\n766\n767\n768\n769\n770\n771\n772\n773\n774\n775\n776\n777\n778\n779\n780\n781\n782\n783\n784\n785\n786\n787\n788\n789\n790\n791\n792\n793\n794\n795\n796\n797\n798\n799\nwork with equal ability.\n5.2 Similarity to Deep Memory Network\nDeep Memory Network is an effective implementation of the neural turing machine. When they use state machine such as GRU or LSTM to read from one memory and write to another, the memory IO addresses are either content-based (via attention mechanism) or location-based (actually sequentially cell-by-cell) (Meng et al., 2015). However, there are much more other methods in the location-based category.\nCell-by-cell vs. Incremental If we consider the hidden of one time step inside pyramid RNN as memory, we can name the operations as incremental read and write. The intuition behind incremental addressing is, when we read little, we know little, we only have the ability to write little. But when we read more, we know more, we are gone to have the ability to write more.\n6 Discussion\nOur experiments clearly show that the proposed pRNN model is quite effective in language modeling and machine translation. This is the because of:\n\u2022 when model predicting, it is provided with all candidate phrases as structure information rather than just surface sequential words.\n\u2022 utilizing attention mechanism to compare and combine to get the weighted sum which best fit for predicting next hidden state.\nThe most significant question that remains is how well the quality of forest generated as a by-product of pRNN, will it get a better result than other supervised parsing model trained on human label data.\n7 Conclusion\nWe introduced phrasal recurrent neural network, an RNN model with all potential candidate phrases considered. Our model does not require any human labeled data to construct the structures. It outperforms the state-of-the-art LSTM language models. Our model does not require any external resources such as human labeled data or word align model to construct the phrases. It outperforms both state-of-the-art PBSMT and RNNsearch model.\nWe make two main contributions:\n\u2022 Instead of packing all information in distributed representation and internal hidden status, which are computing-friendly, we try to represent natural structure in an explicit way, which are human-friendly.\n\u2022 Instead of stacking deeper and deeper layers of RNNs, we explore the possibility of network construction in another dimension: making RNN sequences parallel.\n", "cats": {"READ": 0.0, "DONTREAD": 1.0}, "reviews": [{"IMPACT": "3", "SUBSTANCE": "4", "APPROPRIATENESS": "5", "MEANINGFUL_COMPARISON": "3", "PRESENTATION_FORMAT": "Poster", "comments": "The paper describes an idea to learn phrasal representation and facilitate them\nin RNN-based language models and neural machine translation\n\n-Strengths:\n\nThe  idea to incorporate phrasal information into the task is interesting.\n\n- Weaknesses:\n\n- The description is hard to follow. Proof-reading by an English native speaker\nwould benefit the understanding\n- The evaluation of the approach has several weaknesses\n\n- General discussion\n\n- In Equation 1 and 2 the authors mention a phrase representation give a\nfix-length word embedding vector. But this is not used in the model. The\nrepresentation is generated based on an RNN. What the propose of this\ndescription?\n- Why are you using GRU for the Pyramid and LSTM for the sequential part? Is\nthe combination of two architectures a reason for your improvements?\n- What is the simplified version of the GRU? Why is it performing better? How\nis it performing on the large data set?\n- What is the difference between RNNsearch (groundhog) and RNNsearch(baseline)\nin Table 4?\n-  What is the motivation for only using the ending phrases and e.g. not using\nthe starting phrases?\n- Did you use only the pyramid encoder? How is it performing? That would be a\nmore fair comparison since it normally helps to make the model more complex.\n- Why did you run RNNsearch several times, but PBNMT only once?\n\n- Section 5.2: What is the intent of this section", "SOUNDNESS_CORRECTNESS": "5", "ORIGINALITY": "5", "is_meta_review": null, "RECOMMENDATION": "2", "CLARITY": "2", "REVIEWER_CONFIDENCE": "4"}, {"IMPACT": "3", "SUBSTANCE": "2", "APPROPRIATENESS": "5", "MEANINGFUL_COMPARISON": "3", "PRESENTATION_FORMAT": "Poster", "comments": "This paper proposed a new phrasal RNN architecture for sequence to sequence\ngeneration. They have evaluated their architecture based on (i) the language\nmodelling test evaluated on PTB and FBIS and (ii) Chinese-English machine\ntranslation task on NIST MT02-08 evaluation sets. The phrasal RNN (pRNN)\narchitecture is achieved by generating subnetworks of phrases. \n\nStrengths\n====\n\nA new phrasal architecture. \n\nWeaknesses\n====\n\n**Technical**: \n\nIt's unclear whether there is a limit set  on the phrase length of the pRNN.\nMaybe I've missed this in the paper, if there is, please be more explicit about\nit because it affects the model quite drastically if for every sentence the\nlargest phrase length is the sentence length. \n\n - It's because if the largest phrase length is the sentence length, then model\ncan be simplified into a some sort of convolution RNN where the each state of\nthe RNN goes through some convolution layer before a final softmax and\nattention. \n\n - If there is a limit set on the phrase length of pRNN, then it makes the\nsystem more tractable. But that would also mean that the phrases are determined\nby token ngrams which produces a sliding window of the \"pyramid encoders\" for\neach sentence where there are instance where the parameter for these phrases\nwill be set close to zero to disable the phrases and these phrases would be a\ngood intrinsic evaluation of the pRNN in addition to evaluating it purely on\nperplexity and BLEU extrinsically. \n\nThe usage of attention mechanism without some sort of pruning might be\nproblematic at the phrasal level. The author have opted for some sort of greedy\npruning as described in the caption of figure 4. But I support given a fixed\nset of phrase pairs at train time, the attention mechanism at the phrasal level\ncan be pre-computed but at inference (apply the attention on new data at test\ntime), this might be kind of problematic when the architecture is scaled to a\nlarger dataset. \n\n**Empirical**: \n\nOne issue with the language modelling experiment is the choice of evaluation\nand train set. Possibly a dataset like common crawl or enwiki8 would be more\nappropriate for language modelling experiments. \n\nThe main issue of the paper is in the experiments and results reporting, it\nneeds quite a bit of reworking. \n\n - The evaluation on PTB (table 2) isn't a fair one since the model was trained\non a larger corpus (FBIS) and then tested on PTB. The fact that the previous\nstudy reported a 126 perplexity baseline using LSTM and the LSTM's perplexity\nof 106.9 provided by the author showed that the FBIS gives an advantage to\ncomputing the language model's perplexity when tested on PTB.\n\n - Also, regarding section 3.3, please cite appropriate publications the\n\"previous work\" presented in the tables. And are the previous work using the\nsame training set? \n\n- Additionally, why isn't the the GRU version of pRNNv reported in the FBIS\nevaluation in Table 3?\n\nThe result section cannot be simply presenting a table without explanation:\n\n - Still on the result sections, although it's clear that BLEU and perplexity\nare objective automatic measure to evaluate the new architecture. It's not\nreally okay to put up the tables and show the perplexity and BLEU scores\nwithout some explanation. E.g. in Table 2, it's necessary to explain why the\nLSTM's perplexity from previous work is higher than the author's\nimplementation. Same in Table 3. \n\nThe result presented in Table 4 don't match the description in Section 4.3:\n\n - It's not true that the pRNN outperforms both PBSMT and Enc-Dec model. The\nauthors should make it clear that on different evaluation sets, the scores\ndiffers. And it's the averaged test scores that pRNN performs better\n\n- Please also make it clear whether the \"Test Avg.\" is a micro-average (all\ntestsets are concatenated and evaluated as one set) or macro-average (average\ntaken across the scores of individual test sets) score. \n\nFor table 4, please also include the significance of the BLEU improvement made\nby the pRNN with respect to the the baseline, see\nhttps://github.com/jhclark/multeval\n\nGeneral Discussion\n====\n\nAs the main contribution of this work is on the phrasal effect of the new RNN\narchitecture, it's rather important to show that the phrases are more coherent\nthan the vanilla LSTM / RNN model. Thus the BLEU evaluation is insufficient. A\ncloser look at evaluating the phrases in a subset of the evaluation set would\nbe necessary to support the claims. \n\nDoes the baseline system (groundhog) contains the attention mechanism? \n\n - If so, please be more specific in describing it in section 4.2 and Table 4. \n\n - If not, please remove the attention layer after the encoder in figure 5.\nAlso, the lack of attention mechanism provides a disadvantage to the baseline\nenc-dec system and it's unclear whether the pRNN can outperform or be an\nadditive feature to the enc-dec system with an attention mechanism. The unfair\ndisadvantage is even more prevalent when the pRNN uses multiple phrasal\nattention layers within a single sentence while a simple enc-dec system without\nattention is used as a benchmark =(\n\nQuestion: Wouldn't a simpler way to get phrasal RNN is to put the \"pyramid\"\nRNNs of a phrase into some soft of a average pooling layer?\n\nMinor Issues \n====\n\nFigure 2 is a little redundant, I think figure 1 is enough to compare it\nagainst the pRNN (figure3 and 4).\n\nAlso, possibly figure 3 can be combined into the pyramid part of figure 4. And\nmore space can be freed up to further explain the results section. \n\nPlease don't abuse figure/table captions, whenever possible, please try to keep\nthe description of the tables and figures in-text.  \n\n**Please put the verbose caption description in the main text for Figure 3, 4,\n5 and Table 4**\n\nSpacing in between some of the equations can also be reduced (e.g. in latex use\n\\vspace{-5mm} )", "SOUNDNESS_CORRECTNESS": "5", "ORIGINALITY": "5", "is_meta_review": null, "RECOMMENDATION": "2", "CLARITY": "2", "REVIEWER_CONFIDENCE": "3"}]}
{"text": "1 000\n011\n012\n013\n014\n015\n016\n017\n018\n019\n020\n021\n022\n023\n024\n025\n026\n027\n028\n029\n030\n031\n032\n033\n034\n035\n036\n037\n038\n039\n040\n041\n042\n043\n044\n045\n046\n047\n048\n049\n068\n069\n070\n071\n072\n073\n074\n075\n076\n077\n078\n079\n080\n081\n082\n083\n084\n085\n086\n087\n088\n089\n090\n091\n092\n093\n094\n095\n096\n097\n098\n099\n1 Introduction\nMulti-task learning is an effective approach to improve the performance of a single task with the help of other related tasks. Recently, neuralbased models for multi-task learning have become very popular, ranging from computer vision (Misra et al., 2016; Zhang et al., 2014) to natural language processing (Collobert and Weston, 2008; Luong et al., 2015), since they provide a convenient way of combining information frommultiple tasks.\nHowever, most existing work on multi-task learning attempts to divide the features of different tasks into private and shared spaces, merely based on whether parameters of some components should be shared. As shown in Figure 1- (a), the general shared-private model introduces two feature spaces for any task: one is used to\nstore task-dependent features, the other is used to capture shared features. The major limitation of this framework is that the shared feature space could contain some unnecessary task-specific features, while some sharable features could also be mixed in private space, suffering from feature redundancy. Taking the following two sentences as examples, which are extracted from two different sentiment classification tasks: Movie reviews and Baby products reviews.\nThe infantile cart is simple and easy to use. This kind of humour is infantile and boring.\nThe word \u201cinfantile\u201d indicates negative sentiment in Movie task while it is neutral in Baby task. However, the general shared-private model could place the task-specific word \u201cinfantile\u201d in a shared space, leaving potential hazards for other tasks. Additionally, the capacity of shared space could also be wasted by some unnecessary features. To address this problem, in this paper we propose an adversarial multi-task framework, in which the shared and private feature spaces are inherently disjoint by introducing orthogonality constraints. Specifically, we design a generic sharedprivate learning framework to model the text se-\n2\n101\n102\n103\n104\n105\n106\n107\n108\n109\n110\n111\n112\n113\n114\n115\n116\n117\n118\n119\n120\n121\n122\n123\n124\n125\n126\n127\n128\n129\n130\n131\n132\n133\n134\n135\n136\n137\n138\n139\n140\n141\n142\n143\n144\n145\n146\n147\n148\n149\n150\n151\n152\n153\n154\n155\n156\n157\n158\n159\n160\n161\n162\n163\n164\n165\n166\n167\n168\n169\n170\n171\n172\n173\n174\n175\n176\n177\n178\n179\n180\n181\n182\n183\n184\n185\n186\n187\n188\n189\n190\n191\n192\n193\n194\n195\n196\n197\n198\n199\nquence. To prevent the shared and private latent feature spaces from interfering with each other, we introduce two strategies: adversarial training and orthogonality constraints. The adversarial training is used to ensure that the shared feature space simply contains common and task-invariant information, while the orthogonality constraint is used to eliminate redundant features from the private and shared spaces.\nThe contributions of this paper can be summarized as follows.\n1. Proposed model divides the task-specific and shared space in a more precise way, rather than roughly sharing parameters. 2. We extend the original binary adversarial training to multi-class, which not only enables multiple tasks to be jointly trained, but allows us to utilize unlabeled data. 3. We can condense the shared knowledge among multiple tasks into an off-the-shelf neural layer, which can be easily transferred to new tasks.\n2 Recurrent Models for Text Classification\nThere are many neural sentence models, which can be used for text modelling, involving recurrent neural networks (Sutskever et al., 2014; Chung et al., 2014), convolutional neural networks (Collobert et al., 2011; Kalchbrenner et al., 2014), and recursive neural networks (Socher et al., 2013). Here we adopt recurrent neural network with long short-term memory (LSTM) due to their superior performance in various NLP tasks.\nLong Short-term Memory Long short-term memory network (LSTM) (Hochreiter and Schmidhuber, 1997) is a type of recurrent neural network (RNN) (Elman, 1990), and specifically addresses the issue of learning long-term dependencies. While there are numerous LSTM variants, here we use the LSTM architecture used by (Jozefowicz et al., 2015), which is similar to the architecture of (Graves, 2013) but without peep-hole connections.\nWe define the LSTM units at each time step t to be a collection of vectors in Rd: an input gate it, a forget gate ft, an output gate ot, a memory cell ct and a hidden state ht. d is the number of the LSTM units. The elements of the gating vectors it, ft and ot are in [0, 1].\nThe LSTM is precisely specified as follows.\n c\u0303t ot it ft  =  tanh \u03c3 \u03c3 \u03c3 (Wp [ xtht\u22121 ] + bp ) , (1)\nct = c\u0303t \u2299 it + ct\u22121 \u2299 ft, (2) ht = ot \u2299 tanh (ct) , (3)\nwhere xt \u2208 Re is the input at the current time step; Wp \u2208 R4d\u00d7(d+e) and bp \u2208 R4d are parameters of affine transformation; \u03c3 denotes the logistic sigmoid function and \u2299 denotes elementwise multiplication. The update of each LSTM unit can be written precisely as follows:\nht = LSTM(ht\u22121,xt, \u03b8p). (4)\nHere, the function LSTM(\u00b7, \u00b7, \u00b7, \u00b7) is a shorthand for Eq. (1-3), and \u03b8p represents all the parameters of LSTM.\nText Classification with LSTM Given a text sequence x = {x1, x2, \u00b7 \u00b7 \u00b7 , xT }, we first use a lookup layer to get the vector representation (embeddings) xi of the each word xi. The output at the last moment hT can be regarded as the representation of the whole sequence, which has a fully connected layer followed by a softmax non-linear layer that predicts the probability distribution over classes.\ny\u0302 = softmax(WhT + b) (5)\nwhere y\u0302 is prediction probabilities, W is the weight which needs to be learned, b is a bias term. Given a corpus with N training samples (xi, yi), the parameters of the network are trained to minimise the cross-entropy of the predicted and true distributions.\nL(y\u0302, y) = \u2212 N\u2211 i=1 C\u2211 j=1 yji log(y\u0302 j i ), (6)\nwhere yji is the ground-truth label; y\u0302 j i is prediction probabilities, and C is the class number.\n3 Multi-task Learning for Text Classification\nThe goal of multi-task learning is to utilizes the correlation among these related tasks to improve\n3\n201\n202\n203\n204\n205\n206\n207\n208\n209\n210\n211\n212\n213\n214\n215\n216\n217\n218\n219\n220\n221\n222\n223\n224\n225\n226\n227\n228\n229\n230\n231\n232\n233\n234\n235\n236\n237\n238\n239\n240\n241\n242\n243\n244\n245\n246\n247\n248\n249\n250\n251\n252\n253\n254\n255\n256\n257\n258\n259\n260\n261\n262\n263\n264\n265\n266\n267\n268\n269\n270\n271\n272\n273\n274\n275\n276\n277\n278\n279\n280\n281\n282\n283\n284\n285\n286\n287\n288\n289\n290\n291\n292\n293\n294\n295\n296\n297\n298\n299\nsoftmax Lmtask\nLSTM\nsoftmax Lntask\nxm xn\n(a) Fully Shared Model (FS-MTL)\nxm\nxn\nLSTM\nLSTM\nLSTM\nsoftmax\nsoftmax\nLmtask\nLntask\n(b) Shared-Private Model (SP-MTL)\nFigure 2: Two architectures for learning multiple tasks. Yellow and gray boxes represent shared and private LSTM layers respectively.\nclassification by learning tasks in parallel. To facilitate this, we give some explanation for notations used in this paper. Formally, we refer to Dk as a dataset with Nk samples for task k. Specifically,\nDk = {(xki , yki )} Nk i=1 (7)\nwhere xki and y k i denote a sentence and corresponding label for task k.\n3.1 Two Sharing Schemes for Sentence Modeling\nThe key factor of multi-task learning is the sharing scheme in latent feature space. In neural network based model, the latent features can be regarded as the states of hidden neurons. Specific to text classification, the latent features are the hidden states of LSTM at the end of a sentence. Therefore, the sharing schemes are different in how to group the shared features. Here, we first introduce two sharing schemes with multi-task learning: fully-shared scheme and shared-private scheme.\nFully-SharedModel (FS-MTL) In fully-shared model, we use a single shared LSTM layer to extract features for all the tasks. For example, given two tasks m and n, it takes the view that the features of taskm can be totally shared by task n and vice versa. This model ignores the fact that some features are task-dependent. Figure 2a illustrates the fully-shared model.\nShared-Private Model (SP-MTL) As shown in Figure 2b, the shared-private model introduces two feature spaces for each task: one is used to store task-dependent features, the other is used\nto capture task-invariant features. Accordingly, we can see each task is assigned a private LSTM layer and shared LSTM layer. Formally, for any sentence in task k, we can compute its shared representation skt and task-specific representation h k t as follows:\nskt = LSTM(xt, s k t\u22121, \u03b8s), (8)\nhkt = LSTM(xt,h m t\u22121, \u03b8k) (9)\nwhere LSTM(., \u03b8) is defined as Eq. (4). The final features are concatenation of the features from private space and shared space.\n3.2 Task-Specific Output Layer\nFor a sentence in task k, its feature h(k), emitted by the deep muti-task architectures, is ultimately fed into the corresponding task-specific softmax layer for classification or other tasks. The parameters of the network are trained to minimise the cross-entropy of the predicted and true distributions on all the tasks. The loss Ltask can be computed as:\nLTask = K\u2211 k=1 \u03b1kL(y\u0302 (k), y(k)) (10)\nwhere \u03b1k is the weights for each task k respectively. L(y\u0302, y) is defined as Eq. 6.\n4 Incorporating Adversarial Training\nAlthough the shared-private model separates the feature space into the shared and private spaces, there is no guarantee that sharable features can not exist in private feature space, or vice versa. Thus, some useful sharable features could be ignored in shared-private model, and the shared feature space is also vulnerable to contamination by some taskspecific information. Therefore, a simple principle can be applied into multi-task learning that a good shared feature space should contain more common information and no task-specific information. To address this problem, we introduce adversarial training into multi-task framework as shown in Figure 3 (ASPMTL).\n4.1 Adversarial Network\nAdversarial networks have recently surfaced and are first used for generative model (Goodfellow\n4\n301\n302\n303\n304\n305\n306\n307\n308\n309\n310\n311\n312\n313\n314\n315\n316\n317\n318\n319\n320\n321\n322\n323\n324\n325\n326\n327\n328\n329\n330\n331\n332\n333\n334\n335\n336\n337\n338\n339\n340\n341\n342\n343\n344\n345\n346\n347\n348\n349\n350\n351\n352\n353\n354\n355\n356\n357\n358\n359\n360\n361\n362\n363\n364\n365\n366\n367\n368\n369\n370\n371\n372\n373\n374\n375\n376\n377\n378\n379\n380\n381\n382\n383\n384\n385\n386\n387\n388\n389\n390\n391\n392\n393\n394\n395\n396\n397\n398\n399\nxm\nxn\nLSTM\nLSTM\nLSTM\nLDiff LAdvLDiff\nsoftmax\nsoftmax\nLmtask\nLntask\nFigure 3: Adversarial shared-private model. Yellow and gray boxes represent shared and private LSTM layers respectively.\net al., 2014). The goal is to learn a generative distribution pG(x) that matches the real data distribution Pdata(x) Specifically, GAN learns a generative network G and discriminative model D, in which G generates samples from the generator distribution pG(x). and D learns to determine whether a sample is from pG(x) or Pdata(x). This min-max game can be optimized by the following risk:\n\u03d5 = min G max D\n( Ex\u223cPdata [logD(x)]\n+ Ez\u223cp(z)[log(1\u2212D(G(z)))] )\n(11)\nWhile originally proposed for generating random samples, adversarial network can be used as a general tool to measure equivalence between distributions (Taigman et al., 2016). Formally, (Ajakan et al., 2014) linked the adversarial loss to the H-divergence between two distributions and successfully achieve unsupervised domain adaptation with adversarial network. Motivated by theory on domain adaptation (Ben-David et al., 2010, 2007; Bousmalis et al., 2016) that a transferable feature is one for which an algorithm cannot learn to identify the domain of origin of the input observation.\n4.2 Task Adversarial Loss for MTL\nInspired by adversarial networks (Goodfellow et al., 2014), we proposed an adversarial sharedprivate model for multi-task learning, in which a shared recurrent neural layer is working adversarially towards a learnable multi-layer perceptron, preventing it from making an accurate prediction about the types of tasks. This adversarial training encourages shared space to be more pure and ensure the shared representation not be contaminated by task-specific features.\nTask Discriminator Discriminator is used to map the shared representation of sentences into a\nprobability distribution, estimating what kinds of tasks the encoded sentence comes from.\nD(skT , \u03b8D) = softmax(b+Us k T ) (12)\nwhereU \u2208 Rd\u00d7d is a learnable parameter and b \u2208 Rd is a bias.\nAdversarial Loss Different with most existing multi-task learning algorithm, we add an extra task adversarial loss LAdv to prevent task-specific feature from creeping in to shared space. The task adversarial loss is used to train a model to produce shared features such that a classifier cannot reliably predict the task based on these features. The original loss of adversarial network is limited since it can only be used in binary situation. To overcome this, we extend it to multi-class form, which allow our model can be trained together with multiple tasks:\nLAdv = min \u03b8s\n( \u03bbmax\n\u03b8D ( K\u2211 k=1 Nk\u2211 i=1 dki log[D(E(x k))])\n) (13)\nwhere dki denotes the ground-truth label indicating the type of the current task. Here, there is a minmax optimization and the basic idea is that, given a sentence, the shared LSTM generates a representation to mislead the task discriminator. At the same time, the discriminator tries its best to make a correct classification on the type of task. After the training phase, the shared feature extractor and task discriminator reach a point at which both cannot improve and the discriminator is unable to differentiate among all the tasks.\nSemi-supervised Learning Multi-task Learning We notice that the LAdv requires only the input sentence x and does not require the corresponding label y, which makes it possible to combine our model with semi-supervised learning. Finally, in this semi-supervised multi-task learning framework, our model can not only utilize the data from related tasks, but can employ abundant unlabeled corpora.\n4.3 Orthogonality Constraints\nWe notice that there is a potential drawback of the above model. That is, the task-invariant features can appear both in shared space and private space. Motivated by recently work(Jia et al., 2010; Salzmann et al., 2010; Bousmalis et al., 2016)\n5\n401\n402\n403\n404\n405\n406\n407\n408\n409\n410\n411\n412\n413\n414\n415\n416\n417\n418\n419\n420\n421\n422\n423\n424\n425\n426\n427\n428\n429\n430\n431\n432\n433\n434\n435\n436\n437\n438\n439\n440\n441\n442\n443\n444\n445\n446\n447\n448\n449\n450\n451\n452\n453\n454\n455\n456\n457\n458\n459\n460\n461\n462\n463\n464\n465\n466\n467\n468\n469\n470\n471\n472\n473\n474\n475\n476\n477\n478\n479\n480\n481\n482\n483\n484\n485\n486\n487\n488\n489\n490\n491\n492\n493\n494\n495\n496\n497\n498\n499\nDataset Train Dev. Test Unlab. Avg. L Vocab.\nBooks 1400 200 400 2000 159 62K Elec. 1398 200 400 2000 101 30K DVD 1400 200 400 2000 173 69K Kitchen 1400 200 400 2000 89 28K Apparel 1400 200 400 2000 57 21K Camera 1397 200 400 2000 130 26K Health 1400 200 400 2000 81 26K Music 1400 200 400 2000 136 60K Toys 1400 200 400 2000 90 28K Video 1400 200 400 2000 156 57K Baby 1300 200 400 2000 104 26K Mag. 1370 200 400 2000 117 30K Soft. 1315 200 400 2000 129 26K Sports 1400 200 400 2000 94 30K IMDB 1400 200 400 2000 269 44K MR 1400 200 400 2000 21 12K\nTable 1: Statistics of the 16 datasets. The columns 2-5 denote the number of samples in training, development, test and unlabeled sets. The last two columns represent the average length and vocabulary size of corresponding dataset.\non shared-private latent space analysis, we introduce orthogonality constraints, which penalize redundant latent representations and encourages the shared and private extractors to encode different aspects of the inputs.\nAfter exploring many optional methods, we find below loss is optimal, which is used by Bousmalis et al. (2016) and achieve a better performance:\nLdiff = K\u2211 k=1 \u2225\u2225\u2225Sk\u22a4Hk\u2225\u2225\u22252 F , (14)\nwhere \u2225 \u00b7 \u22252F is the squared Frobenius norm. S k and Hk are two matrics, whose rows are the output of shared extractor Es(, ; \u03b8s) and task-specific extrator Ek(, ; \u03b8k) of a input sentence.\n4.4 Put It All Together\nThe final loss function of our model can be written as:\nL = LTask + \u03bbLAdv + \u03b3LDiff (15)\nwhere \u03bb and \u03b3 are hyper-parameter. The networks are trained with backpropagation and this minimax optimization becomes possible via the use of a gradient reversal layer (Ganin and Lempitsky, 2015).\n5 Experiment\n5.1 Dataset\nTo make an extensive evaluation, we collect 16 different datasets from several popular review corpora. The first 14 datasets are product reviews, which contain Amazon product reviews from different domains, such as Books, DVDs, Electronics, ect. The goal is to classify a product review as either positive or negative. These datasets are collected based on the raw data 1 provided by (Blitzer et al., 2007). Specifically, we extract the sentences and corresponding labels from the unprocessed original data 2. The only preprocessing operation of these sentences is tokenized using the Stanford tokenizer 3. The remaining two datasets are about movie reviews. The IMDB dataset4 consists of movie reviews with binary classes (Maas et al., 2011). One key aspect of this dataset is that each movie review has several sentences. The MR dataset also consists of movie reviews from rotten tomato website with two classes 5(Pang and Lee, 2005). All the datasets in each task are partitioned randomly into training set, development set and testing set with the proportion of 70%, 20% and 10% respectively. The detailed statistics about all the datasets are listed in Table 1.\n5.2 Competitor Methods for Multi-task Learning\nThe multi-task frameworks proposed by previous works are various while not all can be applied to the tasks we focused. Nevertheless, we chose two most related neural models for multi-task learning and implement them as competitor methods.\n\u2022 MT-CNN: This model is proposed by Collobert and Weston (2008) with convolutional layer, in which lookup-tables are shared partially while other layers are task-specific.\n1https://www.cs.jhu.edu/\u02dcmdredze/ datasets/sentiment/\n2Blitzer et al. (2007) also provides two extra processed datasets with the format of Bag-of-Words, which are not proper for neural-based models.\n3http://nlp.stanford.edu/software/ tokenizer.shtml\n4https://www.cs.jhu.edu/\u02dcmdredze/ datasets/sentiment/unprocessed.tar.gz\n5https://www.cs.cornell.edu/people/ pabo/movie-review-data/.\n6\n501\n502\n503\n504\n505\n506\n507\n508\n509\n510\n511\n512\n513\n514\n515\n516\n517\n518\n519\n520\n521\n522\n523\n524\n525\n526\n527\n528\n529\n530\n531\n532\n533\n534\n535\n536\n537\n538\n539\n540\n541\n542\n543\n544\n545\n546\n547\n548\n549\n550\n551\n552\n553\n554\n555\n556\n557\n558\n559\n560\n561\n562\n563\n564\n565\n566\n567\n568\n569\n570\n571\n572\n573\n574\n575\n576\n577\n578\n579\n580\n581\n582\n583\n584\n585\n586\n587\n588\n589\n590\n591\n592\n593\n594\n595\n596\n597\n598\n599\nTask Single Task Multiple Tasks\nLSTM BiLSTM sLSTM Avg. MT-DNN MT-CNN FS-MTL SP-MTL ASP-MTL\nBooks 20.5 19.0 18.0 19.2 17.7(\u22121.5) 15.6(\u22123.6) 17.5(\u22121.7) 18.7(\u22120.5) 13.0(\u22126.2) Electronics 19.5 21.5 23.3 21.4 18.2(\u22123.2) 16.9(\u22124.5) 14.3(\u22127.1) 12.3(\u22129.1) 11.0(\u221210.4) DVD 18.3 19.5 22.0 19.9 15.8(\u22124.1) 16.1(\u22123.8) 16.5(\u22123.4) 16.1(\u22123.8) 12.6(\u22127.3) Kitchen 22.0 18.8 19.5 20.1 19.2(\u22120.9) 16.8(\u22123.3) 14.0(\u22126.1) 14.8(\u22125.3) 12.8(\u22127.3) Apparel 16.8 14.0 16.3 15.7 14.9(\u22120.8) 16.1(+0.4) 15.5(\u22120.2) 13.4(\u22122.3) 11.3(\u22124.4) Camera 14.8 14.0 15.0 14.6 13.7(\u22120.9) 14.0(\u22120.6) 13.5(\u22121.1) 12.1(\u22122.5) 8.7(\u22125.9) Health 15.5 21.3 16.5 17.8 14.3(\u22123.5) 12.9(\u22124.9) 12.0(\u22125.8) 12.8(\u22125.0) 10.9(\u22126.9) Music 23.3 22.8 23.0 23.0 15.3(\u22127.7) 16.3(\u22126.7) 18.8(\u22124.2) 17.0(\u22126.0) 17.4(\u22125.6) Toys 16.8 15.3 16.8 16.3 12.1(\u22124.2) 10.9(\u22125.4) 15.5(\u22120.8) 14.9(\u22121.4) 11.2(\u22125.1) Video 18.5 16.3 16.3 17.0 15.0(\u22122.0) 18.7(+1.7) 16.3(\u22120.7) 16.8(\u22120.2) 14.5(\u22122.5) Baby 15.3 16.5 15.8 15.9 12.1(\u22123.8) 12.4(\u22123.5) 12.0(\u22123.9) 13.2(\u22122.7) 10.2(\u22125.7) Magazines 10.8 8.5 12.3 10.5 10.6(+0.1) 12.3(+1.8) 7.5(\u22123.0) 8.1(\u22122.4) 7.6(\u22122.9) Software 15.3 14.3 14.5 14.7 14.4(\u22120.3) 13.4(\u22121.3) 13.8(\u22120.9) 13.1(\u22121.6) 12.7(\u22122.0) Sports 18.3 16.0 17.5 17.3 16.8(\u22120.5) 16.1(\u22121.2) 14.5(\u22122.8) 12.7(\u22124.6) 13.3(\u22124.0) IMDB 18.3 15.0 18.5 17.3 16.7(\u22120.6) 13.7(\u22123.6) 17.5(+0.2) 15.2(\u22122.1) 14.2(\u22123.1) MR 27.3 25.3 28.0 26.9 24.5(\u22122.4) 25.5(\u22121.4) 25.3(\u22121.6) 24.1(\u22122.8) 22.7(\u22124.2)\nAVG 18.2 17.4 18.3 18.0 15.7(\u22122.3) 15.5(\u22122.5) 15.3(\u22122.7) 14.7(\u22123.3) 12.8(\u22125.2)\nTable 2: Error rates of our models on 16 datasets against typical baselines. The numbers in brackets represent the improvements relative to the average performance (Avg.) of three single task baselines.\n\u2022 MT-DNN: The model is proposed by Liu et al. (2015) with bag-of-words input and multi-layer perceptrons, in which a hidden layer is shared.\n5.3 Hyperparameters\nThe word embeddings for all of the models are initialized with the 200d GloVe vectors (840B token version, (Pennington et al., 2014)). The other parameters are initialized by randomly sampling from uniform distribution in [\u22120.1, 0.1]. The minibatch size is set to 16.\nFor each task, we take the hyperparameters which achieve the best performance on the development set via an small grid search over combinations of the initial learning rate [0.1, 0.01], \u03bb \u2208 [0.01, 0.1], and \u03b3 \u2208 [0.01, 0.1]. Finally, we chose the learning rate as 0.01, \u03bb as 0.05 and \u03b3 as 0.01.\n5.4 Performance Evaluation\nTable 2 shows the error rates on 16 text classification tasks. The column of \u201cSingle Task\u201d shows the results of vanilla LSTM, bidirectional LSTM (BiLSTM), stacked LSTM (sLSTM) and the average error rates of previous three models. The column of \u201cMultiple Tasks\u201d shows the results achieved by corresponding multi-task models. From this table, we can see that the performance of most tasks can be improved with a large margin with the help of multi-task learning, in which our model achieves the lowest error rates.\nMore concretely, compared with SP-MTL, ASPMTL achieves 5.2% average improvement surpassing SP-MTL with 1.9%, which indicates the importance of adversarial learning. It is noteworthy that for FS-MTL, the performances of some tasks are degraded, since this model puts all private and shared information into a unified space.\n5.5 Shared Knowledge Transfer\nWith the help of adversarial learning, the shared feature extractor Es can generate more pure taskinvariant representations, which can be considered as off-the-shelf knowledge and then be used for unseen new tasks. To test the transferability of our learned shared extractor, we also design an experiment, in which we take turns choosing 15 tasks to train our model MS with multi-task learning, then the learned shared layer are transferred to a second network MT that is used for the remaining one task. The parameters of transferred layer are kept frozen, and the rest of parameters of the network MT are randomly initialized. More formally, we investigate two mechanisms towards the transferred shared extractor. As shown in Figure 4. The first one Single Channel (SC) model consists of one shared feature extractor Es from MS , then the extracted representation will be sent to an output layer. By contrast, the BiChannel (BC) model introduces an extra LSTM layer to encode more task-specific information. To evaluate the effectiveness of our introduced adver-\n7\n601\n602\n603\n604\n605\n606\n607\n608\n609\n610\n611\n612\n613\n614\n615\n616\n617\n618\n619\n620\n621\n622\n623\n624\n625\n626\n627\n628\n629\n630\n631\n632\n633\n634\n635\n636\n637\n638\n639\n640\n641\n642\n643\n644\n645\n646\n647\n648\n649\n650\n651\n652\n653\n654\n655\n656\n657\n658\n659\n660\n661\n662\n663\n664\n665\n666\n667\n668\n669\n670\n671\n672\n673\n674\n675\n676\n677\n678\n679\n680\n681\n682\n683\n684\n685\n686\n687\n688\n689\n690\n691\n692\n693\n694\n695\n696\n697\n698\n699\nSource Tasks Single Task Transfer Models\nLSTM BiLSTM sLSTM Avg. SP-MTL-SC SP-MTL-BC ASP-MTL-SC ASP-MTL-BC\n\u03d5 (Books) 20.5 19.0 18.0 19.2 17.8(\u22123.6) 16.2(\u22123.0) 16.7(\u22122.5) 13.3(\u22125.9) \u03d5 (Electronics) 19.5 21.5 23.3 21.4 15.1(\u22124.5) 14.6(\u22126.8) 15.1(\u22126.3) 14.9(\u22126.5) \u03d5 (DVD) 18.3 19.5 22.0 19.9 14.7(\u22123.8) 15.5(\u22124.4) 12.1(\u22127.8) 12.4(\u22127.5) \u03d5 (Kitchen) 22.0 18.8 19.5 20.1 15.0(\u22123.3) 16.6(\u22123.5) 14.6(\u22125.5) 14.1(\u22126.0) \u03d5 (Apparel) 16.8 14.0 16.3 15.7 14.9(+0.4) 12.3(\u22123.4) 11.6(\u22124.1) 13.6(\u22122.1) \u03d5 (Camera) 14.8 14.0 15.0 14.6 13.1(\u22120.6) 12.1(\u22122.5) 11.6(\u22123.0) 10.3(\u22124.3) \u03d5 (Health) 15.5 21.3 16.5 17.8 14.1(\u22124.9) 14.2(\u22123.6) 12.2(\u22125.6) 10.5(\u22127.3) \u03d5 (Music) 23.3 22.8 23.0 23.0 19.9(\u22126.7) 17.9(\u22125.1) 16.4(\u22126.6) 18.2(\u22124.8) \u03d5 (Toys) 16.8 15.3 16.8 16.3 13.8(\u22125.4) 12.2(\u22124.1) 13.0(\u22124.7) 11.2(\u22125.1) \u03d5 (Video) 18.5 16.3 16.3 17.0 14.2(+1.7) 15.1(\u22121.9) 14.8(\u22122.2) 14.8(\u22122.2) \u03d5 (Baby) 15.3 16.5 15.8 15.9 16.6(\u22123.5) 16.9(+1.0) 11.5(\u22124.4) 10.0(\u22125.9) \u03d5 (Magazines) 10.8 8.5 12.3 10.5 10.6(+1.8) 10.2(\u22120.3) 8.6(\u22121.9) 9.7(\u22120.8) \u03d5 (Software) 15.3 14.3 14.5 14.7 13.0(\u22121.3) 12.7(\u22122.0) 14.3(\u22120.4) 11.1(\u22123.6) \u03d5 (Sports) 18.3 16.0 17.5 17.3 16.3(\u22121.2) 16.2(\u22121.1) 13.4(\u22123.9) 13.6(\u22123.7) \u03d5 (IMDB) 18.3 15.0 18.5 17.3 12.4(\u22123.6) 12.8(\u22124.5) 12.5(\u22124.8) 13.3(\u22124.0) \u03d5 (MR) 27.3 25.3 28.0 26.9 26.0(\u22121.4) 26.5(\u22120.4) 22.7(\u22124.2) 23.5(\u22123.4)\nAVG 18.2 17.4 18.3 18.0 15.5(\u22122.5) 15.1(\u22122.9) 13.6(\u22124.2) 13.4(\u22124.6)\nTable 3: Error rates of our models on 16 datasets against vanilla multi-task learning. \u03d5 (Books) means that we transfer the knowledge of the other 15 tasks to the target task Books.\nxt LSTM softmax\nEs\n(a) Single Channel\nxt LSTM\nLSTM\nsoftmax\nEs\n(b) Bi-Channel\nFigure 4: Two transfer strategies using a pretrained shared LSTM layer. Yellow box denotes shared feature extractor Es trained by 15 tasks.\nsarial training framework, we also make a comparison with vanilla multi-task learning method.\nResults and Analysis As shown in Table 3, we can see the shared layer from ASP-MTL achieves a better performance compared with SP-MTL. Besides, for the two kinds of transfer strategies, the Bi-Channel model performs better. The reason is that the task-specific layer introduced in the BiChannel model can store some private features. Overall, the results indicate that we can save the existing knowledge into a shared recurrent layer using adversarial multi-task learning, which is quite useful for a new task.\n5.6 Visualization\nTo get an intuitive understanding of how the introduced orthogonality constraints worked compared with vanilla shared-private model, we design an experiment to examine the behaviors of neurons from private layer and shared layer. More concretely, we refer to htj as the activation of the j-\nneuron at time step t, where t \u2208 {1, . . . , n} and j \u2208 {1, . . . , d}. By visualizing the hidden state hj and analyzing the maximum activation, we can find what kinds of patterns the current neuron focuses on. Figure 5 illustrates this phenomenon. Here, we randomly sample a sentence from the validation set of Baby task and analyze the changes of the predicted sentiment score at different time steps, which are obtained by SP-MTL and our proposed model. Additionally, to get more insights into how neurons in shared layer behave diversely towards different input word, we visualize the activation of two typical neurons. For the positive sentence \u201cFive stars, my baby can fall asleep soon in the stroller\u201d, both models capture the informative pattern \u201cFive stars\u201d 6. However, SP-MTL makes a wrong prediction due to misunderstanding of the word \u201casleep\u201d. By contrast, our model makes a correct prediction and the reason can be inferred from the activation of Figure 5-(b), where the shared layer of SP-MTL is so sensitive that many features related to other tasks are included, such as \u201dasleep\u201d, which misleads the final prediction. This indicates the importance of introducing adversarial learning to prevent the shared layer from being contaminated by task-specific features.\n6For this case, the vanilla LSTM also give a wrong answer due to ignoring the feature \u201cFive stars\u201d.\n8\n701\n702\n703\n704\n705\n706\n707\n708\n709\n710\n711\n712\n713\n714\n715\n716\n717\n718\n719\n720\n721\n722\n723\n724\n725\n726\n727\n728\n729\n730\n731\n732\n733\n734\n735\n736\n737\n738\n739\n740\n741\n742\n743\n744\n745\n746\n747\n748\n749\n750\n751\n752\n753\n754\n755\n756\n757\n758\n759\n760\n761\n762\n763\n764\n765\n766\n767\n768\n769\n770\n771\n772\n773\n774\n775\n776\n777\n778\n779\n780\n781\n782\n783\n784\n785\n786\n787\n788\n789\n790\n791\n792\n793\n794\n795\n796\n797\n798\n799\nFive stars , my baby can fall asleep soon in the stroller 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 SP-MTL Ours\n(a) Predicted Sentiment Score by Two Models (b) Behaviours of Neuron hs18 and h s 21\nFigure 5: (a) The change of the predicted sentiment score at different time steps. Y-axis represents the sentiment score, while X-axis represents the input words in chronological order. The darker grey horizontal line gives a border between the positive and negative sentiments. (b) The blue heat map describes the behaviour of neuron hs18 from shared layer of SP-MTL, while the purple one is used to show the behaviour of neuron hs21, which belongs to the shared layer of our model.\nModel Shared Layer Task-Movie Task-Baby\nSP-MTL\ngood, great bad, love, simple, cut, slow, cheap, infantile good, great, well-directed, pointless, cut, cheap, infantile love, bad, cute, safety, mild, broken simple\nASP-MTL good, great, love, bad poor well-directed, pointless, cut, cheap, infantile cute, safety, mild, broken simple\nTable 4: Typical patterns captured by shared layer and task-specific layer of SP-MTL and ASP-MTL models on Movie and Baby tasks.\nWe also list some typical patterns captured by neurons from shared layer and task-specific layer in Table 4, and we have observed that: 1) for SP-MTL, if some patterns are captured by taskspecific layer, they are likely to be placed into shared space. Clearly, suppose we have many tasks to be trained jointly, the shared layer bear much pressure and must sacrifice substantial amount of capacity to capture the patterns they actually do not need. Furthermore, some typical taskinvariant features also go into task-specific layer. 2) for ASP-MTL, we find the features captured by shared and task-specific layer have a small amount of intersection, which allows these two kinds of layers can work effectively.\n6 Related Work\nThere are two threads of related work. One thread is multi-task learning with neural network. Neural networks based multi-task learning has been proven effective in many NLP problems (Collobert and Weston, 2008; Glorot et al., 2011; Liu et al., 2015, 2016). In most of these models, the\nlower layers are shared across all tasks, while top layers are task-specific. These work has potential limitation of just learning a shared space solely on sharing parameters, while our model introduce two strategies to learn the clear and non-redundant shared-private space. Another thread of work is adversarial network. Adversarial networks have recently surfaced as a general tool measure equivalence between distributions and it has proven to be effective in a variety of tasks. Ajakan et al. (2014); Bousmalis et al. (2016) applied adverarial training to domain adaptation, aiming at transferring the knowledge of one source domain to target domain. Park and Im (2016) proposed a novel approach for multimodal representation learning which uses adversarial back-propagation concept. Different from these models, our model aims to find task-invariant sharable information for multiple related tasks using adversarial training strategy. Moreover, we extend binary adversarial training to multi-class, which enable multiple tasks to be jointly trained.\n7 Conclusion\nIn this paper, we have proposed an adversarial multi-task learning framework, in which the taskspecific and task-invariant features are learned non-redundantly, therefore capturing the sharedprivate separation of different tasks. We have demonstrated the effectiveness of our approach by applying our model to 16 different text classification tasks. We also perform extensive qualitative analysis, deriving insights and indirectly explaining the quantitative improvements in the overall performance.\n9\n801\n802\n803\n804\n805\n806\n807\n808\n809\n810\n811\n812\n813\n814\n815\n816\n817\n818\n819\n820\n821\n822\n823\n824\n825\n826\n827\n828\n829\n830\n831\n832\n833\n834\n835\n836\n837\n838\n839\n840\n841\n842\n843\n844\n845\n846\n847\n848\n849\n850\n851\n852\n853\n854\n855\n856\n857\n858\n859\n860\n861\n862\n863\n864\n865\n866\n867\n868\n869\n870\n871\n872\n873\n874\n875\n876\n877\n878\n879\n880\n881\n882\n883\n884\n885\n886\n887\n888\n889\n890\n891\n892\n893\n894\n895\n896\n897\n898\n899\n", "cats": {"READ": 1.0, "DONTREAD": 0.0}, "reviews": [{"IMPACT": "3", "SUBSTANCE": "4", "APPROPRIATENESS": "5", "MEANINGFUL_COMPARISON": "3", "PRESENTATION_FORMAT": "Oral Presentation", "comments": "This paper introduces new configurations and training objectives for neural\nsequence models in a multi-task setting. As the authors describe well, the\nmulti-task setting is important because some tasks have shared information\nand in some scenarios learning many tasks can improve overall performance.\n\nThe methods section is relatively clear and logical, and I like where it ended\nup, though it could be slightly better organized. The organization that I\nrealized after reading is that there are two problems: 1) shared features end\nup in the private feature space, and 2) private features end up in the \nshared space. There is one novel method for each problem. That organization up\nfront would make the methods more cohesive. In any case, they introduce one \nmethod that keeps task-specific features out of shared representation\n(adversarial\nloss) and another to keep shared features out of task-specific representations\n(orthogonality constraints). My only point of confusion is the adversarial\nsystem.\nAfter LSTM output there is another layer, D(s^k_T, \\theta_D), relying on\nparameters\nU and b. This output is considered a probability distribution which is compared\nagainst the actual. This means it is possible it will just learn U and b that\neffectively mask task-specific information from  the LSTM outputs, and doesn't \nseem like it can guarantee task-specific information is removed.\n\nBefore I read the evaluation section I wrote down what I hoped the experiments\nwould look like and it did most of it. This is an interesting idea and there\nare \na lot more experiments one can imagine but I think here they have the basics\nto show the validity of their methods. It would be helpful to have best known\nresults on these tasks.\n\nMy primary concern with this paper is the lack of deeper motivation for the \napproach. I think it is easy to understand that in a totally shared model\nthere will be problems due to conflicts in feature space. The extension to \npartially shared features seems like a reaction to that issue -- one would \nexpect that the useful shared information is in the shared latent space and \neach task-specific space would learn features for that space. Maybe this works\nand maybe it doesn't, but the logic is clear to me. In contrast, the authors\nseem to start from the assumption that this \"shared-private\" model has this\nissue. I expected the argument flow to be 1) Fully-shared obviously has this\nproblem; 2) shared-private seems to address this; 3) in practice shared-private\ndoes not fully address this issue for reasons a,b,c.; 4) we introduce a method\nthat more effectively constrains the spaces.\nTable 4 helped me to partially understand what's going wrong with\nshared-private\nand what your methods do; some terms are _usually_ one connotation\nor another, and that general trend can probably get them into the shared\nfeature\nspace. This simple explanation, an example, and a more logical argument flow\nwould help the introduction and make this a really nice reading paper.\n\nFinally, I think this research ties into some other uncited MTL work [1],\nwhich does deep hierarchical MTL - supervised POS tagging at a lower level,\nchunking\nat the next level up, ccg tagging higher, etc. They then discuss at the end\nsome of the qualities that make MTL possible and conclude that MTL only works\n\"when tasks are sufficiently similar.\" The ASP-MTL paper made me think of this\nprevious work because potentially this model could learn what sufficiently\nsimilar is -- i.e., if two tasks are not sufficiently similar the shared model\nwould learn nothing and it would fall back to learning two independent systems,\nas compared to a shared-private model baseline that might overfit and perform\npoorly.\n\n[1]\n@inproceedings{sogaard2016deep,\n  title={Deep multi-task learning with low level tasks supervised at lower\nlayers},\n  author={S{\\o}gaard, Anders and Goldberg, Yoav},\n  booktitle={Proceedings of the 54th Annual Meeting of the Association for\nComputational Linguistics},\n  volume={2},\n  pages={231--235},\n  year={2016},\n  organization={Association for Computational Linguistics}\n}", "SOUNDNESS_CORRECTNESS": "5", "ORIGINALITY": "5", "is_meta_review": null, "RECOMMENDATION": "4", "CLARITY": "4", "REVIEWER_CONFIDENCE": "2"}, {"IMPACT": "3", "SUBSTANCE": "5", "APPROPRIATENESS": "5", "MEANINGFUL_COMPARISON": "3", "PRESENTATION_FORMAT": "Oral Presentation", "comments": "# Paper summary\n\nThis paper presents a method for learning well-partitioned shared and\ntask-specific feature spaces for LSTM text classifiers. Multiclass adversarial\ntraining encourages shared space representations from which a discriminative\nclassifier cannot identify the task source (and are thus generic). The models\nevaluates are a fully-shared, shared-private and adversarial shared-private --\nthe lattermost ASP model is one of the main contributions. They also use\northogonality constraints to help reward shared and private spaces that are\ndistinct. The ASP model has lower error rate than single-task and other\nmulti-task neural models. They also experiment with a task-level cross\nvalidation to explore whether the shared representation can transfer across\ntasks, and it seems to favourably. Finally, there is some analysis of shared\nlayer activations suggesting that the ASP model is not being misled by strong\nweights learned on a specific (inappropriate) task.\n\n# Review summary\n\nGood ideas, well expressed and tested. Some minor comments.\n\n# Strengths\n\n* This is a nice set of ideas working well together. I particularly like the\nfocus on explicitly trying to create useful shared representations. These have\nbeen quite successful in the CV community, but it appears that one needs to\nwork quite hard to create them for NLP.\n* Sections 2, 3 and 4 are very clearly expressed.\n* The task-level cross-validation in Section 5.5 is a good way to evaluate the\ntransfer.\n* There is an implementation and data.\n\n# Weaknesses\n\n* There are a few minor typographic and phrasing errors. Individually, these\nare fine, but there are enough of them to warrant fixing:\n** l:84 the \u201cinfantile cart\u201d is slightly odd -- was this a real example\nfrom the data?\n** l:233 \u201care different in\u201d -> \u201cdiffer in\u201d\n** l:341 \u201cworking adversarially towards\u201d -> \u201cworking against\u201d or\n\u201ccompeting with\u201d?\n** l:434 \u201ctwo matrics\u201d -> \u201ctwo matrices\u201d\n** l:445 \u201care hyperparameter\u201d -> \u201care hyperparameters\u201d\n** Section 6 has a number of number agreement errors\n(l:745/746/765/766/767/770/784) and should be closely re-edited.\n** The shading on the final row of Tables 2 and 3 prints strangely\u2026\n* There is mention of unlabelled data in Table 1 and semi-supervised learning\nin Section 4.2, but I didn\u2019t see any results on these experiments. Were they\nomitted, or have I misunderstood?\n* The error rate differences are promising in Tables 2 and 3, but statistical\nsignificance testing would help make them really convincing. Especially between\nSP-MLT and ASP-MTL results to highlight the benefit of adversarial training. It\nshould be pretty straightforward to adapt the non-parametric approximate\nrandomisation test (see\nhttp://www.lr.pi.titech.ac.jp/~takamura/pubs/randtest.pdf for promising notes a\nreference to the Chinchor paper) to produce these.\n* The colours are inconsistent in the caption of Figure 5 (b). In 5 (a), blue\nis used for \u201cOurs\u201d, but this seems to have swapped for 5 (b). This is worth\nchecking, or I may have misunderstood the caption.\n\n# General Discussion\n\n* I wonder if there\u2019s some connection with regularisation here, as the effect\nof the adversarial training with orthogonal training is to help limit the\nshared feature space. It might be worth drawing that connection to other\nregularisation literature.", "SOUNDNESS_CORRECTNESS": "5", "ORIGINALITY": "5", "is_meta_review": null, "RECOMMENDATION": "4", "CLARITY": "4", "REVIEWER_CONFIDENCE": "3"}]}
{"text": "1 INTRODUCTION\nProduct classification is a key issue in e-commerce domains. A product is typically represented by metadata such as its title, image, color, weight and so on, and most of them are assigned manually by the seller. Once a product is uploaded to an e-commerce website, it is typically placed in multiple categories. Categorizing products helps e-commerce websites to provide costumers a better shopping experience, for example by efficiently searching the products catalog or by developing recommendation systems. A few examples of categories are internal taxonomies (for business needs), public taxonomies (such as groceries and office equipment) and the product\u2019s shelf (a group of products that are presented together on an e-commerce web page). These categories vary with time in order to optimize search efficiency and to account for special events such as holidays and sports events. In order to address these needs, e-commerce websites typically hire editors and use crowdsourcing platforms to classify products. However, due to the high amount of new products uploaded daily and the dynamic nature of the categories, machine learning solutions for product classification are very appealing as means to reduce the time and economic costs. Thus, precisely categorizing items emerges as a significant issue in e-commerce domains.\nA shelf is a group of products presented together on an e-commerce website page, and usually contain products with a given theme/category (e.g., Women boots, folding tables). Product to shelf classification is a challenging problem due to data size, category skewness, and noisy metadata and labels. In particular, it presents three fundamental challenges for machine learning algorithms. First, it is typically a multi-class problem with thousands of classes. Second, a product may belong to multiple shelves making it a multi-label problem. And last, a product has both an image and a text input making it a multi-modal problem.\nProducts classification is typically addressed as a text classification problem because most metadata of items are represented as textual features (Pyo et al., 2010). Text classification is a classic topic for natural language processing, in which one needs to assign predefined categories to text inputs.\nStandard methods follow a classical two-stage scheme of extraction of (handcrafted) features, followed by a classification stage. Typical features include bag-of-words or n-grams, and their TF-IDF. On the other hand, Deep Neural Networks use generic priors instead of specific domain knowledge (Bengio et al., 2013) and have been shown to give competitive results on text classification tasks (Zhang et al., 2015). In particular, Convolutional neural networks (CNNs) (Kim, 2014; Zhang et al., 2015; Conneau et al., 2016) and Recurrent NNs (Lai et al., 2015; Pyo et al., 2010; Xiao & Cho, 2016) can efficiently capture the sequentiality of the text. These methods are typically applied directly to distributed embedding of words (Kim, 2014; Lai et al., 2015; Pyo et al., 2010) or characters (Zhang et al., 2015; Conneau et al., 2016; Xiao & Cho, 2016), without any knowledge on the syntactic or semantic structures of a language. However, all of these architectures were only applied on problems with a small amount of labels (\u223c 20) while e-commerce shelf classification problems typically have thousands of labels with multiple labels per product.\nIn Image classification, CNNs are widely considered the best models, and achieve state-of-theart results on the ImageNet Large-Scale Visual Recognition Challenge (Russakovsky et al., 2015; Krizhevsky et al., 2012; Simonyan & Zisserman, 2014; He et al., 2015). However, as good as they are, the classification accuracy of machine learning systems is often limited in problems with many classes of object categories. One remedy is to leverage data from other sources, such as text data. However, the studies on multi-modal deep learning for large-scale item categorization are still rare to the best of our belief. In particular in a setting where there is a significant difference in discriminative power between the two types of signals.\nIn this work, we propose a multi-modal deep neural network model for product classification. Our design principle is to leverage the specific prior for each data type by using the current state-of-\nthe-art classifiers from the image and text domains. The final architecture has 3 main components (Figure 2, Right): a text CNN (Kim, 2014), an image CNN (Simonyan & Zisserman, 2014) and a policy network that learns to choose between them. We collected a large-scale data set of 1.2 million products from the Walmart.com website. Each product has a title and an image and needs to be classified to a shelf (label) with 2890 possible shelves. Examples from this dataset can be seen in Figure 1 and are also available on-line at the Walmart.com website. For most of the products, both the image and the title of each product contain relevant information for customers. However, it is interesting to observe that for some of the products, both input types may not be informative for shelf prediction (Figure 1). This observation motivates our work and raises interesting questions: which input type is more useful for product classification? is it possible to forge the inputs into a better architecture?\nIn our experiments, we show that the text CNN outperforms the image one. However, for a relatively large number of products (\u223c 8%), the image CNN is correct while the text CNN is wrong, indicating a potential gain from using a multi-modal architecture. We also show that the policy is able to choose between the two models and give a performance improvement over both state-of-the-art networks.\nTo the best of our knowledge, this is the first work that demonstrates a performance improvement on top-1 classification accuracy by using images and text on a large-scale classification problem. In particular, our main contributions are:\n\u2022 We demonstrate that the text classification CNN (Kim, 2014) outperforms the VGG network (Simonyan & Zisserman, 2014) on a real-world large-scale product to shelf classification problem.\n\u2022 We analyze the errors made by the different networks and show the potential gain of multimodality.\n\u2022 We propose a novel decision-level fusion policy that learns to choose between the text and image networks and improve over both.\n2 MULTI-MODALITY\nOver the years, a large body of research has been devoted to improving classification using ensembles of classifiers (Kittler et al., 1998; Hansen & Salamon, 1990). Inspired by their success, these methods have also been used in multi-modal settings (e.g.,Guillaumin et al. (2010); Poria et al. (2016)), where the source of the signals, or alternatively their modalities, are different. Some examples include audio-visual speech classification (Ngiam et al., 2011), image and text retrieval (Kiros et al.), sentiment analysis and semi-supervised learning (Guillaumin et al., 2010).\nCombining classifiers from different input sources presents multiple challenges. First, classifiers vary in their discriminative power, thus, an optimal unification method should be able to adapt itself for specific combinations of classifiers. Second, different data sources have different state-ofthe-art architectures, typically deep neural networks, which vary in depth, width, and optimization algorithm; making it non-trivial to merge them. Moreover, a multi-modal architecture potentially has more local minima that may give unsatisfying results. Finally, most of the publicly available real-world big data classification datasets, an essential building block of deep learning systems, typically contain only one data type.\nNevertheless, the potential performance boost of multi-modal architectures has motivated researchers over the years. Frome et al. (2013) combined an image network (Krizhevsky et al., 2012) with a Skip-gram Language Model in order to improve classification results on ImageNet. However, they were not able to improve the top-1 accuracy prediction, possibly because the text input they used (image labels) didn\u2019t contain a lot of information. Other works, used multi-modality to learn good embedding but did not present results on classification benchmarks (Lynch et al., 2015; Kiros et al.; Gong et al., 2014). Kannan et al. (2011) suggested to improve text-based product classification by adding an image signal, training an image classifier and learning a decision rule between the two. However, they only experimented with a small dataset and a low number of labels, and it is not clear how to scale their method for extreme multi-class multi-label applications that characterize real-world problems in e-commerce.\nAdding modalities can improve the classification of products that have a non-informative input source (e.g., image or text). In e-commerce, for example, classifiers that rely exclusively on text suffer from short and non-informative titles, differences in style between vendors and overlapping text across categories (i.e., a word that helps to classify a certain class may appear in other classes). Figure 1 presents a few examples of products that have only one informative input type. These examples suggest that a multi-modal architecture can potentially outperform a classifier with a single input type.\nMost unification techniques for multi-modal learning are partitioned between feature-level fusion techniques and decision-level fusion techniques (Figure 2, Left).\n2.1 FEATURE LEVEL FUSION\nFeature-level fusion is characterized by three phases: (a) learning a representation, (b) supervised training, and (c) testing. The different unification techniques are distinguished by the availability of the data in each phase (Guillaumin et al., 2010). For example, in cross-modality training, the representation is learned from all the modalities, but only one modality is available for supervised training and testing. In other cases, all of the modalities are available at all stages but we may want (or not) to limit their usage given a certain budget. Another source for the distinction is the order in which phases (a) and (b) are made. For example, one may first learn the representation and then learn a classifier from it, or learn both the representation and the classifier in parallel. In the deep learning context, there are two common approaches. In the first approach, we learn an end-to-end deep NN; the NN has multiple input-specific pipes that include a data source followed by input specific layers. After a certain depth, the pipes are concatenated followed by additional layers such that the NN is trained end-to-end. In the second approach, input specific deep NNs are learned first, and a multi-modal representation vector is created by concatenating the input specific feature vectors (e.g., the neural network\u2019s last hidden layer). Then, an additional classifier learns to classify from the multi-modal representation vector. While multi-modal methods have shown potential to boost performance on small datasets (Poria et al., 2016), or on top-k accuracy measures (Frome et al., 2013), we are not familiar with works that succeeded with applying it on a large-scale classification problem and received performance improvement in top-1 accuracy.\n2.2 DECISION-LEVEL FUSION\nIn this approach, an input specific classifier is learned for each modality, and the goal is to find a decision rule between them. The decision rule is typically a pre-defined rule (Guillaumin et al., 2010) and is not learned from the data. For example, Poria et al. (2016) chose the classifier with the maximal confidence, while Krizhevsky et al. (2012) average classifier predictions. However, in this work we show that learning the decision rule yields significantly better results on our data.\n3 METHODS AND ARCHITECTURES\nIn this section, we give the details of our multi-modal product classification architecture. The architecture is composed of a text CNN and an image CNN which are forged together by a policy network, as can be seen in Figure 2, Right.\n3.1 MULTI LABEL COST FUNCTION\nOur cost function is the weighted sigmoid cross entropy with logits, a common cost function for multi-label problems. Let x be the logits, z be the targets, q be a positive weight coefficient, used as a multiplier for the positive targets, and \u03c3(x) = 11+exp(\u2212x) . The loss is given by:\nCost(x,z;q) = \u2212 qz \u00b7 log(\u03c3(x))\u2212 (1\u2212 z) \u00b7 log(1\u2212 \u03c3(x)) = (1\u2212 z) \u00b7 x+ (1 + (q \u2212 1) \u00b7 z) \u00b7 log(1 + exp(\u2212x)).\nThe positive coefficient q, allows one to trade off recall and precision by up- or down-weighting the cost of a positive error relative to a negative error. We found it to have a significant effect in practice.\n3.2 TEXT CLASSIFICATION\nFor the text signal, we use the text CNN architecture of Kim (2014). The first layer embeds words into low-dimensional vectors using random embedding (different than the original paper). The next layer performs convolutions over time on the embedded word vectors using multiple filter sizes (3, 4 and 5), where we use 128 filters from each size. Next, we max-pool-over-time the result of each convolution filter and concatenated all the results together. We add a dropout regularization layer (0.5 dropping rate), followed by a fully connected layer, and classify the result using a softmax layer. An illustration of the Text CNN can be seen in Figure 2.\n3.3 IMAGE CLASSIFICATION\nFor the image signal, we use the VGG Network (Simonyan & Zisserman, 2014). The input to the network is a fixed-size 224 x 224 RGB image. The image is passed through a stack of convolutional layers with a very small receptive field: 3 x 3. The convolution stride is fixed to 1 pixel; the spatial padding of the convolutional layer is 1 pixel. Spatial pooling is carried out by five max-pooling layers, which follow some of the convolutional layers. Max-pooling is performed over a 2 x 2 pixel window, with stride 2. A stack of convolutional layers is followed by three Fully-Connected (FC) layers: the first two have 4096 channels each, the third performs 2890-way product classification and thus contains 2890 channels (one for each class). All hidden layers are followed by a ReLu non-linearity. The exact details can be seen in Figure 2.\n3.4 MULTI-MODAL ARCHITECTURE\nWe experimented with four types of multi-modal architectures. (1) Learning decision-level fusion policies from different inputs. (1a) Policies that use the text and image CNNs class probabilities as input (Figure 2). We experimented with architectures that have one or two fully connected layers (the two-layered policy is using 10 hidden units and a ReLu non-linearity between them). (1b) Policies that use the text and/or image as input. For these policies, the architecture of policy network was either the text CNN or the VGG network. In order to train policies, labels are collected from the image and text networks predictions, i.e., the label is 1 if the image network made a correct prediction while the text network made a mistake, and 0 otherwise. On evaluation, we use the policy predictions to select between the models, i.e., if the policy prediction is 1 we use the image network, and use the text network otherwise. (2) Pre-defined policies that average the predictions of the different CNNs or choose the CNN with the highest confidence. (3) End-to-end feature-level fusion, each input type is processed by its specific CNN. We concatenate the last hidden layers of the CNNs and add one or two fully connected layers. All the layers are trained together end-to-end (we also tried to initialize the input specific weights from pre-trained single-modal networks). (4) Multistep feature-level fusion. As in (3), we create shared representation vector by concatenating the last hidden layers. However, we now keep the shared representation fixed and learn a new classifier from it.\n4 EXPERIMENTS\n4.1 SETUP\nOur dataset contains 1.2 million products (title image and shelf) that we collected from Walmart.com (offered online and can be viewed at the website) and were deemed the hardest to classify by the current production system. We divide the data into training (1.1 million) validation (50k) and test (50k). We train both the image network and the text network on the training dataset and evaluate them on the test dataset. The policy is trained on the validation dataset and is also evaluated on the test dataset. The objective is to classify the product\u2019s shelf, from 2890 possible choices. Each product is typically assigned to more than one shelf (3 on average), and the network is considered accurate if its most probable shelf is one of them.\n4.2 TRAINING THE TEXT ARCHITECTURE\nPreprocess: we build a dictionary of all the words in the training data and embed each word using a random embedding into a one hundred dimensional vector. We trim titles with more than 40 words and pad shorter titles with nulls.\nWe experimented with different batch sizes, dropout rates, and filters stride, but found that the vanilla architecture (Kim, 2014) works well on our data. This is consistent with Zhang & Wallace (2015), who showed that text CNNs are not very sensitive to hyperparameters. We tuned the cost function positive coefficient parameter q, and found out that the value 30 performed best in practice (we will also use this value for the image network). The best CNN that we trained classified 70.1% of the products from the test set correctly (Table 1).\n4.3 TRAINING THE IMAGE ARCHITECTURE\nPreprocess: we re-size all the images into 224 x 224 pixels and reduce the image mean.\nThe VGG network that we trained classified 57% of the products from the test set correctly. This is a bit disappointing if we compare it to the performance of the VGG network on ImageNet (\u223c 75%). There are a few differences between these two datasets that may explain this gap. First, our data has 3 times more classes and contains multiple labels per image making the classification harder, and second, Figure 1 implies that some of our images are not informative for shelf classification. Some works claim that the features learned by VGG on ImageNet are global feature extractors (Lynch et al., 2015). We therefore decided to use the weights learned by VGG on ImageNet and learn only the last layer. This configuration yielded only 36.7% accuracy. We believe that the reason is that some of the ImageNet classes are irrelevant for e-commerce (e.g., vehicles and animals) while some relevant categories are misrepresented (e.g., electronics and office equipment). It could also be that our images follow some specific pattern of white background, well-lit studio etc., that characterizes e-commerce.\n4.4 ERROR ANALYSIS\nIs a picture worth a thousand words? Inspecting Figure 3, we can see that the text network outperformed the image network on this dataset, classifying more products correctly. Similar results were reported before (Pyo et al., 2010; Kannan et al., 2011) but to the best of our knowledge, this is the first work that compares state-of-the-art text and image CNNs on a real-world large-scale ecommerce dataset. What is the potential of multi-modality? We identified that for 7.8% of the products the image network made a correct prediction while the text network was wrong. This observation is encouraging since it implies that there is a relative big potential to harness via multi-modality. We find this large gap surprising since different neural networks applied to the same problem tend to make the same mistakes (Szegedy et al., 2013). Unification techniques for multi-modal problems typically use the last hidden layer of each network as features (Frome et al., 2013; Lynch et al., 2015; Pyo et al., 2010). We therefore decided to visualize the activations of this layer using a tSNE map (Maaten & Hinton, 2008). Figure 3, depicts such a map for the activations of the text model (the image model yielded similar results). In particular,\nwe were looking for regions in the tSNE map where the image predictions are correct and the text is wrong (Figure 3, green). Finding such a region will imply that a policy network can learn good decision boundaries. However, we can see that there are no well-defined regions in the tSNE maps where the image network is correct and the title is wrong (green), thus implying that it might be hard to identify these products using the activations of the last layers.\n4.5 MULTI-MODAL UNIFICATION TECHNIQUES\nOur error analysis experiment highlights the potential of merging image and text. Still, we found it hard to achieve the upper bound provided by the error analysis in practice. We now describe the policies that managed to achieve performance boost in top-1 accuracy % over the text and image networks, and then provide discussion on other approaches that we tried but didn\u2019t work.\nDecision-level fusion: We trained policies from different data sources (e.g., title, image, and each CNN class probabilities), using different architectures and different hyperparameters. Looking at Table 1, we can see that the best policies were trained using the class probabilities (the softmax probabilities) of the image and text CNNs as inputs. The amount of class probabilities that were used (top-1, top-3 or all) did not have a significant effect on the results, indicating that the top-1 probability contains enough information to learn good policies. This result makes sense since the top-1 probability measures the confidence of the network in making a prediction. Still, the top-3 probabilities performed slightly better, indicating that the difference between the top probabilities may also matter. We can also see that the 2-layer architecture outperformed the 1-layer, indicating that a linear policy is too simple, and deeper models can yield better results. Last, the cost function positive coefficient q had a big impact on the results. We can see that for q = 1, the policy network is more accurate in its prediction however it achieves worse results on shelf classification. For q = 5 we get the best results, while higher values of q (e.g., 7 or 10) resulted in inaccurate policies that did not perform well in practice.\nWhile it may not seem surprising that combining text and image will improve accuracy, in practice we found it extremely hard to leverage this potential. To the best of our knowledge, this is the first work that demonstrates a direct performance improvement on top-1 classification accuracy from using images and text on a large-scale classification problem. We experimented with pre-defined policies that do not learn from the data. Specifically, we tried to average the logits, following (Krizhevsky et al., 2012; Simonyan & Zisserman, 2014), and to choose the network with the maximal confidence following (Poria et al., 2016). Both of these experiments yielded significantly worse results, probably, since the text network is much more accurate than the image one (Table 1). We also tried to learn policies from the text and/or the image input, using a policy network which is either a text CNN, a VGG network or a combination. However, all of these experiments resulted in policies that overfit the data and performed worse than the title model on the test data (Table 1). We also experimented with early stopping criteria, various regularization methods (dropout, l1, l2) and reduced model size but none could make the policy network generalize.\nFeature-level fusion: Training a CNN end-to-end can be tricky. For example, each input source has its own specific architecture, with specific learning rate and optimization algorithm. We experimented with training the network end-to-end, but also with first training each part separately and then learning the concatenated parts. We tried different unification approaches such as gating functions (Srivastava et al., 2015), cross products and a different number of fully connected layers after the concatenation. These experiments resulted in models that were inferior to the text model. While this may seem surprising, the only successful feature level fusion that we are aware of (Frome et al., 2013), was not able to gain accuracy improvement on top-1 accuracy.\n5 CONCLUSIONS\nIn this work, we investigated a multi-modal multi-class multi-label product classification problem and presented results on a challenging real-world dataset that we collected from Walmart.com. We discovered that the text network outperforms the image network on our dataset, and observed a big potential of fusing text and image inputs. Finally, we suggested a multi-modal decision-level fusion approach that leverages state-of-the-art results from image and text classification and forges them into a multi-modal architecture that outperforms both.\nState-of-the-art image CNNs are much larger than text CNNs, and take more time to train and to run. Thus, extracting image features during run time, or getting the image network predictions may be prohibitively expensive. In this context, an interesting observation is that feature level fusion methods require using the image signal for each product, while decision level fusion methods require using the image network selectively making them more appealing. Moreover, our experiments suggest that decision-level fusion performs better than feature-level fusion in practice.\nFinally, we were only able to realize a fraction of the potential of multi-modality. In the future, we plan to investigate deeper policy networks and more sophisticated measures of confidence. We also plan to investigate ensembles of image networks (Krizhevsky et al., 2012) and text networks (Pyo et al., 2010). We believe that the insights from training policy networks will eventually lead us to train end to end differential multi-modal networks.\n", "cats": {"READ": 1.0, "DONTREAD": 0.0}, "reviews": [{"IS_META_REVIEW": true, "comments": "This paper presents a system approach to combine multiple modalities to perform classification in a practical scenario (e-commerce).\n\nIn general, I find the proposed approach in the paper sound and solid, but do not see novelty in the paper: feature fusion and decision time fusion are both standard practices in multi-modal analysis, and the rest of the paper offers no surprise in implementing such approaches. This seems to be a better fit for venues that focus more on production systems, and seems to be a bad fit for ICLR where the focus is more on research of novel algorithms and theories."}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "Three knowledgable reviewers recommend rejection. While the application is interesting and of commercial value, the technical contribution falls below the ICLR's bar. I encourage the authors to improve the paper and submit it to a future conference.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"TITLE": "Practical large-scale multi-model architecture but lack technical novelty", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "This paper introduces a large-scale multi-model product classification system. The model consists of three modules, Image CNN (VGG 16 architecture), text CNN (Kim 2014) and decision-level fusion policies. The authors have tried several fusion methods: including policies taking inputs from text and image CNN probabilities; choose either CNN; average the predictions; end-to-end training. Experimental results show that text CNN alone works better than image CNN and multi-model fusion can improve the accuracy by a small margin. It is a little bit surprising that end-to-end feature level fusion works worse than text CNN alone. The writing is clear and there are a lot of useful practical experiences of learning large-scale model. However, I lean toward rejecting the paper because the following:\n\n1) No other dataset reported. The authors haven't mentioned releasing the walmart dataset and it is going to be really hard to reproduce the results without the dataset. \n2) Technical novelty is limited. All the decision-level fusion policies have been investigated by some previous methods before. \n3) Performance gain is also limited. ", "IS_META_REVIEW": false, "RECOMMENDATION": 5, "DATE": "19 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "Review", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "This paper tackles the problem of multi-modal classification of text and images.\n\nPros:\n- Interesting dataset and application.\n\nCons:\n- The results are rather lacklustre, showing a very mild improvement compared to the oracle improvement. But perhaps some insights as to whether the incorrect decisions are humanly possible would help with significance of the results.\n- Could have explored some intermediate architectures such as feature fusion + class probabilities with/without finetuning. There are no feature fusion results reported.\n- No evaluation on standard datasets or comparison to previous works.\n\nWhat is the policy learnt for CP-1? Given 2 input class probabilities, how does the network perform better than max or mean?\n", "IS_META_REVIEW": false, "RECOMMENDATION": 5, "DATE": "19 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "No Title", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "This paper presents a system approach to combine multiple modalities to perform classification in a practical scenario (e-commerce).\n\nIn general, I find the proposed approach in the paper sound and solid, but do not see novelty in the paper: feature fusion and decision time fusion are both standard practices in multi-modal analysis, and the rest of the paper offers no surprise in implementing such approaches. This seems to be a better fit for venues that focus more on production systems, and seems to be a bad fit for ICLR where the focus is more on research of novel algorithms and theories. ", "IS_META_REVIEW": false, "RECOMMENDATION": 4, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"DATE": "06 Dec 2016", "TITLE": "please fix a cited paper ", "IS_META_REVIEW": false, "comments": "Hi, I am Jung-Woo Ha and the authour of a paper you cited in your work. \n\nIn the references of your manuscript, I think that \"Hyuna Pyo, Jung-Woo Ha, and Jeonghee Kim. Large-scale item categorization in e-commerce using\nmultiple recurrent neural networks. 2010.\" should be changed into \"Jung-Woo Ha, Hyuna Pyo, and Jeonghee Kim. Large-scale item categorization in e-commerce using\nmultiple recurrent neural networks. In Proceedings of KDD 2016.\"\nThe url is ", "OTHER_KEYS": "Jung-Woo Ha"}, {"IS_META_REVIEW": true, "comments": "This paper presents a system approach to combine multiple modalities to perform classification in a practical scenario (e-commerce).\n\nIn general, I find the proposed approach in the paper sound and solid, but do not see novelty in the paper: feature fusion and decision time fusion are both standard practices in multi-modal analysis, and the rest of the paper offers no surprise in implementing such approaches. This seems to be a better fit for venues that focus more on production systems, and seems to be a bad fit for ICLR where the focus is more on research of novel algorithms and theories."}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "Three knowledgable reviewers recommend rejection. While the application is interesting and of commercial value, the technical contribution falls below the ICLR's bar. I encourage the authors to improve the paper and submit it to a future conference.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"TITLE": "Practical large-scale multi-model architecture but lack technical novelty", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "This paper introduces a large-scale multi-model product classification system. The model consists of three modules, Image CNN (VGG 16 architecture), text CNN (Kim 2014) and decision-level fusion policies. The authors have tried several fusion methods: including policies taking inputs from text and image CNN probabilities; choose either CNN; average the predictions; end-to-end training. Experimental results show that text CNN alone works better than image CNN and multi-model fusion can improve the accuracy by a small margin. It is a little bit surprising that end-to-end feature level fusion works worse than text CNN alone. The writing is clear and there are a lot of useful practical experiences of learning large-scale model. However, I lean toward rejecting the paper because the following:\n\n1) No other dataset reported. The authors haven't mentioned releasing the walmart dataset and it is going to be really hard to reproduce the results without the dataset. \n2) Technical novelty is limited. All the decision-level fusion policies have been investigated by some previous methods before. \n3) Performance gain is also limited. ", "IS_META_REVIEW": false, "RECOMMENDATION": 5, "DATE": "19 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "Review", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "This paper tackles the problem of multi-modal classification of text and images.\n\nPros:\n- Interesting dataset and application.\n\nCons:\n- The results are rather lacklustre, showing a very mild improvement compared to the oracle improvement. But perhaps some insights as to whether the incorrect decisions are humanly possible would help with significance of the results.\n- Could have explored some intermediate architectures such as feature fusion + class probabilities with/without finetuning. There are no feature fusion results reported.\n- No evaluation on standard datasets or comparison to previous works.\n\nWhat is the policy learnt for CP-1? Given 2 input class probabilities, how does the network perform better than max or mean?\n", "IS_META_REVIEW": false, "RECOMMENDATION": 5, "DATE": "19 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "No Title", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "This paper presents a system approach to combine multiple modalities to perform classification in a practical scenario (e-commerce).\n\nIn general, I find the proposed approach in the paper sound and solid, but do not see novelty in the paper: feature fusion and decision time fusion are both standard practices in multi-modal analysis, and the rest of the paper offers no surprise in implementing such approaches. This seems to be a better fit for venues that focus more on production systems, and seems to be a bad fit for ICLR where the focus is more on research of novel algorithms and theories. ", "IS_META_REVIEW": false, "RECOMMENDATION": 4, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"DATE": "06 Dec 2016", "TITLE": "please fix a cited paper ", "IS_META_REVIEW": false, "comments": "Hi, I am Jung-Woo Ha and the authour of a paper you cited in your work. \n\nIn the references of your manuscript, I think that \"Hyuna Pyo, Jung-Woo Ha, and Jeonghee Kim. Large-scale item categorization in e-commerce using\nmultiple recurrent neural networks. 2010.\" should be changed into \"Jung-Woo Ha, Hyuna Pyo, and Jeonghee Kim. Large-scale item categorization in e-commerce using\nmultiple recurrent neural networks. In Proceedings of KDD 2016.\"\nThe url is ", "OTHER_KEYS": "Jung-Woo Ha"}]}
{"text": "We study the problem of transferring a sample in one domain to an analog sample in another domain. Given two related domains, S and T , we would like to learn a generative function G that maps an input sample from S to the domain T , such that the output of a given representation function f , which accepts inputs in either domains, would remain unchanged. Other than f , the training data is unsupervised and consist of a set of samples from each domain, without any mapping between them. The Domain Transfer Network (DTN) we present employs a compound loss function that includes a multiclass GAN loss, an f preserving component, and a regularizing component that encourages G to map samples from T to themselves. We apply our method to visual domains including digits and face images and demonstrate its ability to generate convincing novel images of previously unseen entities, while preserving their identity.\n1 INTRODUCTION\nHumans excel in tasks that require making analogies between distinct domains, transferring elements from one domain to another, and using these capabilities in order to blend concepts that originated from multiple source domains. Our experience tells us that these remarkable capabilities are developed with very little, if any, supervision that is given in the form of explicit analogies.\nRecent achievements replicate some of these capabilities to some degree: Generative Adversarial Networks (GANs) are able to convincingly generate novel samples that match that of a given training set; style transfer methods are able to alter the visual style of images; domain adaptation methods are able to generalize learned functions to new domains even without labeled samples in the target domain and transfer learning is now commonly used to import existing knowledge and to make learning much more efficient.\nThese capabilities, however, do not address the general analogy synthesis problem that we tackle in this work. Namely, given separated but otherwise unlabeled samples from domains S and T and a perceptual function f , learn a mapping G : S \u2192 T such that f(x) \u223c f(G(x)). In order to solve this problem, we make use of deep neural networks of a specific structure in which the function G is a composition of the input function f and a learned function g. A compound loss that integrates multiple terms is used. One term is a Generative Adversarial Network (GAN) term that encourages the creation of samples G(x) that are indistinguishable from the training samples of the target domain, regardless of x \u2208 S or x \u2208 T . The second loss term enforces that for every x in the source domain training set, ||f(x)\u2212 f(G(x))|| is small. The third loss term is a regularizer that encourages G to be the identity mapping for all x \u2208 T . The type of problems we focus on in our experiments are visual, although our methods are not limited to visual or even to perceptual tasks. Typically, f would be a neural network representation that is taken as the activations of a network that was trained, e.g., by using the cross entropy loss, in order to classify or to capture identity.\nAs a main application challenge, we tackle the problem of emoji generation for a given facial image. Despite a growing interest in emoji and the hurdle of creating such personal emoji manually, no system has been proposed, to our knowledge, that can solve this problem. Our method is able to\nproduce face emoji that are visually appealing and capture much more of the facial characteristics than the emoji created by well-trained human annotators who use the conventional tools.\n2 RELATED WORK\nAs far as we know, the domain transfer problem we formulate is novel despite being ecological (i.e., appearing naturally in the real-world), widely applicable, and related to cognitive reasoning (Fauconnier & Turner, 2003). In the discussion below, we survey recent GAN work, compare our work to the recent image synthesis work and make links to unsupervised domain adaptation.\nGAN (Goodfellow et al., 2014) methods train a generator network G that synthesizes samples from a target distribution given noise vectors. G is trained jointly with a discriminator network D, which distinguishes between samples generated by G and a training set from the target distribution. The goal of G is to create samples that are classified by D as real samples.\nWhile originally proposed for generating random samples, GANs can be used as a general tool to measure equivalence between distributions. Specifically, the optimization of D corresponds to taking the most discriminative D achievable, which in turn implies that the indistinguishability is true for every D. Formally, Ganin et al. (2016) linked the GAN loss to the H-divergence between two distributions of Ben-david et al. (2006).\nThe generative architecture that we employ is based on the successful architecture of Radford et al. (2015). There has recently been a growing concern about the uneven distribution of the samples generated by G \u2013 that they tend to cluster around a set of modes in the target domain (Salimans et al., 2016). In general, we do not observe such an effect in our results, due to the requirement to generate samples that satisfy specific f -constancy criteria.\nA few contributions (\u201cConditional GANs\u201d) have employed GANs in order to generate samples from a specific class (Mirza & Osindero, 2014), or even based on a textual description (Reed et al., 2016). When performing such conditioning, one can distinguish between samples that were correctly generated but fail to match the conditional constraint and samples that were not correctly generated. This is modeled as a ternary discriminative function D (Reed et al., 2016; Brock et al., 2016).\nThe recent work by Dosovitskiy & Brox (2016), has shown promising results for learning to map embeddings to their pre-images, given input-target pairs. Like us, they employ a GAN as well as additional losses in the feature- and the pixel-space. Their method is able to invert the midlevel activations of AlexNet and reconstruct the input image. In contrast, we solve the problem of unsupervised domain transfer and apply the loss terms in different domains: pixel loss in the target domain, and feature loss in the source domain.\nAnother class of very promising generative techniques that has recently gained traction is neural style transfer. In these methods, new images are synthesized by minimizing the content loss with respect to one input sample and the style loss with respect to one or more input samples. The content loss is typically the encoding of the image by a network training for an image categorization task, similar to our work. The style loss compares the statistics of the activations in various layers of the neural network. We do not employ style losses in our method. While initially style transfer was obtained by a slow optimization process (Gatys et al., 2016), recently, the emphasis was put on feed-forward methods (Ulyanov et al., 2016; Johnson et al., 2016).\nThere are many links between style transfer and our work: both are unsupervised and generate a sample under f constancy given an input sample. However, our work is much more general in its scope and does not rely on a predefined family of perceptual losses. Our method can be used in order to perform style transfer, but not the other way around. Another key difference is that the current style transfer methods are aimed at replicating the style of one or several images, while our work considers a distribution in the target space. In many applications, there is an abundance of unlabeled data in the target domain T , which can be modeled accurately in an unsupervised manner.\nGiven the impressive results of recent style transfer work, in particular for face images, one might get the false impression that emoji are just a different style of drawing faces. By way of analogy, this claim is similar to stating that a Siamese cat is a Labrador in a different style. Emoji differ from facial photographs in both content and style. Style transfer can create visually appealing face images; However, the properties of the target domain are compromised.\nIn the computer vision literature, work has been done to automatically generate sketches from images, see Kyprianidis et al. (2013) for a survey. These systems are able to emphasize image edges and facial features in a convincing way. However, unlike our method, they require matching pairs of samples, and were not shown to work across two distant domains as in our method. Due to the lack of supervised training data, we did not try to apply such methods to our problems. However, one can assume that if such methods were appropriate for emoji synthesis, automatic face emoji services would be available.\nUnsupervised domain adaptation addresses the following problem: given a labeled training set in S \u00d7 Y , for some target space Y , and an unlabeled set of samples from domain T , learn a function h : T \u2192 Y (Chen et al., 2012; Ganin et al., 2016). One can solve the sample transfer problem (our problem) using domain adaptation and vice versa. In both cases, the solution is indirect. In order to solve domain adaptation using domain transfer, one would learn a function from S to Y and use it as the input method of the domain transfer algorithm in order to obtain a map from S to T 1. The training samples could then be transferred to T and used to learn a classifier there.\nIn the other direction, given the function f , one can invert f in the domain T by generating training samples (f(x), x) for x \u2208 T and learn from them a function h from f(T ) = {f(x)|x \u2208 T} to T . Domain adaptation can then be used in order to map f(S) = {f(x)|x \u2208 S} to T , thus achieving domain transfer. Based on the work by Zhmoginov & Sandler (2016), we expect that h, even in the target domain of emoji, will be hard to learn, making this solution hypothetical at this point.\n3 A BASELINE PROBLEM FORMULATION\nGiven a set s of unlabeled samples in a source domain S sampled i.i.d according to some distribution DS , a set of samples in the target domain t \u2282 T sampled i.i.d from distribution DT , a function f from the domain S \u222a T , some metric d, and a weight \u03b1, we wish to learn a function G : S \u2192 T that minimizes the combined risk R = RGAN + \u03b1RCONST, which is comprised of\nRGAN = max D Ex\u223cDS log[1\u2212D(G(x))] + Ex\u223cDT log[D(x)], (1)\nwhere D is a binary classification function from T , D(x) the probability of the class 1 it assigns for a sample x \u2208 T , and\nRCONST = Ex\u223cDS d(f(x), f(G(x))) (2)\nThe first term is the adversarial risk, which requires that for every discriminative function D, the samples from the target domain would be indistinguishable from the samples generated by G for samples in the source domain. An adversarial risk is not the only option. An alternative term that does not employ GANs would directly compare the distribution DT to the distribution of G(x) where x \u223c DS , e.g., by using KL-divergence. The second term is the f -constancy term, which requires that f is invariant under G. In practice, we have experimented with multiple forms of d including Mean Squared Error (MSE) and cosine distance, as well as other variants including metric learning losses (hinge) and triplet losses. The performance is mostly unchanged, and we report results using the simplest MSE solution.\nSimilarly to other GAN formulations, one can minimize the loss associated with the risk R over G, while maximizing it over D, where G and D are deep neural networks, and the expectations in R are replaced by summations over the corresponding training sets. However, this baseline solution, as we will show experimentally, does not produce desirable results.\n1The function trained this way would be more accurate on S than on T . This asymmetry is shared with all experiments done in this work.\n4 THE DOMAIN TRANSFER NETWORK\nWe suggest to employ a more elaborate architecture that contains two high level modifications. First, we employ f(x) as the baseline representation to the function G. Second, we consider, during training, the generated samples G(x) for x \u2208 t. The first change is stated as G = g \u25e6 f , for some learned function g. By applying this, we focus the learning effort ofG on the aspects that are most relevant toRCONST. In addition, in most applications, f is not as accurate on T as it on S. The composed function, which is trained on samples from both S and T , adds layers on top of f , which adapt it.\nThe second change alters the form of LGAN, making it multiclass instead of binary. It also introduces a new term LTID that requires G to be the identity matrix on samples from T . Taken together and written in terms of training loss, we now have two losses LD and LG = LGANG + \u03b1LCONST + \u03b2LTID + \u03b3LTV, for some weights \u03b1, \u03b2, \u03b3, where\nLD = \u2212 \u2211 x\u2208s logD1(g(f(x)))\u2212 \u2211 x\u2208t logD2(g(f(x)))\u2212 \u2211 x\u2208t logD3(x) (3)\nLGANG = \u2212 \u2211 x\u2208s logD3(g(f(x)))\u2212 \u2211 x\u2208t logD3(g(f(x))) (4)\nLCONST = \u2211 x\u2208s d(f(x), f(g(f(x)))) (5)\nLTID = \u2211 x\u2208t d2(x,G(x)) (6)\nand where D is a ternary classification function from the domain T to 1, 2, 3, and Di(x) is the probability it assigns to class i = 1, 2, 3 for an input sample x, and d2 is a distance function in T . During optimization, LG is minimized over g and LD is minimized over D. See Fig. 1 for an illustration of our method.\nEq. 3 and 4 make sure that the generated analogy, i.e., the output ofG, is in the target space T . Since D is ternary and can therefore confuse classes in more than one way, this role, which is captured by Eq. 1 in the baseline formulation, is split into two. However, the two equations do not enforce any similarity between the source sample x and the generated G(x). This is done by Eq. 5 and 6: Eq. 5 enforces f -constancy for x \u2208 S, while Eq. 6 enforces that for samples x \u2208 T , which are already in the target space, G is the identity mapping. The latter is a desirable behavior, e.g., for the cartooning task, given an input emoji, one would like it to remain constant under the mapping of G. It can also be seen as an autoencoder type of loss, applied only to samples from T . The experiments reported in Sec. 5 evaluate the contributions of LCONST and LTID and reveal that at least one of these is required, and that when employing only one loss, LCONST leads to a better performance than LTID.\nThe last loss, LTV is an anisotropic total variation loss (Rudin et al., 1992; Mahendran & Vedaldi, 2015), which is added in order to slightly smooth the resulting image. The loss is defined on the generated image z = [zij ] = G(x) as\nLTV (z) = \u2211 i,j ( (zi,j+1 \u2212 zij)2 + (zi+1,j \u2212 zij)2 )B 2 , (7)\nwhere we employ B = 1.\nIn our work, MSE is used for both d and d2. We also experimented with replacing d2, which, in visual domains, compares images, with a second GAN. No noticeable improvement was observed. Throughout the experiments, the adaptive learning rate method Adam by Kingma & Ba (2016) is used as the optimization algorithm.\n5 EXPERIMENTS\nThe Domain Transfer Network (DTN) is evaluated in two application domains: digits and face images. In the first domain, we transfer images from the Street View House Number (SVHN) dataset of Netzer et al. (2011) to the domain of the MNIST dataset by LeCun & Cortes (2010). In\nthe face domain, we transfer a set of random and unlabeled face images to a space of emoji images. In both cases, the source and target domains differ considerably.\n5.1 DIGITS: FROM SVHN TO MNIST\nFor working with digits, we employ the extra training split of SVHN, which contains 531,131 images for two purposes: learning the function f and as an unsupervised training set s for the domain transfer method. The evaluation is done on the test split of SVHN, comprised of 26,032 images. The architecture of f consists of four convolutional layers with 64, 128, 256, 128 filters respectively, each followed by max pooling and ReLU non-linearity. The error on the test split is 4.95%. Even tough this accuracy is far from the best reported results, it seems to be sufficient for the purpose of domain transfer. Within the DTN, f maps a 32 \u00d7 32 RGB image to the activations of the last convolutional layer of size 128\u00d7 1\u00d7 1 (post a 4\u00d7 4 max pooling and before the ReLU). In order to apply f on MNIST images, we replicate the grayscale image three times.\nThe set t contains the test set of the MNIST dataset. For supporting quantitative evaluation, we have trained a classifier on the train set of the MNIST dataset, consisting of the same architecture as f . The accuracy of this classifier on the test set approaches perfect performance at 99.4% accuracy, and is, therefore, trustworthy as an evaluation metric. In comparison, the network f , achieves 76.08% accuracy on t.\nNetwork g, inspired by Radford et al. (2015), maps SVHN-trained f \u2019s 128D representations to 32\u00d7 32 grayscale images. g employs four blocks of deconvolution, batch-normalization, and ReLU, with a hyperbolic tangent terminal. The architecture ofD consists of four batch-normalized convolutional layers and employs ReLU. See Radford et al. (2015) for more details on the networks architecture. In the digit experiments, the results were obtained with the tradeoff hyperparamemters \u03b1 = \u03b2 = 15. We did not observe a need to add a smoothness term and the weight of LTV was set to \u03b3 = 0.\nDespite not being very accurate on both domains (and also considerably worse than the SVHN state of the art), we were able to achieve visually appealing domain transfer, as shown in Fig. 2(a). In order to evaluate the contribution of each of the method\u2019s components, we have employed the MNIST network on the set of samples G(sTEST ) = {G(x)|x \u2208 sTEST }, using the true SVHN labels of the test set.\nWe first compare to the baseline method of Sec. 3, where the generative function, which works directly with samples in S, is composed out of a few additional layers at the bottom of G. The results, shown in Tab. 1, demonstrate that DTN has a clear advantage over the baseline method. In addition, the contribution of each one of the terms in the loss function is shown in the table. The regularization term LTID seems less crucial than the constancy term. However, at least one of them is required in order to obtain good performance. The GAN constraints are also important. Finally, the inclusion of f within the generator function G has a dramatic influence on the results.\nAs explained in Sec. 2, domain transfer can be used in order to perform unsupervised domain adaptation. For this purposes, we transformed the set s to the MNIST domain (as above), and using the true labels of s employed a simple nearest neighbor classifier there. The choice of classifier was\nto emphasize the simplicity of the approach; However, the constraints of the unsupervised domain transfer problem would be respected for any classifier trained on G(s). The results of this experiment are reported in Tab. 2, which shows a clear advantage over the state of the art method of Ganin et al. (2016). This is true both when transferring the samples of the set s and when transferring the test set of SVHN, which is much smaller and was not seen during the training of the DTN.\n5.1.1 UNSEEN DIGITS\nAnother set of experiments was performed in order to study the ability of the domain transfer network to overcome the omission of a class of samples. This type of ablation can occur in the source or the target domain, or during the training of f and can help us understand the importance of each of these inputs. The results are shown visually in Fig. 3, and qualitatively in Tab. 3, based on the accuracy of the MNIST classifier only on the transferred samples from the test set of SVHN that belong to class \u20183\u2019.\nIt is evident that not including the class in the source domain is much less detrimental than eliminating it from the target domain. This is the desirable behavior: never seeing any \u20183\u2019-like shapes in t, the generator should not generate such samples. Results are better when not observing \u20183\u2019 in both s, t than when not seeing it only in t since in the latter case, G learns to map source samples of \u20183\u2019 to target images of other classes.\n5.2 FACES: FROM PHOTOS TO EMOJI\nFor face images, we use a set s of one million random images without identity information. The set t consists of assorted facial avatars (emoji) created by an online service (bitmoji.com). The emoji images were processed by a fully automatic process that localizes, based on a set of heuristics, the center of the irides and the tip of the nose. Based on these coordinates, the emoji were centered and scaled into 152\u00d7 152 RGB images. As the function f , we employ the representation layer of the DeepFace network Taigman et al. (2014). This representation is 256-dimensional and was trained on a labeled set of four million images that does not intersect the set s. Network D takes 152 \u00d7 152 RGB images (either natural or scaled-up emoji) and consists of 6 blocks, each containing a convolution with stride 2, batch normalization, and a leaky ReLU with a parameter of 0.2. Network g maps f \u2019s 256D representations to 64\u00d7 64 RGB images through a network with 5 blocks, each consisting of an upscaling convolution, batch-normalization and ReLU. Adding 1 \u00d7 1 convolution to each block resulted in lower LCONST training errors, and made g 9-layers deep. We set \u03b1 = 100, \u03b2 = 1, \u03b3 = 0.05 as the tradeoff hyperparameters within LG via validation. As expected, higher values of \u03b1 resulted in better f -constancy, however introduced artifacts such as general noise or distortions. The network was trained for 3 epochs, the point where no further reduction of validation error was observed on LCONST.\nIn order to upscale the 64 \u00d7 64 output to print quality, we used the method of Dong et al. (2015), which was shown to work well on art. We did not retrain this network for our application, and apply the published one to the final output of our method after its training was finished. Results without this upscale are shown, for comparison, in Appendix C.\nComparison With Human Annotators For evaluation purposes only, a team of professional annotators manually created an emoji, using a web service, for 118 random images from the CelebA dataset (Yang et al., 2015). Fig. 4 shows side by side samples of the original image, the human generated emoji and the emoji generated by the learned generator function G. As can be seen, the automatically generated emoji tend to be more informative, albeit less restrictive than the ones created manually.\nIn order to evaluate the identifiability of the resulting emoji, we have collected a second example for each identity in the set of 118 CelebA images and a set s\u2032 of 100,000 random face images, which were not included in s. We then employed the VGG face CNN descriptor of Parkhi et al. (2015) in order to perform retrieval as follows. For each image x in our manually annotated set, we create a gallery s\u2032 \u222a x\u2032, where x\u2032 is the other image of the person in x. We then perform retrieval using the VGG face descriptor using either the manually created emoji or G(x) as probe.\nThe VGG network is used in order to avoid a bias that might be caused by using f both for training the DTN and for evaluation. The results are reported in Tab. 4. As can be seen, the emoji generated by G are much more discriminative than the emoji created manually and obtain a median rank of 16 in cross-domain identification out of 105 distractors.\nMultiple Images Per Person We evaluate the visual quality that is obtained per person and not just per image, by testing DTN on the Facescrub dataset (Ng & Winkler, 2014). For each person p, we considered the set of their images Xp, and selected the emoji that was most similar to their\nsource image: argmin x\u2208Xp ||f(x)\u2212 f(G(x))|| (8)\nThis simple heuristic seems to work well in practice; The general problem of mapping a set X \u2282 S to a single output in T is left for future work. Fig. 2(b) contains several examples from the Facescrub dataset. For the complete set of identities, see Appendix A.\nTransferring both identity and expression We also experimented with multiple expressions. As it turns out the face identification network f encodes enough expression information to support a successful transfer of both identity as well as expression, see Appendix B.\nNetwork Visualization The obtained mapping g can serve as a visualization tool for studying the properties of the face representation. This is studied in Appendix D by computing the emoji generated for the standard basis of R256. The resulting images present a large amount of variability, indicating that g does not present a significant mode effect.\n5.3 STYLE TRANSFER AS A SPECIFIC DOMAIN TRANSFER TASK\nFig. 5(a-c) demonstrates that neural style transfer Gatys et al. (2016) cannot solve the photo to emoji transfer task in a convincing way. The output image is perhaps visually appealing; However, it does not belong to the space t of emoji. Our result are given in Fig. 5(d) for comparison. Note that DTN is able to fix the missing hair in the image.\nDomain transfer is more general than style transfer in the sense that we can perform style transfer using a DTN. In order to show this, we have transformed, using the method of Johnson et al. (2016), the training images of CelebA based on the style of a single image (shown in Fig. 5(e)). The original photos were used as the set s, and the transformed images were used as t. Applying DTN, using face representation f , we obtained styled face images such as the one shown in the figure 5(f).\n6 DISCUSSION AND LIMITATIONS\nAsymmetry is central to our work. Not only does our solution handle the two domains S and T differently, the function f is unlikely to be equally effective in both domains since in most practical cases, f would be trained on samples from one domain. While an explicit domain adaptation step can be added in order to make f more effective on the second domain, we found it to be unnecessary. Adaptation of f occurs implicitly due to the application of D downstream.\nUsing the same function f , we can replace the roles of the two domains, S and T . For example, we can synthesize an SVHN image that resembles a given MNIST image, or synthesize a face that matches an emoji. As expected, this yields less appealing results due to the asymmetric nature of f and the lower information content in these new source domains, see Appendix E.\nDomain transfer, as an unsupervised method, could prove useful across a wide variety of computational tasks. Here, we demonstrate the ability to use domain transfer in order to perform unsupervised domain adaptation. While this is currently only shown in a single experiment, the simplicity of performing domain adaptation and the fact that state of the art results were obtained effortlessly with a simple nearest neighbor classifier suggest it to be a promising direction for future research.\nA FACESCRUB DATASET GENERATIONS\nIn Fig. 6 we show the full set of identities of the Facescrub dataset, and their corresponding generated emoji.\nB TRANSFERRING NON-IDENTITY DATA\nf may encode, in addition to identity, other data that is desirable to transfer. In the example of faces, this information might include expression, facial hair, glasses, pose, etc. In order to transfer such information, it is important that the set of samples in the target domain t present variability along the desirable dimensions. Otherwise, the GAN applied in the target domain (Eq. 4) would maintain these dimensions fixed. The set t employed throughout our experiments in Sec. 5.2 was constructed by sampling emoji of neutral expression. To support a smiling expression for example, we simply added to set t random smiling emoji and re-trained the DTN. The results, presented in Fig. 7, demonstrate that f contains expression information in addition to identity information, and that this information is enough in order to transfer smiling photos to smiling emoji.\nC THE EFFECT OF SUPER-RESOLUTION\nAs mentioned in Sec. 5, in order to upscale the 64\u00d7 64 output to print quality, the method of Dong et al. (2015) is used. Fig. 8 shows the effect of applying this postprocessing step.\nD THE BASIS ELEMENTS OF THE FACE REPRESENTATION\nFig. 9 depicts the face emoji generated by g for the standard basis of the face representation (Taigman et al., 2014), viewed as the vector space R256.\nE DOMAIN TRANSFER IN THE REVERSE DIRECTION\nFor completion, we present, in Fig. 10 results obtained by performing domain transfer using DTNs in the reverse direction of the one reported in Sec. 5.\n", "cats": {"READ": 1.0, "DONTREAD": 0.0}, "reviews": [{"IS_META_REVIEW": true, "comments": "Update: After reading the rebuttal comments and the revised paper, I'm leaving the rating as it was before.\n\nThis paper proposes an unsupervised algorithm for transferring samples from one domain to another (related) domain under the constraint that some predefined f returns same result for the input and the result.\n\nPros:\n1. The paper presents an interesting idea of comparing samples from different domains using a fixed perceptual function f.\n\n2. The proposed method produces visually appealing results on several datasets\n\n3. The authors demonstrate how their approach can be used for domain adaptation and obtain improved results on the SVHN->MNIST task\n\n4. The paper is well-written and easy to read\n\nCons:\n1. The novelty of the method is relatively minor (I consider f-constancy term as the main contribution)\n\n2. It feels like the proposed approach would break for more dissimilar domains. The method relies on a fixed f which is trained on the source domain. This f can potentially drop information important for obtaining 1) better reconstructions in the target domain  2) more tightly related x and g(f(x)). I think the authors should consider either training all the modules in the model end-to-end or incorporating target samples into the training of f.\n\n3. A single domain adaptation experiment is definitely not enough to consider the proposed method as a universal alternative to the existing DA approaches.\n\nI would also like to point out that using super-resolved outputs as opposed to the actual model\u2019s outputs can produce a false impression of the visual quality of the transferred samples. I\u2019d suggest moving original outputs from the appendix into the main part."}, {"DATE": "17 Apr 2017 (modified: 18 Apr 2017)", "TITLE": "Training set for SVHN domain?", "IS_META_REVIEW": false, "comments": "In section 5.1, you said that you employ the extra training split of SVHN for two purposes: learning the function f and as a unsupervised training set s. So can I say that, in Table 2, your results are based on SVHN 'extra' training split as s domain traning set? But it is different from other methods since SVHN dataset includes train set, test set and extra training split. As far as I know, other methods are using train set as the s domain unsupervised tranining data. ", "OTHER_KEYS": "(anonymous)"}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "The authors propose a application of GANs to map images to new domains with no labels. E.g., an MNIST 3 is used to generate a SVHN 3. Ablation analysis is given to help understand the model. The results are (subjectively) impressive and the approach could be used for cross-domain transfer, an important problem. All in all, a strong paper.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"DATE": "23 Jan 2017", "TITLE": "Interesting Work", "IS_META_REVIEW": false, "comments": "This paper presents an unsupervised domain transfer from the image of domain S to the image of domain T. \nIt was really refreshing that this conversion was possible without any mapping data. \nFor example, in the paper, the model can transfer the SVHN image '3' to the MNIST image '3' without the mapping data.\nThe model can be roughly divided into GAN and Content Extractor (f in the paper).\n\n1. GAN\nDuring training, the discriminator sees the mnist image and learns to determine it as a real image. \nAnd with GAN loss, the generator learns to get the mnist image as output when it receives an svhn image as input to deceive the discriminator.\n\n2. Content Extractor\nIf the model use only GAN loss, the content in the image may not be retained even if the domain is changed.\nFor example, the generator may convert the svhn image '3' to the mnist image '2' to deceive the discriminator.\nIn this paper, authors introduce a new function called 'f' to maintain the content.\nThe generator includes f and generates a fake mnist image when it receives an svhn image as input.\nThe original svhn image and the generated fake mnist image are put back into f.\nThen additional loss function is set so that the resulting values \u200b\u200bare the same.\nHere, f is learning to extract content regardless of domain.\n\n\nI felt very fresh in this paper so i implemented this paper myself.\nHere is the code I implemented.\n\n", "OTHER_KEYS": "yunjey choi"}, {"DATE": "16 Jan 2017", "TITLE": "Authors' summary of discussion", "IS_META_REVIEW": false, "comments": "We thank the reviewers for their time and insights. All 3 reviewers seem to agree that the work is interesting, well-written and presents extensive experiments. R1 & R2 both note the f-constancy as a novelty of this work. Also, the fact that the method does not require training pairs for the two domains is noted by R3 as a major contribution, which \u201ccould be impactful in broad problem context\u201d. R2 & R3 both agree that the output generations are visually appealing. We have no factual dispute with the reviewers and replied to each individually below.\n\nThe open review discussion has been extremely beneficial to us and the paper has been revised in order to address all actionable items raised throughout the review period. We thank the reviewers for their thoughtful and constructive reviews and all other community members who shared their comments. During the review period, the work has already been cited several times, reimplemented on github, and drawn considerable attention.\n", "OTHER_KEYS": "Yaniv Taigman"}, {"DATE": "02 Jan 2017", "TITLE": "Revision #2", "IS_META_REVIEW": false, "comments": "Following the request of the reviewing team, we have just uploaded a new version of our manuscript, which includes expression preserving experiments. \nThe new experiments are in Appendix B. \n\nIn order to provide a quick way to track the changes from the original submission, we color all modifications in red. \n\nThank you for the extremely useful feedback.\n", "OTHER_KEYS": "Yaniv Taigman"}, {"OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "TITLE": "An interesting work", "comments": "This paper presents an unsupervised image transformation method that maps a sample from source domain to target domain. The major contribution lies in that it does not require aligned training pairs from two domains. The model is based on GANs. To make it work in the unsupervised setting, this paper decomposes the generation function into two modules: an encoder that identify a common feature space between two domains and an decoder that generates samples in the target domain. To avoid trivial solutions, this paper proposed two additional losses that penalize 1) the feature difference between a source sample and its transformed sample and 2) the pixel difference between a target sample and its re-generated sample. This paper presents extensive experiments on transferring SVHN digit images to MNIST style and transferring face images to emoji style. \n+The proposed learning method enables unsupervised domain transfer that could be impactful in broad problem contexts. \n+This paper presents careful ablation studies to analyze the effects of different components of the system, which is helpful for understanding the paper. \n+The transferred images are visually impressive and quantitative results also show the image identities are preserved across domains to some degree. \n-It will be more interesting to show results in other domains such as texts and images. \n-In addition to the face identities, it is also of great interest to analyze how well the facial attributes are preserved when mapping to target domain. ", "SOUNDNESS_CORRECTNESS": 5, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "30 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"IMPACT": 4, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "TITLE": "Interesting work, needs more explanations ", "comments": "Update: thank you for running more experiments, and add more explanations in the manuscript. They addressed most of my concerns, so I updated the score accordingly. \n\n\nThe work aims at learning a generative function G that can maps input from source domain to the target domain, such that a given representation function f remain unchanged accepting inputs from either domain. The criteria is termed f-constancy. The proposed method is evaluated on two visual domain adaptation tasks. \n\nThe paper is relatively easy to follow, and the authors provided quite extensive experimental results on the two datasets. f-constancy is the main novelty of the work. \n\nIt seems counter-intuitive to force the function G to be of g o f, i.e., starting from a restricted function f which might have already lost information. As in the face dataset, f is learned to optimize the performance of certain task on some external dataset. It is not clear if an input from the source or target domain can be recovered from applying G as in equation (5) and (6). Also, the f function is learned with a particular task in mind. As in the two experiments, the representation function f is learned to identify the digits in the source SVHN dataset or the identity of some face dataset.  As a result, the procedure has to be repeated if we were to perform domain adaptation for the same domains but for different tasks, such as recognizing expressions instead of identity. \n\nDo the authors have insight on why the baseline method proposed in equation (1) and (2) perform so poorly?\n\nFigure 5 shows some visual comparison between style transfer and the proposed method. It is not clear though which method is better. Will it be possible to apply style transfer to generate emojis from photos and repeat the experiments shown in table 4?", "ORIGINALITY": 4, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "20 Dec 2016 (modified: 13 Jan 2017)", "CLARITY": 4, "REVIEWER_CONFIDENCE": 3}, {"IMPACT": 2, "SUBSTANCE": 3, "MEANINGFUL_COMPARISON": 3, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "Update: After reading the rebuttal comments and the revised paper, I'm leaving the rating as it was before.\n\nThis paper proposes an unsupervised algorithm for transferring samples from one domain to another (related) domain under the constraint that some predefined f returns same result for the input and the result.\n\nPros:\n1. The paper presents an interesting idea of comparing samples from different domains using a fixed perceptual function f.\n\n2. The proposed method produces visually appealing results on several datasets\n\n3. The authors demonstrate how their approach can be used for domain adaptation and obtain improved results on the SVHN->MNIST task\n\n4. The paper is well-written and easy to read\n\nCons:\n1. The novelty of the method is relatively minor (I consider f-constancy term as the main contribution)\n\n2. It feels like the proposed approach would break for more dissimilar domains. The method relies on a fixed f which is trained on the source domain. This f can potentially drop information important for obtaining 1) better reconstructions in the target domain  2) more tightly related x and g(f(x)). I think the authors should consider either training all the modules in the model end-to-end or incorporating target samples into the training of f.\n\n3. A single domain adaptation experiment is definitely not enough to consider the proposed method as a universal alternative to the existing DA approaches.\n\nI would also like to point out that using super-resolved outputs as opposed to the actual model\u2019s outputs can produce a false impression of the visual quality of the transferred samples. I\u2019d suggest moving original outputs from the appendix into the main part.", "SOUNDNESS_CORRECTNESS": 5, "ORIGINALITY": 2, "IS_ANNOTATED": true, "TITLE": "Final review", "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "19 Dec 2016 (modified: 23 Jan 2017)", "REVIEWER_CONFIDENCE": 3}, {"DATE": "19 Dec 2016", "TITLE": "Revision", "IS_META_REVIEW": false, "comments": "Following the ongoing discussion with the reviewing team as well as with other readers, we have just uploaded a new version of our manuscript, which is aimed at improving clarity. \nIn order to provide a quick way to track the changes, we color all modifications in red. \nIn addition, as mentioned below, we will soon share our open implementation in Torch. \nThank you all for the extremely useful feedback.", "OTHER_KEYS": "Yaniv Taigman"}, {"DATE": "19 Dec 2016", "TITLE": "Accuracy without reconstructing images in target domain", "IS_META_REVIEW": false, "comments": "Given the goal of transferring samples from source domain to target domain, I am wondering whether it is necessary to simultaneously train the model to reconstruct images in the target domain. Any ablation studies on this?", "OTHER_KEYS": "Xun Huang"}, {"IMPACT": 4, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "TITLE": "Explanations", "comments": "", "ORIGINALITY": 4, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "DATE": "10 Dec 2016", "CLARITY": 4}, {"IMPACT": 2, "SUBSTANCE": 3, "MEANINGFUL_COMPARISON": 3, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "", "SOUNDNESS_CORRECTNESS": 5, "ORIGINALITY": 2, "IS_ANNOTATED": true, "TITLE": "Questions", "IS_META_REVIEW": false, "DATE": "02 Dec 2016 (modified: 03 Dec 2016)"}, {"DATE": "12 Nov 2016", "TITLE": "About the configuration of the generator ", "IS_META_REVIEW": false, "comments": "Hi, it is really interesting and I want to re-implement this work. But the paper doesn't provide much information about the configuration of the generator. For example, do you use a fully connected layer as the first layer? If true, what is the dimension?  How many filters of each deconvolution layer do you use ?  Thank you very much.", "OTHER_KEYS": "(anonymous)"}, {"DATE": "11 Nov 2016", "TITLE": "One puzzle", "IS_META_REVIEW": false, "comments": "In section 5.1, you map SVHN-trained representation to 32*32 grayscale images, but the encoder f's input is three-channel, how do you solve this ? Why don't you transform RGB SVHN images into grayscale images or replicate the grayscale MNIST images three times ? I mean why don't you use the same image format for training f and the adversarial network? ", "OTHER_KEYS": "(anonymous)"}, {"DATE": "08 Nov 2016", "TITLE": "How is this work different from style transfer?", "IS_META_REVIEW": false, "comments": "Can the authors comment on what they are doing in terms of end-use/goal in addition to what we know from style transfer already?", "OTHER_KEYS": "(anonymous)"}, {"DATE": "06 Nov 2016", "TITLE": "About the reference and one minor error", "IS_META_REVIEW": false, "comments": "This work is really interesting, but I find the network architecture and loss functions in this work are extremely similar to that in the work \"Generating Images with Perceptual Similarity Metrics based on Deep Networks\" by Alexey Dosoviskiy and Thomas Brox.  \n\nThe loss function in this work vs that of the latter: \nL_const is equal to L_feat;\nL_tid is equall to L_img;\nL_GANG is equal to L_adv.\n\nHowever, I do not see the later's name in your reference paper list.  Besides, have you forgotten to print a 'log' before D_3(x) in your L_D loss  on page 4?", "OTHER_KEYS": "WANG Zongwei"}, {"IS_META_REVIEW": true, "comments": "Update: After reading the rebuttal comments and the revised paper, I'm leaving the rating as it was before.\n\nThis paper proposes an unsupervised algorithm for transferring samples from one domain to another (related) domain under the constraint that some predefined f returns same result for the input and the result.\n\nPros:\n1. The paper presents an interesting idea of comparing samples from different domains using a fixed perceptual function f.\n\n2. The proposed method produces visually appealing results on several datasets\n\n3. The authors demonstrate how their approach can be used for domain adaptation and obtain improved results on the SVHN->MNIST task\n\n4. The paper is well-written and easy to read\n\nCons:\n1. The novelty of the method is relatively minor (I consider f-constancy term as the main contribution)\n\n2. It feels like the proposed approach would break for more dissimilar domains. The method relies on a fixed f which is trained on the source domain. This f can potentially drop information important for obtaining 1) better reconstructions in the target domain  2) more tightly related x and g(f(x)). I think the authors should consider either training all the modules in the model end-to-end or incorporating target samples into the training of f.\n\n3. A single domain adaptation experiment is definitely not enough to consider the proposed method as a universal alternative to the existing DA approaches.\n\nI would also like to point out that using super-resolved outputs as opposed to the actual model\u2019s outputs can produce a false impression of the visual quality of the transferred samples. I\u2019d suggest moving original outputs from the appendix into the main part."}, {"DATE": "17 Apr 2017 (modified: 18 Apr 2017)", "TITLE": "Training set for SVHN domain?", "IS_META_REVIEW": false, "comments": "In section 5.1, you said that you employ the extra training split of SVHN for two purposes: learning the function f and as a unsupervised training set s. So can I say that, in Table 2, your results are based on SVHN 'extra' training split as s domain traning set? But it is different from other methods since SVHN dataset includes train set, test set and extra training split. As far as I know, other methods are using train set as the s domain unsupervised tranining data. ", "OTHER_KEYS": "(anonymous)"}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "The authors propose a application of GANs to map images to new domains with no labels. E.g., an MNIST 3 is used to generate a SVHN 3. Ablation analysis is given to help understand the model. The results are (subjectively) impressive and the approach could be used for cross-domain transfer, an important problem. All in all, a strong paper.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"DATE": "23 Jan 2017", "TITLE": "Interesting Work", "IS_META_REVIEW": false, "comments": "This paper presents an unsupervised domain transfer from the image of domain S to the image of domain T. \nIt was really refreshing that this conversion was possible without any mapping data. \nFor example, in the paper, the model can transfer the SVHN image '3' to the MNIST image '3' without the mapping data.\nThe model can be roughly divided into GAN and Content Extractor (f in the paper).\n\n1. GAN\nDuring training, the discriminator sees the mnist image and learns to determine it as a real image. \nAnd with GAN loss, the generator learns to get the mnist image as output when it receives an svhn image as input to deceive the discriminator.\n\n2. Content Extractor\nIf the model use only GAN loss, the content in the image may not be retained even if the domain is changed.\nFor example, the generator may convert the svhn image '3' to the mnist image '2' to deceive the discriminator.\nIn this paper, authors introduce a new function called 'f' to maintain the content.\nThe generator includes f and generates a fake mnist image when it receives an svhn image as input.\nThe original svhn image and the generated fake mnist image are put back into f.\nThen additional loss function is set so that the resulting values \u200b\u200bare the same.\nHere, f is learning to extract content regardless of domain.\n\n\nI felt very fresh in this paper so i implemented this paper myself.\nHere is the code I implemented.\n\n", "OTHER_KEYS": "yunjey choi"}, {"DATE": "16 Jan 2017", "TITLE": "Authors' summary of discussion", "IS_META_REVIEW": false, "comments": "We thank the reviewers for their time and insights. All 3 reviewers seem to agree that the work is interesting, well-written and presents extensive experiments. R1 & R2 both note the f-constancy as a novelty of this work. Also, the fact that the method does not require training pairs for the two domains is noted by R3 as a major contribution, which \u201ccould be impactful in broad problem context\u201d. R2 & R3 both agree that the output generations are visually appealing. We have no factual dispute with the reviewers and replied to each individually below.\n\nThe open review discussion has been extremely beneficial to us and the paper has been revised in order to address all actionable items raised throughout the review period. We thank the reviewers for their thoughtful and constructive reviews and all other community members who shared their comments. During the review period, the work has already been cited several times, reimplemented on github, and drawn considerable attention.\n", "OTHER_KEYS": "Yaniv Taigman"}, {"DATE": "02 Jan 2017", "TITLE": "Revision #2", "IS_META_REVIEW": false, "comments": "Following the request of the reviewing team, we have just uploaded a new version of our manuscript, which includes expression preserving experiments. \nThe new experiments are in Appendix B. \n\nIn order to provide a quick way to track the changes from the original submission, we color all modifications in red. \n\nThank you for the extremely useful feedback.\n", "OTHER_KEYS": "Yaniv Taigman"}, {"OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "TITLE": "An interesting work", "comments": "This paper presents an unsupervised image transformation method that maps a sample from source domain to target domain. The major contribution lies in that it does not require aligned training pairs from two domains. The model is based on GANs. To make it work in the unsupervised setting, this paper decomposes the generation function into two modules: an encoder that identify a common feature space between two domains and an decoder that generates samples in the target domain. To avoid trivial solutions, this paper proposed two additional losses that penalize 1) the feature difference between a source sample and its transformed sample and 2) the pixel difference between a target sample and its re-generated sample. This paper presents extensive experiments on transferring SVHN digit images to MNIST style and transferring face images to emoji style. \n+The proposed learning method enables unsupervised domain transfer that could be impactful in broad problem contexts. \n+This paper presents careful ablation studies to analyze the effects of different components of the system, which is helpful for understanding the paper. \n+The transferred images are visually impressive and quantitative results also show the image identities are preserved across domains to some degree. \n-It will be more interesting to show results in other domains such as texts and images. \n-In addition to the face identities, it is also of great interest to analyze how well the facial attributes are preserved when mapping to target domain. ", "SOUNDNESS_CORRECTNESS": 5, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "30 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"IMPACT": 4, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "TITLE": "Interesting work, needs more explanations ", "comments": "Update: thank you for running more experiments, and add more explanations in the manuscript. They addressed most of my concerns, so I updated the score accordingly. \n\n\nThe work aims at learning a generative function G that can maps input from source domain to the target domain, such that a given representation function f remain unchanged accepting inputs from either domain. The criteria is termed f-constancy. The proposed method is evaluated on two visual domain adaptation tasks. \n\nThe paper is relatively easy to follow, and the authors provided quite extensive experimental results on the two datasets. f-constancy is the main novelty of the work. \n\nIt seems counter-intuitive to force the function G to be of g o f, i.e., starting from a restricted function f which might have already lost information. As in the face dataset, f is learned to optimize the performance of certain task on some external dataset. It is not clear if an input from the source or target domain can be recovered from applying G as in equation (5) and (6). Also, the f function is learned with a particular task in mind. As in the two experiments, the representation function f is learned to identify the digits in the source SVHN dataset or the identity of some face dataset.  As a result, the procedure has to be repeated if we were to perform domain adaptation for the same domains but for different tasks, such as recognizing expressions instead of identity. \n\nDo the authors have insight on why the baseline method proposed in equation (1) and (2) perform so poorly?\n\nFigure 5 shows some visual comparison between style transfer and the proposed method. It is not clear though which method is better. Will it be possible to apply style transfer to generate emojis from photos and repeat the experiments shown in table 4?", "ORIGINALITY": 4, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "20 Dec 2016 (modified: 13 Jan 2017)", "CLARITY": 4, "REVIEWER_CONFIDENCE": 3}, {"IMPACT": 2, "SUBSTANCE": 3, "MEANINGFUL_COMPARISON": 3, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "Update: After reading the rebuttal comments and the revised paper, I'm leaving the rating as it was before.\n\nThis paper proposes an unsupervised algorithm for transferring samples from one domain to another (related) domain under the constraint that some predefined f returns same result for the input and the result.\n\nPros:\n1. The paper presents an interesting idea of comparing samples from different domains using a fixed perceptual function f.\n\n2. The proposed method produces visually appealing results on several datasets\n\n3. The authors demonstrate how their approach can be used for domain adaptation and obtain improved results on the SVHN->MNIST task\n\n4. The paper is well-written and easy to read\n\nCons:\n1. The novelty of the method is relatively minor (I consider f-constancy term as the main contribution)\n\n2. It feels like the proposed approach would break for more dissimilar domains. The method relies on a fixed f which is trained on the source domain. This f can potentially drop information important for obtaining 1) better reconstructions in the target domain  2) more tightly related x and g(f(x)). I think the authors should consider either training all the modules in the model end-to-end or incorporating target samples into the training of f.\n\n3. A single domain adaptation experiment is definitely not enough to consider the proposed method as a universal alternative to the existing DA approaches.\n\nI would also like to point out that using super-resolved outputs as opposed to the actual model\u2019s outputs can produce a false impression of the visual quality of the transferred samples. I\u2019d suggest moving original outputs from the appendix into the main part.", "SOUNDNESS_CORRECTNESS": 5, "ORIGINALITY": 2, "IS_ANNOTATED": true, "TITLE": "Final review", "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "19 Dec 2016 (modified: 23 Jan 2017)", "REVIEWER_CONFIDENCE": 3}, {"DATE": "19 Dec 2016", "TITLE": "Revision", "IS_META_REVIEW": false, "comments": "Following the ongoing discussion with the reviewing team as well as with other readers, we have just uploaded a new version of our manuscript, which is aimed at improving clarity. \nIn order to provide a quick way to track the changes, we color all modifications in red. \nIn addition, as mentioned below, we will soon share our open implementation in Torch. \nThank you all for the extremely useful feedback.", "OTHER_KEYS": "Yaniv Taigman"}, {"DATE": "19 Dec 2016", "TITLE": "Accuracy without reconstructing images in target domain", "IS_META_REVIEW": false, "comments": "Given the goal of transferring samples from source domain to target domain, I am wondering whether it is necessary to simultaneously train the model to reconstruct images in the target domain. Any ablation studies on this?", "OTHER_KEYS": "Xun Huang"}, {"IMPACT": 4, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "TITLE": "Explanations", "comments": "", "ORIGINALITY": 4, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "DATE": "10 Dec 2016", "CLARITY": 4}, {"IMPACT": 2, "SUBSTANCE": 3, "MEANINGFUL_COMPARISON": 3, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "", "SOUNDNESS_CORRECTNESS": 5, "ORIGINALITY": 2, "IS_ANNOTATED": true, "TITLE": "Questions", "IS_META_REVIEW": false, "DATE": "02 Dec 2016 (modified: 03 Dec 2016)"}, {"DATE": "12 Nov 2016", "TITLE": "About the configuration of the generator ", "IS_META_REVIEW": false, "comments": "Hi, it is really interesting and I want to re-implement this work. But the paper doesn't provide much information about the configuration of the generator. For example, do you use a fully connected layer as the first layer? If true, what is the dimension?  How many filters of each deconvolution layer do you use ?  Thank you very much.", "OTHER_KEYS": "(anonymous)"}, {"DATE": "11 Nov 2016", "TITLE": "One puzzle", "IS_META_REVIEW": false, "comments": "In section 5.1, you map SVHN-trained representation to 32*32 grayscale images, but the encoder f's input is three-channel, how do you solve this ? Why don't you transform RGB SVHN images into grayscale images or replicate the grayscale MNIST images three times ? I mean why don't you use the same image format for training f and the adversarial network? ", "OTHER_KEYS": "(anonymous)"}, {"DATE": "08 Nov 2016", "TITLE": "How is this work different from style transfer?", "IS_META_REVIEW": false, "comments": "Can the authors comment on what they are doing in terms of end-use/goal in addition to what we know from style transfer already?", "OTHER_KEYS": "(anonymous)"}, {"DATE": "06 Nov 2016", "TITLE": "About the reference and one minor error", "IS_META_REVIEW": false, "comments": "This work is really interesting, but I find the network architecture and loss functions in this work are extremely similar to that in the work \"Generating Images with Perceptual Similarity Metrics based on Deep Networks\" by Alexey Dosoviskiy and Thomas Brox.  \n\nThe loss function in this work vs that of the latter: \nL_const is equal to L_feat;\nL_tid is equall to L_img;\nL_GANG is equal to L_adv.\n\nHowever, I do not see the later's name in your reference paper list.  Besides, have you forgotten to print a 'log' before D_3(x) in your L_D loss  on page 4?", "OTHER_KEYS": "WANG Zongwei"}]}
{"text": "DESIGNING NEURAL NETWORK ARCHITECTURES\n1 INTRODUCTION\nDeep convolutional neural networks (CNNs) have seen great success in the past few years on a variety of machine learning problems (LeCun et al., 2015). A typical CNN architecture consists of several convolution, pooling, and fully connected layers. While constructing a CNN, a network designer has to make numerous design choices: the number of layers of each type, the ordering of layers, and the hyperparameters for each type of layer, e.g., the receptive field size, stride, and number of receptive fields for a convolution layer. The number of possible choices makes the design space of CNN architectures extremely large and hence, infeasible for an exhaustive manual search. While there has been some work (Pinto et al., 2009; Bergstra et al., 2013; Domhan et al., 2015) on automated or computer-aided neural network design, new CNN architectures or network design elements are still primarily developed by researchers using new theoretical insights or intuition gained from experimentation.\nIn this paper, we seek to automate the process of CNN architecture selection through a metamodeling procedure based on reinforcement learning. We construct a novel Q-learning agent whose goal is to discover CNN architectures that perform well on a given machine learning task with no human intervention. The learning agent is given the task of sequentially picking layers of a CNN model. By discretizing and limiting the layer parameters to choose from, the agent is left with a finite but large space of model architectures to search from. The agent learns through random exploration and slowly begins to exploit its findings to select higher performing models using the - greedy strategy (Mnih et al., 2015). The agent receives the validation accuracy on the given machine learning task as the reward for selecting an architecture. We expedite the learning process through repeated memory sampling using experience replay (Lin, 1993). We refer to this Q-learning based meta-modeling method as MetaQNN, which is summarized in Figure 1.1\nWe conduct experiments with a space of model architectures consisting of only standard convolution, pooling, and fully connected layers using three standard image classification datasets: CIFAR-10,\n1For more information, model files, and code, please visit https://bowenbaker.github.io/metaqnn/\nSVHN, and MNIST. The learning agent discovers CNN architectures that beat all existing networks designed only with the same layer types (e.g., Springenberg et al. (2014); Srivastava et al. (2015)). In addition, their performance is competitive against network designs that include complex layer types and training procedures (e.g., Clevert et al. (2015); Lee et al. (2016)). Finally, the MetaQNN selected models comfortably outperform previous automated network design methods (Stanley & Miikkulainen, 2002; Bergstra et al., 2013). The top network designs discovered by the agent on one dataset are also competitive when trained on other datasets, indicating that they are suited for transfer learning tasks. Moreover, we can generate not just one, but several varied, well-performing network designs, which can be ensembled to further boost the prediction performance.\n2 RELATED WORK\nDesigning neural network architectures: Research on automating neural network design goes back to the 1980s when genetic algorithm-based approaches were proposed to find both architectures and weights (Schaffer et al., 1992). However, to the best of our knowledge, networks designed with genetic algorithms, such as those generated with the NEAT algorithm (Stanley & Miikkulainen, 2002), have been unable to match the performance of hand-crafted networks on standard benchmarks (Verbancsics & Harguess, 2013). Other biologically inspired ideas have also been explored; motivated by screening methods in genetics, Pinto et al. (2009) proposed a high-throughput network selection approach where they randomly sample thousands of architectures and choose promising ones for further training. In recent work, Saxena & Verbeek (2016) propose to sidestep the architecture selection process through densely connected networks of layers, which come closer to the performance of hand-crafted networks.\nBayesian optimization has also been used (Shahriari et al., 2016) for automatic selection of network architectures (Bergstra et al., 2013; Domhan et al., 2015) and hyperparameters (Snoek et al., 2012; Swersky et al., 2013). Notably, Bergstra et al. (2013) proposed a meta-modeling approach based on Tree of Parzen Estimators (TPE) (Bergstra et al., 2011) to choose both the type of layers and hyperparameters of feed-forward networks; however, they fail to match the performance of handcrafted networks.\nReinforcement Learning: Recently there has been much work at the intersection of reinforcement learning and deep learning. For instance, methods using CNNs to approximate theQ-learning utility function (Watkins, 1989) have been successful in game-playing agents (Mnih et al., 2015; Silver et al., 2016) and robotic control (Lillicrap et al., 2015; Levine et al., 2016). These methods rely on phases of exploration, where the agent tries to learn about its environment through sampling, and exploitation, where the agent uses what it learned about the environment to find better paths. In traditional reinforcement learning settings, over-exploration can lead to slow convergence times, yet over-exploitation can lead to convergence to local minima (Kaelbling et al., 1996). However, in the case of large or continuous state spaces, the -greedy strategy of learning has been empirically shown to converge (Vermorel & Mohri, 2005). Finally, when the state space is large or exploration is costly,\nthe experience replay technique (Lin, 1993) has proved useful in experimental settings (Adam et al., 2012; Mnih et al., 2015). We incorporate these techniques\u2014Q-learning, the -greedy strategy and experience replay\u2014in our algorithm design.\n3 BACKGROUND\nOur method relies on Q-learning, a type of reinforcement learning. We now summarize the theoretical formulation of Q-learning, as adopted to our problem. Consider the task of teaching an agent to find optimal paths as a Markov Decision Process (MDP) in a finite-horizon environment. Constraining the environment to be finite-horizon ensures that the agent will deterministically terminate in a finite number of time steps. In addition, we restrict the environment to have a discrete and finite state space S as well as action space U . For any state si \u2208 S , there is a finite set of actions, U(si) \u2286 U , that the agent can choose from. In an environment with stochastic transitions, an agent in state si taking some action u \u2208 U(si) will transition to state sj with probability ps\u2032|s,u(sj |si, u), which may be unknown to the agent. At each time step t, the agent is given a reward rt, dependent on the transition from state s to s\u2032 and action u. rt may also be stochastic according to a distribution pr|s\u2032,s,u. The agent\u2019s goal is to maximize the total expected reward over all possible trajectories, i.e., maxTi\u2208T RTi , where the total expected reward for a trajectory Ti is\nRTi = \u2211 (s,u,s\u2032)\u2208Ti Er|s,u,s\u2032 [r|s, u, s \u2032]. (1)\nThough we limit the agent to a finite state and action space, there are still a combinatorially large number of trajectories, which motivates the use of reinforcement learning. We define the maximization problem recursively in terms of subproblems as follows. For any state si \u2208 S and subsequent action u \u2208 U(si), we define the maximum total expected reward to be Q\u2217(si, u). Q\u2217(\u00b7) is known as the action-value function and individual Q\u2217(si, u) are know as Q-values. The recursive maximization equation, which is known as Bellman\u2019s Equation, can be written as\nQ\u2217(si, u) = Esj |si,u [ Er|si,u,sj [r|si, u, sj ] + \u03b3maxu\u2032\u2208U(sj)Q\u2217(sj , u\u2032) ] . (2)\nIn many cases, it is impossible to analytically solve Bellman\u2019s Equation (Bertsekas, 2015), but it can be formulated as an iterative update\nQt+1(si, u) = (1\u2212 \u03b1)Qt(si, u) + \u03b1 [ rt + \u03b3maxu\u2032\u2208U(sj)Qt(sj , u \u2032) ] . (3)\nEquation 3 is the simplest form of Q-learning proposed by Watkins (1989). For well formulated problems, limt\u2192\u221eQt(s, u) = Q\u2217(s, u), as long as each transition is sampled infinitely many times (Bertsekas, 2015). The update equation has two parameters: (i) \u03b1 is a Q-learning rate which determines the weight given to new information over old information, and (ii) \u03b3 is the discount factor which determines the weight given to short-term rewards over future rewards. The Q-learning algorithm is model-free, in that the learning agent can solve the task without ever explicitly constructing an estimate of environmental dynamics. In addition, Q-learning is off policy, meaning it can learn about optimal policies while exploring via a non-optimal behavioral distribution, i.e. the distribution by which the agent explores its environment.\nWe choose the behavior distribution using an -greedy strategy (Mnih et al., 2015). With this strategy, a random action is taken with probability and the greedy action, maxu\u2208U(si)Qt(si, u), is chosen with probability 1\u2212 . We anneal from 1\u2192 0 such that the agent begins in an exploration phase and slowly starts moving towards the exploitation phase. In addition, when the exploration cost is large (which is true for our problem setting), it is beneficial to use the experience replay technique for faster convergence (Lin, 1992). In experience replay, the learning agent is provided with a memory of its past explored paths and rewards. At a given interval, the agent samples from the memory and updates its Q-values via Equation 3.\n4 DESIGNING NEURAL NETWORK ARCHITECTURES WITH Q-LEARNING\nWe consider the task of training a learning agent to sequentially choose neural network layers. Figure 2 shows feasible state and action spaces (a) and a potential trajectory the agent may take along with the CNN architecture defined by this trajectory (b). We model the layer selection process as a Markov Decision Process with the assumption that a well-performing layer in one network should\nalso perform well in another network. We make this assumption based on the hierarchical nature of the feature representations learned by neural networks with many hidden layers (LeCun et al., 2015). The agent sequentially selects layers via the -greedy strategy until it reaches a termination state. The CNN architecture defined by the agent\u2019s path is trained on the chosen learning problem, and the agent is given a reward equal to the validation accuracy. The validation accuracy and architecture description are stored in a replay memory, and experiences are sampled periodically from the replay memory to update Q-values via Equation 3. The agent follows an schedule which determines its shift from exploration to exploitation.\nOur method requires three main design choices: (i) reducing CNN layer definitions to simple state tuples, (ii) defining a set of actions the agent may take, i.e., the set of layers the agent may pick next given its current state, and (iii) balancing the size of the state-action space\u2014and correspondingly, the model capacity\u2014with the amount of exploration needed by the agent to converge. We now describe the design choices and the learning process in detail.\n4.1 THE STATE SPACE\nEach state is defined as a tuple of all relevant layer parameters. We allow five different types of layers: convolution (C), pooling (P), fully connected (FC), global average pooling (GAP), and softmax (SM), though the general method is not limited to this set. Table 1 shows the relevant parameters for each layer type and also the discretization we chose for each parameter. Each layer has a parameter layer depth (shown as Layer 1, 2, ... in Figure 2). Adding layer depth to the state space allows us to constrict the action space such that the state-action graph is directed and acyclic (DAG) and also allows us to specify a maximum number of layers the agent may select before terminating.\nEach layer type also has a parameter called representation size (R-size). Convolutional nets progressively compress the representation of the original signal through pooling and convolution. The presence of these layers in our state space may lead the agent on a trajectory where the intermediate signal representation gets reduced to a size that is too small for further processing. For example, five 2\u00d7 2 pooling layers each with stride 2 will reduce an image of initial size 32\u00d7 32 to size 1\u00d7 1. At this stage, further pooling, or convolution with receptive field size greater than 1, would be meaningless and degenerate. To avoid such scenarios, we add the R-size parameter to the state tuple s, which allows us to restrict actions from states with R-size n to those that have a receptive field size less than or equal to n. To further constrict the state space, we chose to bin the representation sizes into three discrete buckets. However, binning adds uncertainty to the state transitions: depending on the true underlying representation size, a pooling layer may or may not change the R-size bin. As a result, the action of pooling can lead to two different states, which we model as stochasticity in state transitions. Please see Figure A1 in appendix for an illustrated example.\n4.2 THE ACTION SPACE\nWe restrict the agent from taking certain actions to both limit the state-action space and make learning tractable. First, we allow the agent to terminate a path at any point, i.e. it may choose a termination state from any non-termination state. In addition, we only allow transitions for a state with layer depth i to a state with layer depth i + 1, which ensures that there are no loops in the graph. This constraint ensures that the state-action graph is always a DAG. Any state at the maximum layer depth, as prescribed in Table 1, may only transition to a termination layer.\nNext, we limit the number of fully connected (FC) layers to be at maximum two, because a large number of FC layers can lead to too may learnable parameters. The agent at a state with type FC may transition to another state with type FC if and only if the number of consecutive FC states is less than the maximum allowed. Furthermore, a state s of type FC with number of neurons d may only transition to either a termination state or a state s\u2032 of type FC with number of neurons d\u2032 \u2264 d. An agent at a state of type convolution (C) may transition to a state with any other layer type. An agent at a state with layer type pooling (P) may transition to a state with any other layer type other than another P state because consecutive pooling layers are equivalent to a single, larger pooling layer which could lie outside of our chosen state space. Furthermore, only states with representation size in bins (8, 4] and (4, 1] may transition to an FC layer, which ensures that the number of weights does not become unreasonably huge. Note that a majority of these constraints are in place to enable faster convergence on our limited hardware (see Section 5) and not a limitation of the method in itself.\n4.3 Q-LEARNING TRAINING PROCEDURE\nFor the iterativeQ-learning updates (Equation 3), we set theQ-learning rate (\u03b1) to 0.01. In addition, we set the discount factor (\u03b3) to 1 to not over-prioritize short-term rewards. We decrease from 1.0 to 0.1 in steps, where the step-size is defined by the number of unique models trained (Table 2). At = 1.0, the agent samples CNN architecture with a random walk along a uniformly weighted Markov chain. Every topology sampled by the agent is trained using the procedure described in Section 5, and the prediction performance of this network topology on the validation set is recorded. We train a larger number of models at = 1.0 as compared to other values of to ensure that the agent has adequate time to explore before it begins to exploit. We stop the agent at = 0.1 (and not at = 0) to obtain a stochastic final policy, which generates perturbations of the global minimum.2 Ideally, we want to identify several well-performing model topologies, which can then be ensembled to improve prediction performance.\nDuring the entire training process (starting at = 1.0), we maintain a replay dictionary which stores (i) the network topology and (ii) prediction performance on a validation set, for all of the sampled\n2 = 0 indicates a completely deterministic policy. Because we would like to generate several good models for ensembling and analysis, we stop at = 0.1, which represents a stochastic final policy.\nmodels. If a model that has already been trained is re-sampled, it is not re-trained, but instead the previously found validation accuracy is presented to the agent. After each model is sampled and trained, the agent randomly samples 100 models from the replay dictionary and applies the Q-value update defined in Equation 3 for all transitions in each sampled sequence. The Q-value update is applied to the transitions in temporally reversed order, which has been shown to speed up Q-values convergence (Lin, 1993).\n5 EXPERIMENT DETAILS\nDuring the model exploration phase, we trained each network topology with a quick and aggressive training scheme. For each experiment, we created a validation set by randomly taking 5,000 samples from the training set such that the resulting class distributions were unchanged. For every network, a dropout layer was added after every two layers. The ith dropout layer, out of a total n dropout layers, had a dropout probability of i2n . Each model was trained for a total of 20 epochs with the Adam optimizer (Kingma & Ba, 2014) with \u03b21 = 0.9, \u03b22 = 0.999, \u03b5 = 10\u22128. The batch size was set to 128, and the initial learning rate was set to 0.001. If the model failed to perform better than a random predictor after the first epoch, we reduced the learning rate by a factor of 0.4 and restarted training, for a maximum of 5 restarts. For models that started learning (i.e., performed better than a random predictor), we reduced the learning rate by a factor of 0.2 every 5 epochs. All weights were initialized with Xavier initialization (Glorot & Bengio, 2010). Our experiments using Caffe (Jia et al., 2014) took 8-10 days to complete for each dataset with a hardware setup consisting of 10 NVIDIA GPUs.\nAfter the agent completed the schedule (Table 2), we selected the top ten models that were found over the course of exploration. These models were then finetuned using a much longer training schedule, and only the top five were used for ensembling. We now provide details of the datasets and the finetuning process.\nThe Street View House Numbers (SVHN) dataset has 10 classes with a total of 73,257 samples in the original training set, 26,032 samples in the test set, and 531,131 additional samples in the extended training set. During the exploration phase, we only trained with the original training set, using 5,000 random samples as validation. We finetuned the top ten models with the original plus extended training set, by creating preprocessed training and validation sets as described by Lee et al. (2016). Our final learning rate schedule after tuning on validation set was 0.025 for 5 epochs, 0.0125 for 5 epochs, 0.0001 for 20 epochs, and 0.00001 for 10 epochs.\nCIFAR-10, the 10 class tiny image dataset, has 50,000 training samples and 10,000 testing samples. During the exploration phase, we took 5,000 random samples from the training set for validation. The maximum layer depth was increased to 18. After the experiment completed, we used the same validation set to tune hyperparameters, resulting in a final training scheme which we ran on the entire training set. In the final training scheme, we set a learning rate of 0.025 for 40 epochs, 0.0125 for 40 epochs, 0.0001 for 160 epochs, and 0.00001 for 60 epochs, with all other parameters unchanged. During this phase, we preprocess using global contrast normalization and use moderate data augmentation, which consists of random mirroring and random translation by up to 5 pixels.\nMNIST, the 10 class handwritten digits dataset, has 60,000 training samples and 10,000 testing samples. We preprocessed each image with global mean subtraction. In the final training scheme, we trained each model for 40 epochs and decreased learning rate every 5 epochs by a factor of 0.2. For further tuning details please see Appendix C.\n6 RESULTS\nModel Selection Analysis: From Q-learning principles, we expect the learning agent to improve in its ability to pick network topologies as reduces and the agent enters the exploitation phase. In\nFigure 3, we plot the rolling mean of prediction accuracy over 100 models and the mean accuracy of models sampled at different values, for the CIFAR-10 and SVHN experiments. The plots show that, while the prediction accuracy remains flat during the exploration phase ( = 1) as expected, the agent consistently improves in its ability to pick better-performing models as reduces from 1 to 0.1. For example, the mean accuracy of models in the SVHN experiment increases from 52.25% at = 1 to 88.02% at = 0.1. Furthermore, we demonstrate the stability of the Q-learning procedure with 10 independent runs on a subset of the SVHN dataset in Section D.1 of the Appendix. Additional analysis of Q-learning results can be found in Section D.2.\nThe top models selected by the Q-learning agent vary in the number of parameters but all demonstrate high performance (see Appendix Tables 1-3). For example, the number of parameters for the top five CIFAR-10 models range from 11.26 million to 1.10 million, with only a 2.32% decrease in test error. We find design motifs common to the top hand-crafted network architectures as well. For example, the agent often chooses a layer of type C(N, 1, 1) as the first layer in the network. These layers generate N learnable linear transformations of the input data, which is similar in spirit to preprocessing of input data from RGB to a different color spaces such as YUV, as found in prior work (Sermanet et al., 2012; 2013).\nPrediction Performance: We compare the prediction performance of the MetaQNN networks discovered by theQ-learning agent with state-of-the-art methods on three datasets. We report the accuracy of our best model, along with an ensemble of top five models. First, we compare MetaQNN with six existing architectures that are designed with standard convolution, pooling, and fully-connected layers alone, similar to our designs. As seen in Table 3, our top model alone, as well as the committee ensemble of five models, outperforms all similar models. Next, we compare our results with six top networks overall, which contain complex layer types and design ideas, including generalized pooling functions, residual connections, and recurrent modules. Our results are competitive with these methods as well (Table 4). Finally, our method outperforms existing automated network de-\nsign methods. MetaQNN obtains an error of 6.92% as compared to 21.2% reported by Bergstra et al. (2011) on CIFAR-10; and it obtains an error of 0.32% as compared to 7.9% reported by Verbancsics & Harguess (2013) on MNIST.\nThe difference in validation error between the top 10 models for MNIST was very small, so we also created an ensemble with all 10 models. This ensemble achieved a test error of 0.28%\u2014which beats the current state-of-the-art on MNIST without data augmentation.\nThe best CIFAR-10 model performs 1-2% better than the four next best models, which is why the ensemble accuracy is lower than the best model\u2019s accuracy. We posit that the CIFAR-10 MetaQNN did not have adequate exploration time given the larger state space compared to that of the SVHN experiment, causing it to not find more models with performance similar to the best model. Furthermore, the coarse training scheme could have been not as well suited for CIFAR-10 as it was for SVHN, causing some models to under perform.\nTransfer Learning Ability: Network designs such as VGGnet (Simonyan & Zisserman, 2014) can be adopted to solve a variety of computer vision problems. To check if the MetaQNN networks provide similar transfer learning ability, we use the best MetaQNN model on the CIFAR-10 dataset for training other computer vision tasks. The model performs well (Table 5) both when training from random initializations, and finetuning from existing weights.\n7 CONCLUDING REMARKS\nNeural networks are being used in an increasingly wide variety of domains, which calls for scalable solutions to produce problem-specific model architectures. We take a step towards this goal and show that a meta-modeling approach using reinforcement learning is able to generate tailored CNN designs for different image classification tasks. Our MetaQNN networks outperform previous metamodeling methods as well as hand-crafted networks which use the same types of layers.\nWhile we report results for image classification problems, our method could be applied to different problem settings, including supervised (e.g., classification, regression) and unsupervised (e.g., autoencoders). The MetaQNN method could also aid constraint-based network design, by optimizing parameters such as size, speed, and accuracy. For instance, one could add a threshold in the state-action space barring the agent from creating models larger than the desired limit. In addition,\n\u2217Results in this column obtained with the top MetaQNN architecture for CIFAR-10, trained from random initialization with CIFAR-100 data.\none could modify the reward function to penalize large models for constraining memory or penalize slow forward passes to incentivize quick inference.\nThere are several future avenues for research in reinforcement learning-driven network design as well. In our current implementation, we use the same set of hyperparameters to train all network topologies during the Q-learning phase and further finetune the hyperparameters for top models selected by the MetaQNN agent. However, our approach could be combined with hyperparameter optimization methods to further automate the network design process. Moreover, we constrict the state-action space using coarse, discrete bins to accelerate convergence. It would be possible to move to larger state-action spaces using methods for Q-function approximation (Bertsekas, 2015; Mnih et al., 2015).\nACKNOWLEDGMENTS\nWe thank Peter Downs for creating the project website and contributing to illustrations. We acknowledge Center for Bits and Atoms at MIT for their help with computing resources. Finally, we thank members of Camera Culture group at MIT Media Lab for their help and support.\nA ALGORITHM\nWe first describe the main components of the MetaQNN algorithm. Algorithm 1 shows the main loop, where the parameter M would determine how many models to run for a given and the parameter K would determine how many times to sample the replay database to update Q-values on each iteration. The function TRAIN refers to training the specified network and returns a validation accuracy. Algorithm 2 details the method for sampling a new network using the -greedy strategy, where we assume we have a function TRANSITION that returns the next state given a state and action. Finally, Algorithm 3 implements theQ-value update detailed in Equation 3, with discounting factor set to 1, for an entire state sequence in temporally reversed order.\nAlgorithm 1 Q-learning For CNN Topologies Initialize:\nreplay memory\u2190 [ ] Q\u2190 {(s, u) \u2200s \u2208 S, u \u2208 U(s) : 0.5}\nfor episode = 1 to M do S, U \u2190 SAMPLE NEW NETWORK( , Q) accuracy\u2190 TRAIN(S) replay memory.append((S, U, accuracy)) for memory = 1 to K do\nSSAMPLE , USAMPLE , accuracySAMPLE \u2190 Uniform{replay memory} Q\u2190 UPDATE Q VALUES(Q, SSAMPLE , USAMPLE , accuracySAMPLE)\nend for end for\nAlgorithm 2 SAMPLE NEW NETWORK( , Q) Initialize:\nstate sequence S = [sSTART] action sequence U = [ ]\nwhile U [\u22121] 6= terminate do \u03b1 \u223c Uniform[0, 1) if \u03b1 > then\nu = argmaxu\u2208U(S[\u22121])Q[(S[\u22121], u)] s\u2032 = TRANSITION(S[\u22121], u)\nelse u \u223c Uniform{U(S[\u22121])} s\u2032 = TRANSITION(S[\u22121], u) end if U.append(u) if u != terminate then\nS.append(s\u2032) end if\nend while return S, U\nAlgorithm 3 UPDATE Q VALUES(Q, S, U , accuracy) Q[S[\u22121], U [\u22121]] = (1\u2212 \u03b1)Q[S[\u22121], U [\u22121]] + \u03b1 \u00b7 accuracy for i = length(S)\u2212 2 to 0 do\nQ[S[i], U [i]] = (1\u2212 \u03b1)Q[S[i], U [i]] + \u03b1maxu\u2208U(S[i+1])Q[S[i+ 1], u] end for return Q\nB REPRESENTATION SIZE BINNING\nAs mentioned in Section 4.1 of the main text, we introduce a parameter called representation size to prohibit the agent from taking actions that can reduce the intermediate signal representation to a size that is too small for further processing. However, this process leads to uncertainties in state transitions, as illustrated in Figure A1, which is handled by the standard Q-learning formulation.\nP(2,2)\nR-size: 18 R-size bin: 1\nR-size: 9 R-size bin: 1\n(a)\nP(2,2)\nR-size: 7 R-size bin: 2\nR-size: 14 R-size bin: 1\n(b)\nStates Actions\np 1 2 p\nR-size bin: 1\nR-size bin: 1 R-size bin: 2\nP(2,2)\n(c)\nFigure A1: Representation size binning: In this figure, we show three example state transitions. The true representation size (R-size) parameter is included in the figure to show the true underlying state. Assuming there are two R-size bins, R-size Bin1: [8,\u221e) and R-size Bin2: (0, 7], Figure A1a shows the case where the initial state is in R-size Bin1 and true representation size is 18. After the agent chooses to pool with a 2\u00d72 filter with stride 2, the true representation size reduces to 9 but the R-size bin does not change. In Figure A1b, the same 2 \u00d7 2 pooling layer with stride 2 reduces the actual representation size of 14 to 7, but the bin changes to R-size Bin2. Therefore, in figures A1a and A1b, the agent ends up in different final states, despite originating in the same initial state and choosing the same action. Figure A1c shows that in our state-action space, when the agent takes an action that reduces the representation size, it will have uncertainty in which state it will transition to.\nC MNIST EXPERIMENT\nWe noticed that the final MNIST models were prone to overfitting, so we increased dropout and did a small grid search for the weight regularization parameter. For both tuning and final training, we warmed the model with the learned weights from after the first epoch of initial training. The final models and solvers can be found on our project website https://bowenbaker.github.io/metaqnn/ . Figure A2 shows the Q-Learning performance for the MNIST experiment.\nD FURTHER ANALYSIS OF Q-LEARNING\nFigure 3 of the main text and Figure A2 show that as the agent begins to exploit, it improves in architecture selection. It is also informative to look at the distribution of models chosen at each . Figure A4 gives further insight into the performance achieved at each for both experiments.\nD.1 Q-LEARNING STABILITY\nBecause the Q-learning agent explores via a random or semi-random distribution, it is natural to ask whether the agent can consistently improve architecture performance. While the success of the three independent experiments described in the main text allude to stability, here we present further evidence. We conduct 10 independent runs of the Q-learning procedure on 10% of the SVHN dataset (which corresponds to \u223c7,000 training examples). We use a smaller dataset to reduce the computation time of each independent run to 10GPU-days, as opposed to the 100GPU-days it would take on the full dataset. As can be seen in Figure A3, the Q-learning procedure with the exploration schedule detailed in Table 2 is fairly stable. The standard deviation at = 1 is notably smaller than at other stages, which we attribute to the large difference in number of samples at each stage.\n0 500 1000 1500 2000 2500 3000 3500 Iterations\n0.00\n0.10\n0.20\n0.30\n0.40\n0.50\n0.60\n0.70\n0.80\n0.90\n1.00\nA cc\nu ra\ncy\nEpsilon = 1.0 .9.8.7 .6 .5 .4 .3 .2 .1\nMNIST Q-Learning Performance\nAverage Accuracy Per Epsilon Rolling Mean Model Accuracy\nFigure A2: MNIST Q-Learning Performance. The blue line shows a rolling mean of model accuracy versus iteration, where in each iteration of the algorithm the agent is sampling a model. Each bar (in light blue) marks the average accuracy over all models that were sampled during the exploration phase with the labeled . As decreases, the average accuracy goes up, demonstrating that the agent learns to select better-performing CNN architectures.\n0.10.20.30.40.50.60.70.80.91.0 Epsilon\n0.45\n0.50\n0.55\n0.60\n0.65\n0.70\n0.75\n0.80\nM ea\nn Ac\ncu ra\ncy\nQ-Learning Stability (Across 10 Runs)\n(a)\n0.10.20.30.40.50.60.70.80.91.0 Epsilon\n0.45\n0.50\n0.55\n0.60\n0.65\n0.70\n0.75 0.80 M ea n Ac cu ra cy\nQ-Learning Individual Runs\n(b)\nFigure A3: Figure A3a shows the mean model accuracy and standard deviation at each over 10 independent runs of the Q-learning procedure on 10% of the SVHN dataset. Figure A3b shows the mean model accuracy at each for each independent experiment. Despite some variance due to a randomized exploration strategy, each independent run successfully improves architecture performance.\nFurthermore, the best model found during each run had remarkably similar performance with a mean accuracy of 88.25% and standard deviation of 0.58%, which shows that each run successfully found at least one very high performing model. Note that we did not use an extended training schedule to improve performance in this experiment.\nD.2 Q-VALUE ANALYSIS\nWe now analyze the actualQ-values generated by the agent during the training process. The learning agent iteratively updates the Q-values of each path during the -greedy exploration. Each Q-value is initialized at 0.5. After the -schedule is complete, we can analyze the final Q-value associated with each path to gain insights into the layer selection process. In the left column of Figure A5, we plot the average Q-value for each layer type at different layer depths (for both SVHN and CIFAR10) datasets. Roughly speaking, a higher Q-value associated with a layer type indicates a higher probability that the agent will pick that layer type. In Figure A5, we observe that, while the average Q-value is higher for convolution and pooling layers at lower layer depths, the Q-values for fullyconnected and termination layers (softmax and global average pooling) increase as we go deeper into the network. This observation matches with traditional network designs.\nWe can also plot the averageQ-values associated with different layer parameters for further analysis. In the right column of Figure A5, we plot the averageQ-values for convolution layers with receptive\nfield sizes 1, 3, and 5 at different layer depths. The plots show that layers with receptive field size of 5 have a higher Q-value as compared to sizes 1 and 3 as we go deeper into the networks. This indicates that it might be beneficial to use larger receptive field sizes in deeper networks.\nIn summary, the Q-learning method enables us to perform analysis on the relative benefits of different design parameters of our state space, and possibly gain insights for new CNN designs.\nE TOP TOPOLOGIES SELECTED BY ALGORITHM\nIn Tables A1 through A3, we present the top five model architectures selected with Q-learning for each dataset, along with their prediction error reported on the test set, and their total number of parameters. To download the Caffe solver and prototext files, please visit https://bowenbaker.github.io/metaqnn/ .\nModel Architecture Test Error (%) # Params (106) [C(512,5,1), C(256,3,1), C(256,5,1), C(256,3,1), P(5,3), C(512,3,1), C(512,5,1), P(2,2), SM(10)] 6.92 11.18 [C(128,1,1), C(512,3,1), C(64,1,1), C(128,3,1), P(2,2), C(256,3,1), P(2,2), C(512,3,1), P(3,2), SM(10)] 8.78 2.17 [C(128,3,1), C(128,1,1), C(512,5,1), P(2,2), C(128,3,1), P(2,2), C(64,3,1), C(64,5,1), SM(10)] 8.88 2.42 [C(256,3,1), C(256,3,1), P(5,3), C(256,1,1), C(128,3,1), P(2,2), C(128,3,1), SM(10)] 9.24 1.10 [C(128,5,1), C(512,3,1), P(2,2), C(128,1,1), C(128,5,1), P(3,2), C(512,3,1), SM(10)] 11.63 1.66\nTable A1: Top 5 model architectures: CIFAR-10.\nModel Architecture Test Error (%) # Params (106) [C(128,3,1), P(2,2), C(64,5,1), C(512,5,1), C(256,3,1), C(512,3,1), P(2,2), C(512,3,1), C(256,5,1), C(256,3,1), C(128,5,1), C(64,3,1), SM(10)] 2.24 9.81 [C(128,1,1), C(256,5,1), C(128,5,1), P(2,2), C(256,5,1), C(256,1,1), C(256,3,1), C(256,3,1), C(256,5,1), C(512,5,1), C(256,3,1), C(128,3,1), SM(10)] 2.28 10.38 [C(128,5,1), C(128,3,1), C(64,5,1), P(5,3), C(128,3,1), C(512,5,1), C(256,5,1), C(128,5,1), C(128,5,1), C(128,3,1), SM(10)] 2.32 6.83 [C(128,1,1), C(256,5,1), C(128,5,1), C(256,3,1), C(256,5,1), P(2,2), C(128,1,1), C(512,3,1), C(256,5,1), P(2,2), C(64,5,1), C(64,1,1), SM(10)] 2.35 6.99 [C(128,1,1), C(256,5,1), C(128,5,1), C(256,5,1), C(256,5,1), C(256,1,1), P(3,2), C(128,1,1), C(256,5,1), C(512,5,1), C(256,3,1), C(128,3,1), SM(10)] 2.36 10.05\nTable A2: Top 5 model architectures: SVHN. Note that we do not report the best accuracy on test set from the above models in Tables 3 and 4 from the main text. This is because the model that achieved 2.28% on the test set performed the best on the validation set.\nModel Architecture Test Error (%) # Params (106) [C(64,1,1), C(256,3,1), P(2,2), C(512,3,1), C(256,1,1), P(5,3), C(256,3,1), C(512,3,1), FC(512), SM(10)] 0.35 5.59 [C(128,3,1), C(64,1,1), C(64,3,1), C(64,5,1), P(2,2), C(128,3,1), P(3,2), C(512,3,1), FC(512), FC(128), SM(10)] 0.38 7.43 [C(512,1,1), C(128,3,1), C(128,5,1), C(64,1,1), C(256,5,1), C(64,1,1), P(5,3), C(512,1,1), C(512,3,1), C(256,3,1), C(256,5,1), C(256,5,1), SM(10)] 0.40 8.28 [C(64,3,1), C(128,3,1), C(512,1,1), C(256,1,1), C(256,5,1), C(128,3,1), P(5,3), C(512,1,1), C(512,3,1), C(128,5,1), SM(10)] 0.41 6.27 [C(64,3,1), C(128,1,1), P(2,2), C(256,3,1), C(128,5,1), C(64,1,1), C(512,5,1), C(128,5,1), C(64,1,1), C(512,5,1), C(256,5,1), C(64,5,1), SM(10)] 0.43 8.10 [C(64,1,1), C(256,5,1), C(256,5,1), C(512,1,1), C(64,3,1), P(5,3), C(256,5,1), C(256,5,1), C(512,5,1), C(64,1,1), C(128,5,1), C(512,5,1), SM(10)] 0.44 9.67 [C(128,3,1), C(512,3,1), P(2,2), C(256,3,1), C(128,5,1), C(64,1,1), C(64,5,1), C(512,5,1), GAP(10), SM(10)] 0.44 3.52 [C(256,3,1), C(256,5,1), C(512,3,1), C(256,5,1), C(512,1,1), P(5,3), C(256,3,1), C(64,3,1), C(256,5,1), C(512,3,1), C(128,5,1), C(512,5,1), SM(10)] 0.46 12.42 [C(512,5,1), C(128,5,1), C(128,5,1), C(128,3,1), C(256,3,1), C(512,5,1), C(256,3,1), C(128,3,1), SM(10)] 0.55 7.25 [C(64,5,1), C(512,5,1), P(3,2), C(256,5,1), C(256,3,1), C(256,3,1), C(128,1,1), C(256,3,1), C(256,5,1), C(64,1,1), C(256,3,1), C(64,3,1), SM(10)] 0.56 7.55\nTable A3: Top 10 model architectures: MNIST. We report the top 10 models for MNIST because we included all 10 in our final ensemble. Note that we do not report the best accuracy on test set from the above models in Tables 3 and 4 from the main text. This is because the model that achieved 0.44% on the test set performed the best on the validation set.\n0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0\nValidation Accuracy\n0\n10\n20\n30\n40\n50\n60\n% M\no d e ls\nModel Accuracy Distribution (SVHN)\nepsilon\n0.1 0.2 0.3 0.4 0.5\n0.6 0.7 0.8 0.9 1.0\n(a)\n0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0\nValidation Accuracy\n0\n10\n20\n30\n40\n50\n60\n% M\no d e ls\nModel Accuracy Distribution (SVHN)\nepsilon\n0.1 1.0\n(b)\n0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9\nValidation Accuracy\n0\n5\n10\n15\n20\n% M\no d e ls\nModel Accuracy Distribution (CIFAR-10)\nepsilon\n0.1 0.2 0.3 0.4 0.5\n0.6 0.7 0.8 0.9 1.0\n(c)\n0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9\nValidation Accuracy\n0\n5\n10\n15 20 % M o d e ls\nModel Accuracy Distribution (CIFAR-10)\nepsilon\n0.1 1.0\n(d)\n0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 Validation Accuracy\n0\n20\n40\n60\n80\n100\n% M\nod el\ns\nModel Accuracy Distribution (MNIST)\nepsilon 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0\n(e)\n0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 Validation Accuracy\n0\n20\n40\n60\n80\n100\n% M\nod el\ns\nModel Accuracy Distribution (MNIST)\nepsilon 0.1 1.0\n(f)\nFigure A4: Accuracy Distribution versus : Figures A4a, A4c, and A4e show the accuracy distribution for each for the SVHN, CIFAR-10, and MNIST experiments, respectively. Figures A4b, A4d, and A4f show the accuracy distributions for the initial = 1 and the final = 0.1. One can see that the accuracy distribution becomes much more peaked in the high accuracy ranges at small for each experiment.\n0 2 4 6 8 10 12 14 Layer Depth\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nA ve\nra ge\nQ -V\nal ue\nAverage Q-Value vs. Layer Depth (SVHN)\nConvolution Fully Connected Pooling Global Average Pooling Softmax\n(a)\n0 2 4 6 8 10 12 Layer Depth\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nA ve\nra ge\nQ -V\nal ue\nAverage Q-Value vs. Layer Depth for Convolution Layers (SVHN)\nReceptive Field Size 1 Receptive Field Size 3 Receptive Field Size 5\n(b)\n0 5 10 15 20 Layer Depth\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nA ve\nra ge\nQ -V\nal ue\nAverage Q-Value vs. Layer Depth (CIFAR10)\nConvolution Fully Connected Pooling Global Average Pooling Softmax\n(c)\n0 2 4 6 8 10 12 14 16 18 Layer Depth\n0.5\n0.6\n0.7\n0.8\n0.9 1.0 A ve ra ge Q -V al ue\nAverage Q-Value vs. Layer Depth for Convolution Layers (CIFAR10)\nReceptive Field Size 1 Receptive Field Size 3 Receptive Field Size 5\n(d)\n0 2 4 6 8 10 12 14 Layer Depth\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nA ve\nra ge\nQ -V\nal ue\nAverage Q-Value vs. Layer Depth (MNIST)\nConvolution Fully Connected Pooling Global Average Pooling Softmax\n(e)\n0 2 4 6 8 10 12 Layer Depth\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nA ve\nra ge\nQ -V\nal ue\nAverage Q-Value vs. Layer Depth for Convolution Layers (MNIST)\nReceptive Field Size 1 Receptive Field Size 3 Receptive Field Size 5\n(f)\nFigure A5: Average Q-Value versus Layer Depth for different layer types are shown in the left column. Average Q-Value versus Layer Depth for different receptive field sizes of the convolution layer are shown in the right column.\n", "cats": {"READ": 1.0, "DONTREAD": 0.0}, "reviews": [{"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "This paper comes up with a novel approach to searching the space of architectures for deep neural networks using reinforcement learning. The idea is straightforward and sensible: use a reinforcement learning strategy to iteratively grow a deep net graph (the space of actions is e.g. adding different layer types) via Q-learning. The reviewers agree that the idea is interesting, novel and promising but are underwhelmed with the execution of the experiments and the empirical results. \n \n The idea behind the paper and the formulation of the problem are quite similar to a concurrent submission (", "OTHER_KEYS": "ICLR 2017 pcs"}, {"DATE": "20 Jan 2017", "TITLE": "A new revision updated", "IS_META_REVIEW": false, "comments": "We have added the results of the stability experiment (as suggested by AnonReviewer1) into the Appendix section D.1. We have also included a citation to the CNF paper and results from the latest ResNet paper.", "OTHER_KEYS": "Nikhil Naik"}, {"IMPACT": 4, "MEANINGFUL_COMPARISON": 3, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "This paper introduces a reinforcement learning framework for designing a neural network architecture. For each time-step, the agent picks a new layer type with corresponding layer parameters (e.g., #filters). In order to reduce the size of state-action space, they used a small set of design choices.\n\nStrengths:\n- A novel approach for automatic design of neural network architectures.\n- Shows quite promising results on several datasets (MNIST, CIFAR-10).\n\nWeakness:\n- Limited architecture design choices due to many prior assumptions (e.g., a set of possible number of convolution filters, at most 2 fully-connected layers, maximum depth, hard-coded dropout, etc.)\n- The method is demonstrated in tabular Q-learning setting, but it is unclear whether the proposed method would work in a large state-action space.\n\nOverall, this is an interesting and novel approach for neural network architecture design, and it seems to be worth publication despite some weaknesses.", "ORIGINALITY": 3, "IS_ANNOTATED": true, "TITLE": "review", "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "21 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"IMPACT": 2, "APPROPRIATENESS": 3, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "TITLE": "No Title", "comments": "The paper looks solid and the idea is natural. Results seem promising as well.\n\nI am mostly concerned about the computational cost of the method. 8-10 days on 10 GPUs for relatively tiny datasets is quite prohibitive for most applications I would ever encounter.\n I think the main question is how this approach scales to larger images and also when applied to more exotic and possibly tiny datasets. Can you run an experiment on Caltech-101 for instance? I would be very curious to see if your approach is suitable for the low-data regime and areas where we all do not know right away how a suitable architecture looks like. For Cifar-10/100, MNIST and SVHN, everyone knows very well what a reasonable model initialization looks like.\n\nIf you show proof that you can discover a competitive architecture for something like Caltech-101, I would recommend the paper for publication.\n\nMinor: \n- ResNets should be mentioned in Table ", "ORIGINALITY": 5, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "19 Dec 2016", "CLARITY": 5, "REVIEWER_CONFIDENCE": 3}, {"OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "TITLE": "Review", "comments": "Authors learn deep architectures on a few small vision problems using Q-learning and obtain solid results, SOTA results when limiting to certain types of layers and competitive against everything else. It would be good to know how well this performs when allowing more complex structures. Paper would be much more convincing on a real-size task such as ImageNet.", "ORIGINALITY": 3, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "16 Dec 2016", "CLARITY": 4, "REVIEWER_CONFIDENCE": 4}, {"OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "TITLE": "Details", "comments": "", "ORIGINALITY": 3, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "DATE": "11 Dec 2016", "CLARITY": 4}, {"IMPACT": 4, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "MEANINGFUL_COMPARISON": 3, "comments": "", "ORIGINALITY": 3, "IS_ANNOTATED": true, "TITLE": "questions", "IS_META_REVIEW": false, "DATE": "05 Dec 2016"}, {"IMPACT": 2, "APPROPRIATENESS": 3, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "TITLE": "CNF", "comments": "", "ORIGINALITY": 5, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "DATE": "03 Dec 2016", "CLARITY": 5}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "This paper comes up with a novel approach to searching the space of architectures for deep neural networks using reinforcement learning. The idea is straightforward and sensible: use a reinforcement learning strategy to iteratively grow a deep net graph (the space of actions is e.g. adding different layer types) via Q-learning. The reviewers agree that the idea is interesting, novel and promising but are underwhelmed with the execution of the experiments and the empirical results. \n \n The idea behind the paper and the formulation of the problem are quite similar to a concurrent submission (", "OTHER_KEYS": "ICLR 2017 pcs"}, {"DATE": "20 Jan 2017", "TITLE": "A new revision updated", "IS_META_REVIEW": false, "comments": "We have added the results of the stability experiment (as suggested by AnonReviewer1) into the Appendix section D.1. We have also included a citation to the CNF paper and results from the latest ResNet paper.", "OTHER_KEYS": "Nikhil Naik"}, {"IMPACT": 4, "MEANINGFUL_COMPARISON": 3, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "This paper introduces a reinforcement learning framework for designing a neural network architecture. For each time-step, the agent picks a new layer type with corresponding layer parameters (e.g., #filters). In order to reduce the size of state-action space, they used a small set of design choices.\n\nStrengths:\n- A novel approach for automatic design of neural network architectures.\n- Shows quite promising results on several datasets (MNIST, CIFAR-10).\n\nWeakness:\n- Limited architecture design choices due to many prior assumptions (e.g., a set of possible number of convolution filters, at most 2 fully-connected layers, maximum depth, hard-coded dropout, etc.)\n- The method is demonstrated in tabular Q-learning setting, but it is unclear whether the proposed method would work in a large state-action space.\n\nOverall, this is an interesting and novel approach for neural network architecture design, and it seems to be worth publication despite some weaknesses.", "ORIGINALITY": 3, "IS_ANNOTATED": true, "TITLE": "review", "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "21 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"IMPACT": 2, "APPROPRIATENESS": 3, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "TITLE": "No Title", "comments": "The paper looks solid and the idea is natural. Results seem promising as well.\n\nI am mostly concerned about the computational cost of the method. 8-10 days on 10 GPUs for relatively tiny datasets is quite prohibitive for most applications I would ever encounter.\n I think the main question is how this approach scales to larger images and also when applied to more exotic and possibly tiny datasets. Can you run an experiment on Caltech-101 for instance? I would be very curious to see if your approach is suitable for the low-data regime and areas where we all do not know right away how a suitable architecture looks like. For Cifar-10/100, MNIST and SVHN, everyone knows very well what a reasonable model initialization looks like.\n\nIf you show proof that you can discover a competitive architecture for something like Caltech-101, I would recommend the paper for publication.\n\nMinor: \n- ResNets should be mentioned in Table ", "ORIGINALITY": 5, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "19 Dec 2016", "CLARITY": 5, "REVIEWER_CONFIDENCE": 3}, {"OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "TITLE": "Review", "comments": "Authors learn deep architectures on a few small vision problems using Q-learning and obtain solid results, SOTA results when limiting to certain types of layers and competitive against everything else. It would be good to know how well this performs when allowing more complex structures. Paper would be much more convincing on a real-size task such as ImageNet.", "ORIGINALITY": 3, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "16 Dec 2016", "CLARITY": 4, "REVIEWER_CONFIDENCE": 4}, {"OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "TITLE": "Details", "comments": "", "ORIGINALITY": 3, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "DATE": "11 Dec 2016", "CLARITY": 4}, {"IMPACT": 4, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "MEANINGFUL_COMPARISON": 3, "comments": "", "ORIGINALITY": 3, "IS_ANNOTATED": true, "TITLE": "questions", "IS_META_REVIEW": false, "DATE": "05 Dec 2016"}, {"IMPACT": 2, "APPROPRIATENESS": 3, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "TITLE": "CNF", "comments": "", "ORIGINALITY": 5, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "DATE": "03 Dec 2016", "CLARITY": 5}]}
{"text": "INTELLIGIBLE LANGUAGE MODELING WITH INPUT SWITCHED AFFINE NETWORKS\n1 INTRODUCTION\nNeural networks and the general field of deep learning have made remarkable progress over the last few years in fields such as object recognition (Krizhevsky et al., 2012), language translation (Sutskever et al., 2014), and speech recognition (Graves et al., 2013). For all of the success of the deep learning approach however, there are certain application domains in which intelligibility of the system is an essential design requirement. One commonly used example is the necessity to understand the decisions that a self-driving vehicle makes when avoiding various obstacles in its path. Another example is the application of neural network methodologies to scientific discovery (Mante et al., 2013). Even where intelligibility is not an overt design requirement, it is fair to say that most users of neural networks would like to better understand the models they deploy.\nThere are at least two approaches to creating intelligible network models. One approach is to build networks as normal, and then apply analysis techniques after training. Often this approach yields systems that perform extremely well, and whose intelligibility is limited. A second approach is to build a neural network where intelligibility is an explicit design constraint. In this case, the typical result is a system that can be understood reasonably well, but may underperform. In this work we follow this second approach and build intelligibility into our network model, yet without sacrificing performance for the task we studied.\nDesigning intelligibility into neural networks for all application domains is a worthy, but daunting goal. Here we contribute to that larger goal by focusing on a commonly studied task, that of character based\n\u2217This work was performed as an intern at Google Brain. \u2020Work done as a member of the Google Brain Residency program (g.co/brainresidency) \u2021Work performed when author was a visiting faculty at Google Brain.\nlanguage modeling. We develop and analyze a model trained on a one-step-ahead prediction task of the Text8 dataset, which is 10 million characters of Wikipedia text (Mahoney, 2011). The model we use is a switched affine system, where the input determines the switching behavior by selecting a transition matrix and bias as a function of that input, and there is no nonlinearity. Surprisingly, we find that this simple architecture performs as well as a vanilla RNN, Gated Recurrent Unit (GRU) (Cho et al., 2014), IRNN (Le et al., 2015), or Long Short Term Memory (LSTM) (Hochreiter & Schmidhuber, 1997) in this task, despite being a simpler and potentially far more computationally efficient architecture.\nIn what follows, we discuss related work, define our Input Switched Affine Network (ISAN) model, demonstrate its performance on the one-step-ahead prediction task, and then analyze the model in a multitude of ways, most of which would be currently difficult or impossible to accomplish with modern nonlinear recurrent architectures.\n2 RELATED WORK\nWork by the authors of (Karpathy et al., 2015) attempted to use character-based language modeling to begin to understand how the LSTM (Hochreiter & Schmidhuber, 1997) functions. In it, they employ n-gram word models to highlight what the LSTM has \u2013 and has not \u2013 learned about the text corpus. They were able to break down LSTM language model errors into classes, such as e.g., \"rare word\" errors. The authors of (Greff et al., 2015) engaged in a large study to understand the relative importance of the various components of an LSTM. The authors of (Collins et al., 2016) performed an enormous hyperparameter study to disentangle the effects of capacity and trainability in a number of RNN architectures.\nAttempts to understand networks in more general contexts include the use of linearization and nonlinear dynamical systems theory to understand RNNs in (Sussillo & Barak, 2013). In feedforward networks the use of linear probes has been suggested by (Alain & Bengio, 2016), and there exist a host of back-propagation techniques used to infer the most important input to drive various components of the feed-forward network, e.g. (Le et al., 2012).\nThe ISAN uses an input-switched affine model. The highly related linear time-varying systems are standard material in undergraduate electrical engineering text books. Probabilistic versions of switching linear models with discrete latent variables have a history in the context of probabilistic graphical models. A recent example is the switched linear dynamical system in (Linderman et al., 2016). Focusing on language modeling, (Belanger & Kakade, 2015) defined a probabilistic linear dynamical system as a generative language model for creating context-dependent token embeddings and then used steady-state Kalman filtering for inference over token sequences. They used singular value decomposition and discovered that the right and left singular vectors were semantically and syntactically related. One difference between the ISAN and the LDS is that the weight matrices of the ISAN are input token dependent (while the biases of both models are input dependent). Finally, multiplicative neural networks (MRNNs) were proposed precisely for character based language modeling in (Sutskever et al., 2011; Martens & Sutskever, 2011). The MRNN architecture is similar to our own, in that the dynamics matrix switches as a function of the input character. However, the MRNN relied on a tanh nonlinearity, while our model is explicitly linear. It is this property of our model which makes it both amenable to analysis, and computationally efficient.\nThe Observable Operator Model (OOM) (Jaeger, 2000) is similar to the ISAN in that the OOM updates a latent state using a separate transition matrix for each input symbol and performs probabilistic sequence modeling. Unlike the ISAN, the OOM requires that a linear projection of the hidden state corresponds to a normalized sequence probability. This imposes strong constraints on both the model parameters and the model dynamics, and restricts the choice of training algorithms. In contrast, the ISAN applies an affine readout to the hidden state to obtain logits, which are then pushed through a SoftMax to obtain probabilities. Therefore no constraints need to be imposed on the ISAN\u2019s parameters and training is easy using backprop. Lastly, the ISAN is formulated as an affine, rather than linear model. While this doesn\u2019t change the class of processes that can be modeled, it enhances the stability of training and greatly enhances interpretability. We elaborate upon these ideas in Section 6.1.\n3 METHODS\n3.1 MODEL DEFINITION\nIn what follows Wx and bx respectively denote a transition matrix and a bias vector for a specific input x, the symbol xt is the input at time t, and ht is the hidden state at time t. Our ISAN model is defined as\nht = Wxt ht\u22121 + bxt . (1)\nThe network also learns an initial hidden state h0. We emphasize the intentional absence of any nonlinear activation function.\n3.2 CHARACTER LEVEL LANGUAGE MODELLING WITH RNNS\nThe RNNs are trained on the Text8 Wikipedia dataset, for one-step-ahead character prediction. The Text8 dataset consists only of the 27 characters \u2018a\u2019-\u2018z\u2019 and \u2018_\u2019 (space). Given a character sequence of x1, ...,xt, the RNNs are trained to minimize the cross-entropy between the true next character, and the output prediction. We map from the hidden state, ht, into a logit space via an affine map. The probabilities are computed as\np (xt+1) = softmax (lt) (2) lt = Wro ht + bro, (3)\nwhere Wro and bro are the readout weights and biases, and lt is the logit vector. In line with (Collins et al., 2016) we split the training data into 80%, 10%, and 10% for train, test, and evaluation set respectively. The network was trained with the same hyperparameter tuning infrastructure as in (Collins et al., 2016). Analysis in this paper is carried out on the best-performing ISAN model, which has 1, 271, 619 parameters, corresponding to 216 hidden units, and 27 dynamics matrices Wx and biases bx.\n4 RESULTS AND ANALYSIS\n4.1 ISAN PERFORMANCE ON THE TEXT8 TASK\nThe results on Text8 are shown in Figure 1a. For the largest parameter count, the ISAN matches almost exactly the performance of all other nonlinear models with the same number of maximum parameters: RNN, IRNN, GRU, LSTM. However, we note that for small numbers of parameters the ISAN performs considerably worse than other architectures. All analyses use ISAN trained with 1.28e6 maximum parameters (1.58 bpc cross entropy). Samples of generated text from this model are relatively coherent. We show two examples, after priming with \"annual reve\", at inverse temperature of 1.5, and 2.0, respectively:\n\u2022 \u201cannual revenue and producer of the telecommunications and former communist action and saving its new state house of replicas and many practical persons\u201d\n\u2022 \u201cannual revenue seven five three million one nine nine eight the rest of the country in the united states and south africa new\u201d.\nAs a preliminary, comparative analysis, we performed PCA on the state sequence over a large set of sequences for the vanilla RNN, GRU of varying sizes, and ISAN. This is shown in Figure 1b. The eigenvalue spectra, in log of variance explained, was significantly flatter for the ISAN than the other architectures.\nWe also compare the ISAN performance to a fully linear RNN without input switched dynamics. This achieves a cross-entropy of 3.1 bits / char, independent of network size. This perplexity is only slightly better than that of a Naive Bayes model on the task, at 3.3 bits / char. The output probability of the fully linear network is a product of contributions from each previous character, as in Naive Bayes. Those factorial contributions are learned however, giving ISAN a slight advantage. We also run a comparison to a fully linear network with a non-linear readout. This achieves 2.15 bits /\nchar, independent of network size. Both of these comparisons illustrate the importance of the input switched dynamics for achieving good results in the absence of non-linear hidden state dynamics.\nLastly we also test to what extent the ISAN can deal with large dictionaries by running it on a byte-pair encoding of the text8 task, where the input dictionary consists of the 272 different possible character combinations. We find that in this setup the LSTM consistently outperforms the ISAN for the same number of parameters. At 1.3m parameters the LSTM achieves a cross entropy of 3.4 bits / char-pair, while ISAN achieves 3.55. One explanation for this finding is that the matrices in ISAN are a factor of 27 smaller than the matrices of the LSTMs. For very large numbers of parameters the performance of any architecture saturates in the number of parameters, at which point the ISAN can \u2018catch-up\u2019 with more parameter efficient architectures like LSTMs.\n4.2 DECOMPOSITION OF CURRENT PREDICTIONS BASED ON PREVIOUS TIME STEPS\nTaking advantage of the linearity of the hidden state dynamics for any sequence of inputs, we can decompose the current latent state ht into contributions originating from different timepoints s in the history of the input:\nht = t\u2211 s=0\n( t\u220f\ns\u2032=s+1\nWxs\u2032 ) bxs , (4)\nwhere the empty product when s + 1 > t is 1 by convention, and bx0 = h0 is the learned initial hidden state. This is useful because we can analyze which factors were important in the past, for determining the current character prediction.\nUsing this decomposition and the linearity of matrix multiplication we can also write the unnormalized logit-vector, lt, as a sum of terms linear in the biases,\nlt = bro + t\u2211 s=0 \u03bats (5)\n\u03bats = Wro\n( t\u220f\ns\u2032=s+1\nWxs\u2032 ) bxs , (6)\nwhere \u03bats is the contribution from timestep s to the logits at timestep t, and \u03ba t t = bxt . For notational convenience we will sometimes replace the subscript s with the corresponding input character xs at step s when referring to \u03bats \u2013 e.g. \u03ba t \u2018q\u2019 to refer to the contribution from the character \u2018q\u2019 in a string.\nSimilarly, when discussing the summed contributions from a word or substring we will sometimes write \u03batword to mean the summed contributions of all the \u03ba t s from that source word, \u2211 s\u2208word \u03ba t s \u2013 e.g. \u03bat\u2018the\u2019 to refer to the total logit contribution from the word \u2018the\u2019.\nWhile in standard RNNs the nonlinearity causes interdependence of the bias terms across time steps, in the ISAN the bias terms can be interpreted as independent linear contributions to the state that are propagated and transformed through time. We emphasize that \u03bats includes the multiplicative contributions from the Wxs\u2032 for s < s\n\u2032 \u2264 t. It is however independent of prior inputs, xs\u2032 for s\u2032 < s. This is the main difference between the analysis we can carry out with the ISAN compared to a non-linear RNN. In general the contribution of a specific character sequence will depend on the hidden state at the start of the sequence. Due to the linearity of the dynamics, this dependency does not exist in the ISAN.\nIn Figure 2 we show an example of how this decomposition allows us to understand why a particular prediction is made at a given point in time, and how previous characters influence the decoding. For example, the sequence \u2018_annual_revenue_\u2019 is processed by the ISAN: Starting with an all-zero hidden state, we use equation (6) to accumulate a sequence of \u03bat\u2018_\u2032 ,\u03ba t \u2018a\u2032 ,\u03ba t \u2018n\u2032 ,\u03ba t \u2018n\u2032 , .... These values can then be used to understand the prediction of the network at some time t, by simple addition across the s index, which is shown in Figure 2.\nIn Figure 3 we provide a detailed view of how past characters contribute to the logits predicting the next character. There are two competing options for the next letter in the word stem \u2018reve\u2019: either \u2018revenue\u2019 or \u2018reverse\u2019. We show that without the contributions from \u2018_annual\u2019 the most likely decoding of the character after the second \u2018e\u2019 is \u2018r\u2019 (to form \u2018reverse\u2019), while the contributions from \u2018_annual\u2019 tip the balance in favor of \u2018n\u2019, decoding to \u2018revenue\u2019. In a standard RNN a similar analysis could be carried out by comparing the prediction given an artificially limited history.\nUsing the decomposition of current step predictions in to \u03bats, we can also investigate how quickly the contributions of \u03bats decay as a function of t\u2212 s. In Figure 4a we can see that this contribution decays on two different exponential timescales. We hypothesize that the first time scale corresponds to the decay within a word, while the next corresponds to the decay of information across words and sentences. This effect is also visible in Figure 5. We note that it would be difficult to carry out this analysis in a non-linear RNN.\nWe can also show the relevance of the \u03bats contributions to the decoding of characters at different positions in the word. For examples, we observe that \u03bat\u2018_\u2019 makes important contributions to the prediction of the next character at time t. We show that using only the \u03bat\u2018_\u2019, the model can achieve\na cross entropy of < 1 / char when the position of the character is more than 3 letters from the beginning of the word.\nFurthermore we can link back from the norm-decay to the importance of past characters for the decoding quality. By artificially limiting the number of past \u03ba available for prediction, Figure 4c, we show that the prediction quality improves rapidly when extending the history from 0 to 10 characters and then saturates. This rapid improvement aligns with the range of faster decay in Figure 4a.\n4.3 FROM CHARACTERS TO WORDS\nThe ISAN provides a natural means of moving from character level representation to word level. Using the linearity of the hidden state dynamics we can aggregate all of the \u03bats belonging to a given\nword and visualize them as a single contribution to the prediction of the letters in the next word. This allows us to understand how each preceding word impacts the decoding for the letters of later words. In Figure 5 we show that the words \u2018higher\u2019 and \u2018than\u2019 make large contributions to the prediction of the characters \u2018h\u2019 and \u2018n\u2019 in \u2018tevenue\u2019, as measured by the norm of the \u03bat\u2018_the\u2019 and \u03bat\u2018_annual\u2019.\nIn Figure 6 we show that these \u03batword are more than a mathematical convenience and even capture word-level semantic information. Shown is a t-SNE embedding of the \u03batword for the most common 4000 words in the data-set, with examples of the kind of clusters that arise.\n4.4 CHANGE OF BASIS\nWe are free to perform a change of basis on the hidden state, and then to run the affine ISAN dynamics in that new basis. Note that this change of basis is not possible for other RNN architectures, since the action of the nonlinearity depends on the choice of basis.\nIn particular we can construct a \u2018readout basis\u2019 that explicitly divides the latent space into a subspace Pro\u2016 spanned by the rows of the readout matrix Wro, and its orthogonal complement P ro \u22a5 . This representation explicitly divides the hidden state dynamics into a 27-dimensional \u2018readout\u2019 subspace that is accessed by the readout matrix to make predictions, and a \u2018computational\u2019 subspace comprising the remaining 216\u2212 27 dimensions that are orthogonal to the readout matrix. We apply this change of basis to analyze an intriguing observation about the hidden offsets bx: As shown in Figure 7, the norm of the bx is strongly correlated to the log-probability of the unigram x in the training data. Re-expressing network parameters using the \u2018readout basis\u2019 shows that this correlation is not related to reading out the next-step prediction. This is because the norm of the projection of bx into Pro\u22a5 remains strongly correlated with character frequency, while the projections into Pro\u2016 have norms that show little correlation. This indicates that the information content or surprise of a letter is encoded through the norm of the component of bx in the computational space, rather than in the readout space.\nSimilarly, in Figure 8 we illustrate that the structure in the correlations between the bx is due to their components in Pro\u2016 , while the correlation in P ro \u22a5 is relatively uniform. We can clearly see two blocks of high correlations between the vowels and consonants respectively, while b\u2018_\u2019 is uncorrelated to either.\n4.5 COMPARISON WITH n-GRAM MODEL WITH BACKOFF\nWe compared the computation performed by n-gram language models and those performed by the ISAN. An n-gram model with back-off weights expresses the conditional probability p (xt|x1...xt\u22121) as a sum of smoothed count ratios of n-grams of different lengths, with the contribution of shorter n-grams down-weighted by backoff weights. On the other hand, the computations performed by the ISAN start with the contribution of bro to the logits, which as shown in Figure 9a) corresponds to the unigram log-probabilities. The logits are then additively updated with contributions from longer n-grams, represented by \u03bats. This additive contribution to the logits corresponds to a multiplicative modification of the emission probabilities from histories of different length. For long time lags, the additive correction to log-probabilities becomes small (Figure 2), which corresponds to multiplication by a uniform distribution. Despite these differences in how n-gram history is incorporated, we\nnevertheless observe an agreement between empirical models estimated on the training set and model predictions for unigrams and bigrams. Figure 9 shows that the bias term bro gives the unigram probabilities of letters, while the addition of the offset terms bx accurately predict the bigram distribution of P (xt+1|xt). Shown are both an example, P (x|\u2018_\u2032), and a summary plot for all 27 letters.\nWe further explore the n-gram comparison by artificially limiting the length of the character history that is available to the ISAN for making predictions, as shown in Figure 4c).\n5 ANALYSES OF PARENTHESES COUNTING TASK\nTo show the interpretability of the ISAN we train a model on the parenthesis counting task. Bringing together ideas from sections 4.4 and 6.1 we re-express the transition dynamics in a new basis that fully reveals computations performed by the ISAN.\nWe analyze the simple task of parentheses counting, which was defined in (Collins et al., 2016). Briefly, the RNN is required keep track of the nesting level of 3 different types of parentheses\nindependently. The inputs are the one-hot encoding of the different opening and closing parentheses (e.g. \u2018(\u2019, \u2018)\u2019, \u2018{\u2019, \u2018}\u2019) as well as a noise character (\u2018a\u2019). The output is the one-hot encoding of the nesting level between (0-5), one set of counts for each parentheses task. One change from (Collins et al., 2016) is that we slightly simplify the problem by exchanging the cross-entropy error with an L2 error and linear readout (this change leads to slightly cleaner figures, but does not qualitatively change the results).\nWe first re-express the transition dynamics in terms of linear, rather than affine operations. Consider the matrix W\u2032 \u2208 R(n+1)\u00d7(n+1):\nW\u2032 = [ W b 0 1 ] , (7)\nwhere 0 is a row vector of zeros. The matrix W\u2032 emulates the affine transition for any hidden state h \u2208 Rn\u00d71: W\u2032 [ h 1 ] = [ Wh+ b 1 ] . (8)\nThe matrices W and W\u2032 are closely connected. Each eigenvalue of W is also an eigenvalue of W\u2032. Moreover, eigenvectors of W become the eigenvectors of W\u2032 when expanded with a zero dimension. In fact, W\u2032 only has one extra eigenvalue of exactly 1 that is necessary to preserve the last dimension of the expanded hidden state.\nTo analyze the the parentheses task we analyze W\u2032. The key to understanding how the network solves the parentheses task is to find a change of bases that clarifies the dynamics necessary to count 3 sets of independent parentheses nesting levels. In this case we use a matrix composed of the readout matrix, modified by adding a set of vectors that spans the null space of the readout (including the additional bias dimension).\nW\u2032ro =\n[ Wro bro\nO\n] (9)\nwhere O is the orthogonal complement of the subspace spanned by the row vectors of [Wro bro] in an N + 1-dimensional space. We perform the following change of basis of the dynamics matrices,\nWx(ro) \u2032 = Wro \u2032Wx \u2032 (W\u2032ro) \u22121 (10)\nand visualize the results in Fig. 10. Figure 10 shows that this system created delay lines which count the nesting level, with fixed point dynamics at the 0 count (5 count), so that the system stays at both numbers when the input would otherwise increment (decrement) the count. The matrices also implement fixed point dynamics, as implemented via an identity submatrix to preserve the memory of the parenthesis nesting counts when an unrelated symbol enters the system (e.g. The \u2018{}\u2019 count is preserved by the \u2018(\u2019 matrix when a \u2018(\u2019 symbol enters the system).\n6 DISCUSSION\nIn this paper we motivated an input-switched affine recurrent network for the purpose of intelligibility. We showed that a switched affine architecture achieves the same performance, for the same number of maximum parameters, on a language modeling task as do more common RNN architectures, including GRUs and LSTMs. We performed a series of analyses, demonstrating that the simplicity of the latent dynamics makes the trained RNN far easier to understand and interpret.\n6.1 BENEFITS OF AFFINE TRANSITIONS OVER LINEAR\nISAN uses affine operators to model state transitions assigned to each input symbol. Following eq. (1) each transition consists of matrix multiplication and bias vector addition. An important question is whether the biases are needed and how the ISAN would be impacted if linear transition operators were used instead of affine ones. The answer is two-fold. First, affine dynamics can be exactly implemented using linear operators in a hidden space expanded by one additional dimension. Therefore, the expressivity of ISAN does not depend on choosing a linear or affine formulation. However, we found that the affine parametrization of transitions is much easier to train. We attempted to train models using only linear transitions, but achieved a loss of only 4.1 bits per character, which\ncorresponds to the performance of a unigram character model. Second, affine operators are easier to interpret because they permit easy visualization of contributions of each input token on the final network\u2019s prediction, as demonstrated in Section 4.2.\n6.2 COMPUTATIONAL BENEFITS\nSwitched affine networks hold the potential to be massively more computationally and memory efficient for text processing than standard RNNs, as explained in the next two subsections.\n6.2.1 SPARSE PARAMETER ACCESS\nAs shown in Figure 1a, the performance for fixed parameter count is nearly identical between the ISAN and other recurrent networks. However, at each time step, only the parameters associated with a single input are used. For K possible inputs and N parameters, the computational cost per update\nstep is O ( N K ) , a factor of K speedup over non-switched architectures. Similarly, the number of\nhidden units is O (\u221a\nN K\n) , a factor of K 1 2 memory improvement for storage of the latent state.\n6.2.2 COMPOSITION OF AFFINE UPDATES\nThe memory and computational benefits in Section 6.2.1 are shared by other switched networks. However, ISAN is unique in its ability to precompute affine transformations corresponding to input strings. This is possible because the composition of affine transformations is also an affine transformation. This property is used in Section 4.3 to evaluate the linear contributions of words, rather than characters. This means that the hidden state update corresponding to an entire input sequence can be computed with identical cost to the update for a single character (plus the dictionary lookup cost for the composed transformation). ISAN can therefore achieve very large speedups on input processing, at the cost of increased memory use, by accumulating large lookup tables of the Wx and bx corresponding to common input sequences. Of course, practical implementations will have to incorporate complexities of memory management, batching, etc.\n6.3 FUTURE WORK\nThere are some obvious future directions to this work. Currently, we define switching behavior using an input set with finite and manageable cardinality. Studying word-level language models with enormous vocabularies may require some additional logic to scale. Adapting this model to continuous-valued inputs is another important direction. One approach is to use a tensor factorization similar to that employed by the MRNN (Sutskever et al., 2014). Another is to build a language model which switches on bigrams or trigrams, rather than characters or words, targeting an intermediate number of affine transformations.\nTraining very large switched linear models has the potential to be extremely fruitful, due both to their improved computational efficiency, and our ability to better understand and manipulate their behavior.\n7 ACKNOWLEDGEMENTS\nWe would like to thank Jasmine Collins for her help and advice, and Quoc Le, David Ha and Mohammad Norouzi for helpful discussions.\n", "cats": {"READ": 1.0, "DONTREAD": 0.0}, "reviews": [{"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "All reviewers have carefully looked at the paper and weakly support acceptance of the paper. Program Chairs also looked at this paper and believe that its contribution is too marginal and incremental in its current form. We encourage the authors to resubmit.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"DATE": "18 Jan 2017", "TITLE": "Parenthesis counting demo", "IS_META_REVIEW": false, "comments": "We have released the demo of a fully-understandable ISAN network for counting parenthesis at ", "OTHER_KEYS": "Jan K Chorowski"}, {"DATE": "16 Jan 2017", "TITLE": "Overall response to reviews, high level summary of new results and analysis", "IS_META_REVIEW": false, "comments": "We would like to thank all reviewers for their careful and thorough reviews, and for recommending paper acceptance. Based on your specific actionable feedback, we have since improved the paper with additional analysis and experiments. We hope that based on these new results, you will raise your scores and more confidently recommend acceptance.\n\nIn particular we have included a new analysis in Section 5 which provides an end-to-end interpretation of the functioning of the ISAN on a parenthesis counting task. For this example we believe we have really `cracked the case\u2019, and fully explain the neural network behavior. We have also included a new analysis (Figure 4c) that quantifies the importance of past characters for current predictions. Furthermore, we have prepared a standalone IPython demo featuring our implementation of the ISAN on the parenthesis counting task and are waiting for approval to release it.\nLastly we have a included a new plot (Figure 6) that uses the \\kappa_word as an embedding space, clearly showing that semantic structure arises on a word level, even though the model is only trained on next character prediction. \n\nIn terms of new experimental validation we have added the following comparisons (see Section 4.1):\n1) fully linear dynamics (without switching) with linear readouts\n2) fully linear dynamics (without switching) but with non-linear readouts \n3) naive bayes\nThese experiments highlight the crucial importance of input switching for the performance of the ISAN. \n\nTo investigate the limits of the input switched architecture we also ran experiments to compare the ISAN to the LSTM on a word-fragment task with a large number of inputs and find that it performs less well than the LSTM, with a gap of around 0.15 bits / char-pair (details given below).\n\nFinally, we have improved the text in a number of places to address reviewer concerns. These changes are detailed in the per-reviewer responses.", "OTHER_KEYS": "Jakob Nicolaus Foerster"}, {"TITLE": "No Title", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "Summary:  The authors present a simple RNN with linear dynamics for language modeling. The linear dynamics greatly enhance the interpretability of the model, as well as provide the potential to improve performance by caching the dynamics for common sub-sequences. Overall, the quantitative comparison on a benchmark task is underwhelming. It\u2019s unclear why the authors didn\u2019t consider a more common dataset, and they only considered a single dataset. On the other hand, they present a number of well-executed techniques for analyzing the behavior of the model, many of which would be impossible to do for a non-linear RNN. \n\nOverall, I recommend that the paper is accepted, despite the results. It provides an interesting read and an important contribution to the research dialogue. \n\nFeedback\n\nThe paper could be improved by shortening the number of analysis experiments and increasing the discussion of related sequence models. Some of the experiments were very compelling, whereas some of them (eg. 4.6) sort of feels like you\u2019re just showing the reader that the model fits the data well, not that the model has any particularly important property. We trust that the model fits the data well, since you get reasonable perplexity results. \n\nLSTMS/GRUs are great for for language modeling for data with rigid combinatorial structure, such as nested parenthesis. It would have been nice if you compared your model to non-linear methods on this sort of data. Don\u2019t be scared of negative results! It would be interesting if the non-linear methods were substantially better on these tasks. \n\nYou should definitely add a discussion of Belanger and Kakade 2015 to the related work. They have different motivations (fast, scalable learning algorithms) rather than you (interpretable latent state dynamics and simple credit assignment for future predictions given past). On the other hand, they also have linear dynamics, and look at the singular vectors of the transition matrix to analyze the model. \n\nMore broadly, it would be useful for readers if you discussed LDS more directly. A lot of this comparison came up in the openreview discussion, and I recommend folding this into the paper. For example, it would be useful to emphasize that the bias vectors correspond to columns of the Kalman gain matrix. \n\nOne last thing regarding LDS: your model corresponds to Kalman filtering but in an LDS you can also do Kalman smoothing, where state vectors are inferred using the future in addition to the past observations. Could you do something similar in your model?\n\nWhat if you said that each matrix is a sparse/convex combination of a set of dictionary matrices? This parameter sharing could provide even more interpretability, since the characters are then represented by the low-dimensional weights used to combine the dictionary elements. This could also provide more scalability to word-level problems. \n", "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "21 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "Original and creative work - hesitation about results.", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer4", "comments": "Summary: The authors propose an input switched affine network to do character-level language modeling, a kind of RNN without pointwise nonlinearity, but with switching the transition matrix & bias based on the input character. This is motivated by intelligibility, since it allows decomposition of output contribution into these kappa_s^t terms, and use of basic linear algebra to probe the network.\n\nRegarding myself as a reviewer, I am quite sure I understood the main ideas and arguments of this paper, but am not an expert on RNN language models or intelligibility/interpretability in ML.\nI did not read any papers with a similar premise - closest related work I'm familiar with would be deconvnet for insight into vision-CNNs.\n\nPRO:\nI think this is original and novel work. This work is high quality, well written, and clearly is the result of a lot of work.\nI found section 4.5 about projecting into readout subspace vs \"computational\" subspace most interesting and meaningful.\n\nCON:\n+ The main hesitation I have is that the results on both parts (ISAN model, and analysis of it) are not entirely convincing:\n   (1) ISAN is only trained on small task (text8), not clear whether it can be a strong char-LM on larger scale tasks,\n   (2) nor do the analysis sections provide all that much real insight in the learned network.\n\n(1b) Other caveat towards ISAN architecture: this model in its proposed form is really only fit for small-vocabulary (i.e. character-based) language modeling, not a general RNN with large-vocab discrete input nor continuous input.\n\n(2a) For analysis: many cute plots and fun ideas of quantities to look at, but not much concrete insights.\n(2b) Not very clear which analysis is specific to the ISAN model, and which ideas will generalize to general nonlinear RNNs.\n(2c) Re sec 4.2 - 4.3: It seems that the quantity \\kappa_s^t on which analysis rests, isn't all that meaningful. Elaborating a bit on what I wrote in the question:\nFor example: Fig 2, for input letter \"u\" in revenue, there's a red spot where '_' character massively positively impacts the logit of 'e'. This seems quite meaningless, what would be the meaning of influence of '_' character? So it looks ot me that the switching matrix W_u (and prior W_n W_e etc) are using previous state in an interesting way to produce that following e. So that metric \\kappa_s^t just doesn't seem very meaningful.\nThis remark relates to the last paragraph of Sec4.2.\n\nEven though the list of cons here is longer than pro's, I recommend accept; specifically because the originality of this work will in any case make it more vulnerable to critiques. This work is well-motivated, very well-executed, and can inspire many more interesting investigations along these lines.", "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "19 Dec 2016", "REVIEWER_CONFIDENCE": 3}, {"TITLE": "A character language model that gains some interpretability without large losses in predictivity", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "The authors present a character language model that gains some interpretability without large losses in predictivity. \n\nCONTRIBUTION:\n\nI'd characterize the paper as some experimental investigation of a cute insight.  Recall that multi-class logistic regression allows you to apportion credit for a prediction to the input features: some features raised the probability of the correct class, while others lowered it.  This paper points out that a sufficiently simple RNN model architecture is log-linear in the same way, so you can apportion credit for a prediction among elements of the past history.  \n\nPROS:\n\nThe paper is quite well-written and was fun to read.  It's nice to see that a simple architecture still does respectably.\nIt's easy to imagine using this model for a classroom assignment.  \nIt should be easy to implement, and the students could replicate the authors' investigation of what influences the network's predictions.\nThe authors present some nice visualizations.\n\nSection 5.2 also describes some computational benefits.\n\nCAVEATS ON PREDICTIVE ACCURACY:\n\n* Figure 1 says that the ISAN has \"near identical performance to other architectures.\"  But this appears true only when comparing the largest models.  \n\nExplanation: It appears that for smaller parameter sizes, a GRU still beats the authors' model by 22% to 39% in the usual metric of perplexity per word (ppw).  (That's how LM people usually report performance, with a 10% reduction in ppw traditionally being considered a good Ph.D. dissertation.  I assumed an average of 7 chars/word when converting cross-entropy/char to perplexity/word.)  \n\n* In addition, it's not known whether this model family will remain competitive beyond the toy situations tested here.\n\nExplanation: The authors tried it only on character-based language modeling, and only on a 10M-char dataset, so their ppw is extremely high: 2135 for the best models in this paper.  By contrast, a word-based RNN LM trained on 44M words gets ppw of 133, and trained on 800M words gets ppw of 51.  [Numbers copied from the paper I cited before: ", "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "17 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"DATE": "07 Dec 2016", "TITLE": "pre-review questions", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer4"}, {"DATE": "03 Dec 2016", "TITLE": "Questions", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2"}, {"DATE": "03 Dec 2016", "TITLE": "relation to observable operator models?", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3"}, {"DATE": "30 Nov 2016", "TITLE": "Related work", "IS_META_REVIEW": false, "comments": "Very cool work. Here's some possibly related work on treating text as one-hots coming from a latent linear dynamical system over unobserved embeddings, \"A Linear Dynamical System Model for Text\" by Belanger and Kakade: ", "OTHER_KEYS": "Luke Vilnis"}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "All reviewers have carefully looked at the paper and weakly support acceptance of the paper. Program Chairs also looked at this paper and believe that its contribution is too marginal and incremental in its current form. We encourage the authors to resubmit.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"DATE": "18 Jan 2017", "TITLE": "Parenthesis counting demo", "IS_META_REVIEW": false, "comments": "We have released the demo of a fully-understandable ISAN network for counting parenthesis at ", "OTHER_KEYS": "Jan K Chorowski"}, {"DATE": "16 Jan 2017", "TITLE": "Overall response to reviews, high level summary of new results and analysis", "IS_META_REVIEW": false, "comments": "We would like to thank all reviewers for their careful and thorough reviews, and for recommending paper acceptance. Based on your specific actionable feedback, we have since improved the paper with additional analysis and experiments. We hope that based on these new results, you will raise your scores and more confidently recommend acceptance.\n\nIn particular we have included a new analysis in Section 5 which provides an end-to-end interpretation of the functioning of the ISAN on a parenthesis counting task. For this example we believe we have really `cracked the case\u2019, and fully explain the neural network behavior. We have also included a new analysis (Figure 4c) that quantifies the importance of past characters for current predictions. Furthermore, we have prepared a standalone IPython demo featuring our implementation of the ISAN on the parenthesis counting task and are waiting for approval to release it.\nLastly we have a included a new plot (Figure 6) that uses the \\kappa_word as an embedding space, clearly showing that semantic structure arises on a word level, even though the model is only trained on next character prediction. \n\nIn terms of new experimental validation we have added the following comparisons (see Section 4.1):\n1) fully linear dynamics (without switching) with linear readouts\n2) fully linear dynamics (without switching) but with non-linear readouts \n3) naive bayes\nThese experiments highlight the crucial importance of input switching for the performance of the ISAN. \n\nTo investigate the limits of the input switched architecture we also ran experiments to compare the ISAN to the LSTM on a word-fragment task with a large number of inputs and find that it performs less well than the LSTM, with a gap of around 0.15 bits / char-pair (details given below).\n\nFinally, we have improved the text in a number of places to address reviewer concerns. These changes are detailed in the per-reviewer responses.", "OTHER_KEYS": "Jakob Nicolaus Foerster"}, {"TITLE": "No Title", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "Summary:  The authors present a simple RNN with linear dynamics for language modeling. The linear dynamics greatly enhance the interpretability of the model, as well as provide the potential to improve performance by caching the dynamics for common sub-sequences. Overall, the quantitative comparison on a benchmark task is underwhelming. It\u2019s unclear why the authors didn\u2019t consider a more common dataset, and they only considered a single dataset. On the other hand, they present a number of well-executed techniques for analyzing the behavior of the model, many of which would be impossible to do for a non-linear RNN. \n\nOverall, I recommend that the paper is accepted, despite the results. It provides an interesting read and an important contribution to the research dialogue. \n\nFeedback\n\nThe paper could be improved by shortening the number of analysis experiments and increasing the discussion of related sequence models. Some of the experiments were very compelling, whereas some of them (eg. 4.6) sort of feels like you\u2019re just showing the reader that the model fits the data well, not that the model has any particularly important property. We trust that the model fits the data well, since you get reasonable perplexity results. \n\nLSTMS/GRUs are great for for language modeling for data with rigid combinatorial structure, such as nested parenthesis. It would have been nice if you compared your model to non-linear methods on this sort of data. Don\u2019t be scared of negative results! It would be interesting if the non-linear methods were substantially better on these tasks. \n\nYou should definitely add a discussion of Belanger and Kakade 2015 to the related work. They have different motivations (fast, scalable learning algorithms) rather than you (interpretable latent state dynamics and simple credit assignment for future predictions given past). On the other hand, they also have linear dynamics, and look at the singular vectors of the transition matrix to analyze the model. \n\nMore broadly, it would be useful for readers if you discussed LDS more directly. A lot of this comparison came up in the openreview discussion, and I recommend folding this into the paper. For example, it would be useful to emphasize that the bias vectors correspond to columns of the Kalman gain matrix. \n\nOne last thing regarding LDS: your model corresponds to Kalman filtering but in an LDS you can also do Kalman smoothing, where state vectors are inferred using the future in addition to the past observations. Could you do something similar in your model?\n\nWhat if you said that each matrix is a sparse/convex combination of a set of dictionary matrices? This parameter sharing could provide even more interpretability, since the characters are then represented by the low-dimensional weights used to combine the dictionary elements. This could also provide more scalability to word-level problems. \n", "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "21 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "Original and creative work - hesitation about results.", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer4", "comments": "Summary: The authors propose an input switched affine network to do character-level language modeling, a kind of RNN without pointwise nonlinearity, but with switching the transition matrix & bias based on the input character. This is motivated by intelligibility, since it allows decomposition of output contribution into these kappa_s^t terms, and use of basic linear algebra to probe the network.\n\nRegarding myself as a reviewer, I am quite sure I understood the main ideas and arguments of this paper, but am not an expert on RNN language models or intelligibility/interpretability in ML.\nI did not read any papers with a similar premise - closest related work I'm familiar with would be deconvnet for insight into vision-CNNs.\n\nPRO:\nI think this is original and novel work. This work is high quality, well written, and clearly is the result of a lot of work.\nI found section 4.5 about projecting into readout subspace vs \"computational\" subspace most interesting and meaningful.\n\nCON:\n+ The main hesitation I have is that the results on both parts (ISAN model, and analysis of it) are not entirely convincing:\n   (1) ISAN is only trained on small task (text8), not clear whether it can be a strong char-LM on larger scale tasks,\n   (2) nor do the analysis sections provide all that much real insight in the learned network.\n\n(1b) Other caveat towards ISAN architecture: this model in its proposed form is really only fit for small-vocabulary (i.e. character-based) language modeling, not a general RNN with large-vocab discrete input nor continuous input.\n\n(2a) For analysis: many cute plots and fun ideas of quantities to look at, but not much concrete insights.\n(2b) Not very clear which analysis is specific to the ISAN model, and which ideas will generalize to general nonlinear RNNs.\n(2c) Re sec 4.2 - 4.3: It seems that the quantity \\kappa_s^t on which analysis rests, isn't all that meaningful. Elaborating a bit on what I wrote in the question:\nFor example: Fig 2, for input letter \"u\" in revenue, there's a red spot where '_' character massively positively impacts the logit of 'e'. This seems quite meaningless, what would be the meaning of influence of '_' character? So it looks ot me that the switching matrix W_u (and prior W_n W_e etc) are using previous state in an interesting way to produce that following e. So that metric \\kappa_s^t just doesn't seem very meaningful.\nThis remark relates to the last paragraph of Sec4.2.\n\nEven though the list of cons here is longer than pro's, I recommend accept; specifically because the originality of this work will in any case make it more vulnerable to critiques. This work is well-motivated, very well-executed, and can inspire many more interesting investigations along these lines.", "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "19 Dec 2016", "REVIEWER_CONFIDENCE": 3}, {"TITLE": "A character language model that gains some interpretability without large losses in predictivity", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "The authors present a character language model that gains some interpretability without large losses in predictivity. \n\nCONTRIBUTION:\n\nI'd characterize the paper as some experimental investigation of a cute insight.  Recall that multi-class logistic regression allows you to apportion credit for a prediction to the input features: some features raised the probability of the correct class, while others lowered it.  This paper points out that a sufficiently simple RNN model architecture is log-linear in the same way, so you can apportion credit for a prediction among elements of the past history.  \n\nPROS:\n\nThe paper is quite well-written and was fun to read.  It's nice to see that a simple architecture still does respectably.\nIt's easy to imagine using this model for a classroom assignment.  \nIt should be easy to implement, and the students could replicate the authors' investigation of what influences the network's predictions.\nThe authors present some nice visualizations.\n\nSection 5.2 also describes some computational benefits.\n\nCAVEATS ON PREDICTIVE ACCURACY:\n\n* Figure 1 says that the ISAN has \"near identical performance to other architectures.\"  But this appears true only when comparing the largest models.  \n\nExplanation: It appears that for smaller parameter sizes, a GRU still beats the authors' model by 22% to 39% in the usual metric of perplexity per word (ppw).  (That's how LM people usually report performance, with a 10% reduction in ppw traditionally being considered a good Ph.D. dissertation.  I assumed an average of 7 chars/word when converting cross-entropy/char to perplexity/word.)  \n\n* In addition, it's not known whether this model family will remain competitive beyond the toy situations tested here.\n\nExplanation: The authors tried it only on character-based language modeling, and only on a 10M-char dataset, so their ppw is extremely high: 2135 for the best models in this paper.  By contrast, a word-based RNN LM trained on 44M words gets ppw of 133, and trained on 800M words gets ppw of 51.  [Numbers copied from the paper I cited before: ", "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "17 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"DATE": "07 Dec 2016", "TITLE": "pre-review questions", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer4"}, {"DATE": "03 Dec 2016", "TITLE": "Questions", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2"}, {"DATE": "03 Dec 2016", "TITLE": "relation to observable operator models?", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3"}, {"DATE": "30 Nov 2016", "TITLE": "Related work", "IS_META_REVIEW": false, "comments": "Very cool work. Here's some possibly related work on treating text as one-hots coming from a latent linear dynamical system over unobserved embeddings, \"A Linear Dynamical System Model for Text\" by Belanger and Kakade: ", "OTHER_KEYS": "Luke Vilnis"}]}
{"text": "1 INTRODUCTION\nIn this paper, we are interested in deep generative models. One may naively classify these models into a family of directed deep generative models trainable by back-propagation (e.g., Kingma & Welling, 2013; Goodfellow et al., 2014), and deep energy-based models, such as deep belief network (Hinton et al., 2006) and deep Boltzmann machine (Salakhutdinov & Hinton, 2009). The building block of deep energy-based models is a bipartite graphical model called restricted Boltzmann machine (RBM). The RBM model consists of two layers, visible and hidden. The resulting graphical model which can account for higher-order interactions of the visible units (visible layer) using the hidden units (hidden layer). It also makes the inference easier that there are no interactions between the variables in each layer.\nThe conventional RBM uses Bernoulli units for both the hidden and visible units (Smolensky, 1986). One extension is using Gaussian visible units to model general natural images (Freund & Haussler, 1994). For hidden units, we can also generalize Bernoulli units to the exponential family (Welling et al., 2004; Ravanbakhsh et al., 2016).\nNair & Hinton (2010) propose a variation using Rectified Linear Unit (ReLU) for the hidden layer with a heuristic sampling procedure, which has promising performance in terms of reconstruction error and classification accuracy. Unfortunately, due to its lack of strict monotonicity, ReLU RBM does not fit within the framework of exponential family RBMs (Ravanbakhsh et al., 2016). Instead we study leaky-ReLU RBM (leaky RBM) in this work and address two important issues i) a better training (sampling) algorithm for ReLU RBM and; ii) a better quantification of leaky RBM \u2013i.e., evaluation of its performance in terms of likelihood.\nWe study some of the fundamental properties of leaky RBM, including its joint and marginal distributions (Section 2). By analyzing these distributions, we show that the leaky RBM is a union of\ntruncated Gaussian distributions. In this paper, we show that training leaky RBM involves underlying positive definite constraints. Because of this, the training can diverge if these constrains are not satisfied. This is an issue that was previously ignored in ReLU RBM, as it was mainly used for pre-training rather than generative modeling.\nOur contribution in this paper is three-fold: I) we systematically identify and address model constraints in leaky RBM (Section 3); II) for the training of leaky RBM, we propose a meta algorithm for sampling, which anneals leakiness during the Gibbs sampling procedure (Section 3) and empirically show that it can boost contrastive divergence with faster mixing (Section 5); III) We demonstrate the power of the proposed sampling algorithm on estimating the partition function. In particular, comparison on several benchmark datasets shows that the proposed method outperforms the conventional AIS (Salakhutdinov & Murray, 2008) in terms of efficiency and accuracy (Section 4). Moreover, we provide an incentive for using leaky RBM by showing that the leaky ReLU hidden units perform better than the Bernoulli units in terms of the model log-likelihood (Section 4).\n2 RESTRICTED BOLTZMANN MACHINE AND RELU\nThe Boltzmann distribution is defined as p(x) = e\u2212E(x)/Z where Z = \u2211 x e \u2212E(x) is the partition function. Restricted Boltzmann Machine (RBM) is a Boltzmann distribution with a bipartite structure It is also the building block for many deep models (e.g., Hinton et al., 2006; Salakhutdinov & Hinton, 2009; Lee et al., 2009), which are widely used in numerous applications (Bengio, 2009). The conventional Bernoulli RBM, models the joint probability p(v, h) for the visible units v \u2208 [0, 1]I and the hidden units h \u2208 [0, 1]J as p(v, h) \u221d exp(\u2212E(v, h)), where E(v, h) = a>v \u2212 v>Wh + b>h. The parameters are a \u2208 RI , b \u2208 RJ and W \u2208 RI\u00d7J . We can derive the conditional probabilities as\np(vi = 1|h) = \u03c3  J\u2211 j=1 Wijhj + ai  and p(hj = 1|v) = \u03c3( I\u2211 i=1 Wijvi + bj ) , (1)\nwhere \u03c3(x) = (1 + e\u2212x)\u22121 is the sigmoid function.\nOne extension of Bernoulli RBM is replacing the binary visible units by linear units v \u2208 RI with independent Gaussian noise. The energy function in this case is given by\nE(v, h) = I\u2211 i=1 (vi \u2212 ai)2 2\u03c32i \u2212 I\u2211 i=1 J\u2211 j=1 vi \u03c3i Wijhj + b >h.\nTo simplify the notation, we assume a normalized data so that ai and \u03c3i is no longer required. The energy function is accordingly simplified to E(v, h) = \u2016v\u2016 2\n2 \u2212 v >Wh + b>h (Note that the\nelimination does not influence the discussion and one can easily extend all the results in this paper to the model that includes ai and \u03c3i.).\nThe conditional distributions are as follows:\np(vi|h) = N  J\u2211 j=1 Wijhj , 1  and p(hj = 1|v) = \u03c3( I\u2211 i=1 Wijvi + bj ) , (2)\nwhere N (\u00b5, V ) is a Gaussian distribution with mean \u00b5 and variance V . To simplify the notation, in the following we define \u03b7j = \u2211I i=1Wijvi + bj \u2013 that is \u03b7j is the input to the j\nth hidden layer neuron and similarly define \u03bdi = \u2211J j=1Wijhj + ai. Using this notation the conditionals in the (2) are p(vi|\u03bdi) = N (\u03bdi, 1) and p(hj = 1|\u03b7j) = \u03c3(\u03b7j).\n2.1 RELU RBM WITH CONTINUOUS VISIBLE UNITS\nFrom (1) and (2), we can see that the mean of the p(hj |v) is the nonlinearity of the hidden unit at \u03b7j = \u2211I i=1Wijvi + bj \u2013 e.g., mean of the Bernoulli unit is the sigmoid function. From this perspective, we can extend the sigmoid function to other functions and thus allow RBM to have more expressive power (Ravanbakhsh et al., 2016). In particular, it would be interesting to use rectified linear unit (ReLU) nonlinearity, f(\u03b7j) = max(0, \u03b7j), for generative modeling.\nNair & Hinton (2010) use an RBM with visible Gaussian unit and ReLU hidden activation functions for pretraining. They suggest sampling from max(0, \u03b7j+N (0, \u03c3(\u03b7j)) for conditional sampling from the hidden units (compare to (2)). However, this sampling heuristic does not suggest the parametric form of the joint ReLU-Gaussian distribution. This also means we cannot evaluate it using methods such as Annealed Importance Sampling that require access to this parametric form. In fact, only strictly monotonic activation functions can derive feasible joint and conditional distributions in the exponential familly RBM and ReLU is not strictly monotonic Ravanbakhsh et al. (2016). Similar activation functions that are monotonic are Softplus, f(\u03b7j) = log(1 + e\u03b7j ) and leaky ReLU (Maas et al., 2013), defined as f(\u03b7j) = max(c\u03b7j , \u03b7j), where c \u2208 (0, 1) is the leakiness parameter. In contrast to the ReLU RBM the joint parametric form of these two distributions are available. However, the energy (logarithm of the joint probability) in the case of Softplus activation function contains a polylogarithmic term that requires evaluation of an infinite series; see Table 1 in Ravanbakhsh et al. (2016). For this reason, here we focus on Leaky-ReLU activation function.\nBy Ravanbakhsh et al. (2016), the conditional probability of the activation, assuming the nonlinearity f(\u03b7j), is generally defined as p(hj |v) = exp (\u2212Df (\u03b7j\u2016hj) + g(hj)), where Df (\u03b7j\u2016hj) is the Bregman Divergence associated with f, and g(hj) is the base (or carrier) measure in the exponential family which ensures the distribution is well-defined. The Bergman divergence, for strictly monotonic function f , is Df (\u03b7j\u2016hj) = \u2212\u03b7jhj + F (\u03b7j) + F \u2217(hj), where F with dd\u03b7j F (\u03b7j) = f(\u03b7j) is the anti-derivative (integral) of f and F \u2217 is the anti-derivative of f\u22121 (i.e., f\u22121(f(\u03b7)) = \u03b7); Note that due to the strict monotonicity of f , f\u22121 is well-defined, and F and F \u2217 are commonly referred to as conjugate duals.\nConsidering the leaky ReLU activation function f(\u03b7) = max(c\u03b7, \u03b7), using this formalism, the conditional distributions of hidden units in the leaky RBM simplifies to (see Appendix A.1 for details)\np(hj |v) = { N (\u03b7j , 1), if \u03b7j > 0 N (c\u03b7j , c), if \u03b7j \u2264 0.\n(3)\nSince the visible units uses the identity function, the corresponding conditional distribution is a Gaussian1\np(vi|h) = N  J\u2211 j=1 Wijhj , 1  , (4) Having these two conditional distributions is enough for training a leaky RBM model using contrastive divergence (Hinton, 2002) or some other alternatives (e.g., Tieleman, 2008; Tieleman & Hinton, 2009).\n3 TRAINING AND SAMPLING FROM LEAKY RBM\nGiven the conditional distributions p(v|h) and p(h|v), the joint distribution p(v, h) from the general treatment for MRF model is (Yang et al., 2012; Ravanbakhsh et al., 2016)\np(v, h) \u221d exp v>Wh\u2212 I\u2211 i=1 (F\u0303 \u2217(vi) + g(vi))\u2212 J\u2211 j=1 (F \u2217(hj) + g(hj))  , (5) where F\u0303 \u2217(vi) and F \u2217(hj) are anti-derivatives of the inverses of the activation functions f\u0303(vi) and f(hj) for visible units vi and hidden units hj , respectively (see Section 2.1). Assuming f(\u03b7j) = max(c\u03b7j , c) and f\u0303(\u03bdi) = \u03bdi in leaky-ReLU RBM, the joint distribution above becomes (see Appendix A.2 for details)\np(v, h) \u221d exp v>Wh\u2212 \u2016v\u20162 2 \u2212 \u2211 \u03b7j>0 ( h2j 2 + log \u221a 2\u03c0 ) \u2212 \u2211 \u03b7j\u22640 ( h2j 2c + log \u221a 2c\u03c0 ) + b>h  , 1which can also be written as p(vi|h) = exp ( \u2212Df\u0303 (\u03bdi\u2016vi) + g(vi) ) , where \u03bdi = \u2211 j=1Wijhj and\nf\u0303(\u03bdi) = \u03bdi and Df\u0303 (\u03bdi\u2016vi) = (\u03bdi \u2212 vi) 2 and g(vi) = \u2212 log\n\u221a 2\u03c0.\nand the corresponding visible marginal distribution is\np(v) \u221d exp \u22121 2 v> I \u2212\u2211 \u03b7j>0 WjW > j \u2212 c \u2211 \u03b7j\u22640 WjW > j  v + \u2211 \u03b7j>0 bjW > j v + c \u2211 \u03b7j\u22640 bjW > j v  . (6)\nwhere Wj is the j-th column of W .\n3.1 LEAKY RBM AS UNION OF TRUNCATED GAUSSIAN DISTRIBUTIONS\nFrom (6) we see that the marginal probability is determined by the affine constraints \u03b7j > 0 or \u03b7j \u2264 0 for all hidden units j. By combinatorics, these constraints divide RI (the visible domain) into at most M = \u2211I i=1 ( J i ) convex regions R1, \u00b7 \u00b7 \u00b7RM . An example with I = 2 and J = 3 is shown in Figure 1. If I > J , then we have at most 2J regions.\nWe discuss the two types of these regions. For bounded regions, such as R1 in Figure 1, the integration of (6) is also bounded, which results in a valid distribution. Before we discuss the unbounded cases, we define \u2126 = I \u2212 \u2211J j=1 \u03b1jWjW > j , where \u03b1j = 1\u03b7j>0 + c1\u03b7j\u22640. For the unbounded region, if \u2126 \u2208 RI\u00d7I is a positive definite (PD) matrix, then the probability density is proportional to a multivariate Gaussian distribution with mean \u00b5 = \u2126\u22121 (\u2211J j=1 \u03b1jbjWj ) and precision matrix \u2126 (covariance matrix \u2126\u22121) but over an affine-constrained region. Therefore, the distribution of each unbounded region can be treated as a truncated Gaussian distribution. The marginal distrubution can be treated as a union of truncated Gaussain distribution. Note that leaky RBM is different from Su et al. (2017), which use single truncated Gaussian distribution to model joint (conditional) distributions and require approximated and more complicated sampling algorithms for truncated Gaussian distribution, while leaky RBM only requires to sample from Gaussian distributions.\nOn the other hand, if \u2126 is not PD, and the region Ri contains the eigenvectors with negative eigenvalues of \u2126, the integration of (6) over Ri is divergent (infinite), which can not result in a valid probability distribution. In practice, with this type of parameter, when we do Gibbs sampling on the conditional distributions, the sampling will diverge. However, it is unfeasible to check exponentially many regions for each gradient update. Theorem 1. If I \u2212WW> is positive definite, then I \u2212 \u2211 j \u03b1jWjW > j is also positive definite, for all \u03b1j \u2208 [0, 1].\nThe proof is shown in Appendix 1. From Theorem 1 we can see that if the constraint I \u2212WW> is PD, then one can guarantee that the distribution of every region is a valid truncated Gaussian distribution. Therefore, we introduce the following projection step for each W after the gradient update.\nargmin W\u0303\n\u2016W \u2212 W\u0303\u20162F\ns.t. I \u2212 W\u0303W\u0303> 0 (7)\nTheorem 2. The above projection step (7) can be done by shrinking the singular values to be less than 1.\nThe proof is shown in Appendix C. The training algorithm of the leaky RBM is shown in Algorithm 1. By using the projection step (7), we could treat the leaky RBM as the union of truncated Gaussian distributions, which uses weight vectors to divide the space of visible units into several regions and use a truncated Gaussian distribution to model each region. Note that the leaky RBM model is different from Su et al. (2016), which uses a truncated Gaussian distribution to model the conditional distribution p(h|v) instead of the marginal distribution. The empirical study about the divergent values and the necessity of the projection step is shown in Appendix D. Without the projection step, when we run Gibbs sampling for several iterations from the model, the sampled values will diverge because the model does not have a valid marginal distribution p(v). It also implies that we cannot train leaky RBM with larger CD steps without projection, which would result in divergent gradients. The detailed discussion is shown in Appendix D.\nAlgorithm 1 Training Leaky RBM for t = 1, . . . , T do\nEstimate gradient g\u03b8 by CD or other algorithms with (13) and (4), where \u03b8 = {W,a, b}. \u03b8(t) \u2190 \u03b8(t\u22121) + \u03b7g\u03b8. Project W (t) by (7).\nend for\n3.2 SAMPLING FROM LEAKY-RELU RBM\nGibbs sampling is the core procedure for RBM, including training, inference, and estimating the partition function (Fischer & Igel, 2012; Tieleman, 2008; Salakhutdinov & Murray, 2008). For every task, we start from randomly initializing v by an arbitrary distribution q, and iteratively sample from the conditional distributions. Gibbs sampling guarantees the procedure result in the stationary distribution in the long run for any initialized distribution q. However, if q is close to the target distribution p, it can significantly shorten the number of iterations to achieve the stationary distribution. If we set the leakiness c to be 1, then (6) becomes a simple multivariate Gaussian distribution N ( (I \u2212WW>)\u22121Wb, (I \u2212WW>)\u22121 ) , which can be easily sampled without Gibbs sampling. Also, the projection step (7) guarantees it is a valid Gaussian distribution. Then we decrease the leakiness with a small , and use samples from the multivariate Gaussian distribution when c = 1 as the initialization to do Gibbs sampling. Note that the distribution of each region is a truncated Gaussian distribution. When we only decrease the leakiness with a small amount, the resulted distribution is a \u201csimilar\u201d truncated Gaussian distribution with more concentrated density. From this observation, we could expect the original multivariate Gaussian distribution serves as a good initialization. The one-dimensional example is shown in Figure 2. We then repeat this procedure until we reach the target leakiness. The algorithm can be seen as annealing the leakiness during the Gibbs sampling procedure. The meta algorithm is shown in Algorithm 2. Next, we show the proposed sampling algorithm can help both the partition function estimation and the training of leaky RBM.\nAlgorithm 2 Meta Algorithm for Sampling from Leaky RBM. Sample v from N ( (I \u2212WW>)\u22121Wb, (I \u2212WW>)\u22121 ) = (1\u2212 c)/T and c\u2032 = 1 for t = 1, . . . , T do\nDecrease c\u2032 = c\u2032 \u2212 and perform Gibbs sampling by using (13) and (4) with leakiness c\u2032 end for\n4 PARTITION FUNCTION ESTIMATION\nIt is known that estimating the partition function of RBM is intractable (Salakhutdinov & Murray, 2008). Existing approaches, including Salakhutdinov & Murray (2008); Grosse et al. (2013); Liu et al. (2015); Carlson et al. (2016) focus on using sampling to approximate the partition function of the conventional Bernoulli RBM instead of the RBM with Gaussian visible units and non-Bernoulli hidden units. In this paper, we focus on extending the classic annealed importance sampling (AIS) algorithm (Salakhutdinov & Murray, 2008) to leaky RBM.\nAssuming that we want to estimate the partition function Z of p(v) with p(v) = p\u2217(v)/Z and p\u2217(v) \u221d \u2211 h exp(\u2212E(v, h)), Salakhutdinov & Murray (2008) start from a initial distribution\np0(v) \u221d \u2211 h exp(\u2212E0(v, h)), where computing the partition Z0 of p0(v) is tractable and we can draw samples from p0(v). They then use the \u201cgeometric path\u201d to anneal the intermediate distribution as pk(v) \u221d p\u2217k(v) = \u2211 h exp (\u2212\u03b2kE0(v, h)\u2212 (1\u2212 \u03b2k)E(v, h)), where they grid \u03b2k from 1 to 0. If we let \u03b20 = 1, we can draw samples vk from pk(v) by using samples vk\u22121 from pk\u22121(v) for k \u2265 1 via Gibbs sampling. The partition function is then estimated via Z = Z0M \u2211M i=1 \u03c9 (i), where\n\u03c9(i) = p\u22171(v (i) 0 )\np\u22170(v (i) 0 )\np\u22172(v (i) 1 ) p\u22171(v (i) 1 ) \u00b7 \u00b7 \u00b7 p\u2217K\u22121(v (i) K\u22122) p\u2217K\u22122(v (i) K\u22122) p\u2217K(v (i) K\u22121) p\u2217K\u22121(v (i) K\u22121) , and \u03b2K = 0.\nSalakhutdinov & Murray (2008) use the initial distribution with independent visible units and without hidden units. We consider application of AIS to the leaky-ReLU case with E0(v, h) = \u2016v\u20162 2 , which results in a multivariate Gaussian distribution p0(v). Compared with the meta algorithm shown in Algorithm 2 which anneals between leakiness, AIS anneals between energy functions.\n4.1 STUDY ON TOY EXAMPLES\nAs we discussed in Section 3.1, leaky RBM with J hidden units is a union of 2J truncated Gaussian distributions. Here we perform a study on the leaky RBM with a small number hidden units. Since in this example the number of hidden units is small, we can integrate out all possible configurations of h. However, integrating a truncated Gaussian distribution with general affine constraints does not have analytical solutions, and several approximations have been developed (e.g., Pakman & Paninski, 2014). To compare our results with the exact partition function, we consider a special case that has the following form:\np(v) \u221d exp \u22121 2 v> I \u2212\u2211 \u03b7j>0 WjW > j \u2212 c \u2211 \u03b7j\u22640 WjW > j  v  . (8)\nCompared to (6), it is equivalent to the setting where b = 0. Geometrically, everyWj passes through the origin. We further put the additional constraint Wi \u22a5 Wj ,\u2200i 6= j. Therefore. we divide the whole space into 2J equally-sized regions. A three dimensional example is shown in Figure 3. Then the partition function of this special case has the analytical form\nZ = 1\n2J \u2211 \u03b1j\u2208{1,c},\u2200j (2\u03c0)\u2212 I 2 \u2223\u2223\u2223\u2223\u2223\u2223\u2223 I \u2212 J\u2211 j=1 \u03b1jWjW > j \u2212 12 \u2223\u2223\u2223\u2223\u2223\u2223\u2223 .\nWe randomly initialize W and use SVD to make columns orthogonal. Also, we scale \u2016Wj\u2016 to satisfy I \u2212WW> 0. The leakiness parameter is set to be 0.01. For Salakhutdinov & Murray (2008) (AIS-Energy), we use 105 particles with 105 intermediate distributions. For the proposed method (AIS-Leaky), we use only 104 particles with 103 intermediate distributions. In this small problem we study the cases when the model has 5, 10, 20 and 30 hidden units and 3072 visible units. The true log partition function logZ is shown in Table 1 and the difference between logZ and the estimates given by the two algorithms are shown in Table 2.\nFrom Table 1, we observe that AIS-Leaky has significantly better and more stable estimations than AIS-Energy especially and this gap increases as we increase the number of hidden units. AIS-Leaky achieves this with orders magnitude reduced computation \u2013e.g., here it uses \u223c.1% of resources used by conventional AIS. For example, when we increase J from 5 to 30, the bias (difference) of AIS-Leaky only increases from 0.02 to 0.13; however, the bias of AIS-Energy increases from 1.76 to 9.6. We further study the implicit connection between the proposed AIS-Leaky and AIS-Energy in Appendix E, which shows AIS-Leaky is a special case of AIS-Energy under certain conditions.\n4.2 COMPARISON BETWEEN LEAKY-RELU RBM AND BERNOULLI-GAUSSIAN RBM\nIt is known that the reconstruction error is not a proper approximation of the likelihood (Hinton, 2012). One commonly adopted way to compare generative models is to sample from the model, and visualize the images to check the quality. However, Theis et al. (2016) show the better visualization does not imply better likelihood. Also, the single layer model cannot adequately model the complicated natural images (the result for Bernoulli-Gaussian RBM has been shown in Ranzato & Hinton (2010)), which makes the visualization comparison difficult (Appendix F has few visualization results).\nFortunately, our accurate estimate of the partition function for leaky RBM can produce a reliable quantitative estimate of the representation power of leaky RBM. We compare the BernoulliGaussian RBM2, which has Bernoulli hidden units and Gaussian visible units. We trained both models with CD-203 and momentum. For both model, we all used 500 hidden units. We initialized W by sampling from Unif(0, 0.01), a = 0, b = 0 and \u03c3 = 1. The momentum parameter was 0.9 and the batch size was set to 100. We tuned the learning rate between 10\u22121 and 10\u22126. We studied two benchmark data sets, including CIFAR10 and SVHN. The data was normalized to have zero mean and standard deviation of 1 for each pixel. The results of the log-likelihood are reported in Table 3.\nFrom Table 3, leaky RBM outperforms Bernoulli-Gaussian RBM significantly. The unsatisfactory performance of Bernoulli-Gaussian RBM may be in part due to the optimization procedure. If we tune the decay schedule of the learning-rate for each dataset in an ad-hoc way, we observe the performance of Bernoulli-Gaussian RBM can be improved by \u223c 300 nats for both datasets. Also, increasing CD-steps brings slight improvement. The other possibility is the bad mixing during the CD iterations. The advanced algorithms Tieleman (2008); Tieleman & Hinton (2009) may help. Although Nair & Hinton (2010) demonstrate the power of ReLU in terms of reconstruction error and classification accuracy, it does not imply its superior generative capability. Our study confirms leaky RBM could have much better generative performance compared to Bernoulli-Gaussian RBM.\n5 BETTER MIXING BY ANNEALING LEAKINESS\nIn this section, we show the idea of annealing between leakiness benefit the mixing in Gibbs sampling in other settings. A common procedure for comparison of sampling methods for RBM is through visualization. Here, we are interested in more quantitative metrics and the practical benefits of improved sampling. For this, we consider optimization performance as the evaluation metric.\nThe gradient of the log-likelihood function L(\u03b8|vdata) of general RBM models is\n\u2202L(\u03b8|vdata) \u2202\u03b8 = Eh|vdata\n[ \u2202E(v, h)\n\u2202\u03b8\n] \u2212 Ev,h [ \u2202E(v, h)\n\u2202\u03b8\n] . (9)\nSince the second expectation in (9) is usually intractable, different approximation algorithms are used (Fischer & Igel, 2012).\n2Our GPU implementation with gnumpy and cudamat can reproduce the results of http://www.cs.toronto.edu/ tang/code/GaussianRBM.m\n3CD-n means that contrastive divergence was run for n steps\nIn this section, we compare two gradient approximation procedures. The baselines are the conventional contrastive divergence (CD) (Hinton, 2002) and persistent contrastive divergence (Tieleman, 2008) (PCD). The second method is using Algorithm 2 (Leaky) with the same number of mixing steps as CD. The experiment setup is the same as that of Section 4.\nThe results are shown in Figure 4. The proposed sampling procedure is slightly better than typical CD steps. The reason is we only anneals the leakiness for 20 steps. To get accurate estimation requires thousands of steps as shown in Section 4 when we estimate the partition function. Therefore, the estimated gradient is still inaccurate. However, it still outperforms the conventional CD algorithm. On the other hand, unlike the binary RBM case shown in Tieleman (2008), PCD does not outperform CD with 20 mixing steps for leaky RBM.\nThe drawback of Algorithm 2 is that sampling v from N ( (I \u2212WW>)\u22121Wb, (I \u2212WW>)\u22121 ) requires computing mean, covariance and the Cholesky decomposition of the covariance matrix in every iteration, which are computationally expensive. We study a mixture algorithm by combining CD and the idea of annealing leakiness. The mixture algorithm replaces the sampling from N ( (I \u2212WW>)\u22121Wb, (I \u2212WW>)\u22121 ) with sampling from the empirical data distribution. The resulted mix algorithm is almost the same as CD algorithm while it anneals the leakiness over the iterations as Algorithm 2. The results of the mix algorithm is also shown in Figure 4.\nThe mix algorithm is slightly worse than the original leaky algorithm, but it also outperforms the conventional CD algorithm without additional computation cost. The comparison in terms of CPU time is shown in Appendix F. Annealing the leakiness helps the mix algorithm explore different modes of the distribution, thereby improves the training. The idea could also be combined with more advanced algorithms (Tieleman, 2008; Tieleman & Hinton, 2009)4.\n6 CONCLUSION\nIn this paper, we study the properties of the exponential family distribution produced by leaky RBM. This study relates the leaky RBM model and truncated Gaussian distribution and reveals an underlying positive definite constraint of training leaky RBM. We further proposed a meta sampling algorithm, which anneals between leakiness during the Gibbs sampling procedure. We first demonstrate the proposed sampling algorithm is significantly more effective and efficient in estimating the partition function than the conventional AIS algorithm. Second, we show that the proposed sampling algorithm has comparatively better mixing properties (compared to CD). A few direction are worth further study; in particular we are investigating on speeding up the naive projection step; either using the barrier function as shown in Hsieh et al. (2011) or by eliminating the need for projection by artificially bounding the domain via additional constraints.\n4We studied the PCD extension of the proposed sampling algorithm. However, the performance is not as stable as CD.\nA DERIVATION OF LEAKY (RELU) RBM\nA.1 CONDITIONAL DISTRIBUTIONS\nFor leaky RBM, the activation function of hidden units is defined as f(\u03b7j) = max(c\u03b7j , \u03b7j), where c \u2208 (0, 1) and \u03b7j = \u2211I i=1Wijvi + bj . The inverse function of f is f\n\u22121(hj) = min(hj , hj/c). Therefore, the anti-derivatives are\nF (\u03b7j) =\n{ 1 2\u03b7 2 j , if \u03b7j > 0\nc 2\u03b7 2 j , else,\n(10)\nand\nF \u2217(hj) =\n{ 1 2h 2 j , if \u03b7j > 0\n1 2ch 2 j , else.\n(11)\nThe activation function of Gaussian visible units can be treated as the linear unit f\u0303(\u03bdi) = \u03bdi, where \u03bdi = \u2211J j=1Wijhj . Following the similar steps for deriving F and F\n\u2217, we get the anti-derivatives F\u0303 (\u03bdi) = 1 2\u03bd 2 i and F\u0303 \u2217(vi) = 1 2v 2 i .\nFrom Ravanbakhsh et al. (2016), the conditional distribution is defined as\np(hj |\u03b7j) = exp (\u2212\u03b7jhj + F (\u03b7j) + F \u2217(hj)) (12)\nBy plugging F and F \u2217 into (12), we get the conditional distribution for leaky RBM\np(hj |v) = { N (\u03b7j , 1)with g(hj) = \u2212 log( \u221a 2\u03c0), if \u03b7j > 0\nN (c\u03b7j , c)with g(hj) = \u2212 log( \u221a 2c\u03c0), if \u03b7j \u2264 0. (13)\nSimilarly, we have p(vi|\u03bdi) = N (\u03bdi, 1) with g(vi) = \u2212 log( \u221a 2\u03c0).\nA.2 JOINT AND MARGINAL DISTRIBUTIONS\nGiven the conditional distributions p(v|h) and p(h|v), the joint distribution p(v, h) from the general treatment for MRF model given by Yang et al. (2012) is\np(v, h) \u221d exp v>Wh\u2212 I\u2211 i=1 (F\u0303 \u2217(vi) + g(vi))\u2212 J\u2211 j=1 (F \u2217(hj) + g(hj))  , (14) By plugging F \u2217, F\u0303 \u2217 and g from Section A.1 into (14), we have\np(v, h) \u221d exp v>Wh\u2212 \u2016v\u20162 2 \u2212 \u2211 \u03b7j>0 ( h2j 2 + log \u221a 2\u03c0 ) \u2212 \u2211 \u03b7j\u22640 ( h2j 2c + log \u221a 2c\u03c0 ) + b>h  , Then the marginal distribution is\np(v) \u221d \u222b h p(v, h)dh\n\u221d \u222b h exp ( \u2212\u2016v\u2016 2 2 ) \u220f \u03b7j>0 exp ( \u2212 h2j 2 + \u03b7jhj \u2212 log \u221a 2\u03c0 ) \u220f \u03b7j\u22640 ( \u2212 h2j 2c + hj\u03b7j \u2212 log \u221a 2c\u03c0 ) dh\n\u221d exp ( \u2212\u2016v\u2016 2\n2 ) \u220f \u03b7j>0 exp ( \u03b72j 2 ) \u220f \u03b7j\u22640 ( c\u03b72j 2 )\n\u221d exp \u22121 2 v> I \u2212\u2211 \u03b7j>0 WjW > j \u2212 c \u2211 \u03b7j\u22640 WjW > j  v + \u2211 \u03b7j>0 bjW > j v + c \u2211 \u03b7j\u22640 bjW > j v  .\nB PROOF OF THEOREM 1\nProof. Since WW>\u2212 \u2211 j \u03b1jWjWj = \u2211 j(1\u2212\u03b1j)WjW>j 0, we have WW> \u2211 j \u03b1jWjWj .\nTherefore, I \u2212 \u2211 j \u03b1jWjW > j I \u2212WW> 0.\nC PROOF OF THEOREM 2\nProof. Let the SVD decomposition of W and W\u0303 as W = USV > and W\u0303 = U\u0303 S\u0303V\u0303 >. Then we have\n\u2016W \u2212 W\u0303\u20162F = \u2016USV > \u2212 U\u0303 S\u0303V\u0303 >\u20162F \u2265 I\u2211 i=1 (Sii \u2212 S\u0303ii)2, (15)\nand the constraint I \u2212 W\u0303W\u0303> 0 can be rewritten as 0 \u2264 S\u0303ii \u2264 1,\u2200i. The transformed problem has a Lasso-like formulation and we can solve it by S\u0303ii = min(Sii, 1) (Parikh & Boyd, 2014). Also, the lower bound \u2211I i=1(Sii \u2212 S\u0303ii)2 in (15) becomes a tight bound when we set U\u0303 = U and V\u0303 = V , which completes the proof.\nD NECESSITY OF THE PROJECTION STEP\nWe conduct a short comparison to demonstrate the projection step is necessary for the leaky RBM on generative tasks. We train two leaky RBM as follows. The first model is trained by the same setting in Section 4. We use the convergence of log likelihood as the stopping criteria. The second model is trained by CD-1 with weight decay and without the projection step. We stop the training when the reconstruction error is less then 10\u22122. After we train these two models, we run Gibbs sampling with 1000 independent chains for several steps and output the average value of the visible units. Note that the visible units are normalized to zero mean. The results on SVHN and CIFAR10 are shown in Figure 5.\nFrom Figure 5, the model trained by weight decay without projection step is suffered by the problem of the diverged values. It confirms the study shown in Section 3.1. It also implies that we cannot\ntrain leaky RBM with larger CD steps when we do not do projection; otherwise, we would have the diverged gradients. Therefore, the projection is necessary for training leaky RBM for the generative purpose. However, we also oberseve that the projection step is not necessary for the classification and reconstruction tasks. he reason may be the independency of different evaluation criteria (Hinton, 2012; Theis et al., 2016) or other implicit reasons to be studied.\nE EQUIVALENCE BETWEEN ANNEALING THE ENERGY AND LEAKINESS\nWe analyze the performance gap between AIS-Leaky and AIS-Energy. One major difference is the initial distribution. The intermediate marginal distribution of AIS-Energy has the following form:\npk(v) \u221d exp \u22121 2 v> I \u2212 (1\u2212 \u03b2k) \u2211 \u03b7j>0 WjW > j \u2212 (1\u2212 \u03b2k)c \u2211 \u03b7j\u22640 WjW > j  v  . (16)\nHere we eliminated the bias terms b for simplicity. Compared with Algorithm 2, (16) not only anneals the leakiness (1 \u2212 \u03b2k)c \u2211 \u03b7j\u22640WjW > j when \u03b7j \u2264 0, but also in the case (1 \u2212\n\u03b2k) \u2211 \u03b7j>0 WjW > j when \u03b7j > 0, which brings more bias to the estimation. In other words, AIS-Leaky is a one-sided leakiness annealing while AIS-Energy is a two-sided leakiness annealing method.\nTo address the higher bias problem of AIS-Energy, we replace the initial distribution with the one used in Algorithm 2. By elementary calculation, the marginal distribution becomes\npk(v) \u221d exp \u22121 2 v> I \u2212\u2211 \u03b7j>0 WjW > j \u2212 (\u03b2k + (1\u2212 \u03b2k)c) \u2211 \u03b7j\u22640 WjW > j  v  , (17)\nwhich recovers the proposed Algorithm 2. From this analysis, we understand AIS-Leaky is a special case of conventional AIS-Energy with better initialization inspired by the study in Section 3. Also, by this connection between AIS-Energy and AIS-Leaky, we note that AIS-Leaky can be combined with other extensions of AIS (Grosse et al., 2013; Burda et al., 2015) as well.\nF MORE EXPERIMENTAL RESULTS FOR SAMPLING\nF.1 SAMPLED IMAGES\nWe show the sampled images from leaky RBM train on CIFAR10 and SVHN datasets. We randomly initialize 20 chains and run Gibbs sampling for 1000 iterations. The sampled results are shown in Figure 6 The results shows that single layer RBM does not adequately model CIFAR10 and SVHN\nwhen compared to multilayer models. The similar results for single layer Bernoulli-Gaussian RBM from Ranzato & Hinton (2010) (in gray scale) is shown in Figure 7. Therefore, we instead focused on quantitative evaluation of the log-likelihood in Table 3.\nF.2 COMPUTATIONAL TIME BETWEEN DIFFERENT SAMPLING STRATEGIES\nThe comparison in terms of CPU time of different sampling algorithms discussed in Section 5 is shown in Figure 8. Please note that the complexity of CD and Mix are the almost the same. Mix only need a few more constant time steps which can be ignored compared with sampling steps. Leaky is more time-consuming because of computing and decomposing the covariance matrix as we discussed in Section 5. We also report the execution time of each step of algorithms in Table 4.\nF.3 STUDY ON RELU-BERNOULLI RBM\nWe study the idea of annealing leakiness on the RBM model with leaky ReLU hidden units and Bernoulli visible units. We create the toy dataset with 20, 25 and 30 visible units as shown in Figure 9. The small datasets allow exact computation of the partition function. For each dataset, we sample 60,000 images for training and 10,000 images for testing. We use 100 hidden units and PCD to train the model. The log likelihood results are shown in Table 5.\nCompared to the Gaussian visible units case we study in Section 3, where p(v) is a multi-variate Gaussian distribution when c = 1, the partition function of p(v) in ReLU-Bernoulli when c = 1 does not have the analytical form. Therefore, we do the following two-stage alternative. We first run the standard AIS algorithm, which anneals the energy, to the distribution with leakiness c = 1. We then change to anneals the leakiness from 1 to the target value. For the typical AIS algorithm (AIS-Energy), we use 104 chains with 2 \u00d7 104 intermediate distributions. For the proposed twostaged algorithm (AIS-Leaky), we use 104 chains with 104 intermediate distributions for annealing to c = 1 and the other 104 distributions for annealing the leakiness. The results are shown in Table 6.\nIn Table 6, the standard AIS algorithm (AIS-Energy) has unsatisfactory performance. We show the performance of AIS for estimating the partition function of models with different leakiness on Toy20. We use the 104 independent chains and 2 \u00d7 104 intermediate distributions. The results are shown in Table 7. From Table 7, we observe that the AIS performances worse when the leakiness is closer to 0. Although we observed that increasing chains and intermediate distributions could improve the performance, but the improvements are limited. The study demonstrates when the\n(a) I = 20\n(b) I = 25\nnon-linearity of the distribution increases (the leakiness value c decreases), the standard AIS cannot effectively estimate the partition function within feasible computational time. On the other hand, it also confirm the proposed idea, annealing the leakiness, can serve as an effective building block for algorithms without enhancing the algorithm complexity. Note that the unsatisfactory performance of AIS may be addressed by Grosse et al. (2013). From Appendix E, the two-stage algorithm used here can also be improved by applying Grosse et al. (2013).\nF.3.1 MNIST AND CALTECH DATASETS\nWe study MNIST and Caltech 101 Silhouettes datasets with 500 hidden units and train the model with CD-25. The results are shown in Table 8 and Table 9. The leaky RBM is better than conventional Bernoulli RBM and some deep models on MNIST data. Although leaky RBM deos not outperform Su et al. (2017), but it enjoys the advantage of the simpler sampling procedure (Gaussian distribution vs truncated Gaussian distribution) in the binary visible unit case.\n", "cats": {"READ": 1.0, "DONTREAD": 0.0}, "reviews": [{"IS_META_REVIEW": true, "comments": "The authors proposed to use leaky rectified linear units replacing binary units in Gaussian RBM.  A sampling method was presented to train the leaky-ReLU RBM. In the experimental section, AIS estimated likelihood on Cifar10 and SVHN were reported.\n\n It's interesting for trying different nonlinear hidden units for RBM. However, there are some concerns for the current work.\n 1. The author did not explain why the proposed sampling method (Alg. 2) is correct. And the additional computation cost (the inner loop and the projection) should be discussed.\n 2. The results (both the resulting likelihood and the generative samples) of Gaussian RBM are much worse than what we have experienced. It seems that the Gaussian RBM were not trained properly.\n 3. The representation learned from a good generative model often helps the classification task when there are fewer label samples. Gaussian RBM works well for texture synthesis tasks in which mixing is an important issue. The authors are encouraged to do more experiments in these two direction."}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "This paper identifies a joint distribution for an RBM variant based on leaky-ReLU activations. It also proposes using a sequence of distributions, both as an annealing-based training method, or to estimate log(Z) with AIS.\n \n This paper was borderline. While there is an interesting idea, the reviewers weren't generally as excited by the work as for other papers. One limitation is that unit-variance Gaussian RBMs aren't a strong baseline for comparison, although that is the focus of the main body of the paper. An update to the paper has results for binary visibles in an appendix, although I'm not sure exactly what was done, if the results are comparable, or if there is a large cost of projection here.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"DATE": "06 Jan 2017", "TITLE": "Upload revision", "IS_META_REVIEW": false, "comments": "We thank all reviewers for their careful reading of the paper and constructive feedback. We have extensively revised the paper to address these requests for clarifications and experiments. Here, we explain some of these and point to related updates in our revision. Please check the individual replies below the reviews.\n", "OTHER_KEYS": "Chun-Liang Li"}, {"TITLE": "No Title", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "The authors proposed to use leaky rectified linear units replacing binary units in Gaussian RBM.  A sampling method was presented to train the leaky-ReLU RBM. In the experimental section, AIS estimated likelihood on Cifar10 and SVHN were reported.\n\n It's interesting for trying different nonlinear hidden units for RBM. However, there are some concerns for the current work.\n 1. The author did not explain why the proposed sampling method (Alg. 2) is correct. And the additional computation cost (the inner loop and the projection) should be discussed.\n 2. The results (both the resulting likelihood and the generative samples) of Gaussian RBM are much worse than what we have experienced. It seems that the Gaussian RBM were not trained properly.\n 3. The representation learned from a good generative model often helps the classification task when there are fewer label samples. Gaussian RBM works well for texture synthesis tasks in which mixing is an important issue. The authors are encouraged to do more experiments in these two direction.", "IS_META_REVIEW": false, "RECOMMENDATION": 5, "DATE": "23 Dec 2016", "REVIEWER_CONFIDENCE": 5}, {"TITLE": "A new model of RBM is proposed, where the conditional of the hidden is a leaky ReLU. In addition an annealed AIS sampler is also proposed to test the learned models quantifiably", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "\nBased on previous work such as the stepped sigmoid units and ReLU hidden units for discriminatively trained supervised models, a Leaky-ReLU model is proposed for generative learning.\n\nPro: what is interesting is that unlike the traditional way of first defining an energy function and then deriving the conditional distributions, this paper propose the forms of the conditional first and then derive the energy function. However this general formulation is not novel to this paper, but was generalized to exponential family GLMs earlier.\n\nCon: \nBecause of the focus on specifying the conditionals, the joint pdf and the marginal p(v) becomes complicated and hard to compute.\n\nOn the experiments, it would been nice to see a RBM with binary visbles and leaky ReLu for hiddens. This would demonstrate the superiority of the leaky ReLU hidden units. In addition, there are more results on binary MNIST modeling with which the authors can compare the results to. While the authors is correct that the annealing distribution is no longer Gaussian, perhaps CD-25 or (Faast) PCD experiments can be run to compare agains the baseline RBM trained using (Fast) PCD.\n\nThis paper is interesting as it combines new hidden function with the easiness of annealed AIS sampling, However, the baseline comparisons to Stepped Sigmoid Units (Nair &Hinton) or other models like the spike-and-slab RBMs (and others) are missing, without those comparisons, it is hard to tell whether leaky ReLU RBMs are better even in continuous visible domain.\n\n", "IS_META_REVIEW": false, "RECOMMENDATION": 5, "DATE": "19 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "No Title", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "The authors propose a novel energy-function for RBMs, using the leaky relu max(cx, x) activation function for the hidden-units. Analogous to ReLU units in feed-forward networks, these leaky relu RBMs split the input space into a combinatorial number of regions, where each region defines p(v) as a truncated Gaussian. A further contribution of the paper is in proposing a novel sampling scheme for the leaky RBM: one can run a much shorter Markov chain by initializing it from a sample of the leaky RBM with c=1 (which yields a standard multi-variate normal over the visibles) and then slowly annealing c. In low-dimension a similar scheme is shown to outperform AIS for estimating the partition function. Experiments are performed on both CIFAR-10 and SVHN.\n\nThis is an interesting paper which I believe would be of interest to the ICLR community. The theoretical contributions are strong: the authors not only introduce a proper energy formulation of ReLU RBMs, but also a novel sampling mechanism and an improvement on AIS for estimating their partition function. \n\nUnfortunately, the experimental results are somewhat limited. The PCD baseline is notably absent. Including (bernoulli visible, leaky-relu hidden) would have allowed the authors to evaluate likelihoods on standard binary RBM datasets. As it stands, performance on CIFAR-10 and SVHN, while improved with leaky-relu, is a far cry from more recent generative models (VAE-based, or auto-regressive models). While this comparison may be unfair, it will certainly limit the wider appeal of the paper to the community. Furthermore, there is the issue of the costly projection method which is required to guarantee that the energy-function remain bounded (covariance matrix over each region be PSD). Again, while it may be fair to leave that for future work given the other contributions, this will further limit the appeal of the paper.\n\nPROS:\nIntroduces an energy function having the leaky-relu as an activation function\nIntroduces a novel sampling procedure based on annealing the leakiness parameter\nSimilar sampling scheme shown to outperform AIS\n\nCONS:\nResults are somewhat out of date\nMissing experiments on binary datasets (more comparable to prior RBM work)\nMissing PCD baseline\nCost of projection method\n", "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "18 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "some claims are incorrect, experiments should be improved", "OTHER_KEYS": "(anonymous)", "comments": "This paper proposed a new variant of RBM, which has a nonlinearity of leaky ReLU, in contrast to the sigmoid function nonlinearity in RBM. By gradually annealing the leakiness coefficient (corresponding to from Gaussian to non-Gaussian model), the authors can sample from their model with a higher mixing rate. With the same idea annealing leakiness, they show they can estimate the partition function of the new model more accurately. \n\n\nMain comments:\n\nThe proposed model can only account for real-valued data. However, RBM is primarily used to model binary data, real-valued RBM (Gaussian-RBM) is not a well-recognized model for real-valued data. So, to demonstrate the superiority of the model, the author should also include the comparison with binary data. And it is also not enough to only compare two datasets for a newly proposed model.\n\nThe claim that the marginal distribution of visible variables is truncated Gaussian is incorrect. For a truncated normal, the values of variables are constrained to be within some region, e.g. requiring variable v from the region a1", "IS_META_REVIEW": false, "RECOMMENDATION": 5, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"DATE": "04 Dec 2016", "TITLE": "Clarity and better comparisons", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1"}, {"DATE": "02 Dec 2016", "TITLE": "Derivations and baselines", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2"}, {"DATE": "02 Dec 2016", "TITLE": "Why not plot generated samples by the Leaky-ReLU RBM? ", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3"}, {"IS_META_REVIEW": true, "comments": "The authors proposed to use leaky rectified linear units replacing binary units in Gaussian RBM.  A sampling method was presented to train the leaky-ReLU RBM. In the experimental section, AIS estimated likelihood on Cifar10 and SVHN were reported.\n\n It's interesting for trying different nonlinear hidden units for RBM. However, there are some concerns for the current work.\n 1. The author did not explain why the proposed sampling method (Alg. 2) is correct. And the additional computation cost (the inner loop and the projection) should be discussed.\n 2. The results (both the resulting likelihood and the generative samples) of Gaussian RBM are much worse than what we have experienced. It seems that the Gaussian RBM were not trained properly.\n 3. The representation learned from a good generative model often helps the classification task when there are fewer label samples. Gaussian RBM works well for texture synthesis tasks in which mixing is an important issue. The authors are encouraged to do more experiments in these two direction."}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "This paper identifies a joint distribution for an RBM variant based on leaky-ReLU activations. It also proposes using a sequence of distributions, both as an annealing-based training method, or to estimate log(Z) with AIS.\n \n This paper was borderline. While there is an interesting idea, the reviewers weren't generally as excited by the work as for other papers. One limitation is that unit-variance Gaussian RBMs aren't a strong baseline for comparison, although that is the focus of the main body of the paper. An update to the paper has results for binary visibles in an appendix, although I'm not sure exactly what was done, if the results are comparable, or if there is a large cost of projection here.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"DATE": "06 Jan 2017", "TITLE": "Upload revision", "IS_META_REVIEW": false, "comments": "We thank all reviewers for their careful reading of the paper and constructive feedback. We have extensively revised the paper to address these requests for clarifications and experiments. Here, we explain some of these and point to related updates in our revision. Please check the individual replies below the reviews.\n", "OTHER_KEYS": "Chun-Liang Li"}, {"TITLE": "No Title", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "The authors proposed to use leaky rectified linear units replacing binary units in Gaussian RBM.  A sampling method was presented to train the leaky-ReLU RBM. In the experimental section, AIS estimated likelihood on Cifar10 and SVHN were reported.\n\n It's interesting for trying different nonlinear hidden units for RBM. However, there are some concerns for the current work.\n 1. The author did not explain why the proposed sampling method (Alg. 2) is correct. And the additional computation cost (the inner loop and the projection) should be discussed.\n 2. The results (both the resulting likelihood and the generative samples) of Gaussian RBM are much worse than what we have experienced. It seems that the Gaussian RBM were not trained properly.\n 3. The representation learned from a good generative model often helps the classification task when there are fewer label samples. Gaussian RBM works well for texture synthesis tasks in which mixing is an important issue. The authors are encouraged to do more experiments in these two direction.", "IS_META_REVIEW": false, "RECOMMENDATION": 5, "DATE": "23 Dec 2016", "REVIEWER_CONFIDENCE": 5}, {"TITLE": "A new model of RBM is proposed, where the conditional of the hidden is a leaky ReLU. In addition an annealed AIS sampler is also proposed to test the learned models quantifiably", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "\nBased on previous work such as the stepped sigmoid units and ReLU hidden units for discriminatively trained supervised models, a Leaky-ReLU model is proposed for generative learning.\n\nPro: what is interesting is that unlike the traditional way of first defining an energy function and then deriving the conditional distributions, this paper propose the forms of the conditional first and then derive the energy function. However this general formulation is not novel to this paper, but was generalized to exponential family GLMs earlier.\n\nCon: \nBecause of the focus on specifying the conditionals, the joint pdf and the marginal p(v) becomes complicated and hard to compute.\n\nOn the experiments, it would been nice to see a RBM with binary visbles and leaky ReLu for hiddens. This would demonstrate the superiority of the leaky ReLU hidden units. In addition, there are more results on binary MNIST modeling with which the authors can compare the results to. While the authors is correct that the annealing distribution is no longer Gaussian, perhaps CD-25 or (Faast) PCD experiments can be run to compare agains the baseline RBM trained using (Fast) PCD.\n\nThis paper is interesting as it combines new hidden function with the easiness of annealed AIS sampling, However, the baseline comparisons to Stepped Sigmoid Units (Nair &Hinton) or other models like the spike-and-slab RBMs (and others) are missing, without those comparisons, it is hard to tell whether leaky ReLU RBMs are better even in continuous visible domain.\n\n", "IS_META_REVIEW": false, "RECOMMENDATION": 5, "DATE": "19 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "No Title", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "The authors propose a novel energy-function for RBMs, using the leaky relu max(cx, x) activation function for the hidden-units. Analogous to ReLU units in feed-forward networks, these leaky relu RBMs split the input space into a combinatorial number of regions, where each region defines p(v) as a truncated Gaussian. A further contribution of the paper is in proposing a novel sampling scheme for the leaky RBM: one can run a much shorter Markov chain by initializing it from a sample of the leaky RBM with c=1 (which yields a standard multi-variate normal over the visibles) and then slowly annealing c. In low-dimension a similar scheme is shown to outperform AIS for estimating the partition function. Experiments are performed on both CIFAR-10 and SVHN.\n\nThis is an interesting paper which I believe would be of interest to the ICLR community. The theoretical contributions are strong: the authors not only introduce a proper energy formulation of ReLU RBMs, but also a novel sampling mechanism and an improvement on AIS for estimating their partition function. \n\nUnfortunately, the experimental results are somewhat limited. The PCD baseline is notably absent. Including (bernoulli visible, leaky-relu hidden) would have allowed the authors to evaluate likelihoods on standard binary RBM datasets. As it stands, performance on CIFAR-10 and SVHN, while improved with leaky-relu, is a far cry from more recent generative models (VAE-based, or auto-regressive models). While this comparison may be unfair, it will certainly limit the wider appeal of the paper to the community. Furthermore, there is the issue of the costly projection method which is required to guarantee that the energy-function remain bounded (covariance matrix over each region be PSD). Again, while it may be fair to leave that for future work given the other contributions, this will further limit the appeal of the paper.\n\nPROS:\nIntroduces an energy function having the leaky-relu as an activation function\nIntroduces a novel sampling procedure based on annealing the leakiness parameter\nSimilar sampling scheme shown to outperform AIS\n\nCONS:\nResults are somewhat out of date\nMissing experiments on binary datasets (more comparable to prior RBM work)\nMissing PCD baseline\nCost of projection method\n", "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "18 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "some claims are incorrect, experiments should be improved", "OTHER_KEYS": "(anonymous)", "comments": "This paper proposed a new variant of RBM, which has a nonlinearity of leaky ReLU, in contrast to the sigmoid function nonlinearity in RBM. By gradually annealing the leakiness coefficient (corresponding to from Gaussian to non-Gaussian model), the authors can sample from their model with a higher mixing rate. With the same idea annealing leakiness, they show they can estimate the partition function of the new model more accurately. \n\n\nMain comments:\n\nThe proposed model can only account for real-valued data. However, RBM is primarily used to model binary data, real-valued RBM (Gaussian-RBM) is not a well-recognized model for real-valued data. So, to demonstrate the superiority of the model, the author should also include the comparison with binary data. And it is also not enough to only compare two datasets for a newly proposed model.\n\nThe claim that the marginal distribution of visible variables is truncated Gaussian is incorrect. For a truncated normal, the values of variables are constrained to be within some region, e.g. requiring variable v from the region a1", "IS_META_REVIEW": false, "RECOMMENDATION": 5, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"DATE": "04 Dec 2016", "TITLE": "Clarity and better comparisons", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1"}, {"DATE": "02 Dec 2016", "TITLE": "Derivations and baselines", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2"}, {"DATE": "02 Dec 2016", "TITLE": "Why not plot generated samples by the Leaky-ReLU RBM? ", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3"}]}
{"text": "HIERARCHICAL MEMORY NETWORKS\n1 INTRODUCTION\nUntil recently, traditional machine learning approaches for challenging tasks such as image captioning, object detection, or machine translation have consisted in complex pipelines of algorithms, each being separately tuned for better performance. With the recent success of neural networks and deep learning research, it has now become possible to train a single model end-to-end, using backpropagation. Such end-to-end systems often outperform traditional approaches, since the entire model is directly optimized with respect to the final task at hand. However, simple encode-decode style neural networks often underperform on knowledge-based reasoning tasks like question-answering or dialog systems. Indeed, in such cases it is nearly impossible for regular neural networks to store all the necessary knowledge in their parameters.\nNeural networks with memory (Graves et al., 2014; Weston et al., 2015b) can deal with knowledge bases by having an external memory component which can be used to explicitly store knowledge. The memory is accessed by reader and writer functions, which are both made differentiable so that the entire architecture (neural network, reader, writer and memory components) can be trained end-to-end using backpropagation. Memory-based architectures can also be considered as generalizations of RNNs and LSTMs, where the memory is analogous to recurrent hidden states. However they are much richer in structure and can handle very long-term dependencies because once a vector (i.e., a memory) is stored, it is copied from time step to time step and can thus stay there for a very long time (and gradients correspondingly flow back time unhampered).\nThere exists several variants of neural networks with a memory component: Memory Networks (Weston et al., 2015b), Neural Turing Machines (NTM) (Graves et al., 2014), Dynamic Memory Net-\n\u2217Corresponding author: apsarathchandar@gmail.com\nworks (DMN) (Kumar et al., 2015). They all share five major components: memory, input module, reader, writer, and output module.\nMemory: The memory is an array of cells, each capable of storing a vector. The memory is often initialized with external data (e.g. a database of facts), by filling in its cells with a pre-trained vector representations of that data.\nInput module: The input module is to compute a representation of the input that can be used by other modules.\nWriter: The writer takes the input representation and updates the memory based on it. The writer can be as simple as filling the slots in the memory with input vectors in a sequential way (as often done in memory networks). If the memory is bounded, instead of sequential writing, the writer has to decide where to write and when to rewrite cells (as often done in NTMs).\nReader: Given an input and the current state of the memory, the reader retrieves content from the memory, which will then be used by an output module. This often requires comparing the input\u2019s representation or a function of the recurrent state with memory cells using some scoring function such as a dot product.\nOutput module: Given the content retrieved by the reader, the output module generates a prediction, which often takes the form of a conditional distribution over multiple labels for the output.\nFor the rest of the paper, we will use the name memory network to describe any model which has any form of these five components. We would like to highlight that all the components except the memory are learnable. Depending on the application, any of these components can also be fixed. In this paper, we will focus on the situation where a network does not write and only reads from the memory.\nIn this paper, we focus on the application of memory networks to large-scale tasks. Specifically, we focus on large scale factoid question answering. For this problem, given a large set of facts and a natural language question, the goal of the system is to answer the question by retrieving the supporting fact for that question, from which the answer can be derived. Application of memory networks to this task has been studied by Bordes et al. (2015). However, Bordes et al. (2015) depended on keyword based heuristics to filter the facts to a smaller set which is manageable for training. However heuristics are invariably dataset dependent and we are interested in a more general solution which can be used when the facts are of any structure. One can design soft attention retrieval mechanisms, where a convex combination of all the cells is retrieved or design hard attention retrieval mechanisms where one or few cells from the memory are retrieved. Soft attention is achieved by using softmax over the memory which makes the reader differentiable and hence learning can be done using gradient descent. Hard attention is achieved by using methods like REINFORCE (Williams, 1992), which provides a noisy gradient estimate when discrete stochastic decisions are made by a model.\nBoth soft attention and hard attention have limitations. As the size of the memory grows, soft attention using softmax weighting is not scalable. It is computationally very expensive, since its complexity is linear in the size of the memory. Also, at initialization, gradients are dispersed so much that it can reduce the effectiveness of gradient descent. These problems can be alleviated by a hard attention mechanism, for which the training method of choice is REINFORCE. However, REINFORCE can be brittle due to its high variance and existing variance reduction techniques are complex. Thus, it is rarely used in memory networks (even in cases of a small memory).\nIn this paper, we propose a new memory selection mechanism based on Maximum Inner Product Search (MIPS) which is both scalable and easy to train. This can be considered as a hybrid of soft and hard attention mechanisms. The key idea is to structure the memory in a hierarchical way such that it is easy to perform MIPS, hence the name Hierarchical Memory Network (HMN). HMNs are scalable at both training and inference time. The main contributions of the paper are as follows:\n\u2022 We explore hierarchical memory networks, where the memory is organized in a hierarchical fashion, which allows the reader to efficiently access only a subset of the memory.\n\u2022 While there are several ways to decide which subset to access, we propose to pose memory access as a maximum inner product search (MIPS) problem.\n\u2022 We empirically show that exact MIPS-based algorithms not only enjoy similar convergence as soft attention models, but can even improve the performance of the memory network. \u2022 Since exact MIPS is as computationally expensive as a full soft attention model, we propose\nto train the memory networks using approximate MIPS techniques for scalable memory access. \u2022 We empirically show that unlike exact MIPS, approximate MIPS algorithms provide a\nspeedup and scalability of training, though at the cost of some performance.\n2 HIERARCHICAL MEMORY NETWORKS\nIn this section, we describe the proposed Hierarchical Memory Network (HMN). In this paper, HMNs only differ from regular memory networks in two of its components: the memory and the reader.\nMemory: Instead of a flat array of cells for the memory structure, HMNs leverages a hierarchical memory structure. Memory cells are organized into groups and the groups can further be organized into higher level groups. The choice for the memory structure is tightly coupled with the choice of reader, which is essential for fast memory access. We consider three classes of approaches for the memory\u2019s structure: hashing-based approaches, tree-based approaches, and clustering-based approaches. This is explained in detail in the next section.\nReader: The reader in the HMN is different from the readers in flat memory networks. Flat memorybased readers use either soft attention over the entire memory or hard attention that retrieves a single cell. While these mechanisms might work with small memories, with HMNs we are more interested in achieving scalability towards very large memories. So instead, HMN readers use soft attention only over a selected subset of the memory. Selecting memory subsets is guided by a maximum inner product search algorithm, which can exploit the hierarchical structure of the organized memory to retrieve the most relevant facts in sub-linear time. The MIPS-based reader is explained in more detail in the next section.\nIn HMNs, the reader is thus trained to create MIPS queries such that it can retrieve a sufficient set of facts. While most of the standard applications of MIPS (Ram & Gray, 2012; Bachrach et al., 2014; Shrivastava & Li, 2014) so far have focused on settings where both query vector and database (memory) vectors are precomputed and fixed, memory readers in HMNs are learning to do MIPS by updating the input representation such that the result of MIPS retrieval contains the correct fact(s).\n3 MEMORY READER WITH K-MIPS ATTENTION\nIn this section, we describe how the HMN memory reader uses Maximum Inner Product Search (MIPS) during learning and inference.\nWe begin with a formal definition of K-MIPS. Given a set of points X = {x1, . . . , xn} and a query vector q, our goal is to find\nargmax (K) i\u2208X q >xi (1)\nwhere the argmax(K) returns the indices of the top-K maximum values. In the case of HMNs, X corresponds to the memory and q corresponds to the vector computed by the input module.\nA simple but inefficient solution for K-MIPS involves a linear search over the cells in memory by performing the dot product of q with all the memory cells. While this will return the exact result for K-MIPS, it is too costly to perform when we deal with a large-scale memory. However, in many practical applications, it is often sufficient to have an approximate result for K-MIPS, trading speed-up at the cost of the accuracy. There exist several approximate K-MIPS solutions in the literature (Shrivastava & Li, 2014; 2015; Bachrach et al., 2014; Neyshabur & Srebro, 2015).\nAll the approximate K-MIPS solutions add a form of hierarchical structure to the memory and visit only a subset of the memory cells to find the maximum inner product for a given query. Hashingbased approaches (Shrivastava & Li, 2014; 2015; Neyshabur & Srebro, 2015) hash cells into multiple bins, and given a query they search for K-MIPS cell vectors only in bins that are close to the bin\nassociated with the query. Tree-based approaches (Ram & Gray, 2012; Bachrach et al., 2014) create search trees with cells in the leaves of the tree. Given a query, a path in the tree is followed and MIPS is performed only for the leaf for the chosen path. Clustering-based approaches (Auvolat et al., 2015) cluster cells into multiple clusters (or a hierarchy of clusters) and given a query, they perform MIPS on the centroids of the top few clusters. We refer the readers to (Auvolat et al., 2015) for an extensive comparison of various state-of-the-art approaches for approximate K-MIPS.\nOur proposal is to exploit this rich approximate K-MIPS literature to achieve scalable training and inference in HMNs. Instead of filtering the memory with heuristics, we propose to organize the memory based on approximate K-MIPS algorithms and then train the reader to learn to perform MIPS. Specifically, consider the following softmax over the memory which the reader has to perform for every reading step to retrieve a set of relevant candidates:\nRout = softmax(h(q)M T ) (2)\nwhere h(q) \u2208 Rd is the representation of the query, M \u2208 RN\u00d7d is the memory with N being the total number of cells in the memory. We propose to replace this softmax with softmax(K) which is defined as follows:\nC = argmax(K) h(q)MT (3)\nRout = softmax (K)(h(q)MT ) = softmax(h(q)M [C]T ) (4)\nwhere C is the indices of top-K MIP candidate cells and M [C] is a sub-matrix of M where the rows are indexed by C.\nOne advantage of using the softmax(K) is that it naturally focuses on cells that would normally receive the strongest gradients during learning. That is, in a full softmax, the gradients are otherwise more dispersed across cells, given the large number of cells and despite many contributing a small gradient. As our experiments will show, this results in slower training.\nOne problematic situation when learning with the softmax(K) is when we are at the initial stages of training and the K-MIPS reader is not including the correct fact candidate. To avoid this issue, we always include the correct candidate to the top-K candidates retrieved by the K-MIPS algorithm, effectively performing a fully supervised form of learning.\nDuring training, the reader is updated by backpropagation from the output module, through the subset of memory cells. Additionally, the log-likelihood of the correct fact computed using Ksoftmax is also maximized. This second supervision helps the reader learn to modify the query such that the maximum inner product of the query with respect to the memory will yield the correct supporting fact in the top K candidate set.\nUntil now, we described the exact K-MIPS-based learning framework, which still requires a linear look-up over all memory cells and would be prohibitive for large-scale memories. In such scenarios, we can replace the exact K-MIPS in the training procedure with the approximate K-MIPS. This is achieved by deploying a suitable memory hierarchical structure. The same approximate K-MIPSbased reader can be used during inference stage as well. Of course, approximate K-MIPS algorithms might not return the exact MIPS candidates and will likely to hurt performance, but at the benefit of achieving scalability.\nWhile the memory representation is fixed in this paper, updating the memory along with the query representation should improve the likelihood of choosing the correct fact. However, updating the memory will reduce the precision of the approximate K-MIPS algorithms, since all of them assume that the vectors in the memory are static. Designing efficient dynamic K-MIPS should improve the performance of HMNs even further, a challenge that we hope to address in future work.\n3.1 READER WITH CLUSTERING-BASED APPROXIMATE K-MIPS\nClustering-based approximate K-MIPS was proposed in (Auvolat et al., 2015) and it has been shown to outperform various other state-of-the-art data dependent and data independent approximate KMIPS approaches for inference tasks. As we will show in the experiments section, clustering-based MIPS also performs better when used to training HMNs. Hence, we focus our presentation on the clustering-based approach and propose changes that were found to be helpful for learning HMNs.\nFollowing most of the other approximate K-MIPS algorithms, Auvolat et al. (2015) convert MIPS to Maximum Cosine Similarity Search (MCSS) problem:\nargmax (K) i\u2208X qTxi ||q|| ||xi|| = argmax (K) i\u2208X qTxi ||xi||\n(5)\nWhen all the data vectors xi have the same norm, then MCSS is equivalent to MIPS. However, it is often restrictive to have this additional constraint. Instead, Auvolat et al. (2015) append additional dimensions to both query and data vectors to convert MIPS to MCSS. In HMN terminology, this would correspond to adding a few more dimensions to the memory cells and input representations.\nThe algorithm introduces two hyper-parameters, U < 1 and m \u2208 N\u2217. The first step is to scale all the vectors in the memory by the same factor, such that maxi ||xi||2 = U . We then apply two mappings, P and Q, on the memory cells and on the input vector, respectively. These two mappings simply concatenate m new components to the vectors and make the norms of the data points all roughly the same (Shrivastava & Li, 2015). The mappings are defined as follows:\nP (x) = [x, 1/2\u2212 ||x||22, 1/2\u2212 ||x||42, . . . , 1/2\u2212 ||x||2 m\n2 ] (6) Q(x) = [x, 0, 0, . . . , 0] (7)\nWe thus have the following approximation of MIPS by MCSS for any query vector q:\nargmax (K) i q >xi ' argmax(K)i Q(q)>P (xi)\n||Q(q)||2 \u00b7 ||P (xi)||2 (8)\nOnce we convert MIPS to MCSS, we can use spherical K-means (Zhong, 2005) or its hierarchical version to approximate and speedup the cosine similarity search. Once the memory is clustered, then every read operation requires only K dot-products, where K is the number of cluster centroids.\nSince this is an approximation, it is error-prone. As we are using this approximation for the learning process, this introduces some bias in gradients, which can affect the overall performance of HMN. To alleviate this bias, we propose three simple strategies.\n\u2022 Instead of using only the top-K candidates for a single read query, we also add top-K candidates retrieved for every other read query in the mini-batch. This serves two purposes. First, we can do efficient matrix multiplications by leveraging GPUs since all the K-softmax in a minibatch are over the same set of elements. Second, this also helps to decrease the bias introduced by the approximation error.\n\u2022 For every read access, instead of only using the top few clusters which has a maximum product with the read query, we also sample some clusters from the rest, based on a probability distribution log-proportional to the dot product with the cluster centroids. This also decreases the bias.\n\u2022 We can also sample random blocks of memory and add it to top-K candidates.\nWe empirically investigate the effect of these variations in Section 5.5.\n4 RELATED WORK\nMemory networks have been introduced in (Weston et al., 2015b) and have been so far applied to comprehension-based question answering (Weston et al., 2015a; Sukhbaatar et al., 2015), large scale question answering (Bordes et al., 2015) and dialogue systems (Dodge et al., 2015). While (Weston et al., 2015b) considered supervised memory networks in which the correct supporting fact is given during the training stage, (Sukhbaatar et al., 2015) introduced semi-supervised memory networks that can learn the supporting fact by itself. (Kumar et al., 2015; Xiong et al., 2016) introduced Dynamic Memory Networks (DMNs) which can be considered as a memory network with two types of memory: a regular large memory and an episodic memory. Another related class of model is the Neural Turing Machine (Graves et al., 2014), which uses softmax-based soft attention. Later (Zaremba & Sutskever, 2015) extended NTM to hard attention using reinforcement learning. (Dodge et al., 2015; Bordes et al., 2015) alleviate the problem of the scalability of soft attention by having\nan initial keyword based filtering stage, which reduces the number of facts being considered. Our work generalizes this filtering by using MIPS for filtering. This is desirable because MIPS can be applied for any modality of data or even when there is no overlap between the words in a question and the words in facts.\nThe softmax arises in various situations and most relevant to this work are scaling methods for large vocabulary neural language modeling. In neural language modeling, the final layer is a softmax distribution over the next word and there exist several approaches to achieve scalability. (Morin & Bengio, 2005) proposes a hierarchical softmax based on prior clustering of the words into a binary, or more generally n-ary tree, that serves as a fixed structure for the learning process of the model. The complexity of training is reduced from O(n) to O(log n). Due to its clustering and tree structure, it resembles the clustering-based MIPS techniques we explore in this paper. However, the approaches differ at a fundamental level. Hierarchical softmax defines the probability of a leaf node as the product of all the probabilities computed by all the intermediate softmaxes on the way to that leaf node. By contrast, an approximate MIPS search imposes no such constraining structure on the probabilistic model, and is better thought as efficiently searching for top winners of what amounts to be a large ordinary flat softmax. Other methods such as Noice Constrastive Estimation (Mnih & Gregor, 2014) and Negative Sampling (Mikolov et al., 2013) avoid an expensive normalization constant by sampling negative samples from some marginal distribution. By contrast, our approach approximates the softmax by explicitly including in its negative samples candidates that likely would have a large softmax value. Jean et al. (2015) introduces an importance sampling approach that considers all the words in a mini-batch as the candidate set. This in general might also not include the MIPS candidates with highest softmax values.\n(Spring & Shrivastava, 2016) is the only work that we know of, proposing to use MIPS during learning. It proposes hashing-based MIPS to sort the hidden layer activations and reduce the computation in every layer. However, a small scale application was considered and data-independent methods like hashing will likely suffer as dimensionality increases. Rae et al. (2016) have also independently proposed a model called SAM to use approximate search methods for memory access in NTM-like architectures. However, our motivation is different. While Rae et al. (2016) focus on architectures where the memory is written by the controller itself, we focus on handling memory access to large external knowledge bases. While both the models fix the memory access mechanism (HMN uses MIPS and SAM uses NNS), our controller works in a much more constrained setting. Moreover, our experiments suggest that the performance of SAM could be improved using a clustering-based approach as in our work, instead of tree/hash-based approaches for memory search used by SAM.\n5 EXPERIMENTS\nIn this section, we report experiments on factoid question answering using hierarchical memory networks. Specifically, we use the SimpleQuestions dataset Bordes et al. (2015). The aim of these experiments is not to achieve state-of-the-art results on this dataset. Rather, we aim to propose and analyze various approaches to make memory networks more scalable and explore the achieved tradeoffs between speed and accuracy.\n5.1 DATASET\nWe use SimpleQuestions (Bordes et al., 2015) which is a large scale factoid question answering dataset. SimpleQuestions consists of 108,442 natural language questions, each paired with a corresponding fact from Freebase. Each fact is a triple (subject,relation,object) and the answer to the question is always the object. The dataset is divided into training (75910), validation (10845), and test (21687) sets. Unlike Bordes et al. (2015) who additionally considered FB2M (10M facts) or FB5M (12M facts) with keyword-based heuristics for filtering most of the facts for each question, we only use SimpleQuestions, with no keyword-based heuristics. This allows us to do a direct comparison with the full softmax approach in a reasonable amount of time. Moreover, we would like to highlight that for this dataset, keyword-based filtering is a very efficient heuristic since all questions have an appropriate source entity with a matching word. Nevertheless, our goal is to design a general purpose architecture without such strong assumptions on the nature of the data.\n5.2 MODEL\nLet Vq be the vocabulary of all words in the natural language questions. Let Wq be a |Vq| \u2217 m matrix where each row is some m dimensional embedding for a word in the question vocabulary. This matrix is initialized with random values and learned during training. Given any question, we represent it with a bag-of-words representation by summing the vector representation of each word in the question. Let q = {wi}pi=1,\nh(q) = p\u2211 i=1 Wq[wi]\nThen, to find the relevant fact from the memory M, we call the K-MIPS-based reader module with h(q) as the query. This uses Equation 3 and 4 to compute the output of the reader Rout. The reader is trained by minimizing the Negative Log Likelihood (NLL) of the correct fact.\nJ\u03b8 = N\u2211 i=1 \u2212log(Rout[fi])\nwhere fi is the index of the correct fact in Wm. We are fixing the memory embeddings to the TransE (Bordes et al., 2013) embeddings and learning only the question embeddings.\nThis model is simpler than the one reported in (Bordes et al., 2015) so that it is esay to analyze the effect of various memory reading strategies.\n5.3 TRAINING DETAILS\nWe trained the model with the Adam optimizer (Kingma & Ba, 2014), with a fixed learning rate of 0.001. We used mini-batches of size 128. We used 200 dimensional embeddings for the TransE entities, yielding 600 dimensional embeddings for facts by concatenating the embeddings of the subject, relation and object. We also experimented with summing the entities in the triple instead of concatenating, but we found that it was difficult for the model to differentiate facts this way. The only learnable parameters by the HMN model are the question word embeddings. The entity distribution in SimpleQuestions is extremely sparse and hence, following Bordes et al. (2015), we also add artificial questions for all the facts for which we do not have natural language questions. Unlike Bordes et al. (2015), we do not add any other additional tasks like paraphrase detection to the model, mainly to study the effect of the reader. We stopped training for all the models when the validation accuracy consistently decreased for 3 epochs.\n5.4 EXACT K-MIPS IMPROVES ACCURACY\nIn this section, we compare the performance of the full soft attention reader and exact K-MIPS attention readers. Our goal is to verify that K-MIPS attention is in fact a valid and useful attention mechanism and see how it fares when compared to full soft attention. For K-MIPS attention, we tried K \u2208 10, 50, 100, 1000. We would like to emphasize that, at training time, along with K candidates for a particular question, we also add the K-candidates for each question in the minibatch. So the exact size of the softmax layer would be higer than K during training. In Table 1, we report the test performance of memory networks using the soft attention reader and K-MIPS attention reader. We also report the average softmax size during training. From the table, it is clear that the K-MIPS attention readers improve the performance of the network compared to soft attention reader. In fact, smaller the value of K is, better the performance. This result suggests that it is better to use a K-MIPS layer instead of softmax layer whenever possible. It is interesting to see that the convergence of the model is not slowed down due to this change in softmax computation (as shown in Figure 1).\nThis experiment confirms the usefulness of K-MIPS attention. However, exact K-MIPS has the same complexity as a full softmax. Hence, to scale up the training, we need more efficient forms of K-MIPS attention, which is the focus of next experiment.\nModel Test Acc. Avg. Softmax Size Full-softmax 59.5 108442\n10-MIPS 62.2 1290 50-MIPS 61.2 6180\n100-MIPS 60.6 11928 1000-MIPS 59.6 70941 Clustering 51.5 20006 PCA-Tree 32.4 21108 WTA-Hash 40.2 20008\nTable 1: Accuracy in SQ test-set and average size of memory used. 10-softmax has high performance while using only smaller amount of memory.\n5.5 APPROXIMATE K-MIPS BASED LEARNING\nAs mentioned previously, designing faster algorithms for K-MIPS is an active area of research. Auvolat et al. (2015) compared several state-of-the-art data-dependent and data-independent methods for faster approximate K-MIPS and it was found that clustering-based MIPS performs significantly better than other approaches. However the focus of the comparison was on performance during the inference stage. In HMNs, K-MIPS must be used at both training stage and inference stages. To verify if the same trend can been seen during learning stage as well, we compared three different approaches:\nClustering: This was explained in detail in section 3.\nWTA-Hash: Winner Takes All hashing (Vijayanarasimhan et al., 2014) is a hashing-based K-MIPS algorithm which also converts MIPS to MCSS by augmenting additional dimensions to the vectors. This method used n hash functions and each hash function does p different random permutations of the vector. Then the prefix constituted by the first k elements of each permuted vector is used to construct the hash for the vector.\nPCA-Tree: PCA-Tree (Bachrach et al., 2014) is the state-of-the-art tree-based method, which converts MIPS to NNS by vector augmentation. It uses the principal components of the data to construct a balanced binary tree with data residing in the leaves.\nFor a fair comparison, we varied the hyper-parameters of each algorithm in such a way that the average speedup is approximately the same. Table 1 shows the performance of all three methods, compared to a full softmax. From the table, it is clear that the clustering-based method performs significantly better than the other two methods. However, performances are lower when compared to the performance of the full softmax.\nAs a next experiment, we analyze various the strategies proposed in Section 3.1 to reduce the approximation bias of clustering-based K-MIPS:\nTop-K: This strategy picks the vectors in the top K clusters as candidates.\nSample-K: This strategy samples K clusters, without replacement, based on a probability distribution based on the dot product of the query with the cluster centroids. When combined with the Top-K strategy, we ignore clusters selected by the Top-k strategy for sampling.\nRand-block: This strategy divides the memory into several blocks and uniformly samples a random block as candidate.\nWe experimented with 1000 clusters and 2000 clusters. While comparing various training strategies, we made sure that the effective speedup is approximately the same. Memory access to facts per query for all the models is approximately 20,000, hence yielding a 5X speedup.\nResults are given in Table 2. We observe that the best approach is to combine the Top-K and SampleK strategies, with Rand-block not being beneficial. Interestingly, the worst performances correspond to cases where the Sample-K strategy is ignored.\n6 CONCLUSION\nIn this paper, we proposed a hierarchical memory network that exploits K-MIPS for its attentionbased reader. Unlike soft attention readers, K-MIPS attention reader is easily scalable to larger memories. This is achieved by organizing the memory in a hierarchical way. Experiments on the SimpleQuestions dataset demonstrate that exact K-MIPS attention is better than soft attention. However, existing state-of-the-art approximate K-MIPS techniques provide a speedup at the cost of some accuracy. Future research will investigate designing efficient dynamic K-MIPS algorithms, where the memory can be dynamically updated during training. This should reduce the approximation bias and hence improve the overall performance.\n", "cats": {"READ": 1.0, "DONTREAD": 0.0}, "reviews": [{"IS_META_REVIEW": true, "comments": "I find this paper not very compelling.  The basic idea seems to be that we can put a fast neighbor searcher into a memory augmented net to make the memory lookups scalable.  However, this was precisely the point of Rae et al.    There are a  number of standardized neighbor searchers; I don't understand why the authors choose to use their own (which they do not benchmark against the standards).   Moreover, they test on a problem where there is no clear need for (vector based) fast-nn, because one can use hashing on the text.     I also find the repeated distinction between \"mips\" and \"nns\" distracting; most libraries that can do one can do the other, or inputs can be modified  to switch between the problems; indeed the authors do this when they convert to the  \"mcss\" problem."}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "This paper was reviewed by three experts. While they find interesting ideas in the manuscript, all three point to deficiencies (unconvincing results, etc) and unanimously recommend rejection.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"TITLE": "No Title", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "1. The hierarchical memory is fixed, not learned, and there is no hierarchical in the experimental section, only one layer for softmax layer.\n2. It shows the 10-mips > 100-mips > 1000-mips, does it mean 1-mips is the best one we should adopt?\n3. Approximated k-mips is worse than even original method. Why does it need exact k-mips? It seems the proposed method is not robust.", "IS_META_REVIEW": false, "RECOMMENDATION": 5, "DATE": "20 Dec 2016", "REVIEWER_CONFIDENCE": 3}, {"TITLE": "Not a very convincing proposal for dealing with very large memories in memory networks", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "The paper proposes an algorithm for training memory networks which have very large memories. Training such models in traditional ways, by using soft-attention mechanism over all the memory slots is not only slow, it is also harder to train due to dispersion of gradients. The paper proposes to use the k-mips algorithm over the memories to choose a subset of the memory slots over which the attention is applied. Since the cost of exact k-mips is the same as doing full attention, the authors propose to use approximate k-mips, which while faster to compute, results in inferior performance. An artifact of using k-mips is that one cannot learn the memory slots. Hence they are pre-trained and kept fixed during entire training. The experimental section shows the efficacy of using k-mips using the SimpleQuestions dataset. The exact k-mips results in the same performance as the full attention. The approximate k-mips results in deterioration in performance. The paper is quite clearly written and easy to understand. \n\nI think the ideas proposed in the paper are not super convincing. I have a number of issues with this paper. \n\n1. The k-mips algorithm forces the memories to be fixed. This to me is a rather limiting constraint, especially on problems/dataset which will require multiple hops of training to do compounded reasoning. As a results I'm not entirely sure about the usefulness of this technique. \n2. Furthermore, the exact k-mips is the sample complexity as the full attention. The only way to achieve speedup is to use approx k-mips. That, as expected, results in a significant drop in performance. \n3. The paper motivates the ideas by proposing solutions to eliminate heuristics used to prune the memories. However in Section 3.1 the authors themselves end up using multiple heuristics to make the training work. Agreed, that the used heuristics are not data dependent, but still, it feels like they are kicking the can down the road as far as heuristics are concerned. \n4. The experimental results are not very convincing. First there is no speed comparison. Second, the authors do not compare with methods other than k-mips which do fast nearest neighbor search, such as, FLANN. ", "IS_META_REVIEW": false, "RECOMMENDATION": 5, "DATE": "19 Dec 2016", "REVIEWER_CONFIDENCE": 5}, {"TITLE": "No Title", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "I find this paper not very compelling.  The basic idea seems to be that we can put a fast neighbor searcher into a memory augmented net to make the memory lookups scalable.  However, this was precisely the point of Rae et al.    There are a  number of standardized neighbor searchers; I don't understand why the authors choose to use their own (which they do not benchmark against the standards).   Moreover, they test on a problem where there is no clear need for (vector based) fast-nn, because one can use hashing on the text.     I also find the repeated distinction between \"mips\" and \"nns\" distracting; most libraries that can do one can do the other, or inputs can be modified  to switch between the problems; indeed the authors do this when they convert to the  \"mcss\" problem.\n", "IS_META_REVIEW": false, "RECOMMENDATION": 4, "DATE": "17 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"DATE": "13 Dec 2016", "TITLE": "FLANN?  other standard ANN libraries?", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1"}, {"DATE": "02 Dec 2016", "TITLE": "Few questions", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2"}, {"DATE": "02 Dec 2016", "TITLE": "Model and timing issues", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3"}, {"IS_META_REVIEW": true, "comments": "I find this paper not very compelling.  The basic idea seems to be that we can put a fast neighbor searcher into a memory augmented net to make the memory lookups scalable.  However, this was precisely the point of Rae et al.    There are a  number of standardized neighbor searchers; I don't understand why the authors choose to use their own (which they do not benchmark against the standards).   Moreover, they test on a problem where there is no clear need for (vector based) fast-nn, because one can use hashing on the text.     I also find the repeated distinction between \"mips\" and \"nns\" distracting; most libraries that can do one can do the other, or inputs can be modified  to switch between the problems; indeed the authors do this when they convert to the  \"mcss\" problem."}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "This paper was reviewed by three experts. While they find interesting ideas in the manuscript, all three point to deficiencies (unconvincing results, etc) and unanimously recommend rejection.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"TITLE": "No Title", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "1. The hierarchical memory is fixed, not learned, and there is no hierarchical in the experimental section, only one layer for softmax layer.\n2. It shows the 10-mips > 100-mips > 1000-mips, does it mean 1-mips is the best one we should adopt?\n3. Approximated k-mips is worse than even original method. Why does it need exact k-mips? It seems the proposed method is not robust.", "IS_META_REVIEW": false, "RECOMMENDATION": 5, "DATE": "20 Dec 2016", "REVIEWER_CONFIDENCE": 3}, {"TITLE": "Not a very convincing proposal for dealing with very large memories in memory networks", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "The paper proposes an algorithm for training memory networks which have very large memories. Training such models in traditional ways, by using soft-attention mechanism over all the memory slots is not only slow, it is also harder to train due to dispersion of gradients. The paper proposes to use the k-mips algorithm over the memories to choose a subset of the memory slots over which the attention is applied. Since the cost of exact k-mips is the same as doing full attention, the authors propose to use approximate k-mips, which while faster to compute, results in inferior performance. An artifact of using k-mips is that one cannot learn the memory slots. Hence they are pre-trained and kept fixed during entire training. The experimental section shows the efficacy of using k-mips using the SimpleQuestions dataset. The exact k-mips results in the same performance as the full attention. The approximate k-mips results in deterioration in performance. The paper is quite clearly written and easy to understand. \n\nI think the ideas proposed in the paper are not super convincing. I have a number of issues with this paper. \n\n1. The k-mips algorithm forces the memories to be fixed. This to me is a rather limiting constraint, especially on problems/dataset which will require multiple hops of training to do compounded reasoning. As a results I'm not entirely sure about the usefulness of this technique. \n2. Furthermore, the exact k-mips is the sample complexity as the full attention. The only way to achieve speedup is to use approx k-mips. That, as expected, results in a significant drop in performance. \n3. The paper motivates the ideas by proposing solutions to eliminate heuristics used to prune the memories. However in Section 3.1 the authors themselves end up using multiple heuristics to make the training work. Agreed, that the used heuristics are not data dependent, but still, it feels like they are kicking the can down the road as far as heuristics are concerned. \n4. The experimental results are not very convincing. First there is no speed comparison. Second, the authors do not compare with methods other than k-mips which do fast nearest neighbor search, such as, FLANN. ", "IS_META_REVIEW": false, "RECOMMENDATION": 5, "DATE": "19 Dec 2016", "REVIEWER_CONFIDENCE": 5}, {"TITLE": "No Title", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "I find this paper not very compelling.  The basic idea seems to be that we can put a fast neighbor searcher into a memory augmented net to make the memory lookups scalable.  However, this was precisely the point of Rae et al.    There are a  number of standardized neighbor searchers; I don't understand why the authors choose to use their own (which they do not benchmark against the standards).   Moreover, they test on a problem where there is no clear need for (vector based) fast-nn, because one can use hashing on the text.     I also find the repeated distinction between \"mips\" and \"nns\" distracting; most libraries that can do one can do the other, or inputs can be modified  to switch between the problems; indeed the authors do this when they convert to the  \"mcss\" problem.\n", "IS_META_REVIEW": false, "RECOMMENDATION": 4, "DATE": "17 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"DATE": "13 Dec 2016", "TITLE": "FLANN?  other standard ANN libraries?", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1"}, {"DATE": "02 Dec 2016", "TITLE": "Few questions", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2"}, {"DATE": "02 Dec 2016", "TITLE": "Model and timing issues", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3"}]}
{"text": "DEEP INFORMATION PROPAGATION\n1 INTRODUCTION\nDeep neural network architectures have become ubiquitous in machine learning. The success of deep networks is due to the fact that they are highly expressive (Montufar et al., 2014) while simultaneously being relatively easy to optimize (Choromanska et al., 2015; Goodfellow et al., 2014) with strong generalization properties (Recht et al., 2015). Consequently, developments in machine learning often accompany improvements in our ability to train increasingly deep networks. Despite this, designing novel network architectures is frequently equal parts art and science. This is, in part, because a general theory for neural networks that might inform design decisions has lagged behind the feverish pace of design.\nA pair of recent papers (Poole et al., 2016; Raghu et al., 2016) demonstrated that random neural networks are exponentially expressive in their depth. Central to their approach was the consideration of networks after random initialization, whose weights and biases were i.i.d. Gaussian distributed. In particular the paper by Poole et al. (2016) developed a \u201cmean field\u201d formalism for treating wide, untrained, neural networks. They showed that these mean field networks exhibit an order-to-chaos transition as a function of the weight and bias variances. Notably the mean field formalism is not closely tied to a specific choice of activation function or loss.\nIn this paper, we demonstrate the existence of several characteristic \u201cdepth\u201d scales that emerge naturally and control signal propagation in these random networks. We then show that one of these depth scales, \u03bec, diverges at the boundary between order and chaos. This result is insensitive to many architectural decisions (such as choice of activation function) and will generically be true at any order-to-chaos transition. We then extend these results to include dropout and we show that even small amounts of dropout destroys the order-to-chaos critical point and consequently removes the divergence in \u03bec. Together these results bound the depth to which signal may propagate through random neural networks.\nWe then develop a corresponding mean field model for gradients and we show that a duality exists between the forward propagation of signals and the backpropagation of gradients. The ordered and chaotic phases that Poole et al. (2016) identified correspond to regions of vanishing and exploding gradients, respectively. We demonstrate the validity of this mean field theory by computing gradients of random networks on MNIST. This provides a formal explanation of the \u2018vanishing gradients\u2019\n\u2217Work done as a member of the Google Brain Residency program (g.co/brainresidency)\nphenomenon that has long been observed in neural networks (Bengio et al., 1993). We continue to show that the covariance between two gradients is controlled by the same depth scale that limits correlated signal propagation in the forward direction.\nFinally, we hypothesize that a necessary condition for a random neural network to be trainable is that information should be able to pass through it. Thus, the depth-scales identified here bound the set of hyperparameters that will lead to successful training. To test this ansatz we train ensembles of deep, fully connected, feed-forward neural networks of varying depth on MNIST and CIFAR10, with and without dropout. Our results confirm that neural networks are trainable precisely when their depth is not much larger than \u03bec. This result is dataset independent and is, therefore, a universal function of network architecture.\nA corollary of these result is that asymptotically deep neural networks should be trainable provided they are initialized sufficiently close to the order-to-chaos transition. The notion of \u201cedge of chaos\u201d initialization has been explored previously. Such investigations have been both direct as in Bertschinger et al. (2005); Glorot & Bengio (2010) or indirect, through initialization schemes that favor deep signal propagation such as batch normalization (Ioffe & Szegedy, 2015), orthogonal matrix initialization (Saxe et al., 2014), random walk initialization (Sussillo & Abbott, 2014), composition kernels (Daniely et al., 2016), or residual network architectures (He et al., 2015). The novelty of the work presented here is two-fold. First, our framework predicts the depth at which networks may be trained even far from the order-to-chaos transition. While a skeptic might ask when it would be profitable to initialize a network far from criticality, we respond by noting that there are architectures (such as neural networks with dropout) where no critical point exists and so this more general framework is needed. Second, our work provides a formal, as opposed to intuitive, explanation for why very deep networks can only be trained near the edge of chaos.\n2 BACKGROUND\nWe begin by recapitulating the mean-field formalism developed in Poole et al. (2016). Consider a fully-connected, untrained, feed-forward, neural network of depth L with layer width Nl and some nonlinearity \u03c6 : R \u2192 R. Since this is an untrained neural network we suppose that its weights and biases are respectively i.i.d. as W lij \u223c N(0, \u03c32w/Nl) and bli \u223c N(0, \u03c32b ). Notationally we set zli to be the pre-activations of the lth layer and yl+1i to be the activations of that layer. Finally, we take the input to the network to be y0i = xi. The propagation of a signal through the network is described by the pair of equations,\nzli = \u2211 j W lijy l j + b l i y l+1 i = \u03c6(z l i). (1)\nSince the weights and biases are randomly distributed, these equations define a probability distribution on the activations and pre-activations over an ensemble of untrained neural networks. The \u201cmean-field\u201d approximation is then to replace zli by a Gaussian whose first two moments match those of zli. For the remainder of the paper we will take the mean field approximation as given.\nConsider first the evolution of a single input, xi;a, as it evolves through the network (as quantified by yli;a and z l i;a). Since the weights and biases are independent with zero mean, the first two moments of the pre-activations in the same layer will be,\nE[zli;a] = 0 E[zli;azlj;a] = qlaa\u03b4ij (2)\nwhere \u03b4ij is the Kronecker delta. Here qlaa is the variance of the pre-activations in the lth layer due to an input xi;a and it is described by the recursion relation,\nqlaa = \u03c3 2 w\n\u222b Dz\u03c62 (\u221a ql\u22121aa z ) + \u03c32b (3)\nwhere \u222b Dz = 1\u221a\n2\u03c0\n\u222b dze\u2212 1 2 z 2\nis the measure for a standard Gaussian distribution. Together these equations completely describe the evolution of a single input through a mean field neural network. For any choice of \u03c32w and \u03c3 2 b with bounded \u03c6, eq. 3 has a fixed point at q \u2217 = liml\u2192\u221e qlaa.\nThe propagation of a pair of signals, x0i;a and x 0 i;b, through this network can be understood similarly. Here the mean pre-activations are trivially the same as in the single-input case. The independence\nof the weights and biases implies that the covariance between different pre-activations in the same layer will be given by, E[zli;azlj;b] = qlab\u03b4ij . The covariance, qlab, will be given by the recurrence relation,\nqlab = \u03c3 2 w \u222b Dz1Dz2\u03c6(u1)\u03c6(u2) + \u03c32b (4)\nwhere u1 = \u221a ql\u22121aa z1 and u2 = \u221a ql\u22121bb ( cl\u22121ab z1 + \u221a 1\u2212 (cl\u22121ab )2z2 ) , with clab = q l ab/ \u221a qlaaq l bb,\nare Gaussian approximations to the pre-activations in the preceding layer with the correct covariance matrix. Moreover clab is the correlation between the two inputs after l layers.\nExamining eq. 4 it is clear that c\u2217 = 1 is a fixed point of the recurrence relation. To determine whether or not the c\u2217 = 1 is an attractive fixed point the quantity,\n\u03c71 = \u2202clab \u2202cl\u22121ab = \u03c32w\n\u222b Dz [ \u03c6\u2032 (\u221a q\u2217z )]2\n(5)\nis introduced. Poole et al. (2016) note that the c\u2217 = 1 fixed point is stable if \u03c71 < 1 and is unstable otherwise. Thus, \u03c71 = 1 represents a critical line separating an ordered phase (in which c\u2217 = 1 and all inputs end up asymptotically correlated) and a chaotic phase (in which c\u2217 < 1 and all inputs end up asymptotically decorrelated). For the case of \u03c6 = tanh, the phase diagram in fig. 1 (a) is observed.\n3 ASYMPTOTIC EXPANSIONS AND DEPTH SCALES\nOur first contribution is to demonstrate the existence of two depth-scales that arise naturally within the framework of mean field neural networks. Motivating the existence of these depth-scales, we iterate eq. 3 and 4 until convergence for many values of \u03c32w between 0.1 and 3.0 and with \u03c3 2 b = 0.05 starting with q0aa = q 0 bb = 0.8 and c 0 ab = 0.6. We see, in fig. 1 (b) and (c), that the manner in which both qlaa approaches q \u2217 and clab approaches c\n\u2217 is exponential over many orders of magnitude. We therefore anticipate that asymptotically |qlaa \u2212 q\u2217| \u223c e\u2212l/\u03beq and |clab \u2212 c\u2217| \u223c e\u2212l/\u03bec for sufficiently large l. Here, \u03beq and \u03bec define depth-scales over which information may propagate about the magnitude of a single input and the correlation between two inputs respectively.\nWe will presently prove that qlaa and c l ab are asymptotically exponential. In both cases we will use the same fundamental strategy wherein we expand one of the recurrence relations (either eq. 3 or eq. 4) about its fixed point to get an approximate \u201casymptotic\u201d recurrence relation. We find that this asymptotic recurrence relation in turn implies exponential decay towards the fixed point over a depth-scale, \u03bex.\nWe first analyze eq. 3 and identify a depth-scale at which information about a single input may propagate. Let qlaa = q \u2217 + l. By construction so long as liml\u2192\u221e qlaa = q \u2217 exists it follows that\nl \u2192 0 as l\u2192\u221e. Eq. 3 may be expanded to lowest order in l to arrive at an asymptotic recurrence relation (see Appendix 7.1),\nl+1 = l [ \u03c71 + \u03c3 2 w \u222b Dz\u03c6\u2032\u2032 (\u221a q\u2217z ) \u03c6 (\u221a q\u2217z )] +O ( ( l)2 ) . (6)\nNotably, the term multiplying l is a constant. It follows that for large l the asymptotic recurrence relation has an exponential solution, l \u223c e\u2212l/\u03beq , with \u03beq given by\n\u03be\u22121q = \u2212 log [ \u03c71 + \u03c3 2 w \u222b Dz\u03c6\u2032\u2032 (\u221a q\u2217z ) \u03c6 (\u221a q\u2217z )] . (7)\nThis establishes \u03beq as a depth scale that controls how deep information from a single input may penetrate into a random neural network.\nNext, we consider eq. 4. Using a similar argument (detailed in Appendix 7.2) we can expand about clab = c \u2217 + l to find an asymptotic recurrence relation,\nl+1 = l [ \u03c32w \u222b Dz1Dz2\u03c6\u2032(u\u22171)\u03c6\u2032(u\u22172) ] +O(( l)2). (8)\nHere u\u22171 = \u221a q\u2217z1 and u\u22172 = \u221a q\u2217(c\u2217z1 + \u221a 1\u2212 (c\u2217)2z2). Thus, once again, we expect that for large l this recurrence will have an exponential solution, l \u223c e\u2212l/\u03bec , with \u03bec given by\n\u03be\u22121c = \u2212 log [ \u03c32w \u222b Dz1Dz2\u03c6\u2032(u\u22171)\u03c6\u2032(u\u22172) ] . (9)\nIn the ordered phase c\u2217 = 1 and so \u03be\u22121c = \u2212 log\u03c71. Since the transition between order and chaos occurs when \u03c71 = 1 it follows that \u03bec diverges at any order-to-chaos transition so long as q\u2217 and c\u2217 exist.\nThese results can be investigated intuitively by plotting cl+1ab vs c l ab in fig. 2 (a). In the ordered phase there is only a single fixed point, clab = 1. In the chaotic regime we see that a second fixed point develops and the clab = 1 point becomes unstable. We see that the linearization about the fixed points becomes significantly closer to the trivial map near the order-to-chaos transition.\nTo test these claims we measure \u03beq and \u03bec directly by iterating the recurrence relations for qlaa and clab as before with q 0 aa = q 0 bb = 0.8 and c 0 ab = 0.6. In this case we consider values of \u03c3 2 w between\n0.1 and 3.0 and \u03c32b between 0.01 and 0.3. For each hyperparameter settings we fit the resulting residuals, |qlaa \u2212 q\u2217| and |clab \u2212 c\u2217|, to exponential functions and infer the depth-scale. We then compare this measured depth-scale to that predicted by the asymptotic expansion. The result of this measurement is shown in fig. 2. In general we see that the agreement is quite good. As expected we see that \u03bec diverges at the critical point.\nAs observed in Poole et al. (2016) we see that the depth scale for the propagation of information in a single input, \u03beq , is consistently finite and significantly shorter than \u03bec. To understand why this is the case consider eq. 6 and note that for tanh nonlinearities the second term is always negative. Thus, even as \u03c71 approaches 1 we expect \u03c71 + \u03c32w \u222b Dz\u03c6\u2032\u2032(\u221aq\u2217z)\u03c6(\u221aq\u2217z) to be substantially smaller than 1.\n3.1 DROPOUT\nThe mean field formalism can be extended to include dropout. The main contribution here will be to argue that even infinitesimal amounts of dropout destroys the mean field critical point, and therefore limits the trainable network depth. In the presence of dropout the propagation equation, eq. 1, becomes,\nzli = 1\n\u03c1 \u2211 j W lijp l jy l j + b l i (10)\nwhere pj \u223c Bernoulli(\u03c1) and \u03c1 is the dropout rate. As is typically the case we have re-scaled the sum by \u03c1\u22121 so that the mean of the pre-activation is invariant with respect to our choice of dropout rate.\nFollowing a similar procedure to the original mean field calculation consider the fate of two inputs, x0i;a and x 0 i;b, as they are propagated through such a random network. We take the dropout masks to be chosen independently for the two inputs mimicking the manner in which dropout is employed in practice. With dropout the diagonal term in the covariance matrix will be (see Appendix 7.3),\nq\u0304laa = \u03c32w \u03c1\n\u222b Dz\u03c62 (\u221a q\u0304l\u22121aa z ) + \u03c32b . (11)\nThe variance of a single input with dropout will therefore propagate in an identical fashion to the vanilla case with a re-scaling \u03c32w \u2192 \u03c32w/\u03c1. Intuitively, this result implies that, for the case of a single input, the presence of dropout simply increases the effective variance of the weights.\nComputing the off-diagonal term of the covariance matrix similarly (see Appendix 7.4),\nq\u0304lab = \u03c3 2 w \u222b Dz1Dz2\u03c6(u\u03041)\u03c6(u\u03042) + \u03c32b (12)\nwith u\u03041, u\u03042, and c\u0304lab defined by analogy to the mean field equations without dropout. Here, unlike in the case of a single input, the recurrence relation is identical to the recurrence relation without dropout. To see that c\u0304\u2217 = 1 is no longer a fixed point of these dynamics consider what happens to eq. 12 when we input c\u0304l = 1. For simplicity, we leverage the short range of \u03beq to replace q\u0304laa = q\u0304 l bb = q\u0304 \u2217. We find (see Appendix 7.5),\nc\u0304l+1ab = 1\u2212 1\u2212 \u03c1 \u03c1q\u0304\u2217 \u03c32w\n\u222b Dz\u03c62 (\u221a q\u0304\u2217z ) . (13)\nThe second term is positive for any \u03c1 < 1. This implies that if c\u0304lab = 1 for any l then c\u0304 l+1 ab < 1. Thus, c\u2217 = 1 is not a fixed point of eq. 12 for any \u03c1 < 1. Since eq. 12 is identical in form to eq. 4 it follows that the depth scale for signal propagation with dropout will likewise be given by eq. 9 with the substitutions q\u2217 \u2192 q\u0304\u2217 and c\u2217 \u2192 c\u0304\u2217 computed using eq. 11 and eq. 12 respectively. Importantly, since there is no longer a sharp critical point with dropout we do not expect a diverging depth scale.\nAs in networks without dropout we plot, in fig. 3 (a), the iterative map c\u0304l+1ab as a function of c\u0304 l ab. Most significantly, we see that the c\u0304lab = 1 is no longer a fixed point of the dynamics. Instead, as the dropout rate increases c\u0304lab gets mapped to decreasing values and the fixed point monotonically decreases.\nTo test these results we plot in fig. 3 (b) the asymptotic correlation, c\u2217, as a function of \u03c32w for different values of dropout from \u03c1 = 0.8 to \u03c1 = 1.0. As expected, we see that for all \u03c1 < 1 there is no sharp transition between c\u2217 = 1 and c\u2217 < 1. Moreover as the dropout rate increases the correlation c\u2217 monotonically decreases. Intuitively this makes sense. Identical inputs passed through two different dropout masks will become increasingly dissimilar as the dropout rate increases. In fig. 3 (c) we show the depth scale, \u03bec, as a function of \u03c32w for the same range of dropout probabilities. We find that, as predicted, the depth of signal propagation with dropout is drastically reduced and, importantly, there is no longer a divergence in \u03bec. Increasing the dropout rate continues to decrease the correlation depth for constant \u03c32w.\n4 GRADIENT BACKPROPAGATION\nThere is a duality between the forward propagation of signals and the backpropagation of gradients. To elucidate this connection consider the backpropagation equations given a loss E,\n\u2202E\n\u2202W lij = \u03b4li\u03c6(z l\u22121 j ) \u03b4 l i = \u03c6 \u2032(zli) \u2211 j \u03b4l+1j W l+1 ji (14)\nwith the identification \u03b4li = \u2202E/\u2202z l i. Within mean field theory, it is clear that the scale of fluctuations of the gradient of weights in a layer will be proportional to E[(\u03b4li)2] (see appendix 7.6). In contrast to the pre-activations in forward propagation (eq. 1), the \u03b4li will typically not be Gaussian distributed even in the large layer width limit.\nNonetheless, we can work out a recurrence relation for the variance of the error, q\u0303 laa = E[(\u03b4li)2], leveraging the Gaussian ansatz on the pre-activations. In order to do this, however, we must first make an additional approximation that the weights used during forward propagation are drawn independently from the weights used in backpropagation. This approximation is similar in spirit to the vanilla mean field approximation and is reminiscent of work on feedback alignment (Lillicrap et al., 2014). With this in mind we arrive at the recurrence (see appendix 7.7),\nq\u0303 laa = q\u0303 l+1 aa Nl+1 Nl \u03c71. (15)\nThe presence of \u03c71 in the above equation should perhaps not be surprising. In Poole et al. (2016) they show that \u03c71 is intimately related to the tangent space of a given layer in mean field neural\nnetworks. We note that the backpropagation recurrence features an explicit dependence on the ratio of widths of adjacent layers of the network, Nl+1/Nl. Here we will consider exclusively constant width networks where this factor is unity. For a discussion of the case of unequal layer widths see Glorot & Bengio (2010).\nSince \u03c71 depends only on the asymptotic q\u2217 it follows that for constant width networks we expect eq. 15 to again have an exponential solution with,\nq\u0303 laa = q\u0303 L aae \u2212(L\u2212l)/\u03be\u2207 \u03be\u22121 \u2207 = \u2212 log\u03c71. (16)\nNote that here \u03be\u22121 \u2207 = \u2212 log\u03c71 both above and below the transition. It follows that \u03be\u2207 can be both positive and negative. We conclude that there should be three distinct regimes for the gradients.\n1. In the ordered phase, \u03c71 < 1 and so \u03be\u2207 > 0. We therefore expect gradients to vanish over a depth |\u03be\u2207 |. 2. At criticality, \u03c71 \u2192 1 and so \u03be\u2207 \u2192\u221e. Here gradients should be stable regardless of depth. 3. In the chaotic phase, \u03c71 > 1 and so \u03be\u2207 < 0. It follows that in this regime gradients should\nexplode over a depth |\u03be\u2207 |.\nIntuitively these three regimes make sense. To see this, recall that perturbations to a weight in layer l can alternatively be viewed as perturbations to the pre-activations in the same layer. In the ordered phase both the perturbed signal and the unperturbed signal will be asymptotically mapped to the same point and the derivative will be small. In the chaotic phase the perturbed and unperturbed signals will become asymptotically decorrelated and the gradient will be large.\nTo investigate these predictions we construct deep random networks of depth L = 240 and layerwidth Nl = 300. We then consider the cross-entropy loss of these networks on MNIST. In fig. 4 (a) we plot the layer-by-layer 2-norm of the gradient, ||\u2207W labE|| 2 2, as a function of layer, l, for different values of \u03c32w. We see that ||\u2207W labE|| 2 2 behaves exponentially over many orders of magnitude. Moreover, we see that the gradient vanishes in the ordered phase and explodes in the chaotic phase. We test the quantitative predictions of eq. 16 in fig. 4 (b) where we compare |\u03be\u2207 | as predicted from theory with the measured depth-scale constructed from exponential fits to the gradient data. Here we see good quantitative agreement between the theoretical predictions from mean field random networks and experimentally realized networks. Together these results suggest that the approximations on the backpropagation equations were representative of deep, wide, random networks.\nFinally, we show that the depth scale for correlated signal propagation likewise controls the depth at which information stored in the covariance between gradients can survive. The existence of\nconsistent gradients across similar samples from a training set ought to be especially important for determining whether or not a given neural network architecture can be trained. To establish this depth-scale first note (see Appendix 7.8) that the covariance between gradients of two different inputs, xi;1 and xi;2, will be proportional to (\u2207W lijEa) \u00b7 (\u2207W lijEb) \u223c E[\u03b4 l i;a\u03b4 l i;b] = q\u0303 l ab where Ea is the loss evaluated on xi;a and \u03b4i;a = \u2202Ea/\u2202zli;a are appropriately defined errors.\nIt can be shown (see Appendix 7.9) that q\u0303 lab features the recurrence relation,\nq\u0303 lab = q\u0303 l+1 ab Nl+1 Nl+2 \u03c32w\n\u222b Dz1Dz2\u03c6\u2032(u1)\u03c6\u2032(u2) (17)\nwhere u1 and u2 are defined similarly as for the forward pass. Expanding asymptotically it is clear that to zeroth order in l, q\u0303lab will have an exponential solution with q\u0303 l ab = q\u0303 L abe \u2212(L\u2212l)/\u03bec with \u03bec as defined in the forward pass.\n5 EXPERIMENTAL RESULTS\nTaken together, the results of this paper lead us to the following hypothesis: a necessary condition for a random network to be trained is that information about the inputs should be able to propagate forward through the network, and information about the gradients should be able to propagate backwards through the network. The preceding analysis shows that networks will have this property precisely when the network depth, L, is not much larger than the depth-scale \u03bec. This criterion is data independent and therefore offers a \u201cuniversal\u201d constraint on the hyperparameters that depends on network architecture alone. We now explore this relationship between depth of signal propagation and network trainability empirically.\nTo investigate this prediction, we consider random networks of depth 10 \u2264 L \u2264 300 and 1 \u2264 \u03c32w \u2264 4 with \u03c32b = 0.05. We train these networks using Stochastic Gradient Descent (SGD) and RMSProp\non MNIST and CIFAR10. We use a learning rate of 10\u22123 for SGD when L . 200, 10\u22124 for larger L, and 10\u22125 for RMSProp. These learning rates were selected by grid search between 10\u22126 and 10\u22122 in exponentially spaced steps of size 10. We note that the depth dependence of learning rate was explored in detail in Saxe et al. (2014). In fig. 5 (a)-(d) we color in red the training accuracy that neural networks achieved as a function of \u03c32w and L for different datasets, training time, and choice of minimizer (see Appendix 7.10 for more comparisons). In all cases the neural networks over-fit the data to give a training accuracy of 100% and test accuracies of 98% on MNIST and 55% on CIFAR10. We emphasize that the purpose of this study is to demonstrate trainability as opposed to optimizing test accuracy.\nWe now make the connection between the depth scale, \u03bec, and the maximum trainable depth more precise. Given the arguments in the preceding sections we note that if L = n\u03bec then signal through the network will be attenuated by a factor of en. To understand how much signal can be lost while still allowing for training, we overlay in fig. 5 (a) curves corresponding to n\u03bec from n = 1 to 6. We find that networks appear to be trainable when L . 6\u03bec. It would be interesting to understand why this is the case.\nMotivated by this argument in fig. 5 (b)-(d) in white, dashed, overlay we plot twice the predicted depth scale, 6\u03bec. There is clearly a relationship between the depth of correlated signal propagation and whether or not these networks are trainable. Networks closer to their critical point appear to train more quickly than those further away. Moreover, this relationship has no obvious dependence on dataset, duration of training, or minimizer. We therefore conclude that these bounds on trainable hyperparameters are universal. This in turn implies that to train increasingly deep networks, one must generically be ever closer to criticality.\nNext we consider the effect of dropout. As we showed earlier, even infinitesimal amounts of dropout disrupt the order-to-chaos phase transition and cause the depth scale to become finite. However, since the effect of a single dropout mask is to simply re-scale the weight variance by \u03c32w \u2192 \u03c32w/\u03c1, the gradient magnitude will be stable near criticality, while the input and gradient correlations will not be. This therefore offers a unique opportunity to test whether the relevant depth-scale is |1/ log\u03c71| or \u03bec.\nIn fig. 6 we repeat the same experimental setup as above on MNIST with dropout rates \u03c1 = 0.99, 0.98, and 0.94. We observe, first and foremost, that even extremely modest amounts of dropout limit the maximum trainable depth to about L = 100. We additionally notice that the depth-scale, \u03bec, predicts the trainable region accurately for varying amounts of dropout.\n6 DISCUSSION\nIn this paper we have elucidated the existence of several depth-scales that control signal propagation in random neural networks. Furthermore, we have shown that the degree to which a neural network can be trained depends crucially on its ability to propagate information about inputs and gradients\nthrough its full depth. At the transition between order and chaos, information stored in the correlation between inputs can propagate infinitely far through these random networks. This in turn implies that extremely deep neural networks may be trained sufficiently close to criticality. However, our contribution goes beyond advocating for hyperparameter selection that brings random networks to be nearly critical. Instead, we offer a general purpose framework that predicts, at the level of mean field theory, which hyperparameters should allow a network to be trained. This is especially relevant when analyzing schemes like dropout where there is no critical point and which therefore imply an upper bound on trainable network depth.\nAn alternative perspective as to why information stored in the covariance between inputs is crucial for training can be understood by appealing to the correspondence between infinitely wide Bayesian neural networks and Gaussian Processes (Neal, 2012). In particular the covariance, qlab, is intimately related to the kernel of the induced Gaussian Process. It follows that cases in which signal stored in the covariance between inputs may propagate through the network correspond precisely to situations in which the associated Gaussian Process is well defined.\nOur work suggests that it may be fruitful to investigate pre-training schemes that attempt to perturb the weights of a neural network to favor information flow through the network. In principle this could be accomplished through a layer-by-layer local criterion for information flow or by selecting the mean and variance in schemes like batch normalization to maximize the covariance depth-scale.\nThese results suggest that theoretical work on random neural networks can be used to inform practical architectural decisions. However, there is still much work to be done. For instance, the framework developed here does not apply to unbounded activations, such as rectified linear units, where it can be shown that there are phases in which eq. 3 does not have a fixed point. Additionally, the analysis here applies directly only to fully connected feed-forward networks, and will need to be extended to architectures with structured weight matrices such as convolutional networks.\nWe close by noting that in physics it has long been known that, through renormalization, the behavior of systems near critical points can control their behavior even far from the idealized critical case. We therefore make the somewhat bold hypothesis that a broad class of neural network topologies will be controlled by the fully-connected mean field critical point.\nACKNOWLEDGMENTS\nWe thank Ben Poole, Jeffrey Pennington, Maithra Raghu, and George Dahl for useful discussions. We are additionally grateful to RocketAI for introducing us to Temporally Recurrent Online Learning and two-dimensional time.\n7 APPENDIX\nHere we present derivations of results from throughout the paper.\n7.1 SINGLE INPUT DEPTH-SCALE\nResult:\nConsider the recurrence relation for the variance of a single input,\nqlaa = \u03c3 2 w\n\u222b Dz\u03c62 (\u221a ql\u22121aa z ) + \u03c32b (18)\nand a fixed point of the dynamics, q\u2217. qlaa can be expanded about the fixed point to yield the asymptotic recurrence relation,\nl+1 = l [ \u03c71 + \u03c3 2 w \u222b Dz\u03c6\u2032\u2032 (\u221a q\u2217z ) \u03c6 (\u221a q\u2217z )] +O ( ( l)2 ) . (19)\nDerivation:\nWe begin by first expanding to order l,\nq\u2217 + l+1 = \u03c32w\n\u222b Dz [ \u03c6 (\u221a q\u2217 + lz )]2\n+ \u03c32b (20)\n\u2248 \u03c32w \u222b Dz [ \u03c6 (\u221a q\u2217z + 1\n2 lz\u221a q\u2217\n)]2 + \u03c32b (21)\n\u2248 \u03c32w \u222b Dz [ \u03c6 (\u221a q\u2217z ) + 1\n2 lz\u221a q\u2217 \u03c6\u2032 (\u221a q\u2217z )]2 + \u03c32b +O(( l)2) (22)\n\u2248 \u03c32w \u222b Dz\u03c62 (\u221a q\u2217z ) + \u03c32b + l \u03c3\n2 w\u221a q\u2217\n\u222b Dzz\u03c6 (\u221a q\u2217z ) \u03c6\u2032 (\u221a q\u2217z ) +O(( l)2) (23)\n\u2248 q\u2217 + l \u03c3 2 w\u221a q\u2217\n\u222b Dzz\u03c6(\u221aq\u2217z)\u03c6\u2032 (\u221a q\u2217z ) +O(( l)2). (24)\nWe therefore arrive at the approximate reccurence relation,\nl+1 = l \u03c32w\u221a q\u2217\n\u222b Dzz\u03c6(\u221aq\u2217z)\u03c6\u2032 (\u221a q\u2217z ) +O(( l)2). (25)\nUsing the identity, \u222b Dzzf(z) = \u222b Dzf \u2032(z) we can rewrite this asymptotic recurrence relation as,\nl+1 = l [ \u03c32w \u222b Dz [ \u03c6\u2032 (\u221a q\u2217z )]2 + \u03c32w \u222b Dz\u03c6\u2032\u2032 (\u221a q\u2217z ) \u03c6 (\u221a q\u2217z )] +O(( l)2) (26)\n= l [ \u03c71 + \u03c3 2 w \u222b Dz\u03c6\u2032\u2032 (\u221a q\u2217z ) \u03c6 (\u221a q\u2217z )] +O(( l)2) (27)\nas required.\n7.2 TWO INPUT DEPTH-SCALE\nResult:\nConsider the recurrence relation for the co-variance of two input,\nqlab = \u03c3 2 w \u222b Dz1Dz2\u03c6(u1)\u03c6(u2) + \u03c32b , (28)\na correlation between the inputs, clab = q l ab/ \u221a qlaaq l bb, and a fixed point of the dynamics, c\n\u2217. clab can be expanded about the fixed point to yield the asymptotic recurrence relation,\nl+1 = l [ \u03c32w \u222b Dz1Dz2\u03c6\u2032(u1)\u03c6\u2032(u2) ] +O ( ( l)2 ) . (29)\nDerivation:\nSince the relaxation of qlaa and q l bb to q \u2217 occurs much more quickly than the convergence of qlab we approximate qlaa = q l bb = q\n\u2217 as in Poole et al. (2016). We therefore consider the perturbation qlab/q \u2217 = clab = c \u2217 + l. It follows that we may make the approximation,\nul2 = \u221a q\u2217 ( clabz1 + \u221a 1\u2212 (clab)2z2 ) (30)\n\u2248 \u221aq\u2217 ( c\u2217z1 + \u221a 1\u2212 (c\u2217)2 \u2212 2c\u2217 lz2 ) + \u221a q\u2217 lz1 +O( 2) (31)\n(32)\nWe now consider the case where c\u2217 < 1 and c\u2217 = 1 separately; we will later show that these two results agree with one another. First we consider the case where c\u2217 < 1 in which case we may safely expand the above equation to get,\nul2 = \u221a q\u2217 ( c\u2217z1 + \u221a 1\u2212 (c\u2217)2z2 ) + \u221a q\u2217 l ( z1 \u2212\nc\u2217\u221a 1\u2212 (c\u2217)2 z2\n) +O( 2). (33)\nThis allows us to in turn approximate the recurrence relation,\ncl+1ab = \u03c32w q\u2217\n\u222b Dz1Dz2\u03c6(u\u22171)\u03c6(ul2) + \u03c32b (34)\n\u2248 \u03c3 2 w\nq\u2217\n\u222b Dz1Dz2\u03c6(u\u22171) [ \u03c6(u\u22172) + \u221a q\u2217 l ( z1 \u2212\nc\u2217\u221a 1\u2212 (c\u2217)2 z2\n) \u03c6\u2032(u\u22172) ] + \u03c32b +O( 2)\n(35) = c\u2217 + \u03c32w\u221a q\u2217 l \u222b Dz1Dz2 ( z1 \u2212 c\u2217\u221a 1\u2212 (c\u2217)2 z2 ) \u03c6(u\u22171)\u03c6 \u2032(u\u22172) (36)\n= c\u2217 + \u03c32w\u221a q\u2217 l\n[\u222b Dz1Dz2z1\u03c6(u\u22171)\u03c6\u2032(u\u22172)\u2212\nc\u2217\u221a 1\u2212 (c\u2217)2\n\u222b Dz1Dz2z2\u03c6(u\u22171)\u03c6\u2032(u\u22172) ] (37)\n= c\u2217 + \u03c32w l [\u222b Dz1Dz2(\u03c6\u2032(u\u22171)\u03c6\u2032(u\u22172) + c\u2217\u03c6(u\u22171)\u03c6\u2032\u2032(u\u22172))\u2212 c\u2217 \u222b Dz1Dz2\u03c6(u\u22171)\u03c6\u2032\u2032(u\u22172) ] (38)\n= c\u2217 + \u03c32w l \u222b Dz1Dz2\u03c6\u2032(u\u22171)\u03c6\u2032(u\u22172). (39)\nwhere u\u22171 and u \u2217 2 are appropriately defined asymptotic random variables. This leads to the asymptotic recurrence relation,\nl+1 = \u03c32w l \u222b Dz1Dz2\u03c6\u2032(u\u22171)\u03c6\u2032(u\u22172) (40)\nas required.\nWe now consider the case where c\u2217 = 1 and clab = 1 \u2212 l. In this case the expansion of ul2 will become,\nul2 = \u221a q\u2217z1 + \u221a 2q\u2217 lz2 \u2212 \u221a q\u2217 lz1 +O( 3/2) (41)\nand so the lowest order correction is of orderO( \u221a l) as opposed toO( l). As usual we now expand the recurrence relation, noting that u\u22172 = u \u2217 1 is independent of z2 when c \u2217 = 1 to find,\ncl+1ab = \u03c32w q\u2217\n\u222b Dz1Dz2\u03c6(u\u22171)\u03c6(ul2) + \u03c32b (42)\n\u2248 \u03c3 2 w\nq\u2217\n\u222b Dz1Dz2\u03c6(u\u22171) [ \u03c6(u\u22172) + (\u221a 2q\u2217 lz2 \u2212 \u221a q\u2217 lz1 ) \u03c6\u2032(u\u22172) + q \u2217 lz22\u03c6 \u2032\u2032(u\u22172) ] + \u03c32b\n(43)\n= c\u2217 + \u03c32w l\n\u222b Dz\u03c6(\u221aq\u2217z) [ \u03c6\u2032\u2032( \u221a q\u2217z)\u2212 1\u221a\nq\u2217 z\u03c6\u2032( \u221a q\u2217z)\n] (44)\n= c\u2217 + \u03c32w l\n[\u222b Dz\u03c6(\u221aq\u2217z)\u03c6\u2032\u2032(\u221aq\u2217z)\u2212 1\u221a\nq\u2217\n\u222b Dzz\u03c6(\u221aq\u2217z)\u03c6\u2032(\u221aq\u2217z) ] (45)\n= c\u2217 \u2212 \u03c32w l \u222b Dz [ \u03c6\u2032( \u221a q\u2217z) ]2 (46)\nIt follows that the asymptotic recurrence relation in this case will be, l+1 = \u2212 l\u03c32w \u222b Dz [ \u03c6\u2032( \u221a q\u2217z) ]2 = \u2212 l\u03c71. (47)\nwhere \u03c71 is the stability condition for the ordered phase. We note that although the approximations were somewhat different the asymptotic recurrence relation for c\u2217 < 1 reduces eq. 47 result for c\u2217 = 1. We may therefore use 4 for all c\u2217.\n7.3 VARIANCE OF AN INPUT WITH DROPOUT\nResult:\nIn the presence of dropout with rate \u03c1, the variance of a single input as it is passed through the network is described by the recurrence relation,\nq\u0304laa = \u03c32w \u03c1\n\u222b Dz\u03c62 (\u221a q\u0304l\u22121aa z ) + \u03c32b . (48)\nDerivation:\nRecall that the recurrence relation for the pre-activations is given by,\nzli = 1\n\u03c1 \u2211 j W lijp l jy l j + b l i (49)\nwhere plj \u223c Bernoulli(\u03c1). It follows that the variance will be given by,\nq\u0304laa = E[(zli)2] (50)\n= 1\n\u03c12 \u2211 j E[(W lij)2]E[(\u03c1lj)2]E[(ylj)2] + E[(bli)2] (51)\n= \u03c32w \u03c1\n\u222b Dz\u03c62 (\u221a q\u0304l\u22121aa z ) + \u03c32b . (52)\nwhere we have used the fact that E[(plj)2] = \u03c1.\n7.4 COVARIANCE OF TWO INPUTS WITH DROPOUT\nResult:\nThe co-variance between two signals, zli;a and z l i;b, with separate i.i.d. dropout masks p l i;a and p l i;b is given by,\nq\u0304lab = \u03c3 2 w \u222b Dz1Dz2\u03c6(u\u03041)\u03c6(u\u03042) + \u03c32b . (53)\nwhere, in analogy to eq. 4, u\u03041 = \u221a q\u0304laaz1 and u\u03042 = \u221a q\u0304lbb ( c\u0304labz1 + \u221a 1\u2212 (c\u0304lab)2z2 ) .\nDerivation:\nProceeding directly we find that,\nE[zli;azli;b] = 1\n\u03c12 \u2211 j E[(W lij)2]E[plj;a]E[plj;b]E[ylj;aylj;b] + E[bli] (54)\n= \u03c32w \u222b Dz1Dz2\u03c6(u\u03041)\u03c6(u\u03042) + \u03c32b (55)\nwhere we have used the fact that E[pli;a] = E[pli;b] = \u03c1. We have also used the same substitution for E[ylj;aylj;b] used in the original mean field calculation with the appropriate substitution.\n7.5 THE LACK OF A c\u2217 = 1 FIXED POINT WITH DROPOUT\nResult:\nIf clab = 1 then it follows that,\nc\u0304l+1ab = 1\u2212 1\u2212 \u03c1 \u03c1q\u0304\u2217 \u03c32w\n\u222b Dz\u03c62 (\u221a q\u0304\u2217z )\n(56)\nsubject to the approximation, qlaa \u2248 qlbb \u2248 q\u2217. This implies that cl+1ab < 1. Derivation:\nPlugging in clab = 1 with q l aa \u2248 qlbb \u2248 q\u2217 we find that u\u03041 = u\u03042 = \u221a q\u2217z1. It follows that,\ncl+1ab = ql+1ab q\u2217\n(57)\n= 1\nq\u2217\n[ \u03c32w \u222b Dz\u03c62 (\u221a q\u2217z ) + \u03c32b ] (58)\n= 1\nq\u2217\n[ \u03c32w(1\u2212 \u03c1\u22121 + \u03c1\u22121) \u222b Dz\u03c62 (\u221a q\u2217z ) + \u03c32b ] (59)\n= 1\nq\u2217 [ \u03c32w \u03c1 \u222b Dz\u03c62 (\u221a q\u2217z ) + \u03c32b ] + \u03c32w q\u2217 (1\u2212 \u03c1\u22121) \u222b Dz\u03c62 (\u221a q\u2217z )\n(60)\n= 1\u2212 1\u2212 \u03c1 \u03c1q\u0304\u2217 \u03c32w\n\u222b Dz\u03c62 (\u221a q\u0304\u2217z )\n(61)\nas required. Here we have integrated out z2 since nether u\u03041 nor u\u03042 depend on it.\n7.6 MEAN FIELD GRADIENT SCALING\nResult:\nIn mean field theory the expected magnitude of the gradient ||\u2207W lijE|| 2 will be proportional to E[(\u03b4li)2].\nDerivation:\nWe first note that since the W lij are i.i.d. it follows that,\n||\u2207W lijE|| 2 = \u2211 ij\n( \u2202E\n\u2202W lij\n)2 (62)\n\u2248 NlNl+1E ( \u2202E \u2202W lij )2 (63) where we have used the fact that the first line is related to the sample expectation over the different realizations of theW lij to approximate it by the analytic expectation in the second line. In mean field theory since the pre-activations in each layer are assumed to be i.i.d. Gaussian it follows that,\nE ( \u2202E \u2202W lij )2 = E[(\u03b4li)2]E[\u03c62(zl\u22121j )] (64) and the result follows.\n7.7 MEAN FIELD BACKPROPAGATION\nResult:\nIn mean field theory the recursion relation for the variance of the errors, q\u0303 l = E[(\u03b4li)2] is given by,\nq\u0303 laa = q\u0303 l+1 aa Nl+1 Nl+2 \u03c71(q l aa). (65)\nDerivation:\nComputing the variance directly and using mean field approximation, q\u0303 laa = E[(\u03b4li;a)2] = E[(\u03c6\u2032(zli;a))2] \u2211 j E[(\u03b4l+1j;a ) 2]E[(W l+1ji ) 2] (66)\n= E[(\u03c6\u2032(zli;a))2] \u03c32w Nl+1 \u2211 j E[(\u03b4l+1j;a ) 2] (67)\n= E[(\u03c6\u2032(zli;a))2] Nl+1 Nl+2 \u03c32w q\u0303 l+1 aa (68)\n= \u03c32w q\u0303 l+1 aa Nl+1 Nl+2\n\u222b Dz [ \u03c6\u2032 (\u221a\nqlaaz\n)]2 (69)\n\u2248 q\u0303 l+1aa Nl+1 Nl+2 \u03c71 (70)\nas required. In the last step we have made the approximation that qlaa \u2248 q\u2217 since the depth scale for the variance is short ranged.\n7.8 MEAN FIELD GRADIENT COVARIANCE SCALING\nResult:\nIn mean field theory we expect the covariance between the gradients of two different inputs to scale as,\n(\u2207W lijEa) \u00b7 (\u2207W lijEb) \u223c E[\u03b4i;a\u03b4i;b]. (71)\nDerivation:\nWe proceed in a manner analogous to Appendix 7.6. Note that in mean field theory since the weights are i.i.d. it follows that\n(\u2207W lijEa) \u00b7 (\u2207W lijEb) = \u2211 ij \u2202Ea \u2202W lij \u2202Eb \u2202W lij\n(72)\n\u2248 NlNl+1E [ \u2202Ea \u2202W lij \u2202Eb \u2202W lij ] (73)\nwhere, as before, the final term is approximating the sample expectation. Since the weights in the forward and backwards passes are chosen independently it follows that we can factor the expectation as,\nE [ \u2202Ea \u2202W lij \u2202Eb \u2202W lij ] = E[\u03b4li;a\u03b4li;b]E[\u03c6(zli;a)\u03c6(zli;b)] (74)\nand the result follows.\n7.9 MEAN FIELD BACKPROPAGATION OF COVARIANCE\nResult:\nThe covariance between the gradients due to two inputs scales as,\nq\u0303 lab = q\u0303 l+1 ab Nl+1 Nl+2 \u03c32w\n\u222b Dz1Dz2\u03c6\u2032(u1)\u03c6\u2032(u2) (75)\nunder backpropagation.\nDerivation\nAs in the analogous derivation for the variance, we compute directly, q\u0303 lab = E[\u03b4li;a\u03b4li;b] = E [\u03c6\u2032(zi;a)\u03c6\u2032(zi;b)] \u2211 j E[\u03b4l+1j;a \u03b4 l+1 j;b ]E[(W l+1 ji ) 2] (76)\n= q\u0303 l+1ab Nl+1 Nl+2 \u03c32w\n\u222b Dz1Dz2\u03c6\u2032(u1)\u03c6\u2032(u2) (77)\nas required.\n7.10 FURTHER EXPERIMENTAL RESULTS\nHere we include some more experimental figures that investigate the effects of training time, minimizer, and dataset more closely.\n", "cats": {"READ": 1.0, "DONTREAD": 0.0}, "reviews": [{"IS_META_REVIEW": true, "comments": "This paper presents a mathematical analysis of how information is propagated through deep feed-forward neural networks, with novel analysis addressing the problem of vanishing and exploding gradients in the backward pass of backpropagation and the use of the dropout algorithm. The paper is clear and well-written, the analysis is thorough, and the experimental results showing agreement with the model are very nice."}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "This is one of the two top papers in my stack. In total the reviews are a little bit on the light side in terms of level of detail and there are some concerns regarding how useful the results are from a practical point of view. However, I am confident that the paper should be accepted. ", "OTHER_KEYS": "ICLR 2017 pcs"}, {"DATE": "31 Dec 2016", "TITLE": "dropout", "IS_META_REVIEW": false, "comments": "Just making sure, your experiments with dropout follows the convention that during evaluation, it is turned off?", "OTHER_KEYS": "Greg Yang"}, {"TITLE": "interesting analysis - empirical results could be clarified", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer4", "comments": "I'm not familiar enough with mean-field techniques to judge the soundness of Eq 2, but I'm willing to roll with it.\n\nMinor point on presentation: Speaking of the \"evolution\" of x_{i;a} as it travels through the network could give some readers helpful intuition, but for me it was confusing because x_{*;a} is the immutable input vector, and it's the just-introduced z and y variables that represent its so-called evolution, no?\n\nIn interpreting this analysis - A network may be trainable if information does not pass through it, if the training steps, by whatever reason, perturb the weights so that information starts to pass through it (without subsequently perturbing the weights to stop information from passing through it.) Perhaps this could be clarified by a definition of \u201ctraining algorithm\u201d?\n\nComments on central claims:\nPrevious work on initializing neural networks to promote information flow (e.g. Glorot & Bengio, ", "IS_META_REVIEW": false, "RECOMMENDATION": 8, "DATE": "23 Dec 2016", "REVIEWER_CONFIDENCE": 2}, {"DATE": "23 Dec 2016", "TITLE": "-", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer4"}, {"TITLE": "An important and thorough contribution to the theoretical analysis of deep neural networks", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "The paper expands a recent mean-field approximation of deep random neural networks to study depth-dependent information propagation, its phase-dependence and the influence of drop-out. The paper is extremely well written, the mathematical analysis is thorough and numerical experiments are included that underscore the theoretical results. Overall the paper stands out as one of the few papers that thoroughly analyses training and performance of deep nets.", "IS_META_REVIEW": false, "RECOMMENDATION": 9, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "Excellent analysis ", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "This paper presents a mathematical analysis of how information is propagated through deep feed-forward neural networks, with novel analysis addressing the problem of vanishing and exploding gradients in the backward pass of backpropagation and the use of the dropout algorithm. The paper is clear and well-written, the analysis is thorough, and the experimental results showing agreement with the model are very nice. ", "IS_META_REVIEW": false, "RECOMMENDATION": 8, "DATE": "13 Dec 2016", "REVIEWER_CONFIDENCE": 3}, {"IS_META_REVIEW": true, "comments": "This paper presents a mathematical analysis of how information is propagated through deep feed-forward neural networks, with novel analysis addressing the problem of vanishing and exploding gradients in the backward pass of backpropagation and the use of the dropout algorithm. The paper is clear and well-written, the analysis is thorough, and the experimental results showing agreement with the model are very nice."}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "This is one of the two top papers in my stack. In total the reviews are a little bit on the light side in terms of level of detail and there are some concerns regarding how useful the results are from a practical point of view. However, I am confident that the paper should be accepted. ", "OTHER_KEYS": "ICLR 2017 pcs"}, {"DATE": "31 Dec 2016", "TITLE": "dropout", "IS_META_REVIEW": false, "comments": "Just making sure, your experiments with dropout follows the convention that during evaluation, it is turned off?", "OTHER_KEYS": "Greg Yang"}, {"TITLE": "interesting analysis - empirical results could be clarified", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer4", "comments": "I'm not familiar enough with mean-field techniques to judge the soundness of Eq 2, but I'm willing to roll with it.\n\nMinor point on presentation: Speaking of the \"evolution\" of x_{i;a} as it travels through the network could give some readers helpful intuition, but for me it was confusing because x_{*;a} is the immutable input vector, and it's the just-introduced z and y variables that represent its so-called evolution, no?\n\nIn interpreting this analysis - A network may be trainable if information does not pass through it, if the training steps, by whatever reason, perturb the weights so that information starts to pass through it (without subsequently perturbing the weights to stop information from passing through it.) Perhaps this could be clarified by a definition of \u201ctraining algorithm\u201d?\n\nComments on central claims:\nPrevious work on initializing neural networks to promote information flow (e.g. Glorot & Bengio, ", "IS_META_REVIEW": false, "RECOMMENDATION": 8, "DATE": "23 Dec 2016", "REVIEWER_CONFIDENCE": 2}, {"DATE": "23 Dec 2016", "TITLE": "-", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer4"}, {"TITLE": "An important and thorough contribution to the theoretical analysis of deep neural networks", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "The paper expands a recent mean-field approximation of deep random neural networks to study depth-dependent information propagation, its phase-dependence and the influence of drop-out. The paper is extremely well written, the mathematical analysis is thorough and numerical experiments are included that underscore the theoretical results. Overall the paper stands out as one of the few papers that thoroughly analyses training and performance of deep nets.", "IS_META_REVIEW": false, "RECOMMENDATION": 9, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "Excellent analysis ", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "This paper presents a mathematical analysis of how information is propagated through deep feed-forward neural networks, with novel analysis addressing the problem of vanishing and exploding gradients in the backward pass of backpropagation and the use of the dropout algorithm. The paper is clear and well-written, the analysis is thorough, and the experimental results showing agreement with the model are very nice. ", "IS_META_REVIEW": false, "RECOMMENDATION": 8, "DATE": "13 Dec 2016", "REVIEWER_CONFIDENCE": 3}]}
{"text": "REINFORCEMENT LEARNING THROUGH NEURAL ENCODING\nINTRODUCTION\nIn the early days of RL, understanding the behavior of trained policies could be done rather easily (Sutton, 1990). Researchers focused on simpler problems (Peng and Williams, 1993), and policies were built using lighter models than today (Tesauro, 1994). As a result, a meaningful analysis of policies was possible even by working with the original state representation and relating to primitive actions. However, in recent years research has made a huge step forward. Fancier models such as Deep Neural Networks (DNNs) have become a commodity (Mnih et al., 2015), and the RL community tackles bigger and more challenging problems (Silver et al., 2016). Artificial agents are even expected to be used in autonomous systems such as self-driving cars. The need to reason the behavior of trained agents, and understand the mechanisms that govern its choice of actions is pressing more than ever.\nAnalyzing a trained policy modeled by a DNN (either graphically using the state-action diagram, or by any other mean) is practically impossible. A typical problem consists of an immense number of states, and policies often rely on skills (Mankowitz, Mann, and Mannor, 2014), creating more than a single level of planning. The resulting Markov reward processes induced by such policies are too complicated to comprehend through observation. Simplifying the behavior requires finding a suitable representation of the state space; a long-standing problem in machine learning, where extensive research has been conducted over the years (Boutilier, Dean, and Hanks, 1999). There, the goal is to come up with a transformation of the state space \u03c6 : s \u2192 s\u0302, that can facilitate learning.\n\u2217These authors contributed equally\nIn the field of RL, where problems are sequential in nature, this problem is exacerbated since the representation of a state needs to account for the dynamics of the problem as well.\nFinding a suitable state representation can be phrased as a learning problem itself (Ng, 2011; Lee et al., 2006). DNNs are very useful in this context since they automatically build a hierarchy of representations with an increasing level of abstraction along the layers. In this work, we show that the state representation that is learned automatically by DNNs is suitable for building abstractions in RL. To this end, we introduce the SAMDP model; a modeling approach that creates abstractions both in space and time. Contrary to other modeling approaches, SAMDP is built in a transformed state space, where the problem of creating spatial abstractions (i.e., state aggregation), and temporal abstractions (i.e., identifying skills) is facilitated using spatiotemporal clustering. We provide an example for building an SAMDP model for a basic gridworld problem where \u03c6(s) is hand-crafted. However, the real strength of the model is demonstrated on challenging Atari2600 games solved using DQNs (Mnih et al., 2015). There, we set \u03c6(s) to be the state representation automatically learned by the DNN (i.e. the last hidden layer). We continue by presenting methods for evaluating the fitness of the SAMDP model to the trained policy at hand. Finally, we describe a method for using the SAMDP as a monitor that alerts when the policy\u2019s performance weakens, and provide initial results showing how the SAMDP model is useful for shared autonomy systems.\nBACKGROUND\nWe briefly review the standard reinforcement learning framework of discrete-time finite Markov decision processes (MDPs). An MDP is defined by a five-tuple < S,A, P,R, \u03b3 >. At time t an agent observes a state st \u2208 S, selects an action at \u2208 A, and receives a reward rt. Following the agent\u2019s action choice, it transitions to the next state st+1 \u2208 S according to a Markovian probability matrix Pa \u2208 P . The cumulative return at time t is given by Rt = \u2211\u221e t\u2032=t \u03b3\nt\u2032\u2212trt, where \u03b3 \u2208 [0, 1] is the discount factor. In this framework, the goal of an RL agent is to maximize the expected return by learning a policy \u03c0 : S \u2192 \u2206A; a mapping from states s \u2208 S to a probability distribution over actions. The action-value function Q\u03c0(s, a) = E[Rt|st = s, at = a, \u03c0] represents the expected return after observing state s, taking action a and then following policy \u03c0. The optimal action-value function obeys a fundamental recursion known as the optimal Bellman Equation: Q\u2217(st, at) = E [ rt + \u03b3max\na\u2032 Q\u2217(st+1, a\n\u2032) ] .\nSkills, Options (Sutton, Precup, and Singh, 1999) are temporally extended control structures, denoted by \u03c3. A skill is defined by a triplet: \u03c3 =< I, \u03c0, \u03b2 >, where I defines the set of states where the skill can be initiated, \u03c0 is the intra-skill policy, and \u03b2 is the set of termination probabilities determining when a skill will stop executing. \u03b2 is typically either a function of state s or time t. Any MDP with a fixed set of skills is a Semi-MDP (SMDP). Planning with skills can be performed by learning for each state the value of choosing each skill. More formally, an SMDP is defined by a five-tuple < S,\u03a3, P,R, \u03b3 >. S is the set of states, \u03a3 is the set of skills, P is the SMDP transition\nmatrix, \u03b3 is the discount factor and the SMDP reward is defined by:\nR\u03c3s = E[r\u03c3s ] = E[rt+1 + \u03b3rt+2 + \u00b7 \u00b7 \u00b7+ \u03b3k\u22121rt+k|st = s, \u03c3]. (1)\nThe Skill Policy \u00b5 : S \u2192 \u2206\u03a3 is a mapping from states to a distribution over skills. The action-value function Q\u00b5(s, \u03c3) = E[ \u2211\u221e t=0 \u03b3\ntRt|(s, \u03c3), \u00b5] represents the value of choosing skill \u03c3 \u2208 \u03a3 at state s \u2208 S, and thereafter selecting skills according to policy \u00b5. The optimal skill value function is given by: Q\u2217\u03a3(s, \u03c3) = E[R\u03c3s + \u03b3kmax\n\u03c3\u2032\u2208\u03a3 Q\u2217\u03a3(s \u2032, \u03c3\u2032)] (Stolle and Precup, 2002).\nTHE SEMI AGGREGATED MDP\nReinforcement Learning problems are typically modeled using the MDP formulation. Given an MDP, a variety of algorithms have been proposed to find an optimal policy. However, when one wishes to analyze a trained policy, MDP may not be the best modeling choice due to the size of the state space and the length of the planning horizon. In this section, we present the SMDP and Aggregated MDP (AMDP) models which can simplify the analysis by using temporal and spatial abstractions respectively. We also introduce the new Semi-Aggregated MDP (SAMDP) model, that combines SMDP and AMDP models in a novel way which leverages the abstractions made in each modeling approach. SMDP (Sutton, Precup, and Singh, 1999), can simplify the analysis of a trained policy by using temporal abstractions. The model extends the MDP action space A to allow the agent to plan with temporally extended actions \u03a3 (i.e., skills). Analyzing policies using the SMDP model shortens the planning horizon and simplifies the analysis. However, there are two problems with this approach. First, one still faces the high complexity of the state space, and second, the SMDP model requires to identify skills.\nSkill identification is an ill-posed problem that can be addressed in many ways, and for which extensive research has been done over the years. The popular approaches are to identify bottlenecks in the state space (McGovern and Barto, 2001),or to search for common behavior trajectories, or common state region policies (McGovern, 2002). A different approach can be to build a graphical model of the agent\u2019s interaction with the environment and to use betweenness centrality measures to identify subtasks (S\u0327ims\u0327ek and Barreto, 2009). No matter what the method is, identifying skills solely by observing an agent play is a challenging task.\nAlternative approach to SMDP modeling is to analyze a policy using spatial abstractions in the state space. If there is a reason to believe that groups of states share common attributes such as similar policy or value function, it is possible to use State Aggregation (Moore, 1991). State Aggregation is a well-studied problem that typically involves identifying clusters as the new states of an Aggregated MDP, where the set of clusters C replaces the MDP states S. Applying RL on aggregated states is potentially advantageous because the dimensions of the transition probability matrix P , the reward signal R and the policy \u03c0 are decreased (Singh, Jaakkola, and Jordan, 1995). However, the AMDP modeling approach has two drawbacks. First, the action space A is not modified, and therefore the planning horizon remains intractable, and second, AMDPs are not necessarily Markovian (Bai, Srivastava, and Russell, 2016). In this paper, we propose a model that combines the advantages of the SMDP and AMDP approaches and denote it by SAMDP. Under SAMDP modeling, aggregation defines both the states and the set of skills, allowing analysis with spatiotemporal abstractions (the state-space dimensions and the planning horizon are reduced). However, SAMDPs are still not necessarily Markovian. We summarize the different modeling approaches in Figure 1. The rest of this section is devoted to explaining the five stages of building an SAMDP model: (0) Feature selection, (1) State Aggregation, (2) Skill identification, (3) Inference, and (4) Model Selection. (0) Feature selection. We define the mapping from MDP states to features, by a mapping function \u03c6 : s \u2192 s\u2032 \u2282 Rm. The features may be raw (e.g., spatial coordinates, frame pixels) or higher level abstractions (e.g., the last hidden layer of an NN). The feature representation has a significant effect on the quality of the resulting SAMDP model and vice versa; a good model can point out a good feature representation. (1) Aggregation via Spatio-temporal clustering. The goal of Aggregation is to find a mapping (clustering) from the MDP feature space S\u2032 \u2282 Rm to the AMDP state space C. Clustering algorithms typically assume that data is drawn from an i.i.d distribution. However, in our problem data\nis generated from an MDP which violates this assumption. We alleviate this problem using two different approaches. First, we decouple the clustering step from the SAMDP model, by creating an ensemble of clustering candidates and building an SAMDP model for each (following stages 2 and 3). In stage 4, we will explain how to run a non-analytic outer optimization loop to choose between these candidates based on spatiotemporal evaluation criteria. Second, we introduce a novel extension of the celebrated K-means algorithm (MacQueen and others, 1967), which enforces temporal coherency along trajectories. In the vanilla K-means algorithm, a point xt is assigned to cluster ci with mean \u00b5i if \u00b5i is the closest cluster center to xt (for further details please see the supplementary material). We modified this step as follows:\nc(xt) = { ci : \u2225\u2225Xt \u2212 \u00b5i\u2225\u22252F \u2264 \u2225\u2225Xt \u2212 \u00b5j\u2225\u22252F ,\u2200j \u2208 [1,K]}, where F stands for the Frobenius norm, K is the number of clusters, t is the time index of xt, and Xt is a set of 2w+ 1 centered at xt from the same trajectory: { xj \u2208 Xt \u21d0\u21d2 j \u2208 [t\u2212w, t+w] } . The dimensions of \u00b5 correspond to a single point, but is expanded to the dimensions of Xt. In this way, we enforce temporal coherency since a point xt is assigned to a cluster ci if its neighbors in time along the trajectory are also close to \u00b5i. We have also experimented with other clustering methods such as spectral clustering, hierarchical agglomerative clustering and entropy minimization (please refer to the supplementary material for more details). (2) Skill identification. We define an SAMDP skill \u03c3i,j \u2208 \u03a3 uniquely by a single initiation state ci \u2208 C and a single termination state cj \u2208 C : \u03c3ij =< ci, \u03c0i,j , cj > . More formally, at time t the agent enters an AMDP state ci at an MDP state st \u2208 ci. It chooses a skill according to its SAMDP policy and follows the skill policy \u03c0i,j for k time steps until it reaches a state st+k \u2208 cj , s.t i 6= j. We do not define the skill length k apriori nor the skill policy but infer the skill length from the data. As for the skill policies, our model does not define them explicitly, but we will observe later that our model successfully identifies skills that are localized in time and space.\n(3) Inference. Given the SAMDP states and skills, we infer the skill length, the SAMDP reward and the SAMDP probability transition matrix from observations. The skill length, is inferred for a skill \u03c3i,j by averaging the number of MDP states visited since entering SAMDP state ci until leaving for SAMDP state cj . The skill reward is inferred similarly using Equation 1. The inference of the SAMDP transition matrices is a bit more puzzling, since the probability of seeing the next SAMDP state depends both on the MDP dynamics and the agent policy in the MDP state space. We now turn to discuss how to infer these matrices by observing transitions in the MDP state space. Our goal is to infer two quantities: (a) The SAMDP transition probability matrices P\u03a3 : P \u03c3\u2208\u03a3 i,j = Pr(cj |ci, \u03c3), measures the probability of moving from state ci to cj given that skill \u03c3 is chosen. These matrices are defined uniquely by our definition of skills as deterministic probability matrices. (b) The probability of moving from state ci to cj given that skill \u03c3 is chosen according to the agent SAMDP policy: P\u03c0i,j = Pr(cj |ci, \u03c3 = \u03c0(ci)). This quantity involves both the SAMDP transition probability matrices and the agent policy. However, since SAMDP transition probability matrices are deterministic, this is equivalent to the agent policy in the SAMDP state space. Therefore by inferring transitions between SAMDP states, we directly infer the agent\u2019s SAMDP policy. Given an MDP with a deterministic environment and an agent with a nearly deterministic MDP policy (e.g., a deterministic policy that uses an -greedy exploration ( 1)), it is intuitive to assume that we would observe a nearly deterministic SAMDP policy. However, there are two mechanisms that cause stochasticity in the SAMDP policy: (1) Stochasticity that is accumulated along skill trajectories. (2) Approximation errors in the aggregation process. A given SAMDP state may contain more than one \u201dreal\u201d state and therefore more than one skill. Performing inference in this setup, we might observe a stochastic policy that chooses randomly between skills. Therefore, it is very likely to infer a stochastic SAMDP transition matrix, even though the SAMDP transition probability matrices and the MDP environment are deterministic, and the MDP policy is nearly deterministic. (4) Model selection. So far we have explained how to build an SAMDP from observations. In this stage, we\u2019ll explain how to choose between different SAMDP model candidates. There are two advantages of choosing between multiple SAMDPs. First, there are different hyperparameters to tune: two examples are the number of SAMDP states (K) and the window size (w) for the clustering algorithm. Second, there is randomness in the aggregation step. Hence, clustering multiple times and picking the best result will potentially yield better models.\nWe developed, therefore, evaluation criteria that allow us to select the best model, motivated by Hallak, Di-Castro, and Mannor (2013). We follow the Occams Razor principle and aim to find the simplest model which best explains the data. (i) Value Mean Square Error(VMSE), measures the consistency of the model with the observations. The estimator is given by\nVMSE = \u2016v\u2212vSAMDP \u2016/\u2016v\u2016, (2)\nwhere v stands for the SAMDP value function of the given policy, and vSAMDP is given by: VSAMDP = (I + \u03b3\nkP )\u22121r, where P is measured under the SAMDP policy. (ii) Inertia, the Kmeans algorithm objective function, is given by : I = \u2211n i=0 min\u00b5j\u2208C(||xj \u2212 \u00b5i||2). Inertia measures the variance inside clusters and encourages spatial coherency. Motivated by Ncut and spectral clustering (Von Luxburg, 2007), we define (iii) The Intensity Factor as the fraction of out/in cluster transitions. However, we define edges between states that are connected along the trajectory (a transition between them was observed) and give them equal weights (instead of defining the edges by euclidean distances as in spectral clustering). Minimizing the intensity factor encourages longer duration skills. (iv)had been Entropy, is defined on the SAMDP probability transition matrix as follows: e = \u2212 \u2211 i{|Ci| \u00b7 \u2211 j Pi,j logPi,j}. Low entropy encourages clusters to have less skills, i.e., clusters that are localized both in time and space.\nSAMDP FOR GRIDWORLD\nWe first illustrate the advantages of SAMDP in a basic gridworld problem (Figure 2). In this task, an agent is placed at the origin (marked in X), where the goal is to reach the green ball and return. The state s \u2208 R3 is given by: s = {x, y, b}, where (x, y) are the coordinates and b \u2208 {0, 1} indicates whether the agent has reached the ball or not. The policy is trained to find skills following the algorithm of Mankowitz, Mann, and Mannor (2014). We are given trajectories of the trained agent, and wish to analyze its behavior by building the state-action graph for all four modeling approaches. For clarity, we plot the graphs on the maze using the coordinates of the state. The MDP graph (Figure 2(a)), consists of a vast number of states. It is also difficult to understand what skills the agent is using. In the SMDP graph (Figure 2(b)), the number of states remain high, however\ncoloring the edges by the skills, helps to understand the agent\u2019s behavior. Unfortunately, producing this graph is seldom possible because we rarely receive information about the skills. On the other hand, abstracting the state space can be done more easily using state aggregation. However, in the AMDP graph (Figure 2(c)), the clusters are not aligned with the played skills because the routes leading to and from the ball overlap. For building the SAMDP model (Figure 2(d)), we transform the state space in a way that disentangles the routes:\n\u03c6(x, y) = { (x, y), if b is 0 (2L\u2212 x, y), if b is 1 ,\nwhere L is the maze width. The transformation \u03c6 flip and translate the states where b = 1. Now that the routes to and from the ball are disentangled, the clusters are perfectly aligned with the skills. Understanding the behavior of the agent is now possible by examining inter-cluster and intra-cluster transitions.\nSAMDPS FOR DQNS\nFeature extraction: We evaluate a pre-trained DQN agent for multiple trajectories with an -greedy policy on three Atari2600 games, Pacman (a game where DQN performs very well), Seaquest (for the opposite reason) and Breakout (for its popularity). We let the trained agent play 120k game states, and record the neural activations of the last hidden layer as well as the Q values. We also keep the time index of each state to be able to find temporal neighbors. Features from other layers can also be used. However, we rely on the results from Zahavy, Zrihem, and Mannor (2016) that showed that the features learned in the last hidden layer capture a spatiotemporal hierarchy and therefore make a good candidate for state aggregation. We then apply t-SNE on the neural activations data, a non-linear dimensionality reduction method that is particularly good at creating a single map that reveals structure at many different scales. We use the two coordinates of the t-SNE map and the value estimation as the MDP state features. Each coordinate is normalized to have zero mean and\nunit variance. We have experimented with other configurations such as using the activations without t-SNE as well as different normalization. However, we found that this configuration results in better SAMDP models. We also use two approximations in the inference stage which we found to work well: 1) overlooking transitions with a small skill length (shorter than 2) and 2) truncating transitions with probability less than 0.1. We only present results for the Breakout game and refer the reader to the supplementary material for results on Pacman and Seaquest. Model Selection: We perform a grid search on two parameters: i) number of clusters K \u2208 [15, 25]. ii) window size w \u2208 [1, 7]. We found that models larger (smaller) than that are too cumbersome (simplistic) to analyze. We select the best model in the following way: we first sort all models by the four evaluation criteria (SAMDP Section, stage 4) from best to worst. Then, we iteratively intersect the p-prefix of all sets (i.e., the first p elements of each set) starting with 1-prefix. We stop when the intersection is nonempty and choose the configuration at the intersection. The resulted SAMDP model for Breakout can be seen in Figure 3. We also measure the p-value of the chosen model. For the null hypothesis, we take the SAMDP model constructed with random clusters. We tested 10000 random SAMDP models, none of which scored better than the chosen model (for all the evaluation criteria). Qualitative Evaluation: Examining the resulting SAMDP (Figure 3) it is interesting to note the sparsity of transitions, which implies low entropy. Inspecting the mean image of each cluster reveals insights about the nature of the skills hiding within and uncovers the policy hierarchy as described in Zahavy, Zrihem, and Mannor (2016). The agent begins to play in low value (blue) clusters (e.g., 1,5,8,9,13,16,18,19). These clusters are well connected between them and are disconnected from other clusters. Once the agent transitions to the \u201dtunnel-digging\u201d option in clusters 4,12,14, it stays in there until it finishes to curve the tunnel, then it transitions to cluster 11. From cluster 11 the agent progresses through the \u201dleft banana\u201d and hops between clusters 2,21,5,10,0,7 and 3 in that order. Model Evaluation: We first measure the VMSE criterion, as defined in Equation 2 (Figure 4, top). We infer v by averaging the DQN value estimates in each cluster: vDQN (cj) =\n1 |Cj | \u2211 i:si\u2208cj v\nDQN (si), and evaluate VSAMDP as defined above. Since the Atari environment is deterministic, the vSAMDP estimate is accurate with respect to the DQN policy. Therefore, the VMSE criterion measures how well the SAMDP model approximates the true MDP. In practice, we observe that the DQN and SAMDP values are very similar; indicating that the SAMDP model fits the data well. Second, we evaluate the greedy policy with respect to the SAMDP value function by: \u03c0greedy(ci) = argmax\nj {R\u03c3i,j + \u03b3 k\u03c3i,j vSAMDP (cj)}. We then measure the correlation between the greedy policy decisions and the trajectory reward. For a given trajectory j we measure P ji : the empirical distribution of choosing the greedy policy at state ci and the cumulative reward Rj . Finally, we present the correlation between these two measures in each state: corri = corr(P j i , R j) in (Figure 4, center). A\npositive correlation indicates that following the greedy policy leads to high reward. Indeed for most of the states, we observe positive correlation, supporting the consistency of the model. The third evaluation is close in spirit to the second one. We partition the data to a train and test sets. We evaluate the greedy policy based on the train set and create two transition matrices T+, T\u2212 using the k top and bottom rewarded trajectories respectively from the test set. We measure the correlation of the greedy policy TG with each of the transition matrices for different values of k (Figure 4 bottom). As clearly seen, the correlation of the greedy policy and the top trajectories is higher than the correlation with the bottom trajectories. Eject Button: The motivation for this experiment stems from the idea of shared autonomy Pitzer et al. (2011). There are domains where errors are dreadful, and performance must be as high as possible. The idea of shared autonomy, is to allow an operator to intervene in the decision loop at critical times. For example, in 20% of commercial flights, the auto-pilot returns the control to the human pilots. In the following experiment, we show how the SAMDP model can help to identify where the agent\u2019s behavior deteriorates. Setup. (a) Evaluate a DQN agent, create a trajectory data set, and evaluate the features for each state (stage 0). (b) Divide the data into two groups: train (100 trajectories) and test (60). then build an SAMDP model (stages 1-4) on the train data. (c) Split the train data to k top and bottom rewarded trajectories T+, T\u2212 and re-infer the model parameters separately for each (stage 3). (d) Project the test data on the SAMDP model (mapping each state to the nearest SAMDP state). (e) Eject when the transitions of the agent are more likely under the T\u2212 matrix rather then under T+ (inspired by the idea of option interruption Sutton, Precup, and Singh (1999)). (f) We average the trajectory reward on (i) the entire test set, and (ii) the un-ejected trajectories sub set. We measure 36% \u00b1 7.7%, 20% \u00b1 8.0%, and 4.7% \u00b1 %1.2 performance gain for Breakout Seaquest and Pacman, respectively. The eject experiment indicates that the SAMDP model can be used to make a given DQN policy robust by identifying when the agent is not going to perform well and return control to a human operator or some other AI agent. Other eject mechanisms are also possible. For example, ejecting by looking at MDP values. However, the Q value is not monotonically decreasing along the trajectory as expected (See Figure 3). The solution we propose is to eject by monitoring transitions and not state values, which makes the MDP impractical in this case because it\u2019s state-action diagram is too large to construct, and too expensive to process.\nDISCUSSION\nSAMDP modeling offers a way to present a trained policy in a concise way by creating abstractions that relate to the spatiotemporal structure of the problem. We showed that by using the right representation, time-aware state aggregation could be used to identify skills. It implies that the crucial step in building an SAMDP is the state aggregation phase. The aggregation depends on the state features and the clustering algorithm at hand.\nIn this work, we presented a basic K-means variant that relies on temporal information. However, other clustering approaches are possible. We also experimented with agglomerative methods but found them to be significantly slower without providing any benefit. We believe that clustering methods that better relate to the topology, such as spectral clustering, would produce the best results. Regarding the state features; in the DQN example, we used the 2D t-SNE map. This map, however, is built under the i.i.d assumption that overlooks the temporal dimension of the problem. An interesting line of future work will be to modify the t-SNE algorithm to take into account temporal distances as well as spatial ones. A tSNE algorithm of this kind may produce 2D maps with even lower entropy which will decrease the aggregation artifacts that affect the quality of the SAMDP model.\nIn this work we analyzed discrete-action policies, however SAMDP can also be applied for continuous-action policies that maintain a value function (since our algorithm depends on it for construction and evaluation), as in the case of actor-critic methods. Another issue we wish to investigate is the question of consistency in re-building an SAMDP. We would like the SAMDP to be unique for a given problem. However, there are several aspects of randomness that may cause divergence. For instance, when using a DQN, randomness exists in the creation of the t-SNE map, and in the clustering phase. From our experience, though, different models built for the same problem are reasonably consistent. In future work, we wish to address the same problem by laying out an optimization problem that will directly account for all of the performance criteria introduced here. It would be interesting to see what clustering method will be drawn out of this process and to compare the principled solution with our current approach.\n", "cats": {"READ": 1.0, "DONTREAD": 0.0}, "reviews": [{"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "The paper combines semi-Markov models with state aggregation models to define a Semi-Aggregated Markov Decision Process (SAMDP). As per the reviews, the need for spatio-temporal representations is recognized as an important problem. Where the work currently falls short is the loose description of framework, which should be formalized for clarity and analysis, and in terms of its contributions and benefits with respect to the broader literature on hierarchical RL methods.\n The anonymous reviewers (and the additional public review/comment) provide a number of clear directions to pursue for the next iteration of this work; indeed, there is strong general enthusiasm for the greater goals (as shared by the relevant body of work in the research community), but the work is seen as requiring further clarity in formalization and presentation.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"DATE": "27 Jan 2017", "TITLE": "Comments and Suggestions", "IS_META_REVIEW": false, "comments": "This work is interesting and one of the few papers out there to address the problem of autonomous skill acquisition in Deep RL. The importance of this problem cannot be overstated since Hierarchical RL is not only about using a meta-controller to appropriately use a specific skill from a repertoire of previously learned subgoals, but also about devising a set of subgoals through experience. Subgoals are inherently tied to the topology of the underlying problem and hence clustering approaches make sense. \n\nI would like to point out that a very similar attempt has been done in one of the past works in this area - ", "OTHER_KEYS": "Aravind Lakshminarayanan"}, {"TITLE": "Great goal, but the PAPER NEEDS MAJOR WORK", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "The paper starts by pointing out the need for methods that perform both state and temporal representation learning for RL and which allow gaining insight into what is being learned (perhaps in order to allow a human operator to intervene if necessary). This is a very important goal from a practical point of view, and it is great to see research in this direction. For this reason, I would like to encourage the authors to pursue this further. However, I am not at all convinced that the current incarnation of this work is the right answer. Part of the issues are more related to presentation, part may require rethinking.\nIn order to get the \"interpretability\", the authors opt for some fairly specific ways of performing abstraction. For example, their skills always start In a single skill initiation state, and likewise end in one state. This seems unnecessarily restrictive, and it is not clear why this restriction is needed (other than convenience). Similarly, clustering is the basis for forming the higher level states, and there is a specific kind of clustering used here. Again, it is not clear why this has to be done via clustering as opposed to other methods. Ensuring temporal coherence in the particular form employed also seems restrictive. There is a reference to supplementary material where some of these choices are explained, but I could not find this in the posted version of the paper. The authors should either explain clearly why these specific choices are necessary, or (even better) try to think if they can be relaxed while still keeping interpretability. \nFrom a presentation point of view, the paper would benefit from formal definitions of AMDP and SAMDP, as well as from formal descriptions of the algorithms employed in constructing these representations (eg Bellman equations for the models, and update rules for the algorithms learning them). While intuitions are given, the math is not precisely stated. The overhead of constructing an SAMDP (computation time and space) should be clarified as well.\nThe experiments are well carried out and it is nice to have both gridworld experiments, where visualization are easy to perform and understand, as well as Atari games (gridworld still have their place despite what other reviewers might say). The results are positive, but because the proposed approach has many moving parts which rely on specific choices, significance and general ease of use are unclear at this point. Perhaps having the complete supplementary would have helped in this respect.\nSmall comment: The two lines after Eq 2 contain typos in the notation and a wrong sign in the equation.", "IS_META_REVIEW": false, "RECOMMENDATION": 4, "DATE": "28 Dec 2016", "REVIEWER_CONFIDENCE": 5}, {"DATE": "27 Dec 2016", "TITLE": "No questions", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1"}, {"TITLE": "Variant of an SMDP model for skill learning ", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "The framework of semi-Markov decision processes (SDMPs) has been long used to model skill learning and temporal abstraction in reinforcement learning. This paper proposes a variant of such a model called a semi-aggregated MDP model. \n\nThe formalism of SAMDP is not defined clearly enough to merit serious attention. The approach is quasi heuristic and explained through examples rather than clear definition. The work also lacks sufficient theoretical rigor. \n\nSimple experiments are proposed using 2D grid worlds to demonstrate skills. Grid worlds have served their purpose long enough in reinforcement learning, and it is time to retire them. More realistic domains are now routinely used and should be used in this paper as well. \n\n", "IS_META_REVIEW": false, "RECOMMENDATION": 4, "DATE": "17 Dec 2016", "REVIEWER_CONFIDENCE": 5}, {"TITLE": "Review of Spatio-Temporal Abstractions .. Paper", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer4", "comments": "The paper presents a method for visualization and analysis of policies from observed trajectories that the policies produce. The method infers higher level skills and clusters states. The result is a simplified, discrete higher-order state and action transition matrix. This model can be used for analysis, modeling, and interpretation. To construct semi-aggregated MDP the authors propose combining ideas for creating semi-MPDs and agregrated-MDPs. The method consists of choosing features, state clustering, skill inference, reward and skill length inference, and model selection. The method was demonstrated on a small grid-world problem, and DQN-trained agent for playing Atari games.\n\nThe authors correctly identify that tools and means for interpretibility of RL methods are important for analysis, and deployment of such methods for real-world applications. This is particularly true in robotics and high-consequence systems.\n\nThe end-result of the presented method is a high-level transition matrix. There is a big body of literature looking into hierarchical RL methods where lower level skills are combined with higher level policies. The presented method has the similar result, but the advantage of the presented method is that it comes up with a structure and analyzes already trained agent, which is very interesting. The paper would benefit from emphasizing this difference, and contrasting with the broader body of literature.\n\nTo build the model, the authors propose combining the ideas from two existing ideas, semi-MPDs and agregrated-MDPs with using modified k-means for state clustering. It appears that the novelty of the presented method is limited. The paper would have been stronger if the authors explicitly stated the contributions over combining existing methods, and better highlighted the practical utility of the method. \n\nThe evaluation section would be made stronger with more analytical results and precise evaluation, showing full strength of the method.\n\nThe paper is difficult to read. To improve readability:\n- The Semi-Aggregated MDP section should include more precise description of the methods. The narrative that builds intuition is welcome. In addition to the existing narrative, algorithms and formulas where applicable should be included as well. \n- The paper should be self-contained. For example, more background on Occams Razor principle should be included.\n- Reduce the number of acronyms, in particular similarly sounding acronyms. Define acronyms before using. \n- Be more clear on the contributions, contrast with relevant literature, and the specific benefits of the presented method.\n- Fix typos, formatting mistakes etc., as they can be distracting for reading. \n\nThe approach of reverse engineering the hierarchy, and learning high-level transition matrix is very interesting and promising. Perhaps the method can be used to outperform single network approach by using the model as an input to more specialized hierarchical trainers and learn complex behaviors more optimally then possible with one large network approach. Unfortunately, the paper falls short in the novelty, precision, and clarity.", "IS_META_REVIEW": false, "RECOMMENDATION": 4, "DATE": "15 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"DATE": "03 Dec 2016", "TITLE": "Clarification", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer4"}, {"DATE": "02 Dec 2016", "TITLE": "Incorrect format for references and inadequate literature survey ", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3"}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "The paper combines semi-Markov models with state aggregation models to define a Semi-Aggregated Markov Decision Process (SAMDP). As per the reviews, the need for spatio-temporal representations is recognized as an important problem. Where the work currently falls short is the loose description of framework, which should be formalized for clarity and analysis, and in terms of its contributions and benefits with respect to the broader literature on hierarchical RL methods.\n The anonymous reviewers (and the additional public review/comment) provide a number of clear directions to pursue for the next iteration of this work; indeed, there is strong general enthusiasm for the greater goals (as shared by the relevant body of work in the research community), but the work is seen as requiring further clarity in formalization and presentation.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"DATE": "27 Jan 2017", "TITLE": "Comments and Suggestions", "IS_META_REVIEW": false, "comments": "This work is interesting and one of the few papers out there to address the problem of autonomous skill acquisition in Deep RL. The importance of this problem cannot be overstated since Hierarchical RL is not only about using a meta-controller to appropriately use a specific skill from a repertoire of previously learned subgoals, but also about devising a set of subgoals through experience. Subgoals are inherently tied to the topology of the underlying problem and hence clustering approaches make sense. \n\nI would like to point out that a very similar attempt has been done in one of the past works in this area - ", "OTHER_KEYS": "Aravind Lakshminarayanan"}, {"TITLE": "Great goal, but the PAPER NEEDS MAJOR WORK", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "The paper starts by pointing out the need for methods that perform both state and temporal representation learning for RL and which allow gaining insight into what is being learned (perhaps in order to allow a human operator to intervene if necessary). This is a very important goal from a practical point of view, and it is great to see research in this direction. For this reason, I would like to encourage the authors to pursue this further. However, I am not at all convinced that the current incarnation of this work is the right answer. Part of the issues are more related to presentation, part may require rethinking.\nIn order to get the \"interpretability\", the authors opt for some fairly specific ways of performing abstraction. For example, their skills always start In a single skill initiation state, and likewise end in one state. This seems unnecessarily restrictive, and it is not clear why this restriction is needed (other than convenience). Similarly, clustering is the basis for forming the higher level states, and there is a specific kind of clustering used here. Again, it is not clear why this has to be done via clustering as opposed to other methods. Ensuring temporal coherence in the particular form employed also seems restrictive. There is a reference to supplementary material where some of these choices are explained, but I could not find this in the posted version of the paper. The authors should either explain clearly why these specific choices are necessary, or (even better) try to think if they can be relaxed while still keeping interpretability. \nFrom a presentation point of view, the paper would benefit from formal definitions of AMDP and SAMDP, as well as from formal descriptions of the algorithms employed in constructing these representations (eg Bellman equations for the models, and update rules for the algorithms learning them). While intuitions are given, the math is not precisely stated. The overhead of constructing an SAMDP (computation time and space) should be clarified as well.\nThe experiments are well carried out and it is nice to have both gridworld experiments, where visualization are easy to perform and understand, as well as Atari games (gridworld still have their place despite what other reviewers might say). The results are positive, but because the proposed approach has many moving parts which rely on specific choices, significance and general ease of use are unclear at this point. Perhaps having the complete supplementary would have helped in this respect.\nSmall comment: The two lines after Eq 2 contain typos in the notation and a wrong sign in the equation.", "IS_META_REVIEW": false, "RECOMMENDATION": 4, "DATE": "28 Dec 2016", "REVIEWER_CONFIDENCE": 5}, {"DATE": "27 Dec 2016", "TITLE": "No questions", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1"}, {"TITLE": "Variant of an SMDP model for skill learning ", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "The framework of semi-Markov decision processes (SDMPs) has been long used to model skill learning and temporal abstraction in reinforcement learning. This paper proposes a variant of such a model called a semi-aggregated MDP model. \n\nThe formalism of SAMDP is not defined clearly enough to merit serious attention. The approach is quasi heuristic and explained through examples rather than clear definition. The work also lacks sufficient theoretical rigor. \n\nSimple experiments are proposed using 2D grid worlds to demonstrate skills. Grid worlds have served their purpose long enough in reinforcement learning, and it is time to retire them. More realistic domains are now routinely used and should be used in this paper as well. \n\n", "IS_META_REVIEW": false, "RECOMMENDATION": 4, "DATE": "17 Dec 2016", "REVIEWER_CONFIDENCE": 5}, {"TITLE": "Review of Spatio-Temporal Abstractions .. Paper", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer4", "comments": "The paper presents a method for visualization and analysis of policies from observed trajectories that the policies produce. The method infers higher level skills and clusters states. The result is a simplified, discrete higher-order state and action transition matrix. This model can be used for analysis, modeling, and interpretation. To construct semi-aggregated MDP the authors propose combining ideas for creating semi-MPDs and agregrated-MDPs. The method consists of choosing features, state clustering, skill inference, reward and skill length inference, and model selection. The method was demonstrated on a small grid-world problem, and DQN-trained agent for playing Atari games.\n\nThe authors correctly identify that tools and means for interpretibility of RL methods are important for analysis, and deployment of such methods for real-world applications. This is particularly true in robotics and high-consequence systems.\n\nThe end-result of the presented method is a high-level transition matrix. There is a big body of literature looking into hierarchical RL methods where lower level skills are combined with higher level policies. The presented method has the similar result, but the advantage of the presented method is that it comes up with a structure and analyzes already trained agent, which is very interesting. The paper would benefit from emphasizing this difference, and contrasting with the broader body of literature.\n\nTo build the model, the authors propose combining the ideas from two existing ideas, semi-MPDs and agregrated-MDPs with using modified k-means for state clustering. It appears that the novelty of the presented method is limited. The paper would have been stronger if the authors explicitly stated the contributions over combining existing methods, and better highlighted the practical utility of the method. \n\nThe evaluation section would be made stronger with more analytical results and precise evaluation, showing full strength of the method.\n\nThe paper is difficult to read. To improve readability:\n- The Semi-Aggregated MDP section should include more precise description of the methods. The narrative that builds intuition is welcome. In addition to the existing narrative, algorithms and formulas where applicable should be included as well. \n- The paper should be self-contained. For example, more background on Occams Razor principle should be included.\n- Reduce the number of acronyms, in particular similarly sounding acronyms. Define acronyms before using. \n- Be more clear on the contributions, contrast with relevant literature, and the specific benefits of the presented method.\n- Fix typos, formatting mistakes etc., as they can be distracting for reading. \n\nThe approach of reverse engineering the hierarchy, and learning high-level transition matrix is very interesting and promising. Perhaps the method can be used to outperform single network approach by using the model as an input to more specialized hierarchical trainers and learn complex behaviors more optimally then possible with one large network approach. Unfortunately, the paper falls short in the novelty, precision, and clarity.", "IS_META_REVIEW": false, "RECOMMENDATION": 4, "DATE": "15 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"DATE": "03 Dec 2016", "TITLE": "Clarification", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer4"}, {"DATE": "02 Dec 2016", "TITLE": "Incorrect format for references and inadequate literature survey ", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3"}]}
{"text": "", "cats": {"READ": 1.0, "DONTREAD": 0.0}, "reviews": [{"IS_META_REVIEW": true, "comments": "This paper proposed COCONET, which is a neural autoregressive model with convolution, to do music composition task. This paper also proposed to use blocked Gibbs sampling instead of the ancestral sampling of the original NADE model to generate better pieces of music. The experimental results showed that the NLL of COCONET is better than the other baselines and the human evaluation task by Amazon\u2019s Mechanical Turk illustrated that the model can generate compelling music.\n\nIn general, I think the paper is good. Using NADE based model with convolution operations on music generation tasks and using blocked Gibbs sampling contains some kind of novelty. However, the novelty of the paper is incremental, since the blocked Gibbs sampling for NADE model is already proposed by Yao et al., (2014) and the using NADE based model for music modeling has also been proposed by Boulanger-Lewandowski  et al., (2012)."}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "This paper applies an existing idea (Yao's block Gibbs sampling of NADE) to a music model. There is also prior art for applying NADE to music. The main novel and interesting result is that block Gibbs sampling (an approximation) actually improves performance, highlighting problems with NADE.\n \n This work is borderline for inclusion. The paper is mainly an application of existing ideas. The implications of the interesting results could perhaps have been explored further for a paper at a meeting on learning representations.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"DATE": "20 Jan 2017", "TITLE": "Response to reviews", "IS_META_REVIEW": false, "comments": "Thank you all for your reviews!  \n\nWe share your concern regarding quantitative comparison to other works, and regarding the generality of our method.  We have looked into the datasets used in prior works, but found that their preprocessing severely degraded the musical structure. For example, the temporal granularity used in Boulanger-Lewandowski et al. is too coarse: in several pieces, discarding all notes that do not have their onsets on the eighth-note grid results in sparse notes with long stretches of silence between them. The pieces become unrecognizable, and (worse) non-musical.  Downsampling or blurring does not work in symbolic music like it does in images: music is more similar to language, and using a coarse grid is analogous to removing words from a sentence and asking a model to learn this new distribution of broken sentences. This turns it into a different task, and makes qualitative judgement of samples not very meaningful.\n\nWe also considered applying our method to image data as suggested. Unlike (symbolic) music which is discrete and intricately structured, the domain of images is smooth and forgiving of small errors. It is plausible that the NADE sampling approach generates fine images (indeed, that's what Yao et al. found), for reasons that don't carry over to our domain of interest, which is music.\n\nThe development of a larger-scale, more diverse and higher-quality MIDI music dataset is a major component of our ongoing project.  We nonetheless believe that the advances we show are of sufficient interest to justify publication at this time.", "OTHER_KEYS": "Cheng-Zhi Anna Huang"}, {"TITLE": "Review", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer5", "comments": "The paper tackles the task of music generation. They use an orderless NADE model for the task of \"fill in the notes\". Given a roll of T timesteps of pitches, they randomly mask out some pitches, and the model is trained to predict the missing notes. This follows how the orderless NADE model can be trained. During sampling, one normally follows an ancestral sampling procedure. For this, an ordering is defined over outputs, and one runs the model on the current input, samples one of the outputs according to the order, adds this output to the next input, and continues this procedure until all outputs have been sampled. The key point of the paper is that this is a bad sampling strategy. Instead, they suggest the strategy of Yao et al. 2014, which uses a blocked Gibbs sampling approach. The blocked Gibbs strategy instead masks N inputs randomly and independently, samples them, and repeats this procedure. The point of this strategy is the make sure the sampling chain mixes well, which will happen for large N. However, since the samples are independent, having a large N gives incoherent samples. Thus, the authors follow an annealed schedule for N, making it smaller over time, which will eventually reduce to ancestral sampling (giving global structure to the sample). They conduct a variety of experiments involving both normal metrics and human evaluations, and find that this blocked Gibbs sampling outperforms other sampling procedures.\n\nThis is a well written paper - great job.\n\nMy main problem with the paper is that having read Uria and Yao, I don't know how much I have learned from this work in the context of this being an ICLR submission. If this was submitted to some computational music / art conference, this paper would be a clear accept. However, for ICLR, I don't see enough novelty compared with previous works this builds upon. Orderless NADE is an established model. The blocked Gibbs sampling and annealing scheme are basically the exact same one used in Yao. Thus, the main novelty of this paper is its application to the music domain, and finding that Yao's method works better for sampling music. This is a good contribution, but more tailored to those working in the music domain. If the authors found that these results also hold for other domains like images (e.g. on CIFAR / tiny Imagenet) and text (e.g. document generation), then I would change my mind and accept this paper for ICLR. Even just trying musical domains other than Bach chorales would be useful. However, as it stands, the experiments are not convincing enough.", "IS_META_REVIEW": false, "RECOMMENDATION": 5, "DATE": "19 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "Review", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer4", "comments": "The paper presents a way to model the distribution of four-part Bach chorales using Convolutional Neural Networks. Furthermore it addresses the task of artificial music generation by sampling from the model using blocked Gibbs sampling and shows\n\nThe CNN model for the distribution seems very appropriate for the data at hand. Also the analysis of the proposed sampling schemes with the analogy between Gibbs sampling and human music composition are very interesting.\nI am not too sure about the evaluation though. Since the reported likelihoods are not directly comparable to previous work, I have difficulties judging the quality of the quantitative results.  For the human evaluation I would like to see the data for the direct comparisons between the models. E.g. How did NADE vs. Bach perform. Also I find the question: \u2018what piece of music do you prefer\u2019 a stronger test than the question \u2018what piece is more musical to you\u2019 because I don\u2019t really know what \u2018musical\u2019 means to the AMT workers.\n\nFinally, while I think the Bach Chorales are interesting musical pieces that deserve to be subject of the analysis but I find it hard to judge how well this modelling approach will transfer to other types of music which might have a very different data distribution.\n\nNevertheless, in conclusion, I believe this is an exciting model for an interesting task that produces non-trivial musical data.\n", "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 3}, {"TITLE": "A nice work that apply NADE-based model to music composition", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "This paper proposed COCONET, which is a neural autoregressive model with convolution, to do music composition task. This paper also proposed to use blocked Gibbs sampling instead of the ancestral sampling of the original NADE model to generate better pieces of music. The experimental results showed that the NLL of COCONET is better than the other baselines and the human evaluation task by Amazon\u2019s Mechanical Turk illustrated that the model can generate compelling music.\n\nIn general, I think the paper is good. Using NADE based model with convolution operations on music generation tasks and using blocked Gibbs sampling contains some kind of novelty. However, the novelty of the paper is incremental, since the blocked Gibbs sampling for NADE model is already proposed by Yao et al., (2014) and the using NADE based model for music modeling has also been proposed by Boulanger-Lewandowski  et al., (2012).\n\n", "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "14 Dec 2016 (modified: 21 Jan 2017)", "REVIEWER_CONFIDENCE": 5}, {"DATE": "10 Dec 2016", "TITLE": "Stopping criterion for sampling procedure and denoising model", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer4"}, {"DATE": "08 Dec 2016", "TITLE": "Nice paper", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3"}, {"IS_META_REVIEW": true, "comments": "This paper proposed COCONET, which is a neural autoregressive model with convolution, to do music composition task. This paper also proposed to use blocked Gibbs sampling instead of the ancestral sampling of the original NADE model to generate better pieces of music. The experimental results showed that the NLL of COCONET is better than the other baselines and the human evaluation task by Amazon\u2019s Mechanical Turk illustrated that the model can generate compelling music.\n\nIn general, I think the paper is good. Using NADE based model with convolution operations on music generation tasks and using blocked Gibbs sampling contains some kind of novelty. However, the novelty of the paper is incremental, since the blocked Gibbs sampling for NADE model is already proposed by Yao et al., (2014) and the using NADE based model for music modeling has also been proposed by Boulanger-Lewandowski  et al., (2012)."}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "This paper applies an existing idea (Yao's block Gibbs sampling of NADE) to a music model. There is also prior art for applying NADE to music. The main novel and interesting result is that block Gibbs sampling (an approximation) actually improves performance, highlighting problems with NADE.\n \n This work is borderline for inclusion. The paper is mainly an application of existing ideas. The implications of the interesting results could perhaps have been explored further for a paper at a meeting on learning representations.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"DATE": "20 Jan 2017", "TITLE": "Response to reviews", "IS_META_REVIEW": false, "comments": "Thank you all for your reviews!  \n\nWe share your concern regarding quantitative comparison to other works, and regarding the generality of our method.  We have looked into the datasets used in prior works, but found that their preprocessing severely degraded the musical structure. For example, the temporal granularity used in Boulanger-Lewandowski et al. is too coarse: in several pieces, discarding all notes that do not have their onsets on the eighth-note grid results in sparse notes with long stretches of silence between them. The pieces become unrecognizable, and (worse) non-musical.  Downsampling or blurring does not work in symbolic music like it does in images: music is more similar to language, and using a coarse grid is analogous to removing words from a sentence and asking a model to learn this new distribution of broken sentences. This turns it into a different task, and makes qualitative judgement of samples not very meaningful.\n\nWe also considered applying our method to image data as suggested. Unlike (symbolic) music which is discrete and intricately structured, the domain of images is smooth and forgiving of small errors. It is plausible that the NADE sampling approach generates fine images (indeed, that's what Yao et al. found), for reasons that don't carry over to our domain of interest, which is music.\n\nThe development of a larger-scale, more diverse and higher-quality MIDI music dataset is a major component of our ongoing project.  We nonetheless believe that the advances we show are of sufficient interest to justify publication at this time.", "OTHER_KEYS": "Cheng-Zhi Anna Huang"}, {"TITLE": "Review", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer5", "comments": "The paper tackles the task of music generation. They use an orderless NADE model for the task of \"fill in the notes\". Given a roll of T timesteps of pitches, they randomly mask out some pitches, and the model is trained to predict the missing notes. This follows how the orderless NADE model can be trained. During sampling, one normally follows an ancestral sampling procedure. For this, an ordering is defined over outputs, and one runs the model on the current input, samples one of the outputs according to the order, adds this output to the next input, and continues this procedure until all outputs have been sampled. The key point of the paper is that this is a bad sampling strategy. Instead, they suggest the strategy of Yao et al. 2014, which uses a blocked Gibbs sampling approach. The blocked Gibbs strategy instead masks N inputs randomly and independently, samples them, and repeats this procedure. The point of this strategy is the make sure the sampling chain mixes well, which will happen for large N. However, since the samples are independent, having a large N gives incoherent samples. Thus, the authors follow an annealed schedule for N, making it smaller over time, which will eventually reduce to ancestral sampling (giving global structure to the sample). They conduct a variety of experiments involving both normal metrics and human evaluations, and find that this blocked Gibbs sampling outperforms other sampling procedures.\n\nThis is a well written paper - great job.\n\nMy main problem with the paper is that having read Uria and Yao, I don't know how much I have learned from this work in the context of this being an ICLR submission. If this was submitted to some computational music / art conference, this paper would be a clear accept. However, for ICLR, I don't see enough novelty compared with previous works this builds upon. Orderless NADE is an established model. The blocked Gibbs sampling and annealing scheme are basically the exact same one used in Yao. Thus, the main novelty of this paper is its application to the music domain, and finding that Yao's method works better for sampling music. This is a good contribution, but more tailored to those working in the music domain. If the authors found that these results also hold for other domains like images (e.g. on CIFAR / tiny Imagenet) and text (e.g. document generation), then I would change my mind and accept this paper for ICLR. Even just trying musical domains other than Bach chorales would be useful. However, as it stands, the experiments are not convincing enough.", "IS_META_REVIEW": false, "RECOMMENDATION": 5, "DATE": "19 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "Review", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer4", "comments": "The paper presents a way to model the distribution of four-part Bach chorales using Convolutional Neural Networks. Furthermore it addresses the task of artificial music generation by sampling from the model using blocked Gibbs sampling and shows\n\nThe CNN model for the distribution seems very appropriate for the data at hand. Also the analysis of the proposed sampling schemes with the analogy between Gibbs sampling and human music composition are very interesting.\nI am not too sure about the evaluation though. Since the reported likelihoods are not directly comparable to previous work, I have difficulties judging the quality of the quantitative results.  For the human evaluation I would like to see the data for the direct comparisons between the models. E.g. How did NADE vs. Bach perform. Also I find the question: \u2018what piece of music do you prefer\u2019 a stronger test than the question \u2018what piece is more musical to you\u2019 because I don\u2019t really know what \u2018musical\u2019 means to the AMT workers.\n\nFinally, while I think the Bach Chorales are interesting musical pieces that deserve to be subject of the analysis but I find it hard to judge how well this modelling approach will transfer to other types of music which might have a very different data distribution.\n\nNevertheless, in conclusion, I believe this is an exciting model for an interesting task that produces non-trivial musical data.\n", "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 3}, {"TITLE": "A nice work that apply NADE-based model to music composition", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "This paper proposed COCONET, which is a neural autoregressive model with convolution, to do music composition task. This paper also proposed to use blocked Gibbs sampling instead of the ancestral sampling of the original NADE model to generate better pieces of music. The experimental results showed that the NLL of COCONET is better than the other baselines and the human evaluation task by Amazon\u2019s Mechanical Turk illustrated that the model can generate compelling music.\n\nIn general, I think the paper is good. Using NADE based model with convolution operations on music generation tasks and using blocked Gibbs sampling contains some kind of novelty. However, the novelty of the paper is incremental, since the blocked Gibbs sampling for NADE model is already proposed by Yao et al., (2014) and the using NADE based model for music modeling has also been proposed by Boulanger-Lewandowski  et al., (2012).\n\n", "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "14 Dec 2016 (modified: 21 Jan 2017)", "REVIEWER_CONFIDENCE": 5}, {"DATE": "10 Dec 2016", "TITLE": "Stopping criterion for sampling procedure and denoising model", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer4"}, {"DATE": "08 Dec 2016", "TITLE": "Nice paper", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3"}]}
{"text": "1 INTRODUCTION\nDeep Reinforcement Learning has achieved super-human performance in fully observable environments, e.g., in Atari Games [Mnih et al. (2015)] and Computer Go [Silver et al. (2016)]. Recently, Asynchronous Advantage Actor-Critic (A3C) [Mnih et al. (2016)] model shows good performance for 3D environment exploration, e.g. labyrinth exploration. However, in general, to train an agent in a partially observable 3D environment from raw frames remains an open challenge. Direct application of A3C to competitive 3D scenarios, e.g. 3D games, is nontrivial, partly due to sparse and long-term rewards in such scenarios.\nDoom is a 1993 First-Person Shooter (FPS) game in which a player fights against other computercontrolled agents or human players in an adversarial 3D environment. Previous works on FPS AI [van Waveren (2001)] focused on using hand-tuned state machines and privileged information, e.g., the geometry of the map, the precise location of all players, to design playable agents. Although state-machine is conceptually simple and computationally efficient, it does not operate like human players, who only rely on visual (and possibly audio) inputs. Also, many complicated situations require manually-designed rules which could be time-consuming to tune.\nIn this paper, we train an AI agent in Doom with a framework that based on A3C with convolutional neural networks (CNN). This model uses only the recent 4 frames and game variables from the AI side, to predict the next action of the agent and the value of the current situation. We follow the curriculum learning paradigm [Bengio et al. (2009); Jiang et al. (2015)]: start from simple tasks and then gradually try harder ones. The difficulty of the task is controlled by a variety of parameters in Doom environment, including different types of maps, strength of the opponents and the design of the reward function. We also develop adaptive curriculum training that samples from a varying distribution of tasks to train the model, which is more stable and achieves higher score than A3C with the same number of epoch. As a result, our trained agent, named F1, won the champion in Track 1 of ViZDoom Competition 1 by a large margin.\nThere are many contemporary efforts on training a Doom AI based on the VizDoom platform [Kempka et al. (2016)] since its release. Arnold [Lample & Chaplot (2016)] also uses game frames and trains an action network using Deep Recurrent Q-learning [Hausknecht & Stone (2015)], and a navigation network with DQN [Mnih et al. (2015)]. However, there are several important differences. To predict the next action, they use a hybrid architecture (CNN+LSTM) that involves more complicated training procedure. Second, in addition to game frames, they require internal\n1http://vizdoom.cs.put.edu.pl/competition-cig-2016/results\ngame status about the opponents as extra supervision during training, e.g., whether enemy is present in the current frame. IntelAct [Dosovitskiy & Koltun (2017)] models the Doom AI bot training in a supervised manner by predicting the future values of game variables (e.g., health, amount of ammo, etc) and acting accordingly. In comparison, we use curriculum learning with asynchronized actorcritic models and use stacked frames (4 most recent frames) and resized frames to mimic short-term memory and attention. Our approach requires no opponent\u2019s information, and is thus suitable as a general framework to train agents for close-source games.\nIn VizDoom AI Competition 2016 at IEEE Computational Intelligence And Games (CIG) Conference2, our AI won the champion of Track1 (limited deathmatch with known map), and IntelAct won the champion of Track2 (full deathmatch with unknown maps). Neither of the two teams attends the other track. Arnold won the second places of both tracks and CLYDE [Ratcliffe et al. (2017)] won the third place of Track1.\n2 THE ACTOR-CRITIC MODEL\nThe goal of Reinforcement Learning (RL) is to train an agent so that its behavior maximizes/minimizes expected future rewards/penalties it receives from a given environment [Sutton & Barto (1998)]. Two functions play important roles: a value function V (s) that gives the expected reward of the current state s, and a policy function \u03c0(a|s) that gives a probability distribution on the candidate actions a for the current state s. Getting the groundtruth value of either function would largely solve RL: the agent just follows \u03c0(a|s) to act, or jumps in the best state provided by V (s) when the number of candidate next states is finite and practically enumerable. However, neither is trivial.\nActor-critic models [Barto et al. (1983); Sutton (1984); Konda & Tsitsiklis (1999); Grondman et al. (2012)] aim to jointly estimate V (s) and \u03c0(a|s): from the current state st, the agent explores the environment by iteratively sampling the policy function \u03c0(at|st;w\u03c0) and receives positive/negative reward, until the terminal state or a maximum number of iterations are reached. The exploration gives a trajectory {(st, at, rt), (st+1, at+1, rt+1), \u00b7 \u00b7 \u00b7 }, from which the policy function and value function are updated. Specifically, to update the value function, we use the expected reward Rt along the trajectory as the ground truth; to update the policy function, we encourage actions that lead to high rewards, and penalize actions that lead to low rewards. To determine whether an action leads to high- or low-rewarding state, a reference point, called baseline [Williams (1992)], is usually needed. Using zero baseline might increase the estimation variance. [Peters & Schaal (2008)] gives a way to estimate the best baseline (a weighted sum of cumulative rewards) that minimizes the variance of the gradient estimation, in the scenario of episodic REINFORCE [Williams (1992)].\nIn actor-critic frameworks, we pick the baseline as the expected cumulative reward V (s) of the current state, which couples the two functions V (s) and \u03c0(a|s) together in the training, as shown in Fig. 1. Here the two functions reinforce each other: a correct \u03c0(a|s) gives high-rewarding trajectories which update V (s) towards the right direction; a correct V (s) picks out the correct actions for \u03c0(a|s) to reinforce. This mutual reinforcement behavior makes actor-critic model converge faster, but is also prone to converge to bad local minima, in particular for on-policy models that follow the very recent policy to sample trajectory during training. If the experience received by the agent in consecutive batches is highly correlated and biased towards a particular subset of the environment, then both \u03c0(a|s) and V (s) will be updated towards a biased direction and the agent may never see\n2http://vizdoom.cs.put.edu.pl/competition-cig-2016\nthe whole picture. To reduce the correlation of game experience, Asynchronous Advantage ActorCritic Model [Mnih et al. (2016)] runs independent multiple threads of the game environment in parallel. These game instances are likely uncorrelated, therefore their experience in combination would be less biased.\nFor on-policy models, the same mutual reinforcement behavior will also lead to highly-peaked \u03c0(a|s) towards a few actions (or a few fixed action sequences), since it is always easy for both actor and critic to over-optimize on a small portion of the environment, and end up \u201cliving in their own realities\u201d. To reduce the problem, [Mnih et al. (2016)] added an entropy term to the loss to encourage diversity, which we find to be critical. The final gradient update rules are listed as follows:\nw\u03c0 \u2190 w\u03c0 + \u03b1(Rt \u2212 V (st))\u2207w\u03c0 log \u03c0(at|st) + \u03b2\u2207w\u03c0H(\u03c0(\u00b7|st)) (1) wV \u2190 wV \u2212 \u03b1\u2207wV (Rt \u2212 V (st)) 2 (2)\nwhere Rt = \u2211 T t\u2032=t \u03b3t \u2032 \u2212trt\u2032 is the expected discounted reward at time t and \u03b1, \u03b2 are the learning rate. In this work, we use Huber loss instead of the L2 loss in Eqn. 2.\nArchitecture. While [Mnih et al. (2016)] keeps a separate model for each asynchronous agent and perform model synchronization once in a while, we use an alternative approach called BatchA3C, in which all agents act on the same model and send batches to the main process for gradient descent optimization. The agents\u2019 models are updated after each gradient update. Note that the contemporary work GA3C [Babaeizadeh et al. (2017)] also proposes a similar architecture. In their architecture, there is a prediction queue that collects agents\u2019 experience and sends them to multiple predictors, and a training queue that collects experience to feed the optimization.\n3 DOOM AS A REINFORCEMENT LEARNING PLATFORM\nIn Doom, the player controls the agent to fight against enemies in a 3D environment (e.g., in a maze). The agent can only see the environment from his viewpoint and thus receives partial information upon which it makes decisions. On modern computers, the original Doom runs in thousands of frames per second, making it suitable as a platform for training AI agent. ViZDoom [Kempka et al. (2016)] is an open-source platform that offers programming interface to communicate with Doom engine, ZDoom3. From the interface, users can obtain current frames of the game, and control the agent\u2019s action. ViZDoom offers much flexibility, including:\nRich Scenarios. Many customized scenarios are made due to the popularity of the game, offering a variety of environments to train from. A scenario consists of many components, including 2D maps for the environment, scripts to control characters and events. Open-source tools, such as\n3https://zdoom.org/\nSLADE4, are also widely available to build new scenarios. We built our customized map (Fig. 2(b)) for training.\nGame variables. In addition to image frames, ViZDoom environment also offers many games variables revealing the internal state of the game. This includes HEALTH, AMMO ? (agent\u2019s health and ammunition), FRAG COUNT (current score) and so on. ViZDoom also offers USER? variables that are computed on the fly via scenario scripts. These USER? variables can provide more information of the agent, e.g., their spatial locations. Enemy information could also be obtained by modifying ViZDoom [Lample & Chaplot (2016)]. Such information is used to construct a reward function, or as a direct supervision to accelerate training [Lample & Chaplot (2016)].\nBuilt-in bots. Built-in bots can be inserted in the battle. They are state machines with privileged information over the map and the player, which results in apparently decent intelligence with minimal computational cost. By competing against built-in bots, the agent learns to improve.\nEvaluation Criterion. In FPS games, to evaluate their strength, multiple AIs are placed to a scenario for a deathmatch, in which every AI plays for itself against the remaining AIs. Frags per episode, the number of kills minus the number of suicides for the agent in one round of game, is often used as a metric. An AI is stronger if its frags is ranked higher against others. In this work, we use an episode of 2-minute game time (4200 frames in total) for all our evaluations unless noted otherwise.\n4 METHOD\n4.1 NETWORK ARCHITECTURE\nWe use convolutional neural networks to extract features from the game frames and then combine its output representation with game variables. Fig. 3 shows the network architecture and Tbl. 1 gives the parameters. It takes the frames as the input (i.e., the state s) and outputs two branches, one that outputs the value function V (s) by regression, while the other outputs the policy function \u03c0(s|a) by a regular softmax. The parameters of the two functions are shared before the branch.\nFor input, we use the most recent 4 frames plus the center part of them, scaled to the same size (120 \u00d7 120). Therefore, these centered \u201cattention frames\u201d have higher resolution than regular game frames, and greatly increase the aiming accuracy. The policy network will give 6 actions, namely MOVE FORWARD, MOVE LEFT, MOVE RIGHT, TURN LEFT, TURN RIGHT, and ATTACK. We found other on-off actions (e.g., MOVE BACKWARD) offered by ViZDoom less important. After feature extraction by convolutional network, game variables are incorporated. This includes the agent\u2019s Health (0-100) and Ammo (how many bullets left). They are related to AI itself and thus legal in the game environment for training, testing and ViZDoom AI competition.\n4.2 TRAINING PIPELINE\nOur training procedure is implemented with TensorFlow [Abadi et al. (2016)] and tensorpack5. We open 255 processes, each running one Doom instance, and sending experience (st, at, rt) to the\n4http://slade.mancubus.net/ 5https://github.com/ppwwyyxx/tensorpack\nmain process which runs the training procedure. The main process collects frames from different game instances to create batches, and optimizes on these batches asynchronously on one or more GPUs using Eqn. 1 and Eqn. 2. The frames from different processes running independent game instances, are likely to be uncorrelated, which stabilizes the training. This procedure is slightly different from the original A3C, where each game instance collects their own experience and updates the parameters asynchronously.\nDespite the use of entropy term, we still find that \u03c0(\u00b7|s) is highly peaked. Therefore, during trajectory exploration, we encourage exploration by the following changes: a) multiply the policy output of the network by an exploration factor (0.2) before softmax b) uniformly randomize the action for 10% random frames.\nAs mentioned in [Kempka et al. (2016)], care should be taken for frame skips. Small frame skip introduces strong correlation in the training set, while big frame skip reduces effective training samples. We set frame skip to be 3. We choose 640x480 as the input frame resolution and do not use high aspect ratio resolution [Lample & Chaplot (2016)] to increase the field of view.\nWe use Adam [Kingma & Ba (2014)] with \u01eb = 10\u22123 for training. Batch size is 128, discount factor \u03b3 = 0.99, learning rate \u03b1 = 10\u22124 and the policy learning rate \u03b2 = 0.08\u03b1. The model is trained from scratch. The training procedure runs on Intel Xeon CPU E5-2680v2 at 2. 80GHz, and 2 TitanX GPUs. It takes several days to obtain a decent result. Our final model, namely the F1 bot, is trained for around 3 million mini-batches on multiple different scenarios.\n4.3 CURRICULUM LEARNING\nWhen the environment only gives very sparse rewards, or adversarial, A3C takes a long time to converge to a satisfying solution. A direct training with A3C on the map CIGTrack1 with 8 builtin bots does not yield sensible performance. To address this, we use curriculum learning [Bengio et al. (2009)] that trains an agent with a sequence of progressively more difficult environments. By varying parameters in Doom (Sec. 3), we could control its difficulty level.\nReward Shaping. Reward shaping has been shown to be an effective technique to apply reinforcement learning in a complicated environment with delayed reward [Ng et al. (1999); Devlin et al. (2011)]. In our case, besides the basic reward for kills (+1) and death (-1), intermediate rewards are used as shown in Tbl. 2. We penalize agent with a living state, encouraging it to explore and encounter more enemies. health loss and ammo loss place linear reward for a decrement of health and ammunition. ammo pickup and health pickup place reward for picking up these two items. In addition, there is extra reward for picking up ammunition when in need (e.g. almost out of ammo). dist penalty and dist reward push the agent away from the previous locations, encouraging it to explore. The penalty is applied every action, when the displacement of the bot relative to the last state is less than a threshold dist penalty thres. And dist reward is applied for every unit displacement the agent makes. Similar to [Lample & Chaplot (2016)], the displacement information is computed from the ground truth location variables provided by Doom engine, and will not be used in the competition. However, unlike [Lample & Chaplot (2016)] that uses enemy-in-sight signal for training, locations can be extracted directly from USER? variables, or can easily be computed roughly with action history.\nCurriculum Design. We train the bot on FlatMap that contains a simple square with a few pillars (Fig. 2(a)) with several curricula (Tbl. 3), and then proceed to CIGTrack1. For each map, we design curricula by varying the strength of built-in bots, i.e., their moving speed, initial health and initial weapon. Our agent always uses RocketLauncher as its only weapon. Training on FlatMap leads to a capable initial model which is quickly adapted to more complicated maps. As shown in Tbl. 2, for CIGTrack1 we increase dist penalty thres to keep the agent moving, and increase num bots so that the agent encounters more enemies per episode.\nAdaptive Curriculum. In addition to staged curriculum learning, we also design adaptive curriculum learning by assigning a probability distribution on different levels for each thread that runs a Doom instance. The probability distribution shifts towards more difficult curriculum when the agent performs well on the current distribution, and shifts towards easier level otherwise. We consider the agent to perform well if its frag count is greater than 10 points.\n4.4 POST-TRAINING RULES\nFor a better performance in the competition, we also put several rules to process the action given by the trained policy network, called post-training (PT) rules. There are two sets of buttons in ViZDoom: on-off buttons and delta buttons. While on-off button maps to the binary states of a keystroke (e.g., pressing the up arrow key will move the agent forward), delta buttons mimic the mouse behavior and could act faster in certain situations. Therefore, we setup rules that detect the intention of the agent and accelerate with delta button. For example, when the agent turns by invoking TURN LEFT repeatedly, we convert its action to TURN LEFT RIGHT DELTA for acceleration. Besides, the trained model might get stuck in rare situations, e.g., keep moving forward but blocked by an explosive bucket. We also designed rules to detect and fix them.\n5 EXPERIMENT\nIn this section, we show the training procedure (Sec. 5.1), evaluate our AIs with ablation analysis (Sec. 5.2) and ViZDoom AI Competition (Sec. 5.3). We mainly compare among three AIs: (1) F1Pre, the bot trained with FlatMap only, (2) F1Plain, the bot trained on both FlatMap and CIGTrack1, but without post-training rules, and (3) the final F1 bot that attends competition.\n5.1 CURRICULUM LEARNING ON FLATMAP\nFig. 4 shows that the curriculum learning increases the performance of the agents over all levels. When an agent becomes stronger in the higher level of class, it is also stronger in the lower level of class without overfitting. Fig. 5 shows comparison between adaptive curriculum learning with pure A3C. We can see that pure A3C can learn on FlatMap but is slower. Moreover, in CIGTrack1, a direct application of A3C does not yield sensible performance.\n5.2 ABLATION ANALYSIS\nVisualization. Fig. 6 shows the visualization of the first convolutional layer of the trained AI agent. We could see that the convolutional kernels of the current frame is less noisy than the kernels of previous frames. This means that the agent makes the most use of the current frames.\nEffect of History Frames. Interestingly, while the agent focuses on the current frame, it also uses motion information. For this, we use (1) 4 duplicated current frames (2) 4 recent frames in reverse order, as the input. This gives 8.50 and 2.39 mean frags, compared to 10.34 in the normal case, showing that the agent heavily uses the motion information for better decision. In particular, the bot is totally confused with the reversed motion feature. Detailed results are shown in Tbl. 5.\nPost-training Rules. Tbl. 5 shows that the post-training rules improve the performance. As a future work, an end-to-end training involving delta buttons could make the bot better.\nInternal Tournament. We also evaluate our AIs with internal tournaments (Tbl. 4). All our bots beat the performance of built-in bots by a large margin, even though they use privileged information. F1Pre, trained with only FlatMap, shows decent performance, but is not as good as the models trained with both FlatMap and CIGTrack1. The final bot F1 performs the best.\nBehaviors. Visually, the three bots behave differently. F1Pre is a bit overtrained in FlatMap and does not move too often, but when it sees enemies, even faraway, it will start to shoot. Occasionally it will move to the corner and pick medkits. In CIGTrack1, F1Pre stays in one place and ambushes opponents who pass by. On the other hand, F1Plain and F1 always move forwards and turn at the corner. As expected, F1 moves and turns faster.\nTactics All bots develop interesting local tactics when exchanging fire with enemy: they slide around when shooting the enemy. This is quite effective for dodging others\u2019 attack. Also when they shoot the enemy, they usually take advantage of the splashing effect of rocket to cause additional damage for enemy, e.g., shooting the wall when the enemy is moving. They do not pick ammunition too often, even if they can no longer shoot. However, such disadvantage is mitigated by the nature of deathmatch: when a player dies, it will respawn with ammunition. We also check states with highest/lowest estimated future value V (s) over a 10-episode evaluation of F1 bot, from which we can speculate its tactics. The highest value is V = 0.97 when the agent fired, and about to hit the enemy. One low value is V = \u22120.44, ammo = 0, when the agent encountered an enemy at the corner but is out of ammunition. Both cases are reasonable.\n5.3 COMPETITION\nWe attended the ViZDoom AI Competition hosted by IEEE CIG. There are 2 tracks in the competition. Track 1 (Limited Deathmatch) uses a known map and fixed weapons, while Track 2 (Full Deathmatch) uses 3 unknown maps and a variety of weapons. Each bot fights against all others for 12 rounds of 10 minutes each. Due to server capacity, each bot skips one match in the first 9 rounds. All bots are supposed to run in real-time (>35fps) on a GTX960 GPU.\nOur F1 bot won 10 out of 11 attended games and won the champion for Track 1 by a large margin. We have achieved 559 frags, 35.4% higher than 413 frags achieved by Arnold [Lample & Chaplot (2016)], that uses extra game state for model training. On the other hand, IntelAct [Dosovitskiy & Koltun (2017)] won Track 2. The full videos for the two tracks have been released67, as well as an additional game between Human and AIs8. Our bot behaves reasonable and very human-like in Track 1. In the match between Human and AIs, our bot was even ahead of the human player for a short period (6:30 to 7:00).\n6 CONCLUSION\nTeaching agents to act properly in complicated and adversarial 3D environment is a very challenging task. In this paper, we propose a new framework to train a strong AI agent in a First-Person Shooter (FPS) game, Doom, using a combination of state-of-the-art Deep Reinforcement Learning and Curriculum Training. Via playing against built-in bots in a progressive manner, our bot wins the champion of Track1 (known map) in ViZDoom AI Competition. Furthermore, it learns to use motion features and build its own tactics during the game, which is never taught explicitly.\nCurrently, our bot is still an reactive agent that only remembers the last 4 frames to act. Ideally, a bot should be able to build a map from an unknown environment and localize itself, is able to have a global plan to act, and visualize its reasoning process. We leave them to future works.\n", "cats": {"READ": 1.0, "DONTREAD": 0.0}, "reviews": [{"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "This paper provides a number of performance enhancements inspired by domain knowledge. Taken together, these produce a compelling system that has shown itself to be the best-in-class as per the related competition.\n Experts agree that the authors do a good job at justifying the majority of the design decisions.\n \n pros:\n - insights into the SOTA Doom player\n \n cons:\n - lack of pure technical novelty: the various elements have existed previously\n \n This paper comes down to a matter of taste in terms of appreciation of SOTA systems or technical novelty.\n With the code being released, I believe that this work will have impact as a benchmark, and as a guidebook\n as to how features can be combined for SOTA performance on FPS-style scenarios.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"DATE": "14 Jan 2017", "TITLE": "Rebuttal", "IS_META_REVIEW": false, "comments": "We thank the reviewers for their insightful comments!\n\nAll reviewers agree that this paper makes a solid contribution with good experimental results. It is not uncommon to see application-oriented papers using a combination of multiple techniques to achieve strong performance. This category covers many seminar works, e.g., deep reinforcement learning for Atari games (applying deep models to traditional Q-learning), or even AlphaGo (supervised learning, policy gradient, value function, Monte-Carlo Tree Search, self-play). It may be a bit shortsighted to judge such strong performing papers with a single criterion.\n\nConfusion about the domain:\nReviewer3 mentions that the paper \"basically applies A3C to 3D spatial navigation tasks.\", which is not true. In the deathmatch game of Doom, multiple players explore the maze and fight against each other to get a higher score, which is defined as #kills - #suicide. In this task, part of the goal is to learn anti-enemy tactics (e.g., dodging the rocket shot from the enemy, e.g., video:  ", "OTHER_KEYS": "Yuandong Tian"}, {"TITLE": "Final Review: Practical techniques for learning in 3D shooters; a lot of domain knowledge, and a few iffy claims", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "The paper describes approaches taken to train learning agents for the 3D game Doom. The authors propose a number of performance enhancements (curriculum learning, attention (zoomed-in centered) frames, reward shaping, game variables, post-training rules) inspired by domain knowledge.\n\nThe enhancements together lead to a clear win as demonstrated by the competition results. From Fig 4, the curriculum learning clearly helps with learning over increasingly difficult settings. A nice result is that there is no overfitting to the harder classes once they have learned (probably because the curriculum is health and speed). The authors conclude from Fig 5 that the adaptive curriculum is better and more stable that pure A3C; however, this is a bit of a stretch given that graph. They go on to say that Pure A3C doesn't learn at all in the harder map but then show no result/graph to back this claim. Tbl 5 shows a clear benefit of the post-training rules.\n\nIf the goal is to solve problems like these (3D shooters), then this paper makes a significant contribution in that it shows which techniques are practical for solving the problem and ultimately improving performance in these kinds of tasks. Still, I am just not excited about this paper, mainly because it relies so heavily of many sources of domain knowledge, it is quite far from the pure reinforcement learning problem. The results are relatively unsurprising. Maybe they are novel for this problem, though.\n\nI'm not sure we can realistically draw any conclusions about Figure 6 in the paper's current form. I recommend the authors increase the resolution or run some actual metrics to determine the fuzziness/clarity of each row/image: something more concrete than an arrow of already low-resolution images.\n\n--- Added after rebuttal:\n\nI still do not see any high-res images for Figure 6 or any link to them, but I trust that the authors will add them if accepted.\n", "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "19 Dec 2016 (modified: 23 Jan 2017)", "REVIEWER_CONFIDENCE": 4}, {"OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "TITLE": "No Title", "comments": "This paper basically applies A3C to 3D spatial navigation tasks. \n\n- This is not the first time A3C has been applied to 3D navigation. In fact the original paper reported these experiments. Although the experimental results are great, I am not sure if this paper has any additional insights to warrant itself as a conference paper. It might make more sense as a workshop paper\n\n-  Are the graphs in Fig 5 constructed using a single hyper-parameter sweep? I think the authors should report results with many random initializations to make the comparisons more robust\n\n- Overall the two main ideas in this paper -- A3C and curriculums -- are not really novel but the authors do make use of them in a real system. ", "SOUNDNESS_CORRECTNESS": 3, "ORIGINALITY": 4, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "RECOMMENDATION": 4, "DATE": "15 Dec 2016", "REVIEWER_CONFIDENCE": 5}, {"OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "TITLE": "No Title", "comments": "This is a solid paper that applies A3C to Doom, enhancing it with a collection of tricks so as to win one of the VizDoom competitions. I think it is fair to expect the competition aspect to overshadow the more scientific approach of justifying every design decision in isolation, but in fact the authors do a decent job at the latter.\n\nTwo of my concerns have remained unanswered (see AnonReviewer2, below). \n\nIn addition, the citation list is rather thin, for example reward shaping has a rich literature, as do incrementally more difficult task setups, dating back at least to Mark Ring\u2019s work in the 1990s. There has also been a lot of complementary work on other FPS games. I\u2019m not asking that the authors do any direct comparisons, but to give the reader a sense of context in which to place this.", "ORIGINALITY": 4, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "15 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"DATE": "03 Dec 2016", "TITLE": "Huber loss", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1"}, {"TITLE": "clarify Fig 4, and hyper-parameter/shaping robustness", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "", "ORIGINALITY": 4, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "DATE": "02 Dec 2016"}, {"OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "TITLE": "experiment robustness", "comments": "", "SOUNDNESS_CORRECTNESS": 3, "ORIGINALITY": 4, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "DATE": "01 Dec 2016"}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "This paper provides a number of performance enhancements inspired by domain knowledge. Taken together, these produce a compelling system that has shown itself to be the best-in-class as per the related competition.\n Experts agree that the authors do a good job at justifying the majority of the design decisions.\n \n pros:\n - insights into the SOTA Doom player\n \n cons:\n - lack of pure technical novelty: the various elements have existed previously\n \n This paper comes down to a matter of taste in terms of appreciation of SOTA systems or technical novelty.\n With the code being released, I believe that this work will have impact as a benchmark, and as a guidebook\n as to how features can be combined for SOTA performance on FPS-style scenarios.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"DATE": "14 Jan 2017", "TITLE": "Rebuttal", "IS_META_REVIEW": false, "comments": "We thank the reviewers for their insightful comments!\n\nAll reviewers agree that this paper makes a solid contribution with good experimental results. It is not uncommon to see application-oriented papers using a combination of multiple techniques to achieve strong performance. This category covers many seminar works, e.g., deep reinforcement learning for Atari games (applying deep models to traditional Q-learning), or even AlphaGo (supervised learning, policy gradient, value function, Monte-Carlo Tree Search, self-play). It may be a bit shortsighted to judge such strong performing papers with a single criterion.\n\nConfusion about the domain:\nReviewer3 mentions that the paper \"basically applies A3C to 3D spatial navigation tasks.\", which is not true. In the deathmatch game of Doom, multiple players explore the maze and fight against each other to get a higher score, which is defined as #kills - #suicide. In this task, part of the goal is to learn anti-enemy tactics (e.g., dodging the rocket shot from the enemy, e.g., video:  ", "OTHER_KEYS": "Yuandong Tian"}, {"TITLE": "Final Review: Practical techniques for learning in 3D shooters; a lot of domain knowledge, and a few iffy claims", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "The paper describes approaches taken to train learning agents for the 3D game Doom. The authors propose a number of performance enhancements (curriculum learning, attention (zoomed-in centered) frames, reward shaping, game variables, post-training rules) inspired by domain knowledge.\n\nThe enhancements together lead to a clear win as demonstrated by the competition results. From Fig 4, the curriculum learning clearly helps with learning over increasingly difficult settings. A nice result is that there is no overfitting to the harder classes once they have learned (probably because the curriculum is health and speed). The authors conclude from Fig 5 that the adaptive curriculum is better and more stable that pure A3C; however, this is a bit of a stretch given that graph. They go on to say that Pure A3C doesn't learn at all in the harder map but then show no result/graph to back this claim. Tbl 5 shows a clear benefit of the post-training rules.\n\nIf the goal is to solve problems like these (3D shooters), then this paper makes a significant contribution in that it shows which techniques are practical for solving the problem and ultimately improving performance in these kinds of tasks. Still, I am just not excited about this paper, mainly because it relies so heavily of many sources of domain knowledge, it is quite far from the pure reinforcement learning problem. The results are relatively unsurprising. Maybe they are novel for this problem, though.\n\nI'm not sure we can realistically draw any conclusions about Figure 6 in the paper's current form. I recommend the authors increase the resolution or run some actual metrics to determine the fuzziness/clarity of each row/image: something more concrete than an arrow of already low-resolution images.\n\n--- Added after rebuttal:\n\nI still do not see any high-res images for Figure 6 or any link to them, but I trust that the authors will add them if accepted.\n", "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "19 Dec 2016 (modified: 23 Jan 2017)", "REVIEWER_CONFIDENCE": 4}, {"OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "TITLE": "No Title", "comments": "This paper basically applies A3C to 3D spatial navigation tasks. \n\n- This is not the first time A3C has been applied to 3D navigation. In fact the original paper reported these experiments. Although the experimental results are great, I am not sure if this paper has any additional insights to warrant itself as a conference paper. It might make more sense as a workshop paper\n\n-  Are the graphs in Fig 5 constructed using a single hyper-parameter sweep? I think the authors should report results with many random initializations to make the comparisons more robust\n\n- Overall the two main ideas in this paper -- A3C and curriculums -- are not really novel but the authors do make use of them in a real system. ", "SOUNDNESS_CORRECTNESS": 3, "ORIGINALITY": 4, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "RECOMMENDATION": 4, "DATE": "15 Dec 2016", "REVIEWER_CONFIDENCE": 5}, {"OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "TITLE": "No Title", "comments": "This is a solid paper that applies A3C to Doom, enhancing it with a collection of tricks so as to win one of the VizDoom competitions. I think it is fair to expect the competition aspect to overshadow the more scientific approach of justifying every design decision in isolation, but in fact the authors do a decent job at the latter.\n\nTwo of my concerns have remained unanswered (see AnonReviewer2, below). \n\nIn addition, the citation list is rather thin, for example reward shaping has a rich literature, as do incrementally more difficult task setups, dating back at least to Mark Ring\u2019s work in the 1990s. There has also been a lot of complementary work on other FPS games. I\u2019m not asking that the authors do any direct comparisons, but to give the reader a sense of context in which to place this.", "ORIGINALITY": 4, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "15 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"DATE": "03 Dec 2016", "TITLE": "Huber loss", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1"}, {"TITLE": "clarify Fig 4, and hyper-parameter/shaping robustness", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "", "ORIGINALITY": 4, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "DATE": "02 Dec 2016"}, {"OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "TITLE": "experiment robustness", "comments": "", "SOUNDNESS_CORRECTNESS": 3, "ORIGINALITY": 4, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "DATE": "01 Dec 2016"}]}
{"text": "DEEP UNSUPERVISED LEARNING THROUGH SPATIAL CONTRASTING\n1 INTRODUCTION\nFor the past few years convolutional networks (ConvNets, CNNs) LeCun et al. (1998) have proven themselves as a successful model for vision related tasks Krizhevsky et al. (2012) Mnih et al. (2015) Pinheiro et al. (2015) Razavian et al. (2014). A convolutional network is composed of multiple convolutional and pooling layers, followed by a fully-connected affine transformations. As with other neural network models, each layer is typically followed by a non-linearity transformation such as a rectified-linear unit (ReLU). A convolutional layer is applied by cross correlating an image with a trainable weight filter. This stems from the assumption of stationarity in natural images, which means that parameters learned for one local region in an image can be shared for other regions and images.\nDeep learning models, including convolutional networks, are usually trained in a supervised manner, requiring large amounts of labeled data (ranging between thousands to millions of examples per-class for classification tasks) in almost all modern applications. These models are optimized using a variant of stochastic-gradient-descent (SGD) over batches of images sampled from the whole training dataset and their ground truth-labels. Gradient estimation for each one of the optimized parameters is done by back propagating the objective error from the final layer towards the input. This is commonly known as \u201dbackpropagation\u201d Rumelhart et al..\nIn early works, unsupervised training was used as a part of pre-training procedure to obtain an effective initial state of the model. The network was later fine-tuned in a supervised manner as displayed by Hinton (2007). Such unsupervised pre-training procedures were later abandoned, since they provided no apparent benefit over other initialization heuristics in more careful fully supervised training regimes. This led to the de-facto almost exclusive usage of neural networks in supervised environments.\nIn this work we will present a novel unsupervised learning criterion for convolutional network based on comparison of features extracted from regions within images. Our experiments indicate that by\nusing this criterion to pre-train networks we can improve their performance and achieve state-ofthe-art results.\n2 PREVIOUS WORKS\nUsing unsupervised methods to improve performance have been the holy grail of deep learning for the last couple of years and vast research efforts have been focused on that. We hereby give a short overview of the most popular and recent methods that tried to tackle this problem.\nAutoEncoders and reconstruction loss These are probably the most popular models for unsupervised learning using neural networks, and ConvNets in particular. Autoencoders are NNs which aim to transform inputs into outputs with the least possible amount of distortion. An Autoencoder is constructed using an encoder G(x;w1) that maps an input to a hidden compressed representation, followed by a decoder F (y;w2), that maps the representation back into the input space. Mathematically, this can be written in the following general form:\nx\u0302 = F (G(x;w1);w2)\nThe underlying encoder and decoder contain a set of trainable parameters that can be tied together and optimized for a predefined criterion. The encoder and decoder can have different architectures, including fully-connected neural networks, ConvNets and others. The criterion used for training is the reconstruction loss, usually the mean squared error (MSE) between the original input and its reconstruction Zeiler et al. (2010)\nmin\u2016x\u2212 x\u0302\u20162\nThis allows an efficient training procedure using the aforementioned backpropagation and SGD techniques. Over the years autoencoders gained fundamental role in unsupervised learning and many modification to the classic architecture were made. Ng (2011) regularized the latent representation to be sparse, Vincent et al. (2008) substituted the input with a noisy version thereof, requiring the model to denoise while reconstructing. Kingma et al. (2014) obtained very promising results with variational autoencoders (VAE). A variational autoencoder model inherits typical autoencoder architecture, but makes strong assumptions concerning the distribution of latent variables. They use variational approach for latent representation learning, which results in an additional loss component which required a new training algorithm called Stochastic Gradient Variational Bayes (SGVB). VAE assumes that the data is generated by a directed graphical model p(x|z) and require the encoder to learn an approximation qw1(z|x) to the posterior distribution pw2(z|x) where w1 and w2 denote the parameters of the encoder and decoder. The objective of the variational autoencoder in that case has the following form:\nL(w1, w2, x) = \u2212DKL(qw1(z|x)||pw2(z)) + Eqw1 (z|x) ( log pw2(x|z) ) Recently, a stacked set of denoising autoencoders architectures showed promising results in both semi-supervised and unsupervised tasks. A stacked what-where autoencoder by Zhao et al. (2015) computes a set of complementary variables that enable reconstruction whenever a layer implements a many-to-one mapping. Ladder networks by Rasmus et al. (2015) - use lateral connections and layer-wise cost functions to allow the higher levels of an autoencoder to focus on invariant abstract features.\nExemplar Networks: The unsupervised method introduced byDosovitskiy et al. (2014) takes a different approach to this task and trains the network to discriminate between a set of pseudo-classes. Each pseudo-class is formed by applying multiple transformations to a randomly sampled image patch. The number of pseudo-classes can be as big as the size of the input samples. This criterion ensures that different input samples would be distinguished while providing robustness to the applied transformations. In this work we will explore an alternative method with a similar motivation.\nContext prediction Another method for unsupervised learning by context was introduced by Doersch et al. (2015). This method uses an auxiliary criterion of predicting the location of an image patch given another from the same image. This is done by classification to 1 of 9 possible locations. Although the work of Doersch et al. (2015) and ours both use patches from an image to perform unsupervised learning, the methods are quite different. Whereas the former used a classification criterion over the spatial location of each patch within a single image, our work is concerned with comparing patches from several images to each other. We claim that this encourages discriminability between images (which we feel to be important aspect of feature learning), and was not an explicit goal in previous work.\nAdversarial Generative Models: This a recently introduced model that can be used in an unsupervised fashion Goodfellow et al. (2014). Adversarial Generative Models uses a set of networks, one trained to discriminate between data sampled from the true underlying distribution (e.g., a set of images), and a separate generative network trained to be an adversary trying to confuse the first network. By propagating the gradient through the paired networks, the model learns to generate samples that are distributed similarly to the source data. As shown by Radford et al. (2015),this model can create useful latent representations for subsequent classification tasks.\nSampling Methods: Methods for training models to discriminate between a very large number of classes often use a noise contrasting criterion. In these methods, roughly speaking, the posterior probability P (t|yt) of the ground-truth target t given the model output on an input sampled from the true distribution yt = F (x) is maximized, while the probability P (t|yn) given a noise measurement y = F (n) is minimized. This was successfully used in a language domain to learn unsupervised representation of words. The most noteworthy case is the word2vec model introduced by Mikolov et al. (2013). When using this setting in language applications, a natural contrasting noise is a smooth approximation of the Unigram distribution. A suitable contrasting distribution is less obvious when data points are sampled from a high dimensional continuous space, such as the case of image patches.\n2.1 PROBLEMS WITH CURRENT APPROACHES\nOnly recently the potential of ConvNets in an unsupervised environment began to bear fruit, still we believe it is not fully uncovered.\nThe majority of unsupervised optimization criteria currently used are based on variations of reconstruction losses. One limitation of this fact is that a pixel level reconstruction is non-compliant with the idea of a discriminative objective, which is expected to be agnostic to low level information in the input. In addition, it is evident that MSE is not best suited as a measurement to compare images, for example, viewing the possibly large square-error between an image and a single pixel shifted copy of it. Another problem with recent approaches such as Rasmus et al. (2015); Zeiler et al. (2010) is their need to extensively modify the original convolutional network model. This leads to a gap between unsupervised method and the state-of-the-art, supervised, models for classification - which can hurt future attempt to reconcile them in a unified framework, as well as efficiently leverage unlabeled data with otherwise supervised regimes.\n3 LEARNING BY COMPARISONS\nThe most common way to train NN is by defining a loss function between the target values and the network output. Learning by comparison approaches the supervised task from a different angle. The main idea is to use distance comparisons between samples to learn useful representations. For example, we consider relative and qualitative examples of the form X1 is closer to X2 than X1 is to X3. Using a comparative measure with neural network to learn embedding space was introduced in the \u201cSiamese network\u201d framework by Bromley et al. (1993) and later used in the works of Chopra et al. (2005). One use for this methods is when the number of classes is too large or expected to vary over time, as in the case of face verification, where a face contained in an image has to compared against another image of a face. This problem was recently tackled by Schroff et al. (2015) for training a convolutional network model on triplets of examples. There, one image served as an anchor x, and an additional pair of images served as a positive example x+ (containing an instance\nof the face of the same person) together with a negative example x\u2212, containing a face of a different person. The training objective was on the embedded distance of the input faces, where the distance between the anchor and positive example is adjusted to be smaller by at least some constant \u03b1 from the negative distance. More precisely, the loss function used in this case was defined as\nL(x, x+, x\u2212) = max {\u2016F (x)\u2212 F (x+)\u20162 \u2212 \u2016F (x)\u2212 F (x\u2212)\u20162 + \u03b1, 0} (1)\nwhere F (x) is the embedding (the output of a convolutional neural network), and \u03b1 is a predefined margin constant. Another similar model used by Hoffer & Ailon (2015) with triplets comparisons for classification, where examples from the same class were trained to have a lower embedded distance than that of two images from distinct classes. This work introduced a concept of a distance ratio loss, where the defined measure amounted to:\nL(x, x+, x\u2212) = e\u2212\u2016F (x)\u2212F (x+)\u20162\ne\u2212\u2016F (x)\u2212F (x+)\u20162 + e\u2212\u2016F (x)\u2212F (x\u2212)\u20162 (2)\nThis loss has a flavor of a probability of a biased coin flip. By \u2018pushing\u2019 this probability to zero, we express the objective that pairs of samples coming from distinct classes should be less similar to each other, compared to pairs of samples coming from the same class. It was shown empirical by Balntas et al. (2016) to provide better feature embeddings than the margin based distance loss 1\n4 OUR CONTRIBUTION: SPATIAL CONTRASTING\nOne implicit assumption in convolutional networks, is that features are gradually learned hierarchically, each level in the hierarchy corresponding to a layer in the network. Each spatial location within a layer corresponds to a region in the original image. It is empirically observed that deeper layers tend to contain more \u2018abstract\u2019 information from the image. Intuitively, features describing different regions within the same image are likely to be semantically similar (e.g. different parts of an animal), and indeed the corresponding deep representations tend to be similar. Conversely, regions from two probably unrelated images (say, two images chosen at random) tend to be far from each other in the deep representation. This logic is commonly used in modern deep networks such as Szegedy et al. (2015) Lin et al. (2013) He et al. (2015), where a global average pooling is used to aggregate spatial features in the final layer used for classification.\nOur suggestion is that this property, often observed as a side effect of supervised applications, can be used as a desired objective when learning deep representations in an unsupervised task. Later, the resulting representation can be used, as typically done, as a starting point or a supervised learning task. We call this idea which we formalize below Spatial contrasting. The spatial contrasting criterion is similar to noise contrasting estimation Gutmann & Hyva\u0308rinen (2010) Mnih & Kavukcuoglu (2013), in trying to train a model by maximizing the expected probability on desired inputs, while minimizing it on contrasting sampled measurements.\n4.1 FORMULATION\nWe will concern ourselves with samples of images patches x\u0303(m) taken from an image x. Our convolutional network model, denoted by F (x), extracts spatial features f so that f (m) = F (x\u0303(m)) for an image patch x\u0303(m). We will also define P (f1|f2) as the probability for two features f1, f2 to occur together in the same image. We wish to optimize our model such that for two features representing patches taken from the same image x\u0303(1)i , x\u0303 (2) i \u2208 xi for which f (1) i = F (x\u0303 (1) i ) and f (2) i = F (x\u0303 (2) i ), P (f (1) i |f (2) i ) will be maximized. This means that features from a patch taken from a specific image can effectively predict, under our model, features extracted from other patches in the same image. Conversely, we want our model to minimize P (fi|fj) for i, j being two patches taken from distinct images. Following the logic presented before, we will need to sample contrasting patch x\u0303(1)j from a different image xj such that P (f (1) i |f (2) i ) > P (f (1) j |f (2) i ), where f (1) j = F (x\u0303 (1) j ). In order to obtain contrasting samples, we use regions from two random images in the training set. We will use a distance ratio, described earlier\nin Eq. (2) for the supervised case, to represent the probability two feature vectors were taken from the same image. The resulting training loss for a pair of images will be defined as\nLSC(x1, x2) = \u2212 log e\u2212\u2016f (1) 1 \u2212f (2) 1 \u20162\ne\u2212\u2016f (1) 1 \u2212f (2) 1 \u20162 + e\u2212\u2016f (1) 1 \u2212f (1) 2 \u20162\n(3)\nEffectively minimizing a log-probability under the SoftMax measure. This formulation is portrayed in figure 4.1. Since we sample our contrasting sample from the same underlying distribution, we can evaluate this loss considering the image patch as both patch compared (anchor) and contrast symmetrically. The final loss will be the average between these estimations:\nL\u0302SC(x1, x2) = 1\n2 [LSC(x1, x2) + LSC(x2, x1)]\n4.2 METHOD\nConvolutional network are usually trained using SGD over mini-batch of samples, therefore we can extract patches and contrasting patches without changing the network architecture. Each image serves as both anchor and positive patches, for which the corresponding features should be closer, as well as contrasting samples for other images in that batch. For a batch of N images, two samples from each image are taken, and N2 different distance comparisons are made. The final loss is defined as the average distance ratio for all images in the batch:\nLSC({x}Ni=1) = 1\nN N\u2211 i=1 LSC(xi, {x}j 6=i) = \u2212 1 N N\u2211 i=1 log e\u2212\u2016f (1) i \u2212f (2) i \u20162\u2211N j=1 e \u2212\u2016f(1)i \u2212f (2) j \u20162\n(4)\nSince the criterion is differentiable with respect to its inputs, it is fully compliant with standard methods for training convolutional network and specifically using backpropagation and gradient descent. Furthermore, SC can be applied to any layer in the network hierarchy. In fact, SC can be used at multiple layers within the same convolutional network. The spatial properties of the\nfeatures means that we can sample directly from feature space f\u0303 (m) \u2208 f instead of from the original image. Therefore SC has a simple implementation which doesn\u2019t require substation amount of computation. The complete algorithm for batch training is described in Algorithm (1). Similar to the batch normalization (BN) layer Ioffe & Szegedy (2015), a recent usage for batch statistics in neural networks, SC also uses the batch statistics. While BN normalize the input based on the batch statistics, SC sample from it. This can be viewed as a simple sampling from the space of possible features describing a patch of image.\nAlgorithm 1 Calculation the spatial contrasting loss Require: X = {x}Ni=1 # Training on batches of images\n# Get the spatial features for the whole batch of images # Size: N \u00d7Wf \u00d7Hf \u00d7 C {f}Ni=1 \u2190 ConvNet(X)\n# Sample spatial features and calculate embedded distance between all pairs of images for i = 1 to N do f\u0303 (1) i \u2190 sample(fi)\nfor j = 1 to N do f\u0303 (2) j \u2190 sample(fj) Dist(i, j)\u2190 \u2016f\u0303 (1)i \u2212 f\u0303 (2) j \u20162\nend for end for\n# Calculate log SoftMax normalized distances di \u2190 \u2212 log e\n\u2212Dist(i,i)\u2211N k=1 e \u2212Dist(i,k)\n# Spatial contrasting loss is the mean of distance ratios return 1N \u2211N i=1 di\n5 EXPERIMENTS\nIn this section we report empirical results showing that using SC loss as an unsupervised pretraining procedure can improve state-of-the-art performance on subsequent classification. We experimented with MNIST, CIFAR-10 and STL10 datasets. We used modified versions of well studied networks such as those of Lin et al. (2013) and Rasmus et al. (2015). A detailed description of our architecture can be found in 4.\nIn each one of the experiments, we used the spatial contrasting criterion to train the network on the unlabeled images. In each usage of SC criterion, patch features were sampled from the preceding layer in uniform. We note that spatial size of sampled patches ranged between datasets, where on STL10 and Cifar10 it covered about 30% of the image, MNIST required the use of larger patches covering almost the entire image.Training was done by using SGD with an initial learning rate of 0.1 that was decreased by a factor of 10 whenever the measured loss stopped decreasing. After convergence, we used the trained model as an initialization for a supervised training on the complete labeled dataset. The supervised training was done following the same regime, only starting with a lower initial learning rate of 0.01. We used mild data augmentations, such as small translations and horizontal mirroring. The datasets we used are:\n\u2022 STL10 (Coates et al. (2011)). This dataset consists of 100, 000 96\u00d7 96 colored, unlabeled images, together with another set of 5, 000 labeled training images and 8, 000 test images . The label space consists of 10 object classes.\n\u2022 Cifar10 (Krizhevsky & Hinton (2009)). The well known CIFAR-10 is an image classification benchmark dataset containing 50, 000 training images and 10, 000 test images. The\nAll experiments were conducted using the Torch7 framework by Collobert et al. (2011). Code reproducing these results will by available at https://github.com/eladhoffer/ SpatialContrasting.\n5.1 RESULTS ON STL10\nSince STL10 dataset is comprised of mostly unlabeled data, it is most suitable to highlight the benefits of the spatial contrasting criterion. The initial training was unsupervised, as described earlier, using the entire set of 105, 000 samples (union of the original unlabeled set and labeled training set). The representation outputted by the training, was used to initialize supervised training on the 5, 000 labeled images. Evaluation was done on a separate test set of 8, 000 samples. Comparing with state of the art results, we see an improvement of 7% in test accuracy over the best model by Zhao et al. (2015), setting the SC as best model at 81.3% test classification accuracy (see Table (1)). We note that the results of Dosovitskiy et al. (2014) are achieved with no fine-tuning over labeled examples, which may be unfair to this work. We also compare with the same network, but without SC initialization, which achieves a lower classification of 72.6%. This is an indication that indeed SC managed to leverage unlabeled examples to provide a better initialization point for the supervised model.\n5.2 RESULTS ON CIFAR10\nFor Cifar10 dataset, we use the same setting as Coates & Ng (2012) and Hui (2013) to test a model\u2019s ability to learn from unlabeled images. Here, only 4, 000 samples out of 50, 000 are used with their label annotation, and the rest of the samples can be used only in an unsupervised manner. The final test accuracy is measured on the entire 10, 000 test set. In our experiments, we trained our model using SC criterion on the entire dataset, and then used only 400 labeled samples per class (for a total of 4000) in a supervised regime over the initialized network. The results are compared with previous efforts in Table (2). Using the SC criterion allowed an improvement of 6.8% over a non-initialized model, and achieved a final test accuracy of 79.2%. This is a competitive result with current state-of-the-art models.\n5.3 RESULTS ON MNIST\nThe MNIST dataset is very different in nature from the Cifar10 and STL10 datasets, we experimented earlier. The biggest difference, relevant to this work, is that spatial regions sampled from MNIST images usually provide very little, or no information. Thus, SC is much less suited for MNIST dataset, and was conjured to have little benefit. We still, however, experimented with initializing a model with SC criterion and continuing with a fully-supervised regime over all labeled\nexamples. We found again that this provided benefit over training the same network without preinitialization, improving results from 0.63% to 0.34% error on test set. As mentioned previously, the effective compared patches of MNIST covered almost the entire image area. This can be attributed to the fact that MNIST requires global features to differentiate between digits. The results, compared with previous attempts are included in Table (3).\n6 CONCLUSIONS AND FUTURE WORK\nIn this work we presented spatial contrasting - a novel unsupervised criterion for training convolutional networks on unlabeled data. Its is based on comparison between spatial features sampled from a number of images. We\u2019ve shown empirically that using spatial contrasting as a pretraining technique to initialize a ConvNet, can improve its performance on a subsequent supervised training. In cases where a lot of unlabeled data is available, such as the STL10 dataset, this translates to state-of-the-art classification accuracy in the final model.\nSince the spatial contrasting loss is a differentiable estimation that can be computed within a network parallel to supervised losses, in future work we plan to embed it as a semi-supervised model. This usage will allow to create models that can leverage both labeled an unlabeled data, and can be compared to similar semi-supervised models such as the ladder network Rasmus et al. (2015). It is is also apparent that contrasting can occur in dimensions other than the spatial, the most straightforward is the temporal dimension. This suggests that similar training procedure can be applied on segments of sequences to learn useful representation without explicit supervision.\n7 APPENDIX\n", "cats": {"READ": 1.0, "DONTREAD": 0.0}, "reviews": [{"IS_META_REVIEW": true, "comments": "This paper presents a novel way to do unsupervised pretraining in a deep convolutional network setting (though likely applicable to fully-connected nets as well). The method is that of \u2018spatial constrasting\u2019, i.e. of building triplets from patches of input images and learning a presentation that assigns a high score for patches coming from the same image and a low score for patches from diferent images. The method is simple enough that I am surprised that no-one has tried this before (at least according to the previous work in the submission). Here are some comments:\n\n\nThe usage of P(f_i^1 | f_i^2) in Section 4.1 is a bit odd. May be worth defining mathematically what kind of probability the authors are talking about, or just taking that part out (\u201cprobability\u201d can be replaced with another word).\n\nI would like to know more about how the method is using the \u201cbatch statistics\u201d (end of Section 4.2) by sampling from it, unless the authors simply mean that the just sample from all the possible triples in their batch.\n\nShouldn\u2019t the number of patches sampled in Algorithm 1 be a hyper-parameter rather than just be 1? Have the authors tried any other value?\n\nI think there are some missing details in the paper, like the patch size or whether the authors have played with it at all (I think this is an important hyper-parameter).\n\nThe STL results are quite impressive, but CIFAR-10 maybe not so much. For CIFAR I\u2019d expect that one can try to pre-train on, say, Imagenet + CIFAR to build a better representation. Have the authors considered this?\n\n\nAll in all, this is an interesting piece of work with some obvious applications, and it seems relatively straightforward to implemenent and try. I think I would\u2019ve liked more understanding of what the spatial contrasting actually learns, more empirical studies on the effects of various parameter choices (e.g., patch size) and more attempts at beating the state of the art (e.g. CIFAR)."}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "The paper proposes a formulation for unsupervised learning of ConvNets based on the distance between patches sampled from the same and different images. The novelty of the method is rather limited as it's similar to [Doersch et al. 2015] and [Dosovitsky et al. 2015]. The evaluation is only performed on the small datasets, which limits the potential impact of the contribution.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"TITLE": "review", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "This paper proposes an unsupervised training objective based on patch contrasting for visual representation learning using deep neural networks. In particular, the feature representations of the patches from the same image are encouraged to be closer than the those from different images. The distance ratios of positive training pairs are optimized. The proposed method are empirically shown to be effective as an initialization method for supervised training. \n\nStrengths:\n\n- The training objective is reasonable. In particular, high-level features show translation invariance. \n\n- The proposed methods are effective for initializing neural networks for supervised training on several datasets. \n\n\nWeaknesses:\n\n- The methods are technically similar to the \u201cexemplar network\u201d (Dosovitskiy 2015). Cropping patches from a single image can be taken as a type of data augmentation, which is comparable to the data augmentation of positive sample (the exemplar) in (Dosovitskiy 2015). \n\n- The paper is experimentally misleading.\nThe results reported in this paper are based on fine-tuning the whole network with supervision. However, in Table 2, the results of exemplar convnets (Dosovitskiy 2015) is from unsupervised feature learning (the network is not finetuned with labeled samples, and only a classifier is trained upon the features). Therefore, the comparison is not fair. I suspect that exemplar convnets (Dosovitskiy 2015) would achieve similar improvements from fine-tuning; so, without such comparisons (head-to-head comparison with and without fine-tuning based on the same architecture except for the loss), the experimental results are not fully convincing. \n\nRegarding the comparison to \u201cWhat-where\u201d autoencoder (Zhao et al, 2015), it will be interesting to compare against it in large-scale settings, as shown by Zhang et al, ICML 2016 (Augmenting Supervised Neural Networks with Unsupervised Objectives for Large-Scale Image Classification). Training an AlexNet is not very time-consuming with latest (e.g., TITAN-X level) GPUs. \n\nThe proposed method seems useful only for natural images where different patches from the same image can be similar to each other. \n", "IS_META_REVIEW": false, "RECOMMENDATION": 5, "DATE": "21 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "Deep Unsupervised Learning through Spatial Contrasting", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "The proposed self supervised loss is formulated using a Siamese architecture and encourages patches from the same image to lie closer in feature space than a contrasting patch taken from a different, random image. The loss is very similar in spirit to that of Doersch et al. ICCV 2015 and Isola et al. ICLR 2016 workshop. It seems that the proposed loss is actually a simplified version of Doersch et al. ICCV 2015 in that it does not make use of the spatial offset, a freely available self supervised signal in natural images. Intuitively, it seems that the self-supervised problem posed by this method is strictly simpler, and therefore less powerful, than that of the aforementioned work. I would like to see more discussion on the comparison of these two approaches. Nevertheless the proposed method seems to be effective in achieving good empirical results using this simple loss. Though more implementation details should be provided, such as the effect of patch size, overlap between sampled patches, and any other important measures taken to avoid trivial solutions. ", "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "No Title", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "This paper presents a novel way to do unsupervised pretraining in a deep convolutional network setting (though likely applicable to fully-connected nets as well). The method is that of \u2018spatial constrasting\u2019, i.e. of building triplets from patches of input images and learning a presentation that assigns a high score for patches coming from the same image and a low score for patches from diferent images. The method is simple enough that I am surprised that no-one has tried this before (at least according to the previous work in the submission). Here are some comments:\n\n\nThe usage of P(f_i^1 | f_i^2) in Section 4.1 is a bit odd. May be worth defining mathematically what kind of probability the authors are talking about, or just taking that part out (\u201cprobability\u201d can be replaced with another word).\n\nI would like to know more about how the method is using the \u201cbatch statistics\u201d (end of Section 4.2) by sampling from it, unless the authors simply mean that the just sample from all the possible triples in their batch.\n\nShouldn\u2019t the number of patches sampled in Algorithm 1 be a hyper-parameter rather than just be 1? Have the authors tried any other value?\n\nI think there are some missing details in the paper, like the patch size or whether the authors have played with it at all (I think this is an important hyper-parameter).\n\nThe STL results are quite impressive, but CIFAR-10 maybe not so much. For CIFAR I\u2019d expect that one can try to pre-train on, say, Imagenet + CIFAR to build a better representation. Have the authors considered this?\n\n\nAll in all, this is an interesting piece of work with some obvious applications, and it seems relatively straightforward to implemenent and try. I think I would\u2019ve liked more understanding of what the spatial contrasting actually learns, more empirical studies on the effects of various parameter choices (e.g., patch size) and more attempts at beating the state of the art (e.g. CIFAR).\n", "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "14 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"DATE": "05 Dec 2016", "TITLE": "questions", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2"}, {"DATE": "02 Dec 2016", "TITLE": "related works", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3"}, {"DATE": "01 Dec 2016", "TITLE": "Does patch size matter?", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1"}, {"IS_META_REVIEW": true, "comments": "This paper presents a novel way to do unsupervised pretraining in a deep convolutional network setting (though likely applicable to fully-connected nets as well). The method is that of \u2018spatial constrasting\u2019, i.e. of building triplets from patches of input images and learning a presentation that assigns a high score for patches coming from the same image and a low score for patches from diferent images. The method is simple enough that I am surprised that no-one has tried this before (at least according to the previous work in the submission). Here are some comments:\n\n\nThe usage of P(f_i^1 | f_i^2) in Section 4.1 is a bit odd. May be worth defining mathematically what kind of probability the authors are talking about, or just taking that part out (\u201cprobability\u201d can be replaced with another word).\n\nI would like to know more about how the method is using the \u201cbatch statistics\u201d (end of Section 4.2) by sampling from it, unless the authors simply mean that the just sample from all the possible triples in their batch.\n\nShouldn\u2019t the number of patches sampled in Algorithm 1 be a hyper-parameter rather than just be 1? Have the authors tried any other value?\n\nI think there are some missing details in the paper, like the patch size or whether the authors have played with it at all (I think this is an important hyper-parameter).\n\nThe STL results are quite impressive, but CIFAR-10 maybe not so much. For CIFAR I\u2019d expect that one can try to pre-train on, say, Imagenet + CIFAR to build a better representation. Have the authors considered this?\n\n\nAll in all, this is an interesting piece of work with some obvious applications, and it seems relatively straightforward to implemenent and try. I think I would\u2019ve liked more understanding of what the spatial contrasting actually learns, more empirical studies on the effects of various parameter choices (e.g., patch size) and more attempts at beating the state of the art (e.g. CIFAR)."}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "The paper proposes a formulation for unsupervised learning of ConvNets based on the distance between patches sampled from the same and different images. The novelty of the method is rather limited as it's similar to [Doersch et al. 2015] and [Dosovitsky et al. 2015]. The evaluation is only performed on the small datasets, which limits the potential impact of the contribution.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"TITLE": "review", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "This paper proposes an unsupervised training objective based on patch contrasting for visual representation learning using deep neural networks. In particular, the feature representations of the patches from the same image are encouraged to be closer than the those from different images. The distance ratios of positive training pairs are optimized. The proposed method are empirically shown to be effective as an initialization method for supervised training. \n\nStrengths:\n\n- The training objective is reasonable. In particular, high-level features show translation invariance. \n\n- The proposed methods are effective for initializing neural networks for supervised training on several datasets. \n\n\nWeaknesses:\n\n- The methods are technically similar to the \u201cexemplar network\u201d (Dosovitskiy 2015). Cropping patches from a single image can be taken as a type of data augmentation, which is comparable to the data augmentation of positive sample (the exemplar) in (Dosovitskiy 2015). \n\n- The paper is experimentally misleading.\nThe results reported in this paper are based on fine-tuning the whole network with supervision. However, in Table 2, the results of exemplar convnets (Dosovitskiy 2015) is from unsupervised feature learning (the network is not finetuned with labeled samples, and only a classifier is trained upon the features). Therefore, the comparison is not fair. I suspect that exemplar convnets (Dosovitskiy 2015) would achieve similar improvements from fine-tuning; so, without such comparisons (head-to-head comparison with and without fine-tuning based on the same architecture except for the loss), the experimental results are not fully convincing. \n\nRegarding the comparison to \u201cWhat-where\u201d autoencoder (Zhao et al, 2015), it will be interesting to compare against it in large-scale settings, as shown by Zhang et al, ICML 2016 (Augmenting Supervised Neural Networks with Unsupervised Objectives for Large-Scale Image Classification). Training an AlexNet is not very time-consuming with latest (e.g., TITAN-X level) GPUs. \n\nThe proposed method seems useful only for natural images where different patches from the same image can be similar to each other. \n", "IS_META_REVIEW": false, "RECOMMENDATION": 5, "DATE": "21 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "Deep Unsupervised Learning through Spatial Contrasting", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "The proposed self supervised loss is formulated using a Siamese architecture and encourages patches from the same image to lie closer in feature space than a contrasting patch taken from a different, random image. The loss is very similar in spirit to that of Doersch et al. ICCV 2015 and Isola et al. ICLR 2016 workshop. It seems that the proposed loss is actually a simplified version of Doersch et al. ICCV 2015 in that it does not make use of the spatial offset, a freely available self supervised signal in natural images. Intuitively, it seems that the self-supervised problem posed by this method is strictly simpler, and therefore less powerful, than that of the aforementioned work. I would like to see more discussion on the comparison of these two approaches. Nevertheless the proposed method seems to be effective in achieving good empirical results using this simple loss. Though more implementation details should be provided, such as the effect of patch size, overlap between sampled patches, and any other important measures taken to avoid trivial solutions. ", "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "No Title", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "This paper presents a novel way to do unsupervised pretraining in a deep convolutional network setting (though likely applicable to fully-connected nets as well). The method is that of \u2018spatial constrasting\u2019, i.e. of building triplets from patches of input images and learning a presentation that assigns a high score for patches coming from the same image and a low score for patches from diferent images. The method is simple enough that I am surprised that no-one has tried this before (at least according to the previous work in the submission). Here are some comments:\n\n\nThe usage of P(f_i^1 | f_i^2) in Section 4.1 is a bit odd. May be worth defining mathematically what kind of probability the authors are talking about, or just taking that part out (\u201cprobability\u201d can be replaced with another word).\n\nI would like to know more about how the method is using the \u201cbatch statistics\u201d (end of Section 4.2) by sampling from it, unless the authors simply mean that the just sample from all the possible triples in their batch.\n\nShouldn\u2019t the number of patches sampled in Algorithm 1 be a hyper-parameter rather than just be 1? Have the authors tried any other value?\n\nI think there are some missing details in the paper, like the patch size or whether the authors have played with it at all (I think this is an important hyper-parameter).\n\nThe STL results are quite impressive, but CIFAR-10 maybe not so much. For CIFAR I\u2019d expect that one can try to pre-train on, say, Imagenet + CIFAR to build a better representation. Have the authors considered this?\n\n\nAll in all, this is an interesting piece of work with some obvious applications, and it seems relatively straightforward to implemenent and try. I think I would\u2019ve liked more understanding of what the spatial contrasting actually learns, more empirical studies on the effects of various parameter choices (e.g., patch size) and more attempts at beating the state of the art (e.g. CIFAR).\n", "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "14 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"DATE": "05 Dec 2016", "TITLE": "questions", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2"}, {"DATE": "02 Dec 2016", "TITLE": "related works", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3"}, {"DATE": "01 Dec 2016", "TITLE": "Does patch size matter?", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1"}]}
{"text": "TOPICRNN: A RECURRENT NEURAL NETWORK WITH LONG-RANGE SEMANTIC DEPENDENCY\n1 INTRODUCTION\nWhen reading a document, short or long, humans have a mechanism that somehow allows them to remember the gist of what they have read so far. Consider the following example:\n\u201cThe U.S.presidential race isn\u2019t only drawing attention and controversy in the United States \u2013 it\u2019s being closely watched across the globe. But what does the rest of the world think about a campaign that has already thrown up one surprise after another? CNN asked 10 journalists for their take on the race so far, and what their country might be hoping for in America\u2019s next \u2014\u201d\nThe missing word in the text above is easily predicted by any human to be either President or Commander in Chief or their synonyms. There have been various language models \u2013 from simple ngrams to the most recent RNN-based language models \u2013 that aim to solve this problem of predicting correctly the subsequent word in an observed sequence of words.\nA good language model should capture at least two important properties of natural language. The first one is correct syntax. In order to do prediction that enjoys this property, we often only need to consider a few preceding words. Therefore, correct syntax is more of a local property. Word order matters in this case. The second property is the semantic coherence of the prediction. To achieve\n\u2217Work was done while at Microsoft Research.\nthis, we often need to consider many preceding words to understand the global semantic meaning of the sentence or document. The ordering of the words usually matters much less in this case.\nBecause they only consider a fixed-size context window of preceding words, traditional n-gram and neural probabilistic language models (Bengio et al., 2003) have difficulties in capturing global semantic information. To overcome this, RNN-based language models (Mikolov et al., 2010; 2011) use hidden states to \u201cremember\u201d the history of a word sequence. However, none of these approaches explicitly model the two main properties of language mentioned above, correct syntax and semantic coherence. Previous work by Chelba and Jelinek (2000) and Gao et al. (2004) exploit syntactic or semantic parsers to capture long-range dependencies in language.\nIn this paper, we propose TopicRNN, a RNN-based language model that is designed to directly capture long-range semantic dependencies via latent topics. These topics provide context to the RNN. Contextual RNNs have received a lot of attention (Mikolov and Zweig, 2012; Mikolov et al., 2014; Ji et al., 2015; Lin et al., 2015; Ji et al., 2016; Ghosh et al., 2016). However, the models closest to ours are the contextual RNN model proposed by Mikolov and Zweig (2012) and its most recent extension to the long-short term memory (LSTM) architecture (Ghosh et al., 2016). These models use pre-trained topic model features as an additional input to the hidden states and/or the output of the RNN. In contrast, TopicRNN does not require pre-trained topic model features and can be learned in an end-to-end fashion. We introduce an automatic way for handling stop words that topic models usually have difficulty dealing with. Under a comparable model size set up, TopicRNN achieves better perplexity scores than the contextual RNN model of Mikolov and Zweig (2012) on the Penn TreeBank dataset 1. Moreover, TopicRNN can be used as an unsupervised feature extractor for downstream applications. For example, we derive document features of the IMDB movie review dataset using TopicRNN for sentiment classification. We reported an error rate of 6.28%. This is close to the state-of-the-art 5.91% (Miyato et al., 2016) despite that we do not use the labels and adversarial training in the feature extraction stage.\nThe remainder of the paper is organized as follows: Section 2 provides background on RNN-based language models and probabilistic topic models. Section 3 describes the TopicRNN network architecture, its generative process and how to perform inference for it. Section 4 presents per-word perplexity results on the Penn TreeBank dataset and the classification error rate on the IMDB 100K dataset. Finally, we conclude and provide future research directions in Section 5.\n2 BACKGROUND\nWe present the background necessary for building the TopicRNN model. We first review RNN-based language modeling, followed by a discussion on the construction of latent topic models.\n2.1 RECURRENT NEURAL NETWORK-BASED LANGUAGE MODELS\nLanguage modeling is fundamental to many applications. Examples include speech recognition and machine translation. A language model is a probability distribution over a sequence of words in a predefined vocabulary. More formally, let V be a vocabulary set and y1, ..., yT a sequence of T words with each yt \u2208 V . A language model measures the likelihood of a sequence through a joint probability distribution,\np(y1, ..., yT ) = p(y1) T\u220f t=2 p(yt|y1:t\u22121).\nTraditional n-gram and feed-forward neural network language models (Bengio et al., 2003) typically make Markov assumptions about the sequential dependencies between words, where the chain rule shown above limits conditioning to a fixed-size context window.\nRNN-based language models (Mikolov et al., 2011) sidestep this Markov assumption by defining the conditional probability of each word yt given all the previous words y1:t\u22121 through a hidden\n1Ghosh et al. (2016) did not publish results on the PTB and we did not find the code online.\nstate ht (typically via a softmax function):\np(yt|y1:t\u22121) , p(yt|ht), ht = f(ht\u22121, xt).\nThe function f(\u00b7) can either be a standard RNN cell or a more complex cell such as GRU (Cho et al., 2014) or LSTM (Hochreiter and Schmidhuber, 1997). The input and target words are related via the relation xt \u2261 yt\u22121. These RNN-based language models have been quite successful (Mikolov et al., 2011; Chelba et al., 2013; Jozefowicz et al., 2016).\nWhile in principle RNN-based models can \u201cremember\u201d arbitrarily long histories if provided enough capacity, in practice such large-scale neural networks can easily encounter difficulties during optimization (Bengio et al., 1994; Pascanu et al., 2013; Sutskever, 2013) or overfitting issues (Srivastava et al., 2014). Finding better ways to model long-range dependencies in language modeling is therefore an open research challenge. As motivated in the introduction, much of the long-range dependency in language comes from semantic coherence, not from syntactic structure which is more of a local phenomenon. Therefore, models that can capture long-range semantic dependencies in language are complementary to RNNs. In the following section, we describe a family of such models called probabilistic topic models.\n2.2 PROBABILISTIC TOPIC MODELS\nProbabilistic topic models are a family of models that can be used to capture global semantic coherency (Blei and Lafferty, 2009). They provide a powerful tool for summarizing, organizing, and navigating document collections. One basic goal of such models is to find groups of words that tend to co-occur together in the same document. These groups of words are called topics and represent a probability distribution that puts most of its mass on this subset of the vocabulary. Documents are then represented as mixtures over these latent topics. Through posterior inference, the learned topics capture the semantic coherence of the words they cluster together (Mimno et al., 2011).\nThe simplest topic model is latent Dirichlet allocation (LDA) (Blei et al., 2003). It assumes K underlying topics \u03b2 = {\u03b21, . . . , \u03b2K} , each of which is a distribution over a fixed vocabulary. The generative process of LDA is as follows: First generate the K topics, \u03b2k \u223ciid Dirichlet(\u03c4). Then for each document containing words y1:T , independently generate document-level variables and data:\n1. Draw a document-specific topic proportion vector \u03b8 \u223c Dirichlet(\u03b1).\n2. For the tth word in the document,\n(a) Draw topic assignment zt \u223c Discrete(\u03b8). (b) Draw word yt \u223c Discrete(\u03b2zt).\nMarginalizing each zt, we obtain the probability of y1:T via a matrix factorization followed by an integration over the latent variable \u03b8,\np(y1:T |\u03b2) = \u222b p(\u03b8) T\u220f t=1 \u2211 zt p(zt|\u03b8)p(yt|zt, \u03b2)d\u03b8 = \u222b p(\u03b8) T\u220f t=1 (\u03b2\u03b8)ytd\u03b8. (1)\nIn LDA the prior distribution on the topic proportions is a Dirichlet distribution; it can be replaced by many other distributions. For example, the correlated topic model (Blei and Lafferty, 2006) uses a log-normal distribution. Most topic models are \u201cbag of words\u201d models in that word order is ignored. This makes it easier for topic models to capture global semantic information. However, this is also one of the reasons why topic models do not perform well on general-purpose language modeling applications such as word prediction. While bi-gram topic models have been proposed (Wallach, 2006), higher order models quickly become intractable.\nAnother issue encountered by topic models is that they do not model stop words well. This is because stop words usually do not carry semantic meaning; their appearance is mainly to make the sentence more readable according to the grammar of the language. They also appear frequently in\nalmost every document and can co-occur with almost any word2. In practice, these stop words are chosen using tf-idf (Blei and Lafferty, 2009).\n3 THE TOPICRNN MODEL\nWe next describe the proposed TopicRNN model. In TopicRNN, latent topic models are used to capture global semantic dependencies so that the RNN can focus its modeling capacity on the local dynamics of the sequences. With this joint modeling, we hope to achieve better overall performance on downstream applications.\nThe model. TopicRNN is a generative model. For a document containing the words y1:T ,\n1. Draw a topic vector3 \u03b8 \u223c N(0, I). 2. Given word y1:t\u22121, for the tth word yt in the document,\n(a) Compute hidden state ht = fW (xt, ht\u22121), where we let xt , yt\u22121. (b) Draw stop word indicator lt \u223c Bernoulli(\u03c3(\u0393>ht)), with \u03c3 the sigmoid function. (c) Draw word yt \u223c p(yt|ht, \u03b8, lt, B), where\np(yt = i|ht, \u03b8, lt, B) \u221d exp ( v>i ht + (1\u2212 lt)b>i \u03b8 ) .\nThe stop word indicator lt controls how the topic vector \u03b8 affects the output. If lt = 1 (indicating yt is a stop word), the topic vector \u03b8 has no contribution to the output. Otherwise, we add a bias to favor those words that are more likely to appear when mixing with \u03b8, as measured by the dot product between \u03b8 and the latent word vector bi for the ith vocabulary word. As we can see, the longrange semantic information captured by \u03b8 directly affects the output through an additive procedure. Unlike Mikolov and Zweig (2012), the contextual information is not passed to the hidden layer of the RNN. The main reason behind our choice of using the topic vector as bias instead of passing it into the hidden states of the RNN is because it enables us to have a clear separation of the contributions of global semantics and those of local dynamics. The global semantics come from the topics which are meaningful when stop words are excluded. However these stop words are needed for the local dynamics of the language model. We hence achieve this separation of global vs local via a binary decision model for the stop words. It is unclear how to achieve this if we pass the topics to the\n2Wallach et al. (2009) described using asymmetric priors to alleviate this issue. Although it is not clear how to use this idea in TopicRNN, we plan to investigate such priors in future work.\n3Instead of using the Dirichlet distribution, we choose the Gaussian distribution. This allows for more flexibility in the sequence prediction problem and also has advantages during inference.\nhidden states of the RNN. This is because the hidden states of the RNN will account for all words (including stop words) whereas the topics exclude stop words.\nWe show the unrolled graphical representation of TopicRNN in Figure 1(a). We denote all model parameters as \u0398 = {\u0393, V, B,W,Wc} (see Appendix A.1 for more details). Parameter Wc is for the inference network, which we will introduce below. The observations are the word sequences y1:T and stop word indicators l1:T .4 The log marginal likelihood of the sequence y1:T is\nlog p(y1:T , l1:T |ht) = log \u222b p(\u03b8) T\u220f t=1 p(yt|ht, lt, \u03b8)p(lt|ht)d\u03b8. (2)\nModel inference. Direct optimization of Equation 2 is intractable so we use variational inference for approximating this marginal (Jordan et al., 1999). Let q(\u03b8) be the variational distribution on the marginalized variable \u03b8. We construct the variational objective function, also called the evidence lower bound (ELBO), as follows:\nL(y1:T , l1:T |q(\u03b8),\u0398) , Eq(\u03b8) [ T\u2211 t=1 log p(yt|ht, lt, \u03b8) + log p(lt|ht) + log p(\u03b8)\u2212 log q(\u03b8) ] \u2264 log p(y1:T , l1:T |ht,\u0398).\nFollowing the proposed variational autoencoder technique, we choose the form of q(\u03b8) to be an inference network using a feed-forward neural network (Kingma and Welling, 2013; Miao et al., 2015). Let Xc \u2208 N |Vc|+ be the term-frequency representation of y1:T excluding stop words (with Vc the vocabulary size without the stop words). The variational autoencoder inference network q(\u03b8|Xc,Wc) with parameter Wc is a feed-forward neural network with ReLU activation units that projects Xc into a K-dimensional latent space. Specifically, we have\nq(\u03b8|Xc,Wc) = N(\u03b8;\u00b5(Xc), diag(\u03c32(Xc))), \u00b5(Xc) = W1g(Xc) + a1,\nlog \u03c3(Xc) = W2g(Xc) + a2,\nwhere g(\u00b7) denotes the feed-forward neural network. The weight matrices W1, W2 and biases a1, a2 are shared across documents. Each document has its own \u00b5(Xc) and \u03c3(Xc) resulting in a unique distribution q(\u03b8|Xc) for each document. The output of the inference network is a distribution on \u03b8, which we regard as the summarization of the semantic information, similar to the topic proportions in latent topic models. We show the role of the inference network in Figure 1(b). During training, the parameters of the inference network and the model are jointly learned and updated via truncated backpropagation through time using the Adam algorithm (Kingma and Ba, 2014). We use stochastic samples from q(\u03b8|Xc) and the reparameterization trick towards this end (Kingma and Welling, 2013; Rezende et al., 2014).\nGenerating sequential text and computing perplexity. Suppose we are given a word sequence y1:t\u22121, from which we have an initial estimation of q(\u03b8|Xc). To generate the next word yt, we compute the probability distribution of yt given y1:t\u22121 in an online fashion. We choose \u03b8 to be a point estimate \u03b8\u0302, the mean of its current distribution q(\u03b8|Xc). Marginalizing over the stop word indicator lt which is unknown prior to observing yt, the approximate distribution of yt is\np(yt|y1:t\u22121) \u2248 \u2211 lt p(yt|ht, \u03b8\u0302, lt)p(lt|ht).\nThe predicted word yt is a sample from this predictive distribution. We update q(\u03b8|Xc) by including yt toXc if yt is not a stop word. However, updating q(\u03b8|Xc) after each word prediction is expensive, so we use a sliding window as was done in Mikolov and Zweig (2012). To compute the perplexity, we use the approximate predictive distribution above.\nModel Complexity. TopicRNN has a complexity of O(H \u00d7 H + H \u00d7 (C + K) + Wc), where H is the size of the hidden layer of the RNN, C is the vocabulary size, K is the dimension of the topic vector, and Wc is the number of parameters of the inference network. The contextual RNN of Mikolov and Zweig (2012) accounts forO(H\u00d7H+H\u00d7(C+K)), not including the pre-training process, which might require more parameters than the additional Wc in our complexity.\n4Stop words can be determined using one of the several lists available online. For example, http://www. lextek.com/manuals/onix/stopwords2.html\n4 EXPERIMENTS\nWe assess the performance of our proposed TopicRNN model on word prediction and sentiment analysis5. For word prediction we use the Penn TreeBank dataset, a standard benchmark for assessing new language models (Marcus et al., 1993). For sentiment analysis we use the IMDB 100k dataset (Maas et al., 2011), also a common benchmark dataset for this application6. We use RNN, LSTM, and GRU cells in our experiments leading to TopicRNN, TopicLSTM, and TopicGRU.\n4.1 WORD PREDICTION\nWe first tested TopicRNN on the word prediction task using the Penn Treebank (PTB) portion of the Wall Street Journal. We use the standard split, where sections 0-20 (930K tokens) are used for training, sections 21-22 (74K tokens) for validation, and sections 23-24 (82K tokens) for testing (Mikolov et al., 2010). We use a vocabulary of size 10K that includes the special token unk for rare words and eos that indicates the end of a sentence. TopicRNN takes documents as inputs. We split the PTB data into blocks of 10 sentences to constitute documents as done by (Mikolov and Zweig, 2012). The inference network takes as input the bag-of-words representation of the input document. For that reason, the vocabulary size of the inference network is reduced to 9551 after excluding 449 pre-defined stop words.\nIn order to compare with previous work on contextual RNNs we trained TopicRNN using different network sizes. We performed word prediction using a recurrent neural network with 10 neurons,\n5Our code will be made publicly available for reproducibility. 6These datasets are publicly available at http://www.fit.vutbr.cz/~imikolov/rnnlm/\nsimple-examples.tgz and http://ai.stanford.edu/~amaas/data/sentiment/.\n100 neurons and 300 neurons. For these experiments, we used a multilayer perceptron with 2 hidden layers and 200 hidden units per layer for the inference network. The number of topics was tuned depending on the size of the RNN. For 10 neurons we used 18 topics. For 100 and 300 neurons we found 50 topics to be optimal. We used the validation set to tune the hyperparameters of the model. We used a maximum of 15 epochs for the experiments and performed early stopping using the validation set. For comparison purposes we did not apply dropout and used 1 layer for the RNN and its counterparts in all the word prediction experiments as reported in Table 2. One epoch for 10 neurons takes 2.5 minutes. For 100 neurons, one epoch is completed in less than 4 minutes. Finally, for 300 neurons one epoch takes less than 6 minutes. These experiments were ran on Microsoft Azure NC12 that has 12 cores, 2 Tesla K80 GPUs, and 112 GB memory. First, we show five randomly drawn topics in Table 1. These results correspond to a network with 100 neurons. We also illustrate some inferred topic distributions for several documents from TopicGRU in Figure 2. Similar to standard topic models, these distributions are also relatively peaky.\nNext, we compare the performance of TopicRNN to our baseline contextual RNN using perplexity. Perplexity can be thought of as a measure of surprise for a language model. It is defined as the exponential of the average negative log likelihood. Table 2 summarizes the results for different network sizes. We learn three things from these tables. First, the perplexity is reduced the larger the network size. Second, RNNs with context features perform better than RNNs without context features. Third, we see that TopicRNN gives lower perplexity than the previous baseline result reported by Mikolov and Zweig (2012). Note that to compute these perplexity scores for word prediction we use a sliding window to compute \u03b8 as we move along the sequences. The topic vector \u03b8 that is used from the current batch of words is estimated from the previous batch of words. This enables fair comparison to previously reported results (Mikolov and Zweig, 2012).7\nAnother aspect of the TopicRNN model we studied is its capacity to generate coherent text. To do this, we randomly drew a document from the test set and used this document as seed input to the inference network to compute \u03b8. Our expectation is that the topics contained in this seed document are reflected in the generated text. Table 3 shows generated text from models learned on the PTB and IMDB datasets. See Appendix A.3 for more examples.\n7We adjusted the scores in Table 2 from what was previously reported after correcting a bug in the computation of the ELBO.\n4.2 SENTIMENT ANALYSIS\nWe performed sentiment analysis using TopicRNN as a feature extractor on the IMDB 100K dataset. This data consists of 100,000 movie reviews from the Internet Movie Database (IMDB) website. The data is split into 75% for training and 25% for testing. Among the 75K training reviews, 50K are unlabelled and 25K are labelled as carrying either a positive or a negative sentiment. All 25K test reviews are labelled. We trained TopicRNN on 65K random training reviews and used the remaining 10K reviews for validation. To learn a classifier, we passed the 25K labelled training reviews through the learned TopicRNN model. We then concatenated the output of the inference network and the last state of the RNN for each of these 25K reviews to compute the feature vectors. We then used these feature vectors to train a neural network with one hidden layer, 50 hidden units, and a sigmoid activation function to predict sentiment, exactly as done in Le and Mikolov (2014).\nTo train the TopicRNN model, we used a vocabulary of size 5,000 and mapped all other words to the unk token. We took out 439 stop words to create the input of the inference network. We used 500 units and 2 layers for the inference network, and used 2 layers and 300 units per-layer for the\nRNN. We chose a step size of 5 and defined 200 topics. We did not use any regularization such as dropout. We trained the model for 13 epochs and used the validation set to tune the hyperparameters of the model and track perplexity for early stopping. This experiment took close to 78 hours on a MacBook pro quad-core with 16GHz of RAM. See Appendix A.4 for the visualization of some of the topics learned from this data.\nTable 4 summarizes sentiment classification results from TopicRNN and other methods. Our error rate is 6.28%.8 This is close to the state-of-the-art 5.91% (Miyato et al., 2016) despite that we do not use the labels and adversarial training in the feature extraction stage. Our approach is most similar to Le and Mikolov (2014), where the features were extracted in a unsupervised way and then a one-layer neural net was trained for classification.\nFigure 3 shows the ability of TopicRNN to cluster documents using the feature vectors as created during the sentiment analysis task. Reviews with positive sentiment are coloured in green while reviews carrying negative sentiment are shown in red. This shows that TopicRNN can be used as an unsupervised feature extractor for downstream applications. Table 3 shows generated text from models learned on the PTB and IMDB datasets. See Appendix A.3 for more examples. The overall generated text from IMDB encodes a negative sentiment.\n5 DISCUSSION AND FUTURE WORK\nIn this paper we introduced TopicRNN, a RNN-based language model that combines RNNs and latent topics to capture local (syntactic) and global (semantic) dependencies between words. The global dependencies as captured by the latent topics serve as contextual bias to an RNN-based language model. This contextual information is learned jointly with the RNN parameters by maximizing the evidence lower bound of variational inference. TopicRNN yields competitive per-word perplexity on the Penn Treebank dataset compared to previous contextual RNN models. We have reported a competitive classification error rate for sentiment analysis on the IMDB 100K dataset. We have also illustrated the capacity of TopicRNN to generate sensible topics and text. In future work, we will study the performance of TopicRNN when stop words are dynamically discovered during training. We will also extend TopicRNN to other applications where capturing context is important such as in dialog modeling. If successful, this will allow us to have a model that performs well across different natural language processing applications.\n8The experiments were solely based on TopicRNN. Experiments using TopicGRU/TopicLSTM are being carried out and will be added as an extended version of this paper.\nA APPENDIX\nA.1 DIMENSION OF THE PARAMETERS OF THE MODEL:\nWe use the following notation: C is the vocabulary size (including stop words), H is the number of hidden units of the RNN, K is the number of topics, and E is the dimension of the inference network hidden layer. Table 5 gives the dimension of each of the parameters of the TopicRNN model (ignoring the biases).\nA.2 DOCUMENTS USED TO INFER THE DISTRIBUTIONS ON FIGURE 2\nFigure on the left: \u2019the\u2019, \u2019market\u2019, \u2019has\u2019, \u2019grown\u2019, \u2019relatively\u2019, \u2019quiet\u2019, \u2019since\u2019, \u2019the\u2019, \u2019china\u2019, \u2019crisis\u2019, \u2019but\u2019, \u2019if\u2019, \u2019the\u2019, \u2019japanese\u2019, \u2019return\u2019, \u2019in\u2019, \u2019force\u2019, \u2019their\u2019, \u2019financial\u2019, \u2019might\u2019, \u2019could\u2019, \u2019compensate\u2019, \u2019to\u2019, \u2019some\u2019, \u2019extent\u2019, \u2019for\u2019, \u2019local\u2019, \u2019investors\u2019, \"\u2019\", \u2019<unk>\u2019, \u2019commitment\u2019, \u2019another\u2019, \u2019and\u2019, \u2019critical\u2019, \u2019factor\u2019, \u2019is\u2019, \u2019the\u2019, \u2019u.s.\u2019, \u2019hong\u2019, \u2019kong\u2019, \"\u2019s\", \u2019biggest\u2019, \u2019export\u2019, \u2019market\u2019, \u2019even\u2019, \u2019before\u2019, \u2019the\u2019, \u2019china\u2019, \u2019crisis\u2019, \u2019weak\u2019, \u2019u.s.\u2019, \u2019demand\u2019, \u2019was\u2019, \u2019slowing\u2019, \u2019local\u2019, \u2019economic\u2019, \u2019growth\u2019, \u2019<unk>\u2019, \u2019strong\u2019, \u2019consumer\u2019, \u2019spending\u2019, \u2019in\u2019, \u2019the\u2019, \u2019u.s.\u2019, \u2019two\u2019, \u2019years\u2019, \u2019ago\u2019, \u2019helped\u2019, \u2019<unk>\u2019, \u2019the\u2019, \u2019local\u2019, \u2019economy\u2019, \u2019at\u2019, \u2019more\u2019, \u2019than\u2019, \u2019twice\u2019, \u2019its\u2019, \u2019current\u2019, \u2019rate\u2019, \u2019indeed\u2019, \u2019a\u2019, \u2019few\u2019, \u2019economists\u2019, \u2019maintain\u2019, \u2019that\u2019, \u2019global\u2019, \u2019forces\u2019, \u2019will\u2019, \u2019continue\u2019, \u2019to\u2019, \u2019govern\u2019, \u2019hong\u2019, \u2019kong\u2019, \"\u2019s\", \u2019economic\u2019, \u2019<unk>\u2019, \u2019once\u2019, \u2019external\u2019, \u2019conditions\u2019, \u2019such\u2019, \u2019as\u2019, \u2019u.s.\u2019, \u2019demand\u2019, \u2019swing\u2019, \u2019in\u2019, \u2019the\u2019, \u2019territory\u2019, \"\u2019s\", \u2019favor\u2019, \u2019they\u2019, \u2019argue\u2019, \u2019local\u2019, \u2019businessmen\u2019, \u2019will\u2019, \u2019probably\u2019, \u2019overcome\u2019, \u2019their\u2019, \u2019N\u2019, \u2019worries\u2019, \u2019and\u2019, \u2019continue\u2019, \u2019doing\u2019, \u2019business\u2019, \u2019as\u2019, \u2019usual\u2019, \u2019but\u2019, \u2019economic\u2019, \u2019arguments\u2019, \u2019however\u2019, \u2019solid\u2019, \u2019wo\u2019, \"n\u2019t\", \u2019necessarily\u2019, \u2019<unk>\u2019, \u2019hong\u2019, \u2019kong\u2019, \"\u2019s\", \u2019N\u2019, \u2019million\u2019, \u2019people\u2019, \u2019many\u2019, \u2019are\u2019, \u2019refugees\u2019, \u2019having\u2019, \u2019fled\u2019, \u2019china\u2019, \"\u2019s\", \u2019<unk>\u2019, \u2019cycles\u2019, \u2019of\u2019, \u2019political\u2019, \u2019repression\u2019, \u2019and\u2019, \u2019poverty\u2019, \u2019since\u2019, \u2019the\u2019, \u2019communist\u2019, \u2019party\u2019, \u2019took\u2019, \u2019power\u2019, \u2019in\u2019, \u2019N\u2019, \u2019as\u2019, \u2019a\u2019, \u2019result\u2019, \u2019many\u2019, \u2019of\u2019, \u2019those\u2019, \u2019now\u2019, \u2019planning\u2019, \u2019to\u2019, \u2019leave\u2019, \u2019hong\u2019, \u2019kong\u2019, \u2019ca\u2019, \"n\u2019t\", \u2019easily\u2019, \u2019be\u2019, \u2019<unk>\u2019, \u2019by\u2019, \u2019<unk>\u2019, \u2019improvements\u2019, \u2019in\u2019, \u2019the\u2019, \u2019colony\u2019, \"\u2019s\", \u2019political\u2019, \u2019and\u2019, \u2019economic\u2019, \u2019climate\u2019\nFigure on the middle: \u2019it\u2019, \u2019said\u2019, \u2019the\u2019, \u2019man\u2019, \u2019whom\u2019, \u2019it\u2019, \u2019did\u2019, \u2019not\u2019, \u2019name\u2019, \u2019had\u2019, \u2019been\u2019, \u2019found\u2019, \u2019to\u2019, \u2019have\u2019, \u2019the\u2019, \u2019disease\u2019, \u2019after\u2019, \u2019hospital\u2019, \u2019tests\u2019, \u2019once\u2019, \u2019the\u2019, \u2019disease\u2019, \u2019was\u2019, \u2019confirmed\u2019, \u2019all\u2019, \u2019the\u2019, \u2019man\u2019, \"\u2019s\", \u2019associates\u2019, \u2019and\u2019, \u2019family\u2019, \u2019were\u2019, \u2019tested\u2019, \u2019but\u2019, \u2019none\u2019, \u2019have\u2019, \u2019so\u2019, \u2019far\u2019, \u2019been\u2019, \u2019found\u2019, \u2019to\u2019, \u2019have\u2019, \u2019aids\u2019, \u2019the\u2019, \u2019newspaper\u2019, \u2019said\u2019, \u2019the\u2019, \u2019man\u2019, \u2019had\u2019, \u2019for\u2019, \u2019a\u2019, \u2019long\u2019, \u2019time\u2019, \u2019had\u2019, \u2019a\u2019, \u2019chaotic\u2019, \u2019sex\u2019, \u2019life\u2019, \u2019including\u2019, \u2019relations\u2019, \u2019with\u2019, \u2019foreign\u2019, \u2019men\u2019, \u2019the\u2019, \u2019newspaper\u2019, \u2019said\u2019, \u2019the\u2019, \u2019polish\u2019, \u2019government\u2019, \u2019increased\u2019, \u2019home\u2019, \u2019electricity\u2019, \u2019charges\u2019, \u2019by\u2019, \u2019N\u2019, \u2019N\u2019, \u2019and\u2019, \u2019doubled\u2019, \u2019gas\u2019, \u2019prices\u2019, \u2019the\u2019, \u2019official\u2019, \u2019news\u2019, \u2019agency\u2019, \u2019<unk>\u2019, \u2019said\u2019, \u2019the\u2019, \u2019increases\u2019, \u2019were\u2019, \u2019intended\u2019, \u2019to\u2019, \u2019bring\u2019, \u2019<unk>\u2019, \u2019low\u2019, \u2019energy\u2019, \u2019charges\u2019, \u2019into\u2019, \u2019line\u2019, \u2019with\u2019, \u2019production\u2019, \u2019costs\u2019, \u2019and\u2019, \u2019compensate\u2019, \u2019for\u2019, \u2019a\u2019, \u2019rise\u2019, \u2019in\u2019, \u2019coal\u2019, \u2019prices\u2019, \u2019in\u2019, \u2019<unk>\u2019, \u2019news\u2019, \u2019south\u2019, \u2019korea\u2019, \u2019in\u2019, \u2019establishing\u2019, \u2019diplomatic\u2019, \u2019ties\u2019, \u2019with\u2019, \u2019poland\u2019, \u2019yesterday\u2019, \u2019announced\u2019, \u2019$\u2019, \u2019N\u2019, \u2019million\u2019, \u2019in\u2019, \u2019loans\u2019, \u2019to\u2019, \u2019the\u2019, \u2019financially\u2019, \u2019strapped\u2019, \u2019warsaw\u2019, \u2019government\u2019, \u2019in\u2019, \u2019a\u2019, \u2019victory\u2019, \u2019for\u2019, \u2019environmentalists\u2019, \u2019hungary\u2019, \"\u2019s\", \u2019parliament\u2019, \u2019terminated\u2019, \u2019a\u2019, \u2019multibillion-dollar\u2019, \u2019river\u2019, \u2019<unk>\u2019, \u2019dam\u2019, \u2019being\u2019, \u2019built\u2019, \u2019by\u2019, \u2019<unk>\u2019, \u2019firms\u2019, \u2019the\u2019, \u2019<unk>\u2019, \u2019dam\u2019, \u2019was\u2019, \u2019designed\u2019, \u2019to\u2019, \u2019be\u2019, \u2019<unk>\u2019, \u2019with\u2019, \u2019another\u2019, \u2019dam\u2019, \u2019now\u2019, \u2019nearly\u2019, \u2019complete\u2019, \u2019N\u2019, \u2019miles\u2019, \u2019<unk>\u2019, \u2019in\u2019, \u2019czechoslovakia\u2019, \u2019in\u2019, \u2019ending\u2019, \u2019hungary\u2019, \"\u2019s\", \u2019part\u2019, \u2019of\u2019, \u2019the\u2019, \u2019project\u2019, \u2019parliament\u2019, \u2019authorized\u2019, \u2019prime\u2019, \u2019minister\u2019, \u2019<unk>\u2019, \u2019<unk>\u2019, \u2019to\u2019, \u2019modify\u2019, \u2019a\u2019, \u2019N\u2019, \u2019agreement\u2019, \u2019with\u2019, \u2019czechoslovakia\u2019, \u2019which\u2019, \u2019still\u2019, \u2019wants\u2019, \u2019the\u2019, \u2019dam\u2019, \u2019to\u2019, \u2019be\u2019, \u2019built\u2019, \u2019mr.\u2019, \u2019<unk>\u2019, \u2019said\u2019, \u2019in\u2019, \u2019parliament\u2019, \u2019that\u2019, \u2019czechoslovakia\u2019, \u2019and\u2019, \u2019hungary\u2019, \u2019would\u2019, \u2019suffer\u2019, \u2019environmental\u2019, \u2019damage\u2019, \u2019if\u2019, \u2019the\u2019, \u2019<unk>\u2019, \u2019<unk>\u2019, \u2019were\u2019, \u2019built\u2019, \u2019as\u2019, \u2019planned\u2019\nFigure on the right: \u2019in\u2019, \u2019hartford\u2019, \u2019conn.\u2019, \u2019the\u2019, \u2019charter\u2019, \u2019oak\u2019, \u2019bridge\u2019, \u2019will\u2019, \u2019soon\u2019, \u2019be\u2019, \u2019replaced\u2019, \u2019the\u2019, \u2019<unk>\u2019, \u2019<unk>\u2019, \u2019from\u2019, \u2019its\u2019, \u2019<unk>\u2019, \u2019<unk>\u2019, \u2019to\u2019, \u2019a\u2019, \u2019park\u2019, \u2019<unk>\u2019, \u2019are\u2019, \u2019possible\u2019, \u2019citizens\u2019, \u2019in\u2019, \u2019peninsula\u2019, \u2019ohio\u2019, \u2019upset\u2019, \u2019over\u2019, \u2019changes\u2019, \u2019to\u2019, \u2019a\u2019, \u2019bridge\u2019, \u2019negotiated\u2019, \u2019a\u2019, \u2019deal\u2019, \u2019the\u2019, \u2019bottom\u2019, \u2019half\u2019, \u2019of\u2019, \u2019the\u2019, \u2019<unk>\u2019, \u2019will\u2019, \u2019be\u2019, \u2019type\u2019, \u2019f\u2019, \u2019while\u2019, \u2019the\u2019, \u2019top\u2019, \u2019half\u2019, \u2019will\u2019, \u2019have\u2019, \u2019the\u2019, \u2019old\u2019, \u2019bridge\u2019, \"\u2019s\", \u2019<unk>\u2019, \u2019pattern\u2019, \u2019similarly\u2019, \u2019highway\u2019, \u2019engineers\u2019, \u2019agreed\u2019, \u2019to\u2019, \u2019keep\u2019, \u2019the\u2019, \u2019old\u2019, \u2019<unk>\u2019, \u2019on\u2019, \u2019the\u2019, \u2019key\u2019, \u2019bridge\u2019, \u2019in\u2019, \u2019washington\u2019, \u2019d.c.\u2019, \u2019as\u2019, \u2019long\u2019, \u2019as\u2019, \u2019they\u2019, \u2019could\u2019, \u2019install\u2019, \u2019a\u2019, \u2019crash\u2019, \u2019barrier\u2019, \u2019between\u2019, \u2019the\u2019, \u2019sidewalk\u2019, \u2019and\u2019, \u2019the\u2019, \u2019road\u2019, \u2019<unk>\u2019, \u2019<unk>\u2019, \u2019drink\u2019, \u2019carrier\u2019, \u2019competes\u2019, \u2019with\u2019, \u2019<unk>\u2019, \u2019<unk>\u2019, \u2019<unk>\u2019, \u2019just\u2019, \u2019got\u2019, \u2019easier\u2019, \u2019or\u2019, \u2019so\u2019, \u2019claims\u2019, \u2019<unk>\u2019, \u2019corp.\u2019, \u2019the\u2019, \u2019maker\u2019, \u2019of\u2019, \u2019the\u2019, \u2019<unk>\u2019, \u2019the\u2019, \u2019chicago\u2019, \u2019company\u2019, \"\u2019s\", \u2019beverage\u2019, \u2019carrier\u2019, \u2019meant\u2019, \u2019to\u2019, \u2019replace\u2019, \u2019<unk>\u2019, \u2019<unk>\u2019, \u2019at\u2019, \u2019<unk>\u2019, \u2019stands\u2019, \u2019and\u2019, \u2019fast-food\u2019, \u2019outlets\u2019, \u2019resembles\u2019, \u2019the\u2019, \u2019plastic\u2019, \u2019<unk>\u2019, \u2019used\u2019, \u2019on\u2019, \u2019<unk>\u2019, \u2019of\u2019, \u2019beer\u2019, \u2019only\u2019, \u2019the\u2019, \u2019<unk>\u2019, \u2019hang\u2019, \u2019from\u2019, \u2019a\u2019, \u2019<unk>\u2019, \u2019of\u2019, \u2019<unk>\u2019, \u2019the\u2019, \u2019new\u2019, \u2019carrier\u2019, \u2019can\u2019, \u2019<unk>\u2019, \u2019as\u2019, \u2019many\u2019, \u2019as\u2019, \u2019four\u2019, \u2019<unk>\u2019, \u2019at\u2019, \u2019once\u2019, \u2019inventor\u2019, \u2019<unk>\u2019, \u2019marvin\u2019, \u2019says\u2019, \u2019his\u2019, \u2019design\u2019, \u2019virtually\u2019, \u2019<unk>\u2019, \u2019<unk>\u2019\nA.3 MORE GENERATED TEXT FROM THE MODEL:\nWe illustrate below some generated text resulting from training TopicRNN on the PTB dataset. Here we used 50 neurons and 100 topics:\nText1: but the refcorp bond fund might have been unk and unk of the point rate eos house in national unk wall restraint in the property pension fund sold willing to zenith was guaranteed by $ N million at short-term rates maturities around unk products eos deposit posted yields slightly\nText2: it had happened by the treasury \u2019s clinical fund month were under national disappear institutions but secretary nicholas instruments succeed eos and investors age far compound average new york stock exchange bonds typically sold $ N shares in the N but paying yields further an average rate of long-term funds\nWe illustrate below some generated text resulting from training TopicRNN on the IMDB dataset. The settings are the same as for the sentiment analysis experiment:\nthe film \u2019s greatest unk unk and it will likely very nice movies to go to unk why various david proves eos the story were always well scary friend high can be a very strange unk unk is in love with it lacks even perfect for unk for some of the worst movies come on a unk gave a rock unk eos whatever let \u2019s possible eos that kyle can \u2019t different reasons about the unk and was not what you \u2019re not a fan of unk unk us rock which unk still in unk \u2019s music unk one as\nA.4 TOPICS FROM IMDB:\nBelow we show some topics resulting from the sentiment analysis on the IMDB dataset. The total number of topics is 200. Note here all the topics turn around movies which is expected since all reviews are about movies.\n", "cats": {"READ": 1.0, "DONTREAD": 0.0}, "reviews": [{"IS_META_REVIEW": true, "comments": "This work combines a LDA-type topic model with a RNN and models this by having an additive effect on the predictive distribution via the topic parameters. A variational auto-encoder is used to infer the topic distribution for a given piece of text and the RNN is trained as a RNNLM. The last hidden state of the RNNLM and the topic parameters are then concatenated to use as a feature representation.\n\nThe paper is well written and easy to understand. Using the topic as an additive effect on the vocabulary allows for easy inference but intuitively I would expect the topic to affect the dynamics too, e.g. the state of the RNN. The results on using this model as a feature extractor for IMDB are quite strong. Is the RNN fine-tuned on the labelled IMDB data? However, the results for PTB are weaker. From the original paper, an ensemble of 2 LSTMs is able to match the topicRNN score. This method of jointly modelling topics and a language model seems effective and relatively easy to implement.\n\nFinally, the IMDB result is no longer state of the art since this result appeared in May (Miyato et al., Adversarial Training Methods for Semi-Supervised Text Classification).\n\nSome questions:\nHow important is the stop word modelling? What do the results look like if l_t = 0.5 for all t?\n\nIt seems surprising that the RNN was more effective than the LSTM. Was gradient clipping tried in the topicLSTM case? Do GRUs also fail to work?\n\nIt is also unfortunate that the model requires a stop-word list. Is the link in footnote 4 the one that is used in the experiments?\n\nDoes factoring out the topics in this way allow the RNN to scale better with more neurons? How reasonable does the topic distribution look for individual documents? How peaked do they tend to be? Can you show some examples of the inferred distribution? The topics look odd for IMDB with the top word of two of the topics being the same: 'campbell'. It would be interesting to compare these topics with those inferred by LDA on the same datasets.\n\nMinor comments:\nBelow figure 2: GHz -> GB\n\\Gamma is not defined."}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "Though the have been attempts to incorporate both \"topic-like\" and \"sequence-like\" methods in the past (e.g, the work of Hanna Wallach, Amit Gruber and other), they were quite computationally expensive, especially when high-order ngrams are incorporated. This is a modern take on this challenge: using RNNs and the VAE / inference network framework. The results are quite convincing, and the paper is well written.\n \n Pros:\n -- clean and simple model\n -- sufficiently convincing experimentation\n \n Cons:\n -- other ways to model interaction between RNN and topic representation could be considered (see comments of R2 and R1)", "OTHER_KEYS": "ICLR 2017 pcs"}, {"DATE": "16 Jan 2017 (modified: 20 Jan 2017)", "TITLE": "General Answer To Reviewers and ACs", "IS_META_REVIEW": false, "comments": "We thank the reviewers and the anonymous commenters for the helpful feedback and questions!\n\u00a0\nWe first summarize the main idea of this paper below:\n\u00a0\nNeural network-based language models have achieved state of the art results on many NLP tasks. One difficult problem is to capture long-range dependencies as motivated in the introduction of this paper. We propose to solve this by integrating latent topics as context and jointly training these contextual features with the parameters of an RNN language model. We provide a natural way of doing this integration by modeling stop words that are excluded by topic models but needed for sequential language models. This is done via binary classification where the probability of being a stop word is dictated by the hidden layer of the RNN. This modeling approach is possible when the contextual features as provided by the topics are passed directly to the softmax output layer of the RNN as additional bias. We illustrate the performance of this approach on two tasks and two datasets: word prediction on PTB and sentiment analysis on IMDB.\u00a0 We provide competitive perplexity scores on PTB showing more generalization capabilities (for example we only need a TopicGRU with 100 neurons to achieve a better perplexity than stacking 2 LSTMs with 200 neurons each ---112.4 vs 115.9). \"This method of jointly modeling topics and a language model seems effective and relatively easy to implement.\" quoted from AnonReviewer1.\n\u00a0\nWe have revised the paper and added the following changes:\n1- we added a line on the middle of page 7 to clarify even more how we compute the topic vector \\theta using a sliding window for word prediction.\n2- we added the test perplexity scores for TopicRNN, TopicLSTM, and TopicGRU as required by AnonReviewer3.\n3- we added the inferred distributions from some documents as required by AnonReviewer1.\n4- we added an explanation of why we passed the topics directly to the output layer at the bottom of page 4. \n\nWe answer each reviewer individually. See below.", "OTHER_KEYS": "Adji Bousso Dieng"}, {"DATE": "30 Dec 2016", "TITLE": "Is it unfair to use a global topic feature first and then do word prediction?", "IS_META_REVIEW": false, "comments": "I have a question regrading on the language modeling part. I believe it seems unfair to get a global word distribution(i.e. document topic) first and then use it to do word prediction. The RNN model would never do this and would perform not very good on the very beginning of this article. So does the ppl performance increase comes from this? \n\nWhat if the RNN model gets a global embedding first and then do the word prediction?", "OTHER_KEYS": "(anonymous)"}, {"TITLE": "No Title", "SUBSTANCE": 3, "MEANINGFUL_COMPARISON": 3, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "This paper introduces a model that blends ideas from generative topic models with those from recurrent neural network language models. The authors evaluate the proposed approach on a document level classification benchmark as well as a language modeling benchmark and it seems to work well. There is also some analysis as to topics learned by the model and its ability to generate text. Overall the paper is clearly written and with the code promised by the authors others should be able to re-implement the approach. I have 2 potentially major questions I would ask the authors to address:\n\n1 - LDA topic models make an exchangability (bag of words) assumption. The discussion of the generative story for TopicRNN should explicitly discuss whether this assumption is also made. On the surface it appears it is since y_t is sampled using only the document topic vector and h_t but we know that in practice h_t comes from a recurrent model that observes y_t-1. Not clear how this clean exposition of the generative model relates to what is actually done. In the Generating sequential text section it\u2019s clear the topic model can\u2019t generate words without using y_1 - t-1 but this seems inconsistent with the generative model specification. This needs to be shown in the paper and made clear to have a complete paper.\n\n\n2 -  The topic model only allows for linear interactions of the topic vector theta. It seems like this might be required to keep the generative model tractable but seems like a very poor assumption. We would expect the topic representation to have rich interactions with a language model to create nonlinear adjustments to word probabilities for a document. Please add discussion as to why this modeling choice exists and if possible how future work could modify that assumption (or explain why it\u2019s not such a bad assumption as one might imagine)\n\n\n\n\nFigure 2 colors very difficult to distinguish. ", "IS_ANNOTATED": true, "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "20 Dec 2016", "CLARITY": 3, "REVIEWER_CONFIDENCE": 3}, {"OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "TITLE": "review", "comments": "This paper presents TopicRNN, a combination of LDA and RNN that augments traditional RNN with latent topics by having a switching variable that includes/excludes additive effects from latent topics when generating a word. \nExperiments on two tasks are performed: language modeling on PTB, and sentiment analysis on IMBD. \nThe authors show that TopicRNN outperforms vanilla RNN on PTB and achieves SOTA result on IMDB.\n\nSome questions and comments:\n- In Table 2, how do you use LDA features for RNN (RNN LDA features)? \n- I would like to see results from LSTM included here, even though it is lower perplexity than TopicRNN. I think it's still useful to see how much adding latent topics close the gap between RNN and LSTM.\n- The generated text in Table 3 are not meaningful to me. What is this supposed to highlight? Is this generated text for topic \"trading\"? What about the IMDB one?\n- How scalable is the proposed method for large vocabulary size (>10K)?\n- What is the accuracy on IMDB if the extracted features is used directly to perform classification? (instead of being passed to a neural network with one hidden state). I think this is a fairer comparison to BoW, LDA, and SVM methods presented as baselines. ", "ORIGINALITY": 2, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "19 Dec 2016", "CLARITY": 3, "REVIEWER_CONFIDENCE": 4}, {"OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "TITLE": "Nice work on feature extraction", "comments": "This work combines a LDA-type topic model with a RNN and models this by having an additive effect on the predictive distribution via the topic parameters. A variational auto-encoder is used to infer the topic distribution for a given piece of text and the RNN is trained as a RNNLM. The last hidden state of the RNNLM and the topic parameters are then concatenated to use as a feature representation.\n\nThe paper is well written and easy to understand. Using the topic as an additive effect on the vocabulary allows for easy inference but intuitively I would expect the topic to affect the dynamics too, e.g. the state of the RNN. The results on using this model as a feature extractor for IMDB are quite strong. Is the RNN fine-tuned on the labelled IMDB data? However, the results for PTB are weaker. From the original paper, an ensemble of 2 LSTMs is able to match the topicRNN score. This method of jointly modelling topics and a language model seems effective and relatively easy to implement.\n\nFinally, the IMDB result is no longer state of the art since this result appeared in May (Miyato et al., Adversarial Training Methods for Semi-Supervised Text Classification).\n\nSome questions:\nHow important is the stop word modelling? What do the results look like if l_t = 0.5 for all t?\n\nIt seems surprising that the RNN was more effective than the LSTM. Was gradient clipping tried in the topicLSTM case? Do GRUs also fail to work?\n\nIt is also unfortunate that the model requires a stop-word list. Is the link in footnote 4 the one that is used in the experiments?\n\nDoes factoring out the topics in this way allow the RNN to scale better with more neurons? How reasonable does the topic distribution look for individual documents? How peaked do they tend to be? Can you show some examples of the inferred distribution? The topics look odd for IMDB with the top word of two of the topics being the same: 'campbell'. It would be interesting to compare these topics with those inferred by LDA on the same datasets.\n\nMinor comments:\nBelow figure 2: GHz -> GB\n\\Gamma is not defined.", "IS_ANNOTATED": true, "IS_META_REVIEW": false, "RECOMMENDATION": 8, "DATE": "17 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "TITLE": "Prereview Questions", "comments": "", "ORIGINALITY": 2, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "DATE": "01 Dec 2016", "CLARITY": 3}, {"IS_META_REVIEW": true, "comments": "This work combines a LDA-type topic model with a RNN and models this by having an additive effect on the predictive distribution via the topic parameters. A variational auto-encoder is used to infer the topic distribution for a given piece of text and the RNN is trained as a RNNLM. The last hidden state of the RNNLM and the topic parameters are then concatenated to use as a feature representation.\n\nThe paper is well written and easy to understand. Using the topic as an additive effect on the vocabulary allows for easy inference but intuitively I would expect the topic to affect the dynamics too, e.g. the state of the RNN. The results on using this model as a feature extractor for IMDB are quite strong. Is the RNN fine-tuned on the labelled IMDB data? However, the results for PTB are weaker. From the original paper, an ensemble of 2 LSTMs is able to match the topicRNN score. This method of jointly modelling topics and a language model seems effective and relatively easy to implement.\n\nFinally, the IMDB result is no longer state of the art since this result appeared in May (Miyato et al., Adversarial Training Methods for Semi-Supervised Text Classification).\n\nSome questions:\nHow important is the stop word modelling? What do the results look like if l_t = 0.5 for all t?\n\nIt seems surprising that the RNN was more effective than the LSTM. Was gradient clipping tried in the topicLSTM case? Do GRUs also fail to work?\n\nIt is also unfortunate that the model requires a stop-word list. Is the link in footnote 4 the one that is used in the experiments?\n\nDoes factoring out the topics in this way allow the RNN to scale better with more neurons? How reasonable does the topic distribution look for individual documents? How peaked do they tend to be? Can you show some examples of the inferred distribution? The topics look odd for IMDB with the top word of two of the topics being the same: 'campbell'. It would be interesting to compare these topics with those inferred by LDA on the same datasets.\n\nMinor comments:\nBelow figure 2: GHz -> GB\n\\Gamma is not defined."}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "Though the have been attempts to incorporate both \"topic-like\" and \"sequence-like\" methods in the past (e.g, the work of Hanna Wallach, Amit Gruber and other), they were quite computationally expensive, especially when high-order ngrams are incorporated. This is a modern take on this challenge: using RNNs and the VAE / inference network framework. The results are quite convincing, and the paper is well written.\n \n Pros:\n -- clean and simple model\n -- sufficiently convincing experimentation\n \n Cons:\n -- other ways to model interaction between RNN and topic representation could be considered (see comments of R2 and R1)", "OTHER_KEYS": "ICLR 2017 pcs"}, {"DATE": "16 Jan 2017 (modified: 20 Jan 2017)", "TITLE": "General Answer To Reviewers and ACs", "IS_META_REVIEW": false, "comments": "We thank the reviewers and the anonymous commenters for the helpful feedback and questions!\n\u00a0\nWe first summarize the main idea of this paper below:\n\u00a0\nNeural network-based language models have achieved state of the art results on many NLP tasks. One difficult problem is to capture long-range dependencies as motivated in the introduction of this paper. We propose to solve this by integrating latent topics as context and jointly training these contextual features with the parameters of an RNN language model. We provide a natural way of doing this integration by modeling stop words that are excluded by topic models but needed for sequential language models. This is done via binary classification where the probability of being a stop word is dictated by the hidden layer of the RNN. This modeling approach is possible when the contextual features as provided by the topics are passed directly to the softmax output layer of the RNN as additional bias. We illustrate the performance of this approach on two tasks and two datasets: word prediction on PTB and sentiment analysis on IMDB.\u00a0 We provide competitive perplexity scores on PTB showing more generalization capabilities (for example we only need a TopicGRU with 100 neurons to achieve a better perplexity than stacking 2 LSTMs with 200 neurons each ---112.4 vs 115.9). \"This method of jointly modeling topics and a language model seems effective and relatively easy to implement.\" quoted from AnonReviewer1.\n\u00a0\nWe have revised the paper and added the following changes:\n1- we added a line on the middle of page 7 to clarify even more how we compute the topic vector \\theta using a sliding window for word prediction.\n2- we added the test perplexity scores for TopicRNN, TopicLSTM, and TopicGRU as required by AnonReviewer3.\n3- we added the inferred distributions from some documents as required by AnonReviewer1.\n4- we added an explanation of why we passed the topics directly to the output layer at the bottom of page 4. \n\nWe answer each reviewer individually. See below.", "OTHER_KEYS": "Adji Bousso Dieng"}, {"DATE": "30 Dec 2016", "TITLE": "Is it unfair to use a global topic feature first and then do word prediction?", "IS_META_REVIEW": false, "comments": "I have a question regrading on the language modeling part. I believe it seems unfair to get a global word distribution(i.e. document topic) first and then use it to do word prediction. The RNN model would never do this and would perform not very good on the very beginning of this article. So does the ppl performance increase comes from this? \n\nWhat if the RNN model gets a global embedding first and then do the word prediction?", "OTHER_KEYS": "(anonymous)"}, {"TITLE": "No Title", "SUBSTANCE": 3, "MEANINGFUL_COMPARISON": 3, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "This paper introduces a model that blends ideas from generative topic models with those from recurrent neural network language models. The authors evaluate the proposed approach on a document level classification benchmark as well as a language modeling benchmark and it seems to work well. There is also some analysis as to topics learned by the model and its ability to generate text. Overall the paper is clearly written and with the code promised by the authors others should be able to re-implement the approach. I have 2 potentially major questions I would ask the authors to address:\n\n1 - LDA topic models make an exchangability (bag of words) assumption. The discussion of the generative story for TopicRNN should explicitly discuss whether this assumption is also made. On the surface it appears it is since y_t is sampled using only the document topic vector and h_t but we know that in practice h_t comes from a recurrent model that observes y_t-1. Not clear how this clean exposition of the generative model relates to what is actually done. In the Generating sequential text section it\u2019s clear the topic model can\u2019t generate words without using y_1 - t-1 but this seems inconsistent with the generative model specification. This needs to be shown in the paper and made clear to have a complete paper.\n\n\n2 -  The topic model only allows for linear interactions of the topic vector theta. It seems like this might be required to keep the generative model tractable but seems like a very poor assumption. We would expect the topic representation to have rich interactions with a language model to create nonlinear adjustments to word probabilities for a document. Please add discussion as to why this modeling choice exists and if possible how future work could modify that assumption (or explain why it\u2019s not such a bad assumption as one might imagine)\n\n\n\n\nFigure 2 colors very difficult to distinguish. ", "IS_ANNOTATED": true, "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "20 Dec 2016", "CLARITY": 3, "REVIEWER_CONFIDENCE": 3}, {"OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "TITLE": "review", "comments": "This paper presents TopicRNN, a combination of LDA and RNN that augments traditional RNN with latent topics by having a switching variable that includes/excludes additive effects from latent topics when generating a word. \nExperiments on two tasks are performed: language modeling on PTB, and sentiment analysis on IMBD. \nThe authors show that TopicRNN outperforms vanilla RNN on PTB and achieves SOTA result on IMDB.\n\nSome questions and comments:\n- In Table 2, how do you use LDA features for RNN (RNN LDA features)? \n- I would like to see results from LSTM included here, even though it is lower perplexity than TopicRNN. I think it's still useful to see how much adding latent topics close the gap between RNN and LSTM.\n- The generated text in Table 3 are not meaningful to me. What is this supposed to highlight? Is this generated text for topic \"trading\"? What about the IMDB one?\n- How scalable is the proposed method for large vocabulary size (>10K)?\n- What is the accuracy on IMDB if the extracted features is used directly to perform classification? (instead of being passed to a neural network with one hidden state). I think this is a fairer comparison to BoW, LDA, and SVM methods presented as baselines. ", "ORIGINALITY": 2, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "19 Dec 2016", "CLARITY": 3, "REVIEWER_CONFIDENCE": 4}, {"OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "TITLE": "Nice work on feature extraction", "comments": "This work combines a LDA-type topic model with a RNN and models this by having an additive effect on the predictive distribution via the topic parameters. A variational auto-encoder is used to infer the topic distribution for a given piece of text and the RNN is trained as a RNNLM. The last hidden state of the RNNLM and the topic parameters are then concatenated to use as a feature representation.\n\nThe paper is well written and easy to understand. Using the topic as an additive effect on the vocabulary allows for easy inference but intuitively I would expect the topic to affect the dynamics too, e.g. the state of the RNN. The results on using this model as a feature extractor for IMDB are quite strong. Is the RNN fine-tuned on the labelled IMDB data? However, the results for PTB are weaker. From the original paper, an ensemble of 2 LSTMs is able to match the topicRNN score. This method of jointly modelling topics and a language model seems effective and relatively easy to implement.\n\nFinally, the IMDB result is no longer state of the art since this result appeared in May (Miyato et al., Adversarial Training Methods for Semi-Supervised Text Classification).\n\nSome questions:\nHow important is the stop word modelling? What do the results look like if l_t = 0.5 for all t?\n\nIt seems surprising that the RNN was more effective than the LSTM. Was gradient clipping tried in the topicLSTM case? Do GRUs also fail to work?\n\nIt is also unfortunate that the model requires a stop-word list. Is the link in footnote 4 the one that is used in the experiments?\n\nDoes factoring out the topics in this way allow the RNN to scale better with more neurons? How reasonable does the topic distribution look for individual documents? How peaked do they tend to be? Can you show some examples of the inferred distribution? The topics look odd for IMDB with the top word of two of the topics being the same: 'campbell'. It would be interesting to compare these topics with those inferred by LDA on the same datasets.\n\nMinor comments:\nBelow figure 2: GHz -> GB\n\\Gamma is not defined.", "IS_ANNOTATED": true, "IS_META_REVIEW": false, "RECOMMENDATION": 8, "DATE": "17 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "TITLE": "Prereview Questions", "comments": "", "ORIGINALITY": 2, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "DATE": "01 Dec 2016", "CLARITY": 3}]}
{"text": "AUTOENCODING VARIATIONAL INFERENCE FOR TOPIC MODELS\n1 INTRODUCTION\nTopic models (Blei, 2012) are among the most widely used models for learning unsupervised representations of text, with hundreds of different model variants in the literature, and have have found applications ranging from the exploration of the scientific literature (Blei & Lafferty, 2007) to computer vision (Fei-Fei & Perona, 2005), bioinformatics (Rogers et al., 2005), and archaeology (Mimno, 2009). A major challenge in applying topic models and developing new models is the computational cost of computing the posterior distribution. Therefore a large body of work has considered approximate inference methods, the most popular methods being variational methods, especially mean field methods, and Markov chain Monte Carlo, particularly methods based on collapsed Gibbs sampling.\nBoth mean-field and collapsed Gibbs have the drawback that applying them to new topic models, even if there is only a small change to the modeling assumptions, requires re-deriving the inference methods, which can be mathematically arduous and time consuming, and limits the ability of practitioners to freely explore the space of different modeling assumptions. This has motivated the development of black-box inference methods (Ranganath et al., 2014; Mnih & Gregor, 2014; Kucukelbir et al., 2016; Kingma & Welling, 2014) which require only very limited and easy to compute information from the model, and hence can be applied automatically to new models given a simple declarative specification of the generative process.\nAutoencoding variational Bayes (AEVB) (Kingma & Welling, 2014; Rezende et al., 2014) is a particularly natural choice for topic models, because it trains an inference network (Dayan et al., 1995), a neural network that directly maps a document to an approximate posterior distribution,\n\u2217Additional affiliation: Alan Turing Institute, British Library, 96 Euston Road, London NW1 2DB\nwithout the need to run further variational updates. This is intuitively appealing because in topic models, we expect the mapping from documents to posterior distributions to be well behaved, that is, that a small change in the document will produce only a small change in topics. This is exactly the type of mapping that a universal function approximator like a neural network should be good at representing. Essentially, the inference network learns to mimic the effect of probabilistic inference, so that on test data, we can enjoy the benefits of probabilistic modeling without paying a further cost for inference.\nHowever, despite some notable successes for latent Gaussian models, black box inference methods are significantly more challenging to apply to topic models. For example, in initial experiments, we tried to apply ADVI (Kucukelbir et al., 2016), a recent black-box variational method, but it was difficult to obtain any meaningful topics. Two main challenges are: first, the Dirichlet prior is not a location scale family, which hinders reparameterisation, and second, the well known problem of component collapsing (Dinh & Dumoulin, 2016), in which the inference network becomes stuck in a bad local optimum in which all topics are identical.\nIn this paper, we present what is, to our knowledge, the first effective AEVB inference method for topic models, which we call Autoencoded Variational Inference for Topic Models or AVITM1. On several data sets, we find that AVITM yields topics of equivalent quality to standard mean-field inference, with a large decrease in training time. We also find that the inference network learns to mimic the process of approximate inference highly accurately, so that it is not necessary to run variational optimization at all on test data.\nBut perhaps more important is that AVITM is a black-box method that is easy to apply to new models. To illustrate this, we present a new topic model, called ProdLDA, in which the distribution over individual words is a product of experts rather than the mixture model used in LDA. We find that ProdLDA consistently produces better topics than standard LDA, whether measured by automatically determined topic coherence or qualitative examination. Furthermore, because we perform probabilistic inference using a neural network, we can fit a topic model on roughly a one million documents in under 80 minutes on a single GPU, and because we are using a black box inference method, implementing ProdLDA requires a change of only one line of code from our implementation of standard LDA.\nTo summarize, the main advantages of our methods are:\n1. Topic coherence: ProdLDA returns consistently better topics than LDA, even when LDA is trained using Gibbs sampling.\n2. Computational efficiency: Training AVITM is fast and efficient like standard mean-field. On new data, AVITM is much faster than standard mean field, because it requires only one forward pass through a neural network.\n3. Black box: AVITM does not require rigorous mathematical derivations to handle changes in the model, and can be easily applied to a wide range of topic models.\nOverall, our results suggest that AVITM is ready to take its place alongside mean field and collapsed Gibbs as one of the workhorse inference methods for topic models.\n2 BACKGROUND\nTo fix notation, we begin by describing topic modelling and AVITM.\n2.1 LATENT DIRICHLET ALLOCATION\nWe describe the most popular topic model, latent Dirichlet allocation (LDA). In LDA, each document of the collection is represented as a mixture of topics, where each topic \u03b2k is a probability distribution over the vocabulary. We also use \u03b2 to denote the matrix \u03b2 = (\u03b21 . . . \u03b2K). The generative process is then as described in Algorithm 1. Under this generative model, the marginal likelihood of\n1Code available at https://github.com/akashgit/autoencoding_vi_for_topic_models\nfor each document w do Draw topic distribution \u03b8 \u223c Dirichlet(\u03b1); for each word at position n do\nSample topic zn \u223c Multinomial(1, \u03b8); Sample word wn \u223c Multinomial(1, \u03b2zn);\nend end\nAlgorithm 1: LDA as a generative model.\na document w is\n(1)p(w|\u03b1, \u03b2) = \u222b \u03b8 ( N\u220f n=1 k\u2211 zn=1 p(wn|zn, \u03b2)p(zn|\u03b8) ) p(\u03b8|\u03b1)d\u03b8.\nPosterior inference over the hidden variables \u03b8 and z is intractable due to the coupling between the \u03b8 and \u03b2 under the multinomial assumption (Dickey, 1983).\n2.2 MEAN FIELD AND AEVB\nA popular approximation for efficient inference in topic models is mean field variational inference, which breaks the coupling between \u03b8 and z by introducing free variational parameters \u03b3 over \u03b8 and \u03c6 over z and dropping the edges between them. This results in an approximate variational posterior q(\u03b8, z|\u03b3, \u03c6) = q\u03b3(\u03b8) \u220f n q\u03c6(zn), which is optimized to best approximate the true posterior p(\u03b8, z|w, \u03b1, \u03b2). The optimization problem is to minimize\n(2)L(\u03b3, \u03c6 | \u03b1, \u03b2) = DKL [q(\u03b8, z|\u03b3, \u03c6)||p(\u03b8, z|w, \u03b1, \u03b2)]\u2212 log p(w|\u03b1, \u03b2).\nIn fact the above equation is a lower bound to the marginal log likelihood, sometimes called an evidence lower bound (ELBO), a fact which can be easily verified by multiplying and dividing (1) by the variational posterior and then applying Jensen\u2019s inequality on its logarithm. Note that the mean field method optimizes over an independent set of variational parameters for each document. To emphasize this, we will refer to this standard method by the non-standard name of Decoupled Mean-Field Variational Inference (DMFVI).\nFor LDA, this optimization has closed form coordinate descent equations due to the conjugacy between the Dirichlet and multinomial distributions. Although this is a computationally convenient aspect of DMFVI, it also limits its flexibility. Applying DMFVI to new models relies on the practitioner\u2019s ability to derive the closed form updates, which can be impractical and sometimes impossible.\nAEVB (Kingma & Welling, 2014; Rezende et al., 2014) is one of several recent methods that aims at \u201cblack box\u201d inference methods to sidestep this issue. First, rewrite the ELBO as\n(3)L(\u03b3, \u03c6 | \u03b1, \u03b2) = \u2212DKL [q(\u03b8, z|\u03b3, \u03c6)||p(\u03b8, z|\u03b1)] + Eq(\u03b8,z|\u03b3,\u03c6)[log p(w|z, \u03b8, \u03b1, \u03b2)]\nThis form is intuitive. The first term attempts to match the variational posterior over latent variables to the prior on the latent variables, while the second term ensures that the variational posterior favors values of the latent variables that are good at explaining the data. By analogy to autoencoders, this second term is referred to as a reconstruction term.\nWhat makes this method \u201cAutoencoding,\u201d and in fact the main difference from DMFVI, is the parameterization of the variational distribution. In AEVB, the variational parameters are computed by using a neural network called an inference network that takes the observed data as input. For example, if the model prior p(\u03b8) were Gaussian, we might define the inference network as a feedforward neural network (\u00b5(w),v(w)) = f(w, \u03b3), where \u00b5(w) and v(w) are both vectors of length k, and \u03b3 are the network\u2019s parameters. Then we might choose a Gaussian variational distribution q\u03b3(\u03b8) = N(\u03b8;\u00b5(w), diag(v(w))), where diag(\u00b7 \u00b7 \u00b7) produces a diagonal matrix from a column vector. The variational parameters \u03b3 can then be chosen by optimizing the ELBO (3). Note that we have\nnow, unlike DMFVI, coupled the variational parameters for different documents because they are all computed from the same neural network. To compute the expectations with respect to q in (3), Kingma & Welling (2014); Rezende et al. (2014) use a Monte Carlo estimator which they call the \u201creparameterization trick\u201d (RT; appears also in Williams (1992)). In the RT, we define a variate U with a simple distribution that is independent of all variational parameters, like a uniform or standard normal, and a reparameterization function F such that F (U, \u03b3) has distribution q\u03b3 . This is always possible, as we could choose F to be the inverse cumulative distribution function of q\u03b3 , although we will additionally want F to be easy to compute and differentiable. If we can determine a suitable F , then we can approximate (3) by taking Monte Carlo samples of U , and optimize \u03b3 using stochastic gradient descent.\n3 AUTOENCODING VARIATIONAL BAYES IN LATENT DIRICHLET ALLOCATION\nAlthough simple conceptually, applying AEVB to topic models raises several practical challenges. The first is the need to determine a reparameterization function for q(\u03b8) and q(zn) to use the RT. The zn are easily dealt with, but \u03b8 is more difficult; if we choose q(\u03b8) to be Dirichlet, it is difficult to apply the RT, whereas if we choose q to be Gaussian or logistic normal, then the KL divergence in (3) becomes more problematic. The second issue is the well known problem of component collapsing (Dinh & Dumoulin, 2016), which a type of bad local optimum that is particularly endemic to AEVB and similar methods. We describe our solutions to each of those problems in the next few subsections.\n3.1 COLLAPSING z\u2019S\nDealing with discrete variables like z using reparameterization can be problematic, but fortunately in LDA the variable z can be conveniently summed out. By collapsing z we are left with having to sample from \u03b8 only, reducing (1) to\n(4)p(w|\u03b1, \u03b2) = \u222b \u03b8 ( N\u220f n=1 p(wn|\u03b2, \u03b8) ) p(\u03b8|\u03b1)d\u03b8.\nwhere the distribution of wn|\u03b2, \u03b8 is Multinomial(1, \u03b2\u03b8), recalling that \u03b2 denotes the matrix of all topic-word probability vectors.\n3.2 WORKING WITH DIRICHLET BELIEFS: LAPLACE APPROXIMATION\nLDA gets its name from the Dirichlet prior on the topic proportions \u03b8, and the choice of Dirichlet prior is important to obtaining interpretable topics (Wallach et al., 2009). But it is difficult to handle the Dirichlet within AEVB because it is difficult to develop an effective reparameterization function for the RT. Fortunately, a RT does exist for the Gaussian distribution and has been shown to perform quite well in the context of variational autoencoder (VAE) (Kingma & Welling, 2014).\nWe resolve this issue by constructing a Laplace approximation to the Dirichlet prior. Following MacKay (1998), we do so in the softmax basis instead of the simplex. There are two benefits of this choice. First, Dirichlet distributions are unimodal in the softmax basis with their modes coinciding with the means of the transformed densities. Second, the softmax basis also allows for carrying out unconstrained optimization of the cost function without the simplex constraints. The Dirichlet probability density function in this basis over the softmax variable h is given by\n(5)P (\u03b8(h)|\u03b1) = \u0393( \u2211 k \u03b1k)\u220f\nk \u0393(\u03b1k) \u220f k \u03b8\u03b1kk g(1 Th).\nHere \u03b8 = \u03c3(h), where \u03c3(.) represents the softmax function. Recall that the Jacobian of \u03c3 is proportional to \u220f k \u03b8k and g(\u00b7) is an arbitrary density that ensures integrability by constraining the redundant degree of freedom. We use the Laplace approximation of Hennig et al. (2012), which\nhas the property that the covariance matrix becomes diagonal for large k (number of topics). This approximation to the Dirichlet prior p(\u03b8|\u03b1) is results in the distribution over the softmax variables h as a multivariate normal with mean \u00b51 and covariance matrix \u03a31 where\n\u00b51k = log\u03b1k \u2212 1\nK \u2211 i log\u03b1i\n\u03a31kk = 1\n\u03b1k\n( 1\u2212 2\nK\n) + 1\nK2 \u2211 i 1 \u03b1k . (6)\nFinally, we approximate p(\u03b8|\u03b1) in the simplex basis with p\u0302(\u03b8|\u00b51,\u03a31) = LN (\u03b8|\u00b51,\u03a31) where LN is a logistic normal distribution with parameters \u00b51,\u03a31. Although we approximate the Dirichlet prior in LDA with a logistic normal, this is not the same idea as a correlated topic model (Blei & Lafferty, 2006), because we use a diagonal covariance matrix. Rather, it is an approximation to standard LDA.\n3.3 VARIATIONAL OBJECTIVE\nNow we can write the modified variational objective function. We use a logistic normal variational distribution over \u03b8 with diagonal covariance. More precisely, we define two inference networks as feed forward neural networks f\u00b5 and f\u03a3 with parameters \u03b4; the output of each network is a vector in RK . Then for a document w, we define q(\u03b8) to be logistic normal with mean \u00b50 = f\u00b5(w, \u03b4) and diagonal covariance \u03a30 = diag(f\u03a3(w, \u03b4)), where diag converts a column vector to a diagonal matrix. Note that we can generate samples from q(\u03b8) by sampling \u223c N (0, I) and computing \u03b8 = \u03c3(\u00b50 + \u03a3 1/2 0 ).\nWe can now write the ELBO as\nL(\u0398) = D\u2211 d=1\n[ \u2212 ( 1\n2\n{ tr(\u03a3\u221211 \u03a30) + (\u00b51 \u2212\u00b50)T\u03a3 \u22121 1 (\u00b51 \u2212\u00b50)\u2212K + log\n|\u03a31| |\u03a30|\n}) (7)\n+E \u223cN (0,I) [ w>d log ( \u03c3(\u03b2)\u03c3(\u00b50 + \u03a3 1/2 0 ) )]] ,\nwhere \u0398 represents the set of all the model and variational parameters and w1 . . .wD are the documents in the corpus. The first line in this equation arises from the KL divergence between the two logistic normal distributions q and p\u0302, while the second line is the reconstruction error.\nIn order to impose the simplex constraint on the \u03b2 matrix during the optimization, we apply the softmax transformation. That is, each topic \u03b2k \u2208 RV is unconstrained, and the notation \u03c3(\u03b2) means to apply the softmax function separately to each column of the matrix \u03b2. Note that the mixture of multinomials for each word wn can then be written as p(wn|\u03b2, \u03b8) = [\u03c3(\u03b2)\u03b8]wn , which explains the dot product in (7). To optimize (7), we use stochastic gradient descent using Monte Carlo samples from , following the Law of the Unconscious Statistician.\n3.4 TRAINING AND PRACTICAL CONSIDERATIONS: DEALING WITH COMPONENT COLLAPSING\nAEVB is prone to component collapsing (Dinh & Dumoulin, 2016), which is a particular type of local optimum very close to the prior belief, early on in the training. As the latent dimensionality of the model is increased, the KL regularization in the variational objective dominates, so that the outgoing decoder weights collapse for the components of the latent variable that reach close to the prior and do not show any posterior divergence. In our case, the collapsing specifically occurs because of the inclusion of the softmax transformation to produce \u03b8. The result is that the k inferred topics are identical as shown in table 7.\nWe were able to resolve this issue by tweaking the optimization. Specifically, we train the network with the ADAM optimizer (Kingma & Ba, 2015) using high moment weight (\u03b21) and learning rate (\u03b7). Through training at higher rates, early peaks in the functional space can be easily avoided. The\nproblem is that momentum based training coupled with higher learning rate causes the optimizer to diverge. While explicit gradient clipping helps to a certain extent, we found that batch normalization (Ioffe & Szegedy, 2015) does even better by smoothing out the functional space and hence curbing sudden divergence.\nFinally, we also found an increase in performance with dropout units when applied to \u03b8 to force the network to use more of its capacity.\nWhile more prominent in the AEVB framework, the collapsing can also occurs in DMFVI if the learning offset (referred to as the \u03c4 parameter (Hofmann, 1999)) is not set properly. Interestingly, a similar learning offset or annealing based approach can also be used to down-weight the KL term in early iterations of the training to avoid local optima.\n4 PRODLDA: LATENT DIRICHLET ALLOCATION WITH PRODUCTS OF EXPERTS\nIn LDA, the distribution p(w|\u03b8, \u03b2) is a mixture of multinomials. A problem with this assumption is that it can never make any predictions that are sharper than the components that are being mixed (Hinton & Salakhutdinov, 2009). This can result in some topics appearing that are poor quality and do not correspond well with human judgment. One way to resolve this issue is to replace this word-level mixture with a weighted product of experts which by definition is capable of making sharper predictions than any of the constituent experts (Hinton, 2002). In this section we present a novel topic model PRODLDA that replaces the mixture assumption at the word-level in LDA with a weighted product of experts, resulting in a drastic improvement in topic coherence. This is a good illustration of the benefits of a black box inference method, like AVITM, to allow exploration of new models.\n4.1 MODEL\nThe PRODLDA model can be simply described as latent Dirichlet allocation where the word-level mixture over topics is carried out in natural parameter space, i.e. the topic matrix is not constrained to exist in a multinomial simplex prior to mixing. In other words, the only changes from LDA are that \u03b2 is unnormalized, and that the conditional distribution of wn is defined as wn|\u03b2, \u03b8 \u223c Multinomial(1, \u03c3(\u03b2\u03b8)).\nThe connection to a product of experts is straightforward, as for the multinomial, a mixture of natural parameters corresponds to a weighted geometric average of the mean parameters. That is, consider two N dimensional multinomials parametrized by mean vectors p and q. Define the corresponding natural parameters as p = \u03c3(r) and q = \u03c3(s), and let \u03b4 \u2208 [0, 1]. It is then easy to show that\nP ( x|\u03b4r + (1\u2212 \u03b4)s ) \u221d N\u220f i=1 \u03c3(\u03b4ri + (1\u2212 \u03b4)si)xi \u221d N\u220f i=1 [r\u03b4i \u00b7 s (1\u2212\u03b4) i ] xi .\nSo the PRODLDA model can be simply described as a product of experts, that is, p(wn|\u03b8, \u03b2) \u221d\u220f k p(wn|zn = k, \u03b2)\u03b8k . PRODLDA is an instance of the exponential-family PCA (Collins et al., 2001) class, and relates to the exponential-family harmoniums (Welling et al., 2004) but with nonGaussian priors.\n5 RELATED WORK\nFor an overview of topic modeling, see Blei (2012). There are several examples of topic models based on neural networks and neural variational inference (Hinton & Salakhutdinov, 2009; Larochelle & Lauly, 2012; Mnih & Gregor, 2014; Miao et al., 2016) but we are unaware of methods that apply AEVB generically to a topic model specified by an analyst, or even of a successful application of AEVB to the most widely used topic model, latent Dirichlet allocation.\nRecently, Miao et al. (2016) introduced a closely related model called the Neural Variational Document Model (NVDM). This method uses a latent Gaussian distribution over topics, like probabilistic latent semantic indexing, and averages over topic-word distributions in the logit space. However,\nthey do not use either of the two key aspects of our work: explicitly approximating the Dirichlet prior using a Gaussian, or high-momentum training. In the experiments we show that these aspects lead to much improved training and much better topics.\n6 EXPERIMENTS AND RESULTS\nQualitative evaluation of topic models is a challenging task and consequently a large body of work has developed automatic evaluation metrics that attempt to match human judgment of topic quality. Traditionally, perplexity has been used to measure the goodness-of-fit of the model but it has been repeatedly shown that perplexity is not a good metric for qualitative evaluation of topics (Newman et al., 2010). Several new metrics of topic coherence evaluation have thus been proposed; see Lau et al. (2014) for a comparative review. Lau et al. (2014) showed that among all the competing metrics, normalized pointwise mutual information (NPMI) between all the pairs of words in a set of topics matches human judgment most closely, so we adopt it in this work. We also report perplexity, primarily as a way of evaluating the capability of different optimizers. Following standard practice (Blei et al., 2003), for variational methods we use the ELBO to calculate perplexity. For AEVB methods, we calculate the ELBO using the same Monte Carlo approximation as for training.\nWe run experiments on both the 20 Newsgroups (11,000 training instances with 2000 word vocabulary) and RCV1 Volume 2 ( 800K training instances with 10000 word vocabulary) datasets. Our preprocessing involves tokenization, removal of some non UTF-8 characters for 20 Newsgroups and English stop word removal. We first compare our AVITM inference method with the standard online mean-field variational inference (Hoffman et al., 2010) and collapsed Gibbs sampling (Griffiths & Steyvers, 2004) on the LDA model. We use standard implementations of both methods, scikit-learn for DMFVI and mallet (McCallum, 2002) for collapsed Gibbs. Then we compare two autoencoding inference methods on three different topic models: standard LDA, PRODLDA using our inference method and the Neural Variational Document Model (NVDM) (Miao et al., 2016), using the inference described in the paper.2\nTables 1 and 2 show the average topic coherence values for all the models for two different settings of k, the number of topics. Comparing the different inference methods for LDA, we find that, consistent with previous work, collapsed Gibbs sampling yields better topics than mean-field methods. Among the variational methods, we find that VAE-LDA model (AVITM) 3 yields similar topic coherence and perplexity to the standard DMFVI (although in some cases, VAE-LDA yields significantly better topics). However, AVITM is significantly faster to train than DMFVI. It takes 46 seconds on 20 Newsgroup compared to 18 minutes for DMFVI. Whereas for a million document corpus of RCV1 it only under 1.5 hours while scikit-learn\u2019s implementation of DMFVI failed to return any results even after running for 24 hours.4\nComparing the new topic models than LDA, it is clear that PRODLDA finds significantly better topics than LDA, even when trained by collapsed Gibbs sampling. To verify this qualitatively, we display examples of topics from all the models in Table 6. The topics from ProdLDA appear visually more coherent than NVDM or LDA. Unfortunately, NVDM does not perform comparatively to LDA\n2We have used both https://github.com/carpedm20/variational-text-tensorflow and the NVDM author\u2019s (Miao et al., 2016) implementation.\n3We recently found that \u2019whitening\u2019 the topic matrix significantly improves the topic coherence for VAELDA. Manuscript in preparation.\n4Therefore, we were not able to report topic coherence for DMFVI on RCV1\nfor any value of k. To avoid any training dissimilarities we train all the competing models until we reach the perplexities that were reported in previous work. These are reported in Table 35.\nA major benefit of AVITM inference is that it does not require running variational optimization, which can be costly, for new data. Rather, the inference network can be used to obtain topic proportions for new documents for new data points without running any optimization. We evaluate whether this approximation is accurate, i.e. whether the neural network effectively learns to mimic probabilistic inference. We verify this by training the model on the training set, then on the test set, holding the topics (\u03b2 matrix) fixed, and comparing the test perplexity if we obtain topic proportions by running the inference neural network directly, or by the standard method of variational optimization of the inference network on the test set. As shown in Table 4, the perplexity remains practically un-changed. The computational benefits of this are remarkable. On both the datasets, computing perplexity using the neural network takes well under a minute, while running the standard variational approximation takes \u223c 3 minutes even on the smaller 20 Newsgroups data. Finally, we investigate the reasons behind the improved topic coherence in PRODLDA. First, Table 5 explores the effects of each of our two main ideas separately. In this table, \u201cDirichlet\u201d means that the prior is the Laplace approximation to Dirichlet(\u03b1 = 0.02), while \u201cGaussian\u201d indicates that we use a standard Gaussian as prior. \u2018High Learning Rate\u201d training means we use \u03b21 > 0.8 and 0.1 > \u03b7 > 0.0016 with batch normalization, whereas \u201cLow Learning Rate\u201d means \u03b21 > 0.8 and 0.0009 > \u03b7 > 0.00009 without batch normalization. (For both parameters, the precise value was chosen by Bayesian optimization. We found that these values in the \u201dwith BN\u201d cases were close to the default settings in the Adam optimizer.) We find that the high topic coherence that we achieve in this work is only possible if we use both tricks together. In fact the high learning rates with momentum is required to avoid local minima in the beginning of the training and batch-normalization is required to be able to train the network at these values without diverging. If trained at a lower momentum value or at a lower learning rate PRODLDA shows component collapsing. Interestingly, if we choose a Gaussian prior, rather than the logistic normal approximation used in ProdLDA or NVLDA, the model is easier to train even with low learning rate without any momentum or batch normalization.\nThe main advantage of AVITM topic models as opposed to NVDM is that the Laplace approximation allows us to match a specific Dirichlet prior of interest. As pointed out by Wallach et al. (2009), the choice of Dirichlet hyperparameter is important to the topic quality of LDA. Following this reasoning, we hypothesize that AVITM topics are higher quality than those of NVDM because they are much more focused, i.e., apply to a more specific subset of documents of interest. We provide support for this hypothesis in Figure 1, by evaluating the sparsity of the posterior proportions over topics, that is, how many of the model\u2019s topics are typically used to explain each document. In order to estimate the sparsity in topic proportions, we project samples from the Gaussian latent spaces of PRODLDA and NVDM in the simplex and average them across documents. We compare the topic\n5We note that much recent work follows Hinton & Salakhutdinov (2009) in reporting perplexity for the LDA Gibbs sampler on only a small subset of the test data. Our results are different because we use the entire test dataset.\n6\u03b21 is the weight on the average of the gradients from the previous time step and \u03b7 refers to the learning rate.\nsparsity for the standard Gaussian prior used by NVDM to the Laplace approximation of Dirichlet priors with different hyperparameters. Clearly the Laplace approximation to the Dirichlet prior significantly promotes sparsity, providing support for our hypothesis that preserving the Dirichlet prior explains the the increased topic coherence in our method.\nThe inference network architecture can be found in figure 2 in the appendix.\n7 DISCUSSION AND FUTURE WORK\nWe present what is to our knowledge the first effective AEVB inference algorithm for latent Dirichlet allocation. Although this combination may seem simple in principle, in practice this method is difficult to train because of the Dirichlet prior and because of the component collapsing problem. By addressing both of these problems, we presented a black-box inference method for topic models with the notable advantage that the neural network allows computing topic proportions for new documents without the need to run any variational optimization. As an illustration of the advantages of\n1. write article get thanks like anyone please know look one\nblack box inference techniques, we presented a new topic model, ProdLDA, which achieves significantly better topics than LDA, while requiring a change of only one line of code from AVITM for LDA. Our results suggest that AVITM inference is ready to take its place alongside mean field and collapsed Gibbs as one of the workhorse inference methods for topic models. Future work could include extending our inference methods to handle dynamic and correlated topic models.\nACKNOWLEDGMENTS\nWe thank Andriy Mnih, Chris Dyer, Chris Russell, David Blei, Hannah Wallach, Max Welling, Mirella Lapata and Yishu Miao for helpful comments, discussions and feedback.\nA NETWORK ARCHITECTURE\n", "cats": {"READ": 1.0, "DONTREAD": 0.0}, "reviews": [{"IS_META_REVIEW": true, "comments": "This is an interesting paper on a VAE framework for topic models. The main idea is to train a recognition model for the inference phase which, because of so called \u201camortized inference\u201d can be much faster than normal inference where inference must be run iteratively for every document. Some comments:\nEqn 5: I find the notation p(theta(h)|alpha) awkward. Why not P(h|alpha) ?\nThe generative model seems agnostic to document length, meaning that the latent variables only generate probabilities over word space. However, the recognition model is happy to radically change the probabilities q(z|x) if the document length changes because the input to q changes. This seems undesirable. Maybe they should normalize the input to the recognition network?\nThe ProdLDA model might well be equivalent to exponential family PCA or some variant thereof:"}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "The reviewers agree that the approach is interesting and the paper presents useful findings. They also raise enough questions and suggestions for improvements that I believe the paper will be much stronger after further revision, though these seem straightforward to address.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"IMPACT": 3, "SUBSTANCE": 2, "MEANINGFUL_COMPARISON": 3, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "The authors propose NVI for LDA variants. The authors compare NVI-LDA to standard inference schemes such as CGS and online SVI. The authors also evaluate NVI on a different model ProdLDA (not sure this model has been proposed before in the topic modeling literature though?)\n\nIn general, I like the direction of this paper and NVI looks promising for LDA. The experimental results however confound model vs inference which makes it hard to understand the significance of the results. Furthermore, the authors don't discuss hyper-parameter selection which is known to significantly impact performance of topic models. This makes it hard to understand when the proposed method can be expected to work. \n\nCan you maybe generate synthetic datasets with different Dirichlet distributions and assess when the proposed method recovers the true parameters?\n\nFigure 1: Is this prior or posterior? The text talks about sparsity whereas the y-axis reads \"log p(topic proportions)\" which is a bit confusing.\n\nSection 3.2: it is not clear what you mean by unimodal in softmax basis. Consider a Dirichlet on K-dimensional simplex with concentration parameter alpha/K where alpha<1 makes it multimodal. Isn't the softmax basis still multimodal?\n\nNone of the numbers include error bars. Are the results statistically significant?\n\n\nMinor comments:\n\nLast term in equation (3) is not \"error\"; reconstruction accuracy or negative reconstruction error perhaps?\n\nThe idea of using an inference network is much older, cf. Helmholtz machine. \n", "SOUNDNESS_CORRECTNESS": 2, "ORIGINALITY": 4, "IS_ANNOTATED": true, "TITLE": "Promising direction, but the paper needs more work", "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "17 Dec 2016 (modified: 21 Jan 2017)", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "Nice paper to read", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer4", "comments": "This paper proposes the use of neural variational inference method for topic models. The paper shows a nice trick to approximate Dirichlet prior using softmax basis with a Gaussian and then the model is trained to maximize the variational lower bound. Also, the authors study a better way to alleviate the component collapsing issue, which has been problematic for continuous latent variables that follow Gaussian distribution. The results look promising and the experimental protocol sounds fine.\n\nMinor comments:\nPlease add citation to [1] or [2] for neural variational inference, and [2] for VAE. \nA typo in \u201cThis approximation to the Dirichlet prior p(\u03b8|\u03b1) is results in the distribution\u201d, it should be \u201cThis approximation to the Dirichlet prior p(\u03b8|\u03b1) results in the distribution\u201d\n\nIn table 2, it is written that DMFVI was trained more than 24hrs but failed to deliver any result, but why not wait until the end and report the numbers?\n\nIn table 3, why are the perplexities of LDA-Collapsed Gibbs and NVDM are lower while the proposed models (ProdLDA) generates more coherent topics? What is your intuition on this?\n\nHow does the training speed (until the convergence) differs by using different learning-rate and momentum scheduling approaches shown as in figure 1?\n\nIt may be also interesting to add some more analysis on the latent variables z (component collapsing and etc., although your results indirectly show that the learning-rate and momentum scheduling trick removes this issue).\n\nOverall, the paper clearly proposes its main idea, explain why it is good to use NVI, and its experimental results support the original claim. It explains well what are the challenges and demonstrate their solutions. \n\n[1] Minh et al., Neural Variational Inference and Learning in Belief Networks, ICML\u201914\n[2] Rezende et al., Stochastic Backpropagation and Approximate Inference in Deep Generative Models, ICML\u201914", "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "16 Dec 2016 (modified: 18 Dec 2016)", "REVIEWER_CONFIDENCE": 3}, {"TITLE": "VAE model for LDA. Interesting idea, but a incremental.  ", "MEANINGFUL_COMPARISON": 3, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "This is an interesting paper on a VAE framework for topic models. The main idea is to train a recognition model for the inference phase which, because of so called \u201camortized inference\u201d can be much faster than normal inference where inference must be run iteratively for every document. Some comments:\nEqn 5: I find the notation p(theta(h)|alpha) awkward. Why not P(h|alpha) ?\nThe generative model seems agnostic to document length, meaning that the latent variables only generate probabilities over word space. However, the recognition model is happy to radically change the probabilities q(z|x) if the document length changes because the input to q changes. This seems undesirable. Maybe they should normalize the input to the recognition network?\nThe ProdLDA model might well be equivalent to exponential family PCA or some variant thereof: ", "SOUNDNESS_CORRECTNESS": 3, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "14 Dec 2016", "REVIEWER_CONFIDENCE": 5}, {"TITLE": "Comparison to NVDM looks unfair", "OTHER_KEYS": "(anonymous)", "comments": "The comparison to NVDM looks unfair since the user introduces a couples tricks (Dirichlet prior, batch normalisation, high momentum training, etc.) which NVDM doesn't use. A more convincing experimental design is to explore the effect of each trick separately in neural variational inference. \n", "IS_META_REVIEW": false, "RECOMMENDATION": 5, "DATE": "05 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"IMPACT": 3, "SUBSTANCE": 2, "MEANINGFUL_COMPARISON": 3, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "", "SOUNDNESS_CORRECTNESS": 2, "ORIGINALITY": 4, "IS_ANNOTATED": true, "TITLE": "improvements to model (ProdLDA / LDA) vs improvements to inference (NVI / CGS)", "IS_META_REVIEW": false, "DATE": "03 Dec 2016"}, {"DATE": "08 Nov 2016", "TITLE": "Perplexity", "IS_META_REVIEW": false, "comments": "The perplexity you're reporting for the 20 Newsgroups dataset using LDA Collapsed Gibbs is better than for any other method I've seen.  Would you mind sharing the parameters you used and/or the preprocessed dataset?", "OTHER_KEYS": "Erik Holmer"}, {"IS_META_REVIEW": true, "comments": "This is an interesting paper on a VAE framework for topic models. The main idea is to train a recognition model for the inference phase which, because of so called \u201camortized inference\u201d can be much faster than normal inference where inference must be run iteratively for every document. Some comments:\nEqn 5: I find the notation p(theta(h)|alpha) awkward. Why not P(h|alpha) ?\nThe generative model seems agnostic to document length, meaning that the latent variables only generate probabilities over word space. However, the recognition model is happy to radically change the probabilities q(z|x) if the document length changes because the input to q changes. This seems undesirable. Maybe they should normalize the input to the recognition network?\nThe ProdLDA model might well be equivalent to exponential family PCA or some variant thereof:"}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "The reviewers agree that the approach is interesting and the paper presents useful findings. They also raise enough questions and suggestions for improvements that I believe the paper will be much stronger after further revision, though these seem straightforward to address.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"IMPACT": 3, "SUBSTANCE": 2, "MEANINGFUL_COMPARISON": 3, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "The authors propose NVI for LDA variants. The authors compare NVI-LDA to standard inference schemes such as CGS and online SVI. The authors also evaluate NVI on a different model ProdLDA (not sure this model has been proposed before in the topic modeling literature though?)\n\nIn general, I like the direction of this paper and NVI looks promising for LDA. The experimental results however confound model vs inference which makes it hard to understand the significance of the results. Furthermore, the authors don't discuss hyper-parameter selection which is known to significantly impact performance of topic models. This makes it hard to understand when the proposed method can be expected to work. \n\nCan you maybe generate synthetic datasets with different Dirichlet distributions and assess when the proposed method recovers the true parameters?\n\nFigure 1: Is this prior or posterior? The text talks about sparsity whereas the y-axis reads \"log p(topic proportions)\" which is a bit confusing.\n\nSection 3.2: it is not clear what you mean by unimodal in softmax basis. Consider a Dirichlet on K-dimensional simplex with concentration parameter alpha/K where alpha<1 makes it multimodal. Isn't the softmax basis still multimodal?\n\nNone of the numbers include error bars. Are the results statistically significant?\n\n\nMinor comments:\n\nLast term in equation (3) is not \"error\"; reconstruction accuracy or negative reconstruction error perhaps?\n\nThe idea of using an inference network is much older, cf. Helmholtz machine. \n", "SOUNDNESS_CORRECTNESS": 2, "ORIGINALITY": 4, "IS_ANNOTATED": true, "TITLE": "Promising direction, but the paper needs more work", "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "17 Dec 2016 (modified: 21 Jan 2017)", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "Nice paper to read", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer4", "comments": "This paper proposes the use of neural variational inference method for topic models. The paper shows a nice trick to approximate Dirichlet prior using softmax basis with a Gaussian and then the model is trained to maximize the variational lower bound. Also, the authors study a better way to alleviate the component collapsing issue, which has been problematic for continuous latent variables that follow Gaussian distribution. The results look promising and the experimental protocol sounds fine.\n\nMinor comments:\nPlease add citation to [1] or [2] for neural variational inference, and [2] for VAE. \nA typo in \u201cThis approximation to the Dirichlet prior p(\u03b8|\u03b1) is results in the distribution\u201d, it should be \u201cThis approximation to the Dirichlet prior p(\u03b8|\u03b1) results in the distribution\u201d\n\nIn table 2, it is written that DMFVI was trained more than 24hrs but failed to deliver any result, but why not wait until the end and report the numbers?\n\nIn table 3, why are the perplexities of LDA-Collapsed Gibbs and NVDM are lower while the proposed models (ProdLDA) generates more coherent topics? What is your intuition on this?\n\nHow does the training speed (until the convergence) differs by using different learning-rate and momentum scheduling approaches shown as in figure 1?\n\nIt may be also interesting to add some more analysis on the latent variables z (component collapsing and etc., although your results indirectly show that the learning-rate and momentum scheduling trick removes this issue).\n\nOverall, the paper clearly proposes its main idea, explain why it is good to use NVI, and its experimental results support the original claim. It explains well what are the challenges and demonstrate their solutions. \n\n[1] Minh et al., Neural Variational Inference and Learning in Belief Networks, ICML\u201914\n[2] Rezende et al., Stochastic Backpropagation and Approximate Inference in Deep Generative Models, ICML\u201914", "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "16 Dec 2016 (modified: 18 Dec 2016)", "REVIEWER_CONFIDENCE": 3}, {"TITLE": "VAE model for LDA. Interesting idea, but a incremental.  ", "MEANINGFUL_COMPARISON": 3, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "This is an interesting paper on a VAE framework for topic models. The main idea is to train a recognition model for the inference phase which, because of so called \u201camortized inference\u201d can be much faster than normal inference where inference must be run iteratively for every document. Some comments:\nEqn 5: I find the notation p(theta(h)|alpha) awkward. Why not P(h|alpha) ?\nThe generative model seems agnostic to document length, meaning that the latent variables only generate probabilities over word space. However, the recognition model is happy to radically change the probabilities q(z|x) if the document length changes because the input to q changes. This seems undesirable. Maybe they should normalize the input to the recognition network?\nThe ProdLDA model might well be equivalent to exponential family PCA or some variant thereof: ", "SOUNDNESS_CORRECTNESS": 3, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "14 Dec 2016", "REVIEWER_CONFIDENCE": 5}, {"TITLE": "Comparison to NVDM looks unfair", "OTHER_KEYS": "(anonymous)", "comments": "The comparison to NVDM looks unfair since the user introduces a couples tricks (Dirichlet prior, batch normalisation, high momentum training, etc.) which NVDM doesn't use. A more convincing experimental design is to explore the effect of each trick separately in neural variational inference. \n", "IS_META_REVIEW": false, "RECOMMENDATION": 5, "DATE": "05 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"IMPACT": 3, "SUBSTANCE": 2, "MEANINGFUL_COMPARISON": 3, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "", "SOUNDNESS_CORRECTNESS": 2, "ORIGINALITY": 4, "IS_ANNOTATED": true, "TITLE": "improvements to model (ProdLDA / LDA) vs improvements to inference (NVI / CGS)", "IS_META_REVIEW": false, "DATE": "03 Dec 2016"}, {"DATE": "08 Nov 2016", "TITLE": "Perplexity", "IS_META_REVIEW": false, "comments": "The perplexity you're reporting for the 20 Newsgroups dataset using LDA Collapsed Gibbs is better than for any other method I've seen.  Would you mind sharing the parameters you used and/or the preprocessed dataset?", "OTHER_KEYS": "Erik Holmer"}]}
{"text": "GENERATIVE PARAGRAPH VECTOR\n1 INTRODUCTION\nA central problem in many text based applications, e.g., sentiment classification (Pang & Lee, 2008), question answering (Stefanie Tellex & Marton., 2003) and machine translation (I. Sutskever & Le, 2014), is how to capture the essential meaning of a piece of text in a fixed-length vector. Perhaps the most popular fixed-length vector representations for texts is the bag-of-words (or bag-of-ngrams) (Harris, 1954). Besides, probabilistic latent semantic indexing (PLSI) (Hofmann, 1999) and latent Dirichlet allocation (LDA) (Blei & Jordan, 2003) are two widely adopted alternatives.\nA recent paradigm in this direction is to use a distributed representation for texts (T. Mikolov & Dean, 2013a). In particular, Le and Mikolov (Quoc Le, 2014; Andrew M.Dai, 2014) show that their method, Paragraph Vector (PV), can capture text semantics in dense vectors and outperform many existing representation models. Although PV is an efficient method for learning high-quality distributed text representations, it suffers a similar problem as PLSI that it provides no model on text vectors: it is unclear how to infer the distributed representations for texts outside of the training set with the learned model (i.e., learned text and word vectors). Such a limitation largely restricts the usage of the PV model, especially in those prediction focused scenarios.\nInspired by the completion and improvement of LDA over PLSI, we first introduce the Generative Paragraph Vector (GPV) with a complete generation process for a corpus. Specifically, GPV can be viewed as a probabilistic extension of the Distributed Bag of Words version of Paragraph Vector (PVDBOW), where the text vector is viewed as a hidden variable sampled from some prior distributions, and the words within the text are then sampled from the softmax distribution given the text and word vectors. With a complete generative process, we are able to infer the distributed representations of new texts based on the learned model. Meanwhile, the prior distribution over text vectors also acts as a regularization factor from the view of optimization, thus can lead to higher-quality text representations.\nMore importantly, with the ability to infer the distributed representations for unseen texts, we now can directly incorporate labels paired with the texts into the model to guide the representation learning, and turn the model into a supervised version, namely Supervised Generative Paragraph Vector (SGPV). Note that supervision cannot be directly leveraged in the original PV model since it has no\ngeneralization ability on new texts. By learning the SGPV model, we can directly employ SGPV to predict labels for new texts. As we know, when the goal is prediction, fitting a supervised model would be a better choice than learning a general purpose representations of texts in an unsupervised way. We further show that SGPV can be easily extended to accommodate n-grams so that we can take into account word order information, which is important in learning semantics of texts.\nWe evaluated our proposed models on five text classification benchmark datasets. For the unsupervised GPV, we show that its superiority over the existing counterparts, such as bag-of-words, LDA, PV and FastSent (Felix Hill, 2016). For the SGPV model, we take into comparison both traditional supervised representation models, e.g. MNB (S. Wang, 2012), and a variety of state-of-the-art deep neural models for text classification (Kim, 2014; N. Kalchbrenner, 2014; Socher & Potts, 2013; Irsoy & Cardie, 2014). Again we show that the proposed SGPV can outperform the baseline methods by a substantial margin, demonstrating it is a simple yet effective model.\nThe rest of the paper is organized as follows. We first review the related work in section 2 and briefly describe PV in section 3. We then introduce the unsupervised generative model GPV and supervised generative model SGPV in section 4 and section 5 respectively. Experimental results are shown in section 6 and conclusions are made in section 7.\n2 RELATED WORK\nMany text based applications require the text input to be represented as a fixed-length feature vector. The most common fixed-length representation is bag-of-words (BoW) (Harris, 1954). For example, in the popular TF-IDF scheme (Salton & McGill, 1983), each document is represented by tfidf values of a set of selected feature-words. However, the BoW representation often suffers from data sparsity and high dimension. Meanwhile, due to the independent assumption between words, BoW representation has very little sense about the semantics of the words.\nTo address this shortcoming, several dimensionality reduction methods have been proposed, such as latent semantic indexing (LSI) (S. Deerwester & Harshman, 1990), Probabilistic latent semantic indexing (PLSI) (Hofmann, 1999) and latent Dirichlet allocation (LDA) (Blei & Jordan, 2003). Both PLSI and LDA have a good statistical foundation and proper generative model of the documents, as compared with LSI which relies on a singular value decomposition over the term-document cooccurrence matrix. In PLSI, each word is generated from a single topic, and different words in a document may be generated from different topics. While PLSI makes great effect on probabilistic modeling of documents, it is not clear how to assign probability to a document outside of the training set with the learned model. To address this issue, LDA is proposed by introducing a complete generative process over the documents, and demonstrated as a state-of-the-art document representation method. To further tackle the prediction task, Supervised LDA (David M.Blei, 2007) is developed by jointly modeling the documents and the labels.\nRecently, distributed models have been demonstrated as efficient methods to acquire semantic representations of texts. A representative method is Word2Vec (Tomas Mikolov & Dean, 2013b), which can learn meaningful word representations in an unsupervised way from large scale corpus. To represent sentences or documents, a simple approach is then using a weighted average of all the words. A more sophisticated approach is combing the word vectors in an order given by a parse tree (Richard Socher & Ng, 2012). Later, Paragraph Vector (PV) (Quoc Le, 2014) is introduced to directly learn the distributed representations of sentences and documents. There are two variants in PV, namely the Distributed Memory Model of Paragraph Vector (PV-DM) and the Distributed Bag of Words version of Paragraph Vector (PV-DBOW), based on two different model architectures. Although PV is a simple yet effective distributed model on sentences and documents, it suffers a similar problem as PLSI that it provides no model on text vectors: it is unclear how to infer the distributed representations for texts outside of the training set with the learned model.\nBesides these unsupervised representation learning methods, there have been many supervised deep models with directly learn sentence or document representations for the prediction tasks. Recursive Neural Network (RecursiveNN) (Richard Socher & Ng, 2012) has been proven to be efficient in terms of constructing sentence representations. Recurrent Neural Network (RNN) (Ilya Sutskever & Hinton, 2011) can be viewed as an extremely deep neural network with weight sharing across time. Convolution Neural Network (CNN) (Kim, 2014) can fairly determine discriminative phrases in a\ntext with a max-pooling layer. However, these deep models are usually quite complex and thus the training would be time-consuming on large corpus.\n3 PARAGRAPH VECTOR\nSince our model can be viewed as a probabilistic extension of the PV-DBOW model with a complete generative process, we first briefly review the PV-DBOW model for reference.\nIn PV-DBOW, each text is mapped to a unique paragraph vector and each word is mapped to a unique word vector in a continuous space. The paragraph vector is used to predict target words randomly sampled from the paragraph as shown in Figure 1. More formally, Let D={d1, . . . ,dN} denote a corpus of N texts, where each text dn = (wn1 , w n 2 , . . . , w n ln\n), n \u2208 1, 2, . . . , N is an lnlength word sequence over the word vocabulary V of size M . Each text d \u2208 D and each word w \u2208 V is associated with a vector ~d \u2208 RK and ~w \u2208 RK , respectively, where K is the embedding dimensionality. The predictive objective of the PV-DBOW for each word wnl \u2208 dn is defined by the softmax function\np(wni |dn) = exp(~wni \u00b7 ~dn)\u2211 w\u2032\u2208V exp(~w \u2032 \u00b7 ~dn) (1)\nThe PV-DBOW model can be efficiently trained using the stochastic gradient descent (Rumelhart & Williams, 1986) with negative sampling (T. Mikolov & Dean, 2013a).\nAs compared with traditional topic models, e.g. PLSI and LDA, PV-DBOW conveys the following merits. Firstly, PV-DBOW using negative sampling can be interpretated as a matrix factorization over the words-by-texts co-occurrence matrix with shifted-PMI values (Omer Levy & Ramat-Gan, 2015). In this way, more discriminative information (i.e., PMI) can be modeled in PV as compared with the generative topic models which learn over the words-by-texts co-occurrence matrix with raw frequency values. Secondly, PV-DBOW does not have the explicit \u201ctopic\u201d layer and allows words automatically clustered according to their co-occurrence patterns during the learning process. In this way, PV-DBOW can potentially learn much finer topics than traditional topic models given the same hidden dimensionality of texts. However, a major problem with PV-DBOW is that it provides no model on text vectors: it is unclear how to infer the distributed representations for unseen texts.\n4 GENERATIVE PARAGRAPH VECTOR\nIn this section, we introduce the GPV model in detail. Overall, GPV is a generative probabilistic model for a corpus. We assume that for each text, a latent paragraph vector is first sampled from some prior distributions, and the words within the text are then generated from the normalized exponential (i.e. softmax) distribution given the paragraph vector and word vectors. In our work, multivariate normal distribution is employed as the prior distribution for paragraph vectors. It could\nbe replaced by other prior distributions and we will leave this as our future work. The specific generative process is as follows:\nFor each text dn \u2208D, n = 1, 2, . . . , N : (a) Draw paragraph vector ~dn \u223c N (\u00b5,\u03a3) (b) For each word wni \u2208 dn, i = 1, 2, . . . , ln :\nDraw word wni \u223c softmax(~dn \u00b7W )i\nwhere W denotes a k \u00d7M word embedding matrix with W\u2217j = ~wj , and softmax(~dn \u00b7W )i is the softmax function defined the same as in Equation (1). Figure 2 (Left) provides the graphical model of this generative process. Note that GPV differs from PV-DBOW in that the paragraph vector is a hidden variable generated from some prior distribution, which allows us to infer the paragraph vector over future texts given the learned model. Based on the above generative process, the probability of the whole corpus can be written as follows:\np(D)= N\u220f n=1 \u222b p(~dn|\u00b5,\u03a3) \u220f wni \u2208dn p(wni |W, ~dn)d~dn\nTo learn the model, direct maximum likelihood estimation is not tractable due to non-closed form of the integral. We approximate this learning problem by using MAP estimates for ~dn, which can be formulated as follows:\n(\u00b5\u2217,\u03a3\u2217,W \u2217) = arg max \u00b5,\u03a3,W\n\u220f p(d\u0302n|\u00b5,\u03a3) \u220f wni \u2208dn p(wni |W, d\u0302n)\nwhere d\u0302n denotes the MAP estimate of ~dn for dn, (\u00b5\u2217,\u03a3\u2217,W \u2217) denotes the optimal solution. Note that for computational simplicity, in this work we fixed \u00b5 as a zero vector and \u03a3 as a identity matrix. In this way, all the free parameters to be learned in our model are word embedding matrix W . By taking the logarithm and applying the negative sampling idea to approximate the softmax function, we obtain the final learning problem\nL= N\u2211 n=1 ( \u22121 2 ||d\u0302n||2+ \u2211 wni \u2208dn ( log \u03c3(~wni \u00b7d\u0302n)+k\u00b7Ew\u2032\u223cPnw log \u03c3(\u2212 ~w\u2032 \u00b7 d\u0302n) )) where \u03c3(x) = 1/(1 + exp(\u2212x)), k is the number of \u201cnegative\u201d samples, w\u2032 denotes the sampled word and Pnw denotes the distribution of negative word samples. As we can see from the final objective function, the prior distribution over paragraph vectors actually act as a regularization term. From the view of optimization, such regularization term could constrain the learning space and usually produces better paragraph vectors.\nFor optimization, we use coordinate ascent, which first optimizes the word vectors W while leaving the MAP estimates (d\u0302) fixed. Then we find the new MAP estimate for each document while leaving the word vectors fixed, and continue this process until convergence. To accelerate the learning, we adopt a similar stochastic learning framework as in PV which iteratively updates W and estimates ~d by randomly sampling text and word pairs.\nAt prediction time, given a new text, we perform an inference step to compute the paragraph vector for the input text. In this step, we freeze the vector representations of each word, and apply the same MAP estimation process of ~d as in the learning phase. With the inferred paragraph vector of the test text, we can feed it to other prediction models for different applications.\n5 SUPERVISED GENERATIVE PARAGRAPH VECTOR\nWith the ability to infer the distributed representations for unseen texts, we now can incorporate the labels paired with the texts into the model to guide the representation learning, and turn the model into a more powerful supervised version directly towards prediction tasks. Specifically, we introduce an additional label generation process into GPV to accommodate text labels, and obtain the Supervised Generative Paragraph Vector (SGPV) model. Formally, in SGPV, the n-th text dn and the corresponding class label yn \u2208 {1, 2, . . . , C} arise from the following generative process:\nFor each text dn \u2208D, n = 1, 2, . . . , N : (a) Draw paragraph vector ~dn \u223c N (\u00b5,\u03a3) (b) For each word wni \u2208 dn, i = 1, 2, . . . , ln :\nDraw word wni \u223c softmax(~dn \u00b7W )i (c) Draw label yn|~dn, U, b \u223c softmax(U \u00b7 ~dn+b)\nwhere U is a C \u00d7K matrix for a dataset with C output labels, and b is a bias term. The graphical model of the above generative process is depicted in Figure 2 (Right). SGPV defines the probability of the whole corpus as follows\np(D)= N\u220f n=1 \u222b p(~dn|\u00b5,\u03a3) ( \u220f wni \u2208dn p(wni |W, ~dn) ) p(yn|~dn, U, b)d~dn\nWe adopt a similar learning process as GPV to estimate the model parameters. Since the SGPV includes the complete generative process of both paragraphs and labels, we can directly leverage it to predict the labels of new texts. Specifically, at prediction time, given all the learned model parameters, we conduct an inference step to infer the paragraph vector as well as the label using MAP estimate over the test text.\nThe above SGPV may have limited modeling ability on text representation since it mainly relies on uni-grams. As we know, word order information is often critical in capturing the meaning of texts. For example, \u201cmachine learning\u201d and \u201clearning machine\u201d are totally different in meaning with the same words. There has been a variety of deep models using complex architectures such as convolution layers or recurrent structures to help capture such order information at the expense of large computational cost.\nHere we propose to extend SGPV by introducing an additional generative process for n-grams, so that we can incorporate the word order information into the model and meanwhile keep its simplicity in learning. We name this extension as SGPV-ngram. Here we take the generative process of SGPVbigram as an example.\nFor each text dn \u2208D, n = 1, 2, . . . , N : (a) Draw paragraph vector ~dn \u223c N (\u00b5,\u03a3) (b) For each word wni \u2208 dn, i = 1, 2, . . . , ln :\nDraw word wni \u223c softmax(~dn \u00b7W )i\n(c) For each bigram gni \u2208 dn, i = 1, 2, . . . , sn : Draw bigram gni \u223c softmax(~dn \u00b7G)i\n(d) Draw label yn|~dn, U, b \u223c softmax(U \u00b7 ~dn+b)\nwhere G denotes a K \u00d7 S bigram embedding matrix with G\u2217j = ~gj , and S denotes the size of bigram vocabulary. The joint probability over the whole corpus is then defined as\np(D)= N\u220f n=1 \u222b p(~dn|\u00b5,\u03a3) ( \u220f wni \u2208dn p(wni |W, ~dn) )( \u220f gni \u2208dn p(gni |G, ~dn) ) p(yn|~dn, U, b)d~dn\n6 EXPERIMENTS\nIn this section, we introduce the experimental settings and empirical results on a set of text classification tasks.\n6.1 DATASET AND EXPERIMENTAL SETUP\nWe made use of five publicly available benchmark datasets in comparison.\nTREC: The TREC Question Classification dataset (Li & Roth, 2002)1 which consists of 5, 452 train questions and 500 test questions. The goal is to classify a question into 6 different types depending on the answer they seek for.\nSubj: Subjectivity dataset (Pang & Lee, 2004) which contains 5, 000 subjective instances and 5, 000 objective instances. The task is to classify a sentence as being subjective or objective.\nMR: Movie reviews (Pang & Lee, 2005) 2 with one sentence per review. There are 5, 331 positive sentences and 5, 331 negative sentences. The objective is to classify each review into positive or negative category.\nSST-1: Stanford Sentiment Treebank (Socher & Potts, 2013) 3. SST-1 is provided with train/dev/test splits of size 8, 544/1, 101/2, 210. It is a fine-grained classification over five classes: very negative, negative, neutral, positive, and very positive.\nSST-2: SST-2 is the same as SST-1 but with neutral reviews removed. We use the standard train/dev/test splits of size 6, 920/872/1, 821 for the binary classification task.\nPreprocessing steps were applied to all datasets: words were lowercased, non-English characters and stop words occurrence in the training set are removed. For fair comparison with other published results, we use the default train/test split for TREC, SST-1 and SST-2 datasets. Since explicit split of train/test is not provided by subj and MR datasets, we use 10-fold cross-validation instead.\nIn our model, text and word vectors are randomly initialized with values uniformly distributed in the range of [-0.5, +0.5]. Following the practice in (Tomas Mikolov & Dean, 2013b) , we set the noise distributions for context and words as pnw(w) \u221d #(w)0.75. We adopt the same linear learning rate strategy where the initial learning rate of our models is 0.025. For unsupervised methods, we use support vector machines (SVM) 4 as the classifier.\n6.2 BASELINES\nWe adopted both unsupervised and supervised methods on text representation as baselines.\n6.2.1 UNSUPERVISED BASELINES\nBag-of-word-TFIDF and Bag-of-bigram-TFIDF. In the bag-of-word-TFIDF scheme (Salton & McGill, 1983) , each text is represented as the tf-idf value of chosen feature-words. The bag-of-\n1http://cogcomp.cs.illinois.edu/Data/QA/QC/ 2https://www.cs.cornell.edu/people/pabo/movie-review-data/ 3http://nlp.stanford.edu/sentiment/ 4http://www.csie.ntu.edu.tw/\u02dccjlin/libsvm/\nbigram-TFIDF model is constructed by selecting the most frequent unigrams and bigrams from the training subset. We use the vanilla TFIDF in the gensim library5.\nLSI (S. Deerwester & Harshman, 1990) and LDA (Blei & Jordan, 2003). LSI maps both texts and words to lower-dimensional representations in a so-called latent semantic space using SVD decomposition. In LDA, each word within a text is modeled as a finite mixture over an underlying set of topics. We use the vanilla LSI and LDA in the gensim library with topic number set as 100.\ncBow (Tomas Mikolov & Dean, 2013b). Continuous Bag-Of-Words model. We use average pooling as the global pooling mechanism to compose a sentence vector from a set of word vectors.\nPV (Quoc Le, 2014). Paragraph Vector is an unsupervised model to learn distributed representations of words and paragraphs.\nFastSent (Felix Hill, 2016). In FastSent, given a simple representation of some sentence in context, the model attempts to predict adjacent sentences.\nNote that unlike LDA and GPV, LSI, cBow, and FastSent cannot infer the representations of unseen texts. Therefore, these four models need to fold-in all the test data to learn representations together with training data, which makes it not efficient in practice.\n6.2.2 SUPERVISED BASELINES\nNBSVM and MNB (S. Wang, 2012). Naive Bayes SVM and Multinomial Naive Bayes with unigrams and bi-grams.\nDAN (Mohit Iyyer & III, 2015). Deep averaging network uses average word vectors as the input and applies multiple neural layers to learn text representation under supervision.\nCNN-multichannel (Kim, 2014). CNN-multichannel employs convolutional neural network for sentence modeling.\nDCNN (N. Kalchbrenner, 2014). DCNN uses a convolutional architecture that replaces wide convolutional layers with dynamic pooling layers.\nMV-RNN (Richard Socher & Ng, 2012). Matrix-Vector RNN represents every word and longer phrase in a parse tree as both a vector and a matrix.\nDRNN (Irsoy & Cardie, 2014). Deep Recursive Neural Networks is constructed by stacking multiple recursive layers.\nDependency Tree-LSTM (Kai Sheng Tai & Manning, 2015). The Dependency Tree-LSTM based on LSTM structure uses dependency parses of each sentence.\n6.3 PERFORMANCE OF GENERATIVE PARAGRAPH VECTOR\nWe first evaluate the GPV model by comparing with the unsupervised baselines on the TREC, Subj and MR datasets. As shown in table 1, GPV works better than PV over the three tasks. It demonstrates the benefits of introducing a prior distribution (i.e., regularization) over the paragraph vectors. Moreover, GPV can also outperform almost all the baselines on three tasks except Bow-TFIDF and Bigram-TFIDF on the TREC collection. The results show that for unsupervised text representation, bag-of-words representation is quite simple yet powerful which can beat many embedding models. Meanwhile, by using a complete generative process to infer the paragraph vectors, our model can achieve the state-of-the-art performance among the embedding based models.\n6.4 PERFORMANCE OF SUPERVISED GENERATIVE PARAGRAPH VECTOR\nWe compare SGPV model to supervised baselines on all the five classification tasks. Empirical results are shown in Table 2. We can see that SGPV achieves comparable performance against other deep learning models. Note that SGPV is much simpler than these deep models with significantly less parameters and no complex structures. Moreover, deep models with convolutional layers or recurrent structures can potentially capture compositional semantics (e.g., phrases), while SGPV only\n5http://radimrehurek.com/gensim/\nrelies on uni-gram. In this sense, SGPV is quite effective in learning text representation. Meanwhile, if we take Table 1 into consideration, it is not surprising to see that SGPV can consistently outperform GPV on all the three classification tasks. This also demonstrates that it is more effective to directly fit supervised representation models than to learn a general purpose representation in prediction scenarios.\nBy introducing bi-grams, SGPV-bigram can outperform all the other deep models on four tasks. In particular, the improvements of SGPV-bigram over other baselines are significant on SST-1 and SST-2. These results again demonstrated the effectiveness of our proposed SGPV model on text representations. It also shows the importance of word order information in modeling text semantics.\n7 CONCLUSIONS\nIn this paper, we introduce GPV and SGPV for learning distributed representations for pieces of texts. With a complete generative process, our models are able to infer vector representations as well as labels over unseen texts. Our models keep as simple as PV models, and thus can be efficiently learned over large scale text corpus. Even with such simple structures, both GPV and SGPV can produce state-of-the-art results as compared with existing baselines, especially those complex deep models. For future work, we may consider other probabilistic distributions for both paragraph vectors and word vectors.\n", "cats": {"READ": 0.0, "DONTREAD": 1.0}, "reviews": [{"IS_META_REVIEW": true, "comments": "This work reframes paragraph vectors from a generative point of view and in so doing, motivates the existing method of inferring paragraph vectors as well as applying a L2 regularizer on the paragraph embeddings. The work also motivates joint learning of a classifier on the paragraph vectors to perform text classification.\n\nThe paper has numerous citation issues both in formatting within the text and the formatting of the bibliography, e.g. on some occasions including first names, on others not. I suggest the authors use a software package like BibTex to have a more consistent bibliography. There seems to be little novelty in this work. \n\nThe authors claim that there is no proposed method for inferring unseen documents for paragraph vectors. This is untrue. In the original paragraph vector paper, the authors show that to get a new vector, the rest of the model parameters are held fixed and gradient descent is performed on the new paragraph vector. This means the original dataset is not needed when inferring a paragraph vector for new text. This work seems to be essentially doing the same thing when finding the MAP estimate for a new vector. Thus the only contribution from the generative paragraph vector framing is the regularization on the embedding matrix.\n\nThe supervised generative paragraph vector amounts to jointly training a linear classifier on the paragraph vectors, while inference for the paragraph vector is unchanged. For the n-gram based approach, the authors should cite Li et al., 2015.\n\nIn the experiments, table 1 and 2 are badly formatted with .0 being truncated. The authors also do not state the size of the paragraph vector. Finally the SGPV results are actually worse than that reported in the original paragraph vector paper where SST-1 got 48.7 and SST-2 got 86.3.\n\nBofang Li, Tao Liu, Xiaoyong Du, Deyuan Zhang, Zhe Zhao, Learning Document Embeddings by Predicting N-grams for Sentiment Classification of Long Movie Reviews, 2015."}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "The contribution of this paper generally boils down to adding a prior to the latent representations of the paragraph in the Paragraph Vector model. An especially problematic point about this paper is the claim that the original paper considered only the transductive setting (i.e. it could not induce representations of new documents). It is not accurate, they also used gradient descent at test time. Though I agree that regularizing the original model is a reasonable thing to do, I share the reviewers' feeling that the contribution is minimal. There are also some serious issues with presentation (as noted by the reviewers), I am surprised that the authors have not addressed them during the review period.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"TITLE": "below borderline", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "While this paper has some decent accuracy numbers, it is hard to argue for acceptance given the following:\n\n1) motivation based on the incorrect assumption that the Paragraph Vector wouldn't work on unseen data\n2) Numerous basic formatting and Bibtex citation issues.\n\nLack of novelty of yet another standard directed LDA-like bag of words/bigram model.\n", "IS_META_REVIEW": false, "RECOMMENDATION": 4, "DATE": "24 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "Not convincing", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "It feels that this paper is structured around a shortcoming of the original paragraph vectors paper, namely an alleged inability to infer representation for text outside of the training data. I am reasonably sure that this is not the case. Unfortunately on that basis, the premise for the work presented here no longer holds, which renders most of the subsequent discussion void.\n\nWhile I recommend this paper be rejected, I encourage the authors to revisit the novel aspects of the idea presented here and see if that can be turned into a different type of paper going forward.", "IS_META_REVIEW": false, "RECOMMENDATION": 3, "DATE": "19 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "Very limited in novelty", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "This work reframes paragraph vectors from a generative point of view and in so doing, motivates the existing method of inferring paragraph vectors as well as applying a L2 regularizer on the paragraph embeddings. The work also motivates joint learning of a classifier on the paragraph vectors to perform text classification.\n\nThe paper has numerous citation issues both in formatting within the text and the formatting of the bibliography, e.g. on some occasions including first names, on others not. I suggest the authors use a software package like BibTex to have a more consistent bibliography. There seems to be little novelty in this work. \n\nThe authors claim that there is no proposed method for inferring unseen documents for paragraph vectors. This is untrue. In the original paragraph vector paper, the authors show that to get a new vector, the rest of the model parameters are held fixed and gradient descent is performed on the new paragraph vector. This means the original dataset is not needed when inferring a paragraph vector for new text. This work seems to be essentially doing the same thing when finding the MAP estimate for a new vector. Thus the only contribution from the generative paragraph vector framing is the regularization on the embedding matrix.\n\nThe supervised generative paragraph vector amounts to jointly training a linear classifier on the paragraph vectors, while inference for the paragraph vector is unchanged. For the n-gram based approach, the authors should cite Li et al., 2015.\n\nIn the experiments, table 1 and 2 are badly formatted with .0 being truncated. The authors also do not state the size of the paragraph vector. Finally the SGPV results are actually worse than that reported in the original paragraph vector paper where SST-1 got 48.7 and SST-2 got 86.3.\n\nBofang Li, Tao Liu, Xiaoyong Du, Deyuan Zhang, Zhe Zhao, Learning Document Embeddings by Predicting N-grams for Sentiment Classification of Long Movie Reviews, 2015.", "IS_META_REVIEW": false, "RECOMMENDATION": 2, "DATE": "17 Dec 2016", "REVIEWER_CONFIDENCE": 5}, {"DATE": "02 Dec 2016", "TITLE": "intro", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3"}, {"IS_META_REVIEW": true, "comments": "This work reframes paragraph vectors from a generative point of view and in so doing, motivates the existing method of inferring paragraph vectors as well as applying a L2 regularizer on the paragraph embeddings. The work also motivates joint learning of a classifier on the paragraph vectors to perform text classification.\n\nThe paper has numerous citation issues both in formatting within the text and the formatting of the bibliography, e.g. on some occasions including first names, on others not. I suggest the authors use a software package like BibTex to have a more consistent bibliography. There seems to be little novelty in this work. \n\nThe authors claim that there is no proposed method for inferring unseen documents for paragraph vectors. This is untrue. In the original paragraph vector paper, the authors show that to get a new vector, the rest of the model parameters are held fixed and gradient descent is performed on the new paragraph vector. This means the original dataset is not needed when inferring a paragraph vector for new text. This work seems to be essentially doing the same thing when finding the MAP estimate for a new vector. Thus the only contribution from the generative paragraph vector framing is the regularization on the embedding matrix.\n\nThe supervised generative paragraph vector amounts to jointly training a linear classifier on the paragraph vectors, while inference for the paragraph vector is unchanged. For the n-gram based approach, the authors should cite Li et al., 2015.\n\nIn the experiments, table 1 and 2 are badly formatted with .0 being truncated. The authors also do not state the size of the paragraph vector. Finally the SGPV results are actually worse than that reported in the original paragraph vector paper where SST-1 got 48.7 and SST-2 got 86.3.\n\nBofang Li, Tao Liu, Xiaoyong Du, Deyuan Zhang, Zhe Zhao, Learning Document Embeddings by Predicting N-grams for Sentiment Classification of Long Movie Reviews, 2015."}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "The contribution of this paper generally boils down to adding a prior to the latent representations of the paragraph in the Paragraph Vector model. An especially problematic point about this paper is the claim that the original paper considered only the transductive setting (i.e. it could not induce representations of new documents). It is not accurate, they also used gradient descent at test time. Though I agree that regularizing the original model is a reasonable thing to do, I share the reviewers' feeling that the contribution is minimal. There are also some serious issues with presentation (as noted by the reviewers), I am surprised that the authors have not addressed them during the review period.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"TITLE": "below borderline", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "While this paper has some decent accuracy numbers, it is hard to argue for acceptance given the following:\n\n1) motivation based on the incorrect assumption that the Paragraph Vector wouldn't work on unseen data\n2) Numerous basic formatting and Bibtex citation issues.\n\nLack of novelty of yet another standard directed LDA-like bag of words/bigram model.\n", "IS_META_REVIEW": false, "RECOMMENDATION": 4, "DATE": "24 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "Not convincing", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "It feels that this paper is structured around a shortcoming of the original paragraph vectors paper, namely an alleged inability to infer representation for text outside of the training data. I am reasonably sure that this is not the case. Unfortunately on that basis, the premise for the work presented here no longer holds, which renders most of the subsequent discussion void.\n\nWhile I recommend this paper be rejected, I encourage the authors to revisit the novel aspects of the idea presented here and see if that can be turned into a different type of paper going forward.", "IS_META_REVIEW": false, "RECOMMENDATION": 3, "DATE": "19 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "Very limited in novelty", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "This work reframes paragraph vectors from a generative point of view and in so doing, motivates the existing method of inferring paragraph vectors as well as applying a L2 regularizer on the paragraph embeddings. The work also motivates joint learning of a classifier on the paragraph vectors to perform text classification.\n\nThe paper has numerous citation issues both in formatting within the text and the formatting of the bibliography, e.g. on some occasions including first names, on others not. I suggest the authors use a software package like BibTex to have a more consistent bibliography. There seems to be little novelty in this work. \n\nThe authors claim that there is no proposed method for inferring unseen documents for paragraph vectors. This is untrue. In the original paragraph vector paper, the authors show that to get a new vector, the rest of the model parameters are held fixed and gradient descent is performed on the new paragraph vector. This means the original dataset is not needed when inferring a paragraph vector for new text. This work seems to be essentially doing the same thing when finding the MAP estimate for a new vector. Thus the only contribution from the generative paragraph vector framing is the regularization on the embedding matrix.\n\nThe supervised generative paragraph vector amounts to jointly training a linear classifier on the paragraph vectors, while inference for the paragraph vector is unchanged. For the n-gram based approach, the authors should cite Li et al., 2015.\n\nIn the experiments, table 1 and 2 are badly formatted with .0 being truncated. The authors also do not state the size of the paragraph vector. Finally the SGPV results are actually worse than that reported in the original paragraph vector paper where SST-1 got 48.7 and SST-2 got 86.3.\n\nBofang Li, Tao Liu, Xiaoyong Du, Deyuan Zhang, Zhe Zhao, Learning Document Embeddings by Predicting N-grams for Sentiment Classification of Long Movie Reviews, 2015.", "IS_META_REVIEW": false, "RECOMMENDATION": 2, "DATE": "17 Dec 2016", "REVIEWER_CONFIDENCE": 5}, {"DATE": "02 Dec 2016", "TITLE": "intro", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3"}]}
{"text": "TENSORIAL MIXTURE MODELS\n1 INTRODUCTION\nGenerative models have played a crucial part in the early development of the field of Machine Learning. However, in recent years they were mostly cast aside in favor of discriminative models, lead by the rise of ConvNets (LeCun et al., 2015), which were found to perform equally well or better than classical generative counter-parts on almost any task. Despite the increased interest in unsupervised learning, many of the recent studies on generative models choose to focus solely on the generation capabilities of these models (Goodfellow et al., 2014; Gregor et al., 2015; van den Oord et al., 2016; Dinh et al., 2016; Tran et al., 2016; Chen et al., 2016; Kingma et al., 2016; Kim and Bengio, 2016). There is much less emphasis on leveraging generative models to solve actual tasks, e.g. semi-supervised learning (Kingma et al., 2014; Springenberg, 2016; Maal\u00f8e et al., 2016; Forster et al., 2015; Salimans et al., 2016), image restoration (Dinh et al., 2014; Bengio et al., 2014; van den Oord et al., 2016; Zoran and Weiss, 2011; Rosenbaum and Weiss, 2015; Sohl-Dickstein et al., 2015; Theis and Bethge, 2015) or unsupervised feature representation (Radford et al., 2016; Coates et al., 2011). Nevertheless, work on generative models for solving actual problems are yet to show a meaningful advantage over competing discriminative models.\nOn the most fundamental level, the difference between a generative model and a discriminative one is simply the difference between learning P (X,Y ) and learning P (Y |X), respectively. While it is always possible to infer P (Y |X) given P (X,Y ), it might not be immediately apparent why the generative objective is preferred over the discriminative one. In Ng and Jordan (2002), this question was studied w.r.t. the sample complexity, proving that under some cases it can be significantly lesser in favor of the generative classifier. However, their analysis was limited only to specific pairs of discriminative and generative classifiers, and they did not present a general case where the the generative method is undeniably preferred. We wish to highlight one such case, where learning\nP (X,Y ) is provenly better regardless of the models in question, by examining the problem of classification with missing data. Despite the artificially well-behave nature of the typical classification benchmarks presented in current publications, real-world data is usually riddled with noise and missing values \u2013 instead of observing X we only have a partial observation X\u0302 \u2013 a situation that tends to be ignored in modern research. Discriminative models have no natural mechanisms to handle missing data and instead must rely on data imputation, i.e. filling missing data by a preprocessing step prior to prediction. Unlike the discriminative approaches, generative models are naturally fitted to handle missing data by simply marginalizing over the unknown values in P (X,Y ), from which we can attain P (Y |X\u0302) by an application of Bayes Rule. Moreover, under mild assumptions which apply to many real-world settings, this method is proven to be optimal regardless of the process by which values become missing (see sec. 5 for a more detailed discussion).\nWhile almost all generative models can represent P (X,Y ), only few can actually infer its exact value efficiently. Models which possess this property are said to have tractable inference. Many studies specifically address the hard problem of learning generative models that do not have this property. Notable amongst those are works based on Variational Inference (Kingma and Welling, 2014; Kingma et al., 2014; Blei et al., 2003; Wang and Grimson, 2007; Makhzani et al., 2015; Kingma et al., 2016), which only provide approximated inference, and ones based on Generative Adversarial Networks (Goodfellow et al., 2014; Radford et al., 2016; Springenberg, 2016; Chen et al., 2016; Salimans et al., 2016; Makhzani et al., 2015), which completely circumvent the inference problem by restructuring the learning problem as a two-player game of discriminative objectives \u2013 both of these approaches are incapable of tractable inference.\nThere are several advantages to models with tractable inference (e.g. they could be simpler to train), and as we have shown above, this property is also a requirement for proper handling of missing data in the form of marginalization. In practice, to marginalize over P (X,Y ) means to perform integration on it, thus, even if it is tractable to compute P (X,Y ), it still might not be tractable to compute every possible marginalization. Models which are capable of this are said to have tractable marginalization. Mixture Models (e.g. Gaussian Mixture Models) are the classical example of a generative model with tractable inference, as well as tractable marginalization. Though they are simple to understand, easy to train and even known to be universal \u2013 can approximate any distribution given sufficient capacity \u2013 they do not scale well to high-dimensional data. The Gaussian Mixture Model is an example of a shallow model \u2013 containing just a single latent variable \u2013 with limited expressive efficiency. More generally, Graphical Models are deep and exponentially more expressive, capable of representing intricate relations between many latent variables. While not all kinds of Graphical Models are tractable, many are, e.g. Latent Tree Models (Zhang, 2004; Mourad et al., 2013) and Sum-Product Networks (Poon and Domingos, 2011). The main issue with generic graphical models is that by virtue of being too general they lack the inductive bias needed to efficiently model unstructured data, e.g. images or text. Despite the success of structure learning algorithms (Huang et al., 2015; Gens and Domingos, 2013; Adel et al., 2015) on structured datasets, such as discovering a hierarchy among diseases in patients health records, there are no similar results on unstructured datasets. Indeed some recent works on the subject have failed to solve even simple handwritten digit classification tasks (Adel et al., 2015). Thus deploying graphical models on such cases requires experts to manually design the model. Other attempts which harness neural networks blocks (Dinh et al., 2014; 2016) offer tractable inference, but not tractable marginalization.\nTo summarize, most generative models do not have tractable inference, and of the few models which do, they all possess one or more of the following shortcomings: (i) they do not possess the expressive capacity to model high-dimensional data (e.g. images), (ii) they require explicitly designing all the dependencies of the data, or (iii) they do not have tractable marginalization.\nWe present in this paper a family of generative models we call Tensorial Mixture Models (TMMs), which aim to address the above shortcomings of alternative models. Under TMMs, we assume that the data generated by our model is composed of a sequence of local-structures (e.g. patches in an image), where each local-structure is generated from a small set of simple component distributions (e.g. Gaussian), and the dependencies between the local-structures are represented by a prior tensor holding the prior probabilities of assigning a component distribution to each local-structure. In their general form, TMMs are intractable as the prior tensor is typically of exponential size. However, by decomposing the prior tensor, inference of TMMs becomes realizable by Convolutional Arithmetic Circuits (ConvACs) \u2013 a recently proposed (Cohen et al., 2016a) ConvNet architecture based on two\noperations, weighted sum and product pooling \u2013 which enables both tractable inference as well as tractable marginalization. While Graphical Models are typically hard to design, ConvACs follow the same design conventions of modern ConvNets, which reduces the task of designing a model to simply choosing the number of channels at each layer, and size of pooling windows. ConvACs were also the subject of several theoretical studies on its expressive capacity (Cohen et al., 2016a; Cohen and Shashua, 2016b) and comparing them to ConvNets (Cohen and Shashua, 2016a), showing they are especially suitable for high-dimensional natural data (images, audio, etc.) with a non-negligible advantage over standard ConvNets. Sum-Product Networks are another kind of Graphical Model realizable by Arithmetic Circuits, but they do not posses the same theoretical guarantees, nor do they provide a simple method to design efficient and expressive models.\nThe rest of the article is organized as follows. In sec. 2 we briefly review mathematical background on tensors required in order to follow our work. This is followed by sec. 3 which presents our generative model and its theoretical properties. How our model is trained is covered in sec. 4, and a thorough discussion on the importance of marginalization and its implications on our model is given in sec. 5. We conclude the article by presenting our experiments on classification with missing data in sec. 6, and revisit the main points of the article and future research in sec. 7.\n2 PRELIMINARIES\nWe begin by establishing the minimal background in the field of tensor analysis required for following our work (see app. A for a more detailed review of the subject). A tensor is best thought of as a multi-dimensional array Ad1,...,dN \u2208 R, where \u2200i \u2208 [N ], di \u2208 [Mi] and N is referred to as the order of the tensor. For our purposes we typically assume that M1 = . . . = MN = M , and denote it as A \u2208 (RM )\u2297N . It is immediately apparent that performing operations with tensors, or simply storing them, quickly becomes intractable due to their exponential size of MN . That is one of the primary motivations behind tensor decomposition, which can be seen as a generalization of low-rank matrix factorization.\nThe relationship between tensor decomposition and networks arises from the simple observation, that through decomposition one can tradeoff storage complexity with computation, where the type of computation consists of sums and products. Specifically, the decompositions could be described by a compact representation coupled with a decoding algorithm of polynomial complexity to retrieve the entries of the tensor. Most tensor decompositions have a decoding algorithm representable via computation graphs of products and weighted sums, also known as Arithmetic Circuits (Shpilka and Yehudayoff, 2010) or Sum-Product Networks (Poon and Domingos, 2011). More specifically, these circuits take as input N indicator vectors \u03b41, . . . , \u03b4N , representing the coordinates (d1, . . . , dN ), where \u03b4i = 1[j=di], and output the value ofAd1,...,dN , where the weights of these circuits form the compact representation of tensors.\nApplying this perspective to two of the most common decomposition formats, CANDECOMP/PARFAC (CP) and Hierarchical Tucker (HT), give rise to a shared framework for representing their decoding circuits by convolutional networks as illustrated in fig. 1, where a shallow network with one hidden layer corresponds to the CP decomposition, and a deep network with log2(N) hidden layers corresponds to the HT decomposition. The networks consists of just product pooling and 1\u00d71 conv layers. Having no point-wise activations between the layers, the non-linearity of the models stems from the product pooling operation itself. The pooling layers also control the depth of the network by the choice of the size and the shape of pooling windows. The conv operator is not unlike the standard convolutional layer of ConvNets, with the sole difference being that it may operate without coefficient sharing, i.e. the filters that generate feature maps by sliding across the\nprevious layer may have different coefficients at different spatial locations. This is often referred to in the deep learning community as a locally-connected operator (Taigman et al., 2014).\nArithmetic Circuits constructed from the above conv and product pooling layers are called Convolutional Arithmetic Circuits, or ConvACs for short, first suggested by Cohen et al. (2016a) as a theoretical framework for studying standard convolutional networks, sharing many of the defining traits of the latter, most noteworthy, the locality, sharing and pooling properties of ConvNets. Unlike general circuits, the structure of the network is determined solely by two parameters, the number of channels of each conv layer and the size of pooling windows, which indirectly controls the depth of the network. Any decomposition that corresponds to a ConvAC can represent any tensor, given sufficient number of channels, though deeper circuits result in more efficient representations (Cohen et al., 2016a).\nFinally, since we are dealing with generative models, the tensors we study are non-negative and sum to one, i.e. the vectorization of A (rearranging its entries to the shape of a vector), denoted by vec(A), is constrained to lie in the multi-dimensional simplex, denoted by:\n4k := { x \u2208 Rk+1|\n\u2211k+1 i=1 xi = 1,\u2200i \u2208 [k + 1] : xi \u2265 0 }\n(1)\n3 TENSORIAL MIXTURE MODELS\nWe represent the input signal X by a sequence of low-dimensional local structures\nX = (x1, . . . ,xN ) \u2208 (Rs)N This representation is quite natural for many high-dimensional input domains such as images \u2013 where the local structures represent patches consisting of s pixels \u2013 voice through spectrograms, and text through words.\nA well-known observation, which has been verified in several empirical studies (e.g. by Zoran and Weiss (2011)), is that the distributions of local structures typically found in natural data could be sufficiently modeled by a mixture model consisting of only few components (on the order of 100) of simple distributions (e.g. Gaussian). Assuming the above holds for X \u2208 (Rs)N and let {P (x|d; \u03b8d)}Md=1 be the mixing components, parameterized by \u03b81, . . . , \u03b8M , from which local structures are generated, i.e. for all i \u2208 [N ] there exist di \u2208 [M ] such that xi \u223c P (x|di; \u03b8di), where di is a hidden variable specifying the matching component for the i-th local structure, then the probability density of sampling X is fully described by:\nP (X) = \u2211M\nd1,...,dN=1 P (d1, . . . , dN ) \u220fN i=1\nP (xi|di; \u03b8di) (2) where P (d1, . . . , dN ) represents the prior probability of assigning components d1, . . . , dN to their respective local structures x1, . . . ,xN . Even though we had to make an assumption on X to derive eq. 2, it is important to note that if we allow M to become unbounded, then any distribution with support in (Rs)N could be approximated by this equation. The argument follows from the universality property of the common parametric families of distributions (Gaussian, Laplacian, etc.), where any distribution can be approximated given sufficient number of components from these families, and thus the assumption always holds to some degree (see app. B for the complete proof).\nThe prior probabilities P (d1, . . . , dN ) can also be represented by a tensorA \u2208 (RM )\u2297N of orderN , given that the vectorization of A is constrained to the simplex, i.e. vec(A) \u2208 4(MN\u22121) (see eq. 1). Thus, we refer to eq. 2 as a Tensorial Mixture Model (TMM) with priors tensor A and mixing components P (x|d1; \u03b81), . . . , P (x|dN ; \u03b8N ). Notice that if N = 1 then we obtain the standard mixture model, whereas for a general N it is equivalent to a mixture model with tensorised mixing weights and conditionally independent mixing components.\nUnlike standard mixture models, we cannot perform inference directly from eq. 2, nor can we even store the priors tensor directly given its exponential size of MN entries. Therefore the TMM as presented by eq. 2 is not tractable. The way to make the TMM tractable is to replace the tensor Ad1,...,dN by a tensor decomposition and, as described in the previous section, this gives rise to arithmetic circuits. But before we present our approach for tractable TMMs through tensor decompositions, it is worth examining some of the TMM special cases and how they relate to other known generative models.\n3.1 SPECIAL CASES\nWe have already shown that TMMs can be thought of as a special case of mixture models, but it is important to also note that diagonal Gaussian Mixture Models (GMMs), probably the most common type of mixture models, are a strict subset of TMMs. Assume M = N \u00b7K, as well as:\nP (d1, . . . , dN ) = { wk \u2200i \u2208 [N ], di=N \u00b7(k\u22121)+i 0 Otherwise\nP (x|d; \u03b8d) = N (x;\u00b5ki, diag(\u03c32ki)), d=N \u00b7(k\u22121)+i then eq. 2 reduces to:\nP (X) = \u2211K k=1 wk \u220fN i=1 N (x;\u00b5ki, diag(\u03c32ki)) = \u2211K k=1\nwkN (x; \u00b5\u0303k, diag(\u03c3\u03032k)) \u00b5\u0303k = (\u00b5 T k1, . . . ,\u00b5 T kN ) T \u03c3\u03032k = ((\u03c3 2 k1) T , . . . , (\u03c32kN ) T )T\nwhich is equivalent to a diagonal GMM with mixing weights w \u2208 4K\u22121 and Gaussian mixture components with means {\u00b5\u0303k}Kk=1 and covariances {diag(\u03c3\u03032k)}Kk=1. While the previous example highlights another connection between TMMs and mixture models, it does not take full advantage of the priors tensor, setting most of its entries to zero. Perhaps the simplest assumption we could make about the priors tensor, without it becoming degenerate, would be to assume that that the hidden variables d1, . . . , dN are statistically independent, i.e. P (d1, . . . , dN )= \u220fN i=1 P (di). Then rearranging eq. 2 will result in a product of mixture models:\nP (X) = \u220fN\ni=1 \u2211M d=1 P (di = d)P (xi|di = d; \u03b8d)\nIf we also assume that the priors are identical in addition to being independent, i.e. P (d1 = d) = . . . = P (dN = d), then this model becomes a bag-of-words model, where the components {P (x|d; \u03b8d)}Md=1 define a soft dictionary for translating local-structures into \u201dwords\u201d, as is often done when applying bag-of-words models to images. Despite this familiar setting, had we subscribed to only using independent priors, we would lose the universality property of the general TMM model \u2013 it would not be capable of modeling dependencies between the local-structures.\n3.2 DECOMPOSING THE PRIORS TENSOR\nWe have just seen that TMMs could be made tractable through constraints on the priors tensor, but it was at the expense of either not taking advantage of its tensor structure, or losing its universality property. Our approach for tractable TMMs is to apply tensor decompositions to the priors tensor, which is the conventional method for tackling the exponential size of high-order tensors.\nWe have already mentioned in sec. 2 that any decomposition representable by ConvACs, including the well-known CP and HT decompositions, can represent any tensor, and thus applying them would not limit the expressivity of our model. Fixing a ConvAC representing the priors tensor, i.e. \u03a6\u0398(\u03b41, . . . , \u03b4N ) = Ad1,...,dN where \u0398 are the parameters of the ConvAC and {\u03b4i}Ni=1 are the indicators representation of {di}Ni=1, and simply rearranging the terms of eq. 2 after substituting the entries of the priors tensor with the sums and products expression of \u03a6\u0398(\u03b41, . . . , \u03b4N ) results in:\nP (X) = \u03a6\u0398(q 1, . . . ,qN ) \u2200i \u2208 [N ]\u2200d \u2208 [M ], qid = P (xi|di = d) (3)\nwhich is nearly equivalent to how the ConvAC is used for computing the entries of the priors tensor, differing only in the way the input vectors are defined. Namely, eq. 3 is a result of\nreplacing indicator vectors \u03b4i with probability vectors qi, which could be interpreted as a soft variant of indicator vectors. Viewed as a network, it begins with a representation layer, mapping the local structures to the likelihood probabilities of belonging to each mixing component, i.e. {xi}Ni=1\u2192{P (xi|di=d; \u03b8d)}N,Mi=1,d=1. Following the representation layer is the same ConvAC described by \u03a6\u0398(\u00b7, . . . , \u00b7). The complete network is illustrated by fig. 2. Unlike general tensors, for a TMM to represent a valid distribution, the priors tensor is constrained to the simplex and thus not every choice of parameters for the decomposition would result in a tensor holding this constraint. By restricting ourselves to non-negative decomposition parameters, i.e. use positive weights in the 1\u00d71 conv layers, it guarantees the resulting tensors would be nonnegative as well. Additionally, normalizing the non-negative tensor is equivalent to requiring the parameters to be restricted to the simplex, i.e. for every layer l and spatial position j the weight vector wl,j \u2208 4rl\u22121\u22121 of the respective 1\u00d71 conv kernel is normalized to sum to one. Under these constraints we refer to it as a generative decomposition. Notice that restricting ourselves to generative decompositions does not limit the expressivity of our model, as we can still represent any non-negative tensor and thus any distribution that the original TMM could represent. In discussing the above, it helps to distinguish between the two extreme cases of generative decompositions representable by ConvACs, namely, the shallow Generative CP decomposition referred to as the GCP-model, and the deep Generative HT decomposition referred to as the GHT-model.\nNon-negative matrix and tensor decompositions have a long history together with the development of corresponding generative models, e.g., pLSA (Hofmann, 1999) which uses non-negative matrix decompositions for text analysis, which was later extended for images with the help of \u201cvisual words\u201d (Li and Perona, 2005). The non-negative variant of the CP decomposition presented above is related to the more general Latent Class Models (Zhang, 2004), which could be seen as a multi-dimensional pLSA. Likewise, the non-negative HT decomposition is related to the Latent Tree Model (Zhang, 2004; Mourad et al., 2013) with the structure of a complete binary tree. Thus both the GCP and GHT models can be represented as a two-level graphical model, where the top level is either an LCM or an LTM, and the bottom level represent the local structures which are conditionally sampled from the mixing components of the TMM.\nTo conclude, the application of ConvACs to decompose the priors tensor leads to tractable TMMs with inference implemented by convolutional networks, has deep roots to classical use of nonnegative factorizations of generative models, and given sufficient resources does not limit expressivity. However, practical considerations raise the question on the extent of the expressive capacity of our models when the size of the ConvAC is polynomial with respect to the number of local structures and mixing components. This question was thoroughly studied in a series of works analyzing the importance of depth (Cohen et al., 2016a), compared them to the expressive capacity of ConvNets (Cohen and Shashua, 2016a), showing the latter is less capable than ConvACs, and the ability of ConvACs to model the dependency structure typically found in natural data (Cohen and Shashua, 2016b). We prove in app. D that their main results are not hindered by the introduction of simplex constraints to ConvACs as we did above. Together these results give us a detailed understanding of how the number of channels and size of pooling windows control the expressivity of the model. A more in depth overview of their results and its application to our models can be found in app. C.\n3.3 COMPARISON TO SUM-PRODUCT NETWORKS\nSum-Product Networks (SPNs) are a related class of generative models which are also realized by Arithmetic Circuits, though not strictly convolutional circuits as defined above. While SPNs can realize any ConvAC and thus are universal and posses tractable inference, their lack of structure puts them at a disadvantage.\nPicking the right SPN structure from the infinite possible combinations of sum and product nodes could be perplexing even for experts in the field. Indeed Poon and Domingos (2011); Gens and Domingos (2012) had to hand-engineer complex structures for each dataset guided by prior knowledge and heuristics, and while their results were impressive for their time, they are poor by current measures. This lead to many works studying the task of learning the structure directly from the data itself (Peharz et al., 2013; Gens and Domingos, 2013; Adel et al., 2015; Rooshenas and Lowd, 2014), which indeed improved upon manually designed SPNs on some tasks. Nevertheless, when\ncompared in absolute terms compared to other models, and not just average log-likelihood, they do not perform well even on simple handwritten digit classification datasets (Adel et al., 2015).\nAs opposed to SPNs, TMMs implemented with ConvACs have an easily designed architecture with only two set of parameters, size of pooling windows and number of channels, both of which can be directly related to the expressivity of the model as detailed in app. C. Additionally, while SPNs are typically trained using special EM-type algorithms, TMMs are trained using the stochastic gradient descent type algorithms as is common in training neural networks (see sec. 4 for details), thereby benefiting from the shared experience of a large and growing community.\n4 CLASSIFICATION AND LEARNING WITH TMMS\nUntil this point we presented the TMM as a generative model for high-dimensional data, which is universal, and whose structure is tightly coupled to that of convolutional networks. We have yet to incorporate classification and learning into our framework. This is the purpose of the current section.\nThe common way to introduce object classes into a generative framework is to consider a class variable Y , and the distributions P (X|Y ) of the instanceX conditioned on Y . Under our model this is equivalent to having shared mixing components, but different priors tensors P (d1, . . . , dN |Y=y) for each class. Though it is possible to decompose each priors tensor separately, it is much more efficient to employ the concept of joint tensor decomposition, and use a shared ConvAC instead. This results in a single ConvAC computing inference, where instead of a single scalar output, multiple outputs are driven by the network \u2013 one for each class \u2013 as illustrated through the network in fig. 3.\nHeading on to predicting the class of a given instance, we note that in practice, na\u0131\u0308ve implementation of ConvACs is not numerically stable, the reason being that high degree polynomials (as computed by such networks) are easily susceptible to numerical underflow or overflow. The conventional method for tackling this issue is to perform all computations in log-space. This transforms ConvACs into SimNets, a recently introduced deep learning architecture (Cohen and Shashua, 2014; Cohen et al., 2016b). Finally, prediction is carried by returning the most likely class, which in the common setting of uniform class priors (P\u0398(Y=y)\u22611/K), translates to simply predicting the class for which the corresponding network output is maximal, in accordance with standard neural network practice:\nY\u0302 (X) = argmaxy P (Y = y|X) = argmaxy logP (X|Y = y)\nSuppose now that we are given a training set S = {(X(i)\u2208(Rs)N , Y (i)\u2208[K])}|S|i=1 of instances and labels, and would like to fit the parameters \u0398 of multi-class TMM according to the Maximum Likelihood method. Equivalently, we minimize the Negative Log-Likelihood (NLL) loss function: L(\u0398) = E[\u2212 logP\u0398(X,Y )], which can be factorized into two separate loss functions:\nL(\u0398) = E[\u2212 logP\u0398(Y |X)] + E[\u2212 logP\u0398(X)] where E[\u2212 logP\u0398(Y |X)] is commonly known as the cross-entropy loss, which we refer to as the discriminative loss, while E[\u2212 logP\u0398(X)] corresponds to maximizing the prior likelihood P (X), and has no analogy in standard discriminative neural networks. It is this term that captures the generative nature of our model, and we accordingly refer to it as the generative loss. Now, let N\u0398(X (i); y):= logP\u0398(X (i)|Y=y) stand for the y\u2019th output of the SimNet (ConvAC in log-space) realizing the TMM with parameters \u0398, then in the case of uniform class priors, the empirical estimation of L(\u0398) may be written as:\nL(\u0398;S) = \u2212 1|S| \u2211|S| i=1 log eN\u0398(X (i);Y (i))\n\u2211K y=1 e\nN\u0398(X(i);y) \u2212 1|S| \u2211|S| i=1 log \u2211K y=1 eN\u0398(X (i);y) (4)\nMaximum likelihood training of generative models is oftentimes based on dedicated algorithms such as Expectation-Maximization, which are typically difficult to apply at scale. We leverage the resemblance between our objective (eq. 4) and that of standard neural networks, and apply the same optimization procedures used for the latter, which have proven to be extremely effective for training classifiers at scale. Whereas other works have used tensor decompositions for the optimization of probabilistic models (Song et al., 2013; Anandkumar et al., 2014), we employ them strictly for modeling and instead make use of conventional methods. In particular, our implementation of TMMs is based on the SimNets extension of Caffe toolbox (Cohen et al., 2016b; Jia et al., 2014), and uses standard Stochastic Gradient Descent-type methods for optimization (see sec. 6 for more details).\n5 CLASSIFICATION WITH MISSING DATA THROUGH MARGINALIZATION\nA major advantage of generative models over discriminative ones lies in the ability to cope with missing data, specifically in the context of classification. By and large, discriminative methods either attempt to complete missing parts of the data before classification, known as data imputation, or learn directly to classify data with missing values (Little and Rubin, 2002). The first of these approaches relies on the quality of data completion, a much more difficult task than the original one of classification with missing data. Even if the completion was optimal, the resulting classifier is known to be sub-optimal (see app. E). The second approach does not make this assumption, but nonetheless assumes that the distribution of missing values at train and test times are similar, a condition which often does not hold in practice. Indeed, Globerson and Roweis (2006) coined the term \u201cnightmare at test time\u201d to refer to the common situation where a classifier must cope with missing data whose distribution is different from that encountered in training.\nAs opposed to discriminative methods, generative models are endowed with a natural mechanism for classification with missing data. Namely, a generative model can simply marginalize over missing values, effectively classifying under all possible completions, weighing each completion according to its probability. This, however, requires tractable inference and marginalization. We have already shown in sec. 3 that TMM support the former, and will show in sec. 5.1 bring forth marginalization which is just as efficient. Beforehand, we lay out the formulation of classification with missing data.\nLet X be a random vector in Rs representing an object, and Y be a random variable in [K]:={1, . . . ,K} representing its label. Denote byD(X ,Y) the joint distribution of (X ,Y), and by (x\u2208Rs, y\u2208[K]) specific realizations thereof. Assume that after sampling a specific instance (x, y), a random binary vectorM is drawn conditioned on X=x. More concretely, we sample a binary mask m\u2208{0, 1}s (realization ofM) according to a distributionQ(\u00b7|X=x). xi is considered missing ifmi is equal to zero, and observed otherwise. Formally, we consider the vector x m, whose i\u2019th coordinate is defined to hold xi if mi=1, and the wildcard \u2217 if mi=0. The classification task is then to predict y given access solely to x m. Following the works of Rubin (1976); Little and Rubin (2002), we consider three cases for the missingness distribution Q(M=m|X=x): missing completely at random (MCAR), where M is independent of X , i.e. Q(M=m|X=x) is a function of m but not of x; missing at random (MAR), whereM is independent of the missing values in X , i.e. Q(M=m|X=x) is a function of both m and x, but is not affected by changes in xi if mi=0; and missing not at random (MNAR), covering the rest of the distributions for whichM depends on missing values in X , i.e.Q(M=m|X=x) is a function of both m and x, which at least sometimes is sensitive to changes in xi when mi=0.\nLet P be the joint distribution of the object X , label Y , and missingness maskM: P(X=x,Y=y,M=m) = D (X=x,Y=y) \u00b7 Q(M=m|X=x)\nFor given x \u2208 Rs and m \u2208 {0, 1}s, denote by o(x,m) the event where the random vector X coincides with x on the coordinates i for which mi = 1. For example, if m is an all-zero vector o(x,m) covers the entire probability space, and if m is an all-one vector o(x,m) corresponds to the event X = x. With these notations in hand, we are now in a position to characterize the optimal predictor in the presence of missing data: Claim 1. For any data distribution D and missingness distribution Q, the optimal classification rule in terms of 0-1 loss is given by:\nh\u2217(x m) = argmaxy P(Y=y|o(x,m))P(M=m|o(x,m),Y=y)\nProof. See app. E.\nWhen the distributionQ is MAR (or MCAR), the classifier admits a simpler form, referred to as the marginalized Bayes predictor: Corollary 1. Under the conditions of claim 1, if the distributionQ is MAR (or MCAR), the optimal classification rule may be written as:\nh\u2217(x m) = argmaxy P(Y=y|o(x,m)) (5)\nProof. See app. E.\nCorollary 1 indicates that in the MAR setting, which is frequently encountered in practice, optimal classification does not require prior knowledge regarding the missingness distribution Q. As long as one is able to realize the marginalized Bayes predictor (eq. 5), or equivalently, to compute the likelihoods of observed values conditioned on labels (P(o(x,m)|Y=y)), classification with missing data is guaranteed to be optimal, regardless of the corruption process taking place. This is in stark contrast to discriminative methods, which require access to the missingness distribution during training, and thus are not able to cope with unknown conditions at test time.\nMost of this section dealt with the task of prediction given an input with missing data, where we assumed we had access to a complete and uncorrupted training set, and only faced missingness during prediction. However, many times we wish to tackle the reverse problem, where the training set itself is riddled with missing data. Generative methods can once again leverage their natural ability to handle missing data in the form of marginalization during the learning stage. Generative models are typically learned through the Maximum Likelihood principle. When it comes to learning from missing data, the marginalized likelihood objective is used instead. Under the MAR assumption, this method results in an unbiased classifier (Little and Rubin, 2002).\n5.1 EFFICIENT MARGINALIZATION WITH TMMS\nAs discussed above, with generative models optimal classification with missing data (in the MAR setting) is oblivious to the specific missingness distribution. However, it requires tractable computation of the likelihood of observed values conditioned on labels, i.e. tractable marginalization over missing values. The plurality of generative models that have recently gained attention in the deep learning community (Goodfellow et al., 2014; Kingma and Welling, 2014; Dinh et al., 2014; 2016) do not meet this requirement, and thus are not suitable for classification with missing data. TMMs on the other hand bring forth extremely efficient marginalization, requiring only a single forward pass through the corresponding network. Details follow.\nRecall from sec. 3 and 4 that a multi-class TMM realizes the following form:\nP (x1, . . . ,xN |Y=y) = \u2211M\nd1,...,dN P (d1, . . . , dN |Y=y) \u220fN i=1\nP (xi|di; \u03b8di) (6) Suppose now that only the local structures xi1 . . .xiV are observed, and we would like to marginalize over the rest. Integrating eq. 6 gives:\nP (xi1 , . . . ,xiV |Y=y) = \u2211M\nd1,...,dN P (d1, . . . , dN |Y=y) \u220fV v=1\nP (xiv |div ; \u03b8div ) from which it is evident that the same ConvAC used to compute P (x1, . . . ,xN |Y=y), can be used to compute P (xi1 , . . . ,xiV |Y=y) \u2013 all it requires is a slight adaptation of the representation layer. Namely, the latter would represent observed values through the usual likelihoods, whereas missing (marginalized) values would now be represented via constant ones:\nrep(i, d) = {\n1 , xi is missing (marginalized) P (xi|d; \u0398) , xi is visible (not marginalized)\nTo conclude, with TMMs marginalizing over missing values is just as efficient as plain inference \u2013 requires only a single pass through the corresponding ConvAC. Accordingly, the marginalized Bayes predictor (eq. 5) is realized efficiently, and classification with missing data (in the MAR setting) is optimal, regardless of the missingness distribution. This capability is not provided by discriminative methods, which rely on the distribution of missing values being know at training, and by contemporary generative models, which do not bring forth tractable marginalization.\n6 EXPERIMENTS\nWe demonstrate the properties of our models through both qualitative and quantitative experiments. In subsec. 6.1 we present our state-of-the-art results on image classification with missing data, with robustness to various missingness distributions. In app. G we show visualizations produced by our models, which gives us insight into its inner workings. Our experiments were conducted on the MNIST digit classification dataset, consisting of 60000 grayscale images of single digit numbers, as well as the small NORB 3D object recognition dataset, consisting of 48600 grayscale stereo images of toys belonging to 5 categories: four-legged animals, human figures, airplanes, trucks, and cars\nIn all our experiments we use either the GCP or GHT model with Gaussian mixing components. The weights of the conv layers are partially shared as described in sec 3.2, and are represented in log-space. For the case of the GHT model, we use 2\u00d7 2 pooling windows for all pooling layers. We train our model according to the loss described in sec. 4, using the Adam (Kingma and Ba, 2015) variant of SGD and decaying learning rates. We apply L2-regularization to the weights while taking into account they are stored in log-space. Additionally, we also adapt a probabilistic interpretation of dropout (?) by introducing random marginalization layers, that randomly select spatial locations in the input and marginalize over them. We provide a complete and detailed description of our experiments in app. F.\nOur implementation, which is based on Caffe (Jia et al., 2014) and MAPS (Ben-Nun et al., 2015), as well as other code for reproducing our experiments, is available through our Github repository: https://github.com/HUJI-Deep/TMM.\n6.1 IMAGE CLASSIFICATION WITH MISSING DATA\nWe demonstrate the effectiveness of our method for classification with missing data of unknown missingness distribution (see sec. 5), by conducting three kinds of experiments on the MNIST dataset, and an additional experiment on the NORB dataset. We begin by following the protocol of Globerson and Roweis (2006) \u2013 the binary classification problem of digit pairs with feature deletion noise \u2013 where we compare our method to the best known result on that benchmark (Dekel and Shamir, 2008). For our main experiment, we move to the harder multi-class digit classification under two different MAR missingness distributions, comparing against other methods which do not assume a specific missingness distribution. We repeat this experiment on the NORB dataset as well. Finally, our last experiment demonstrates the failure of purely discriminative methods to adapt to previously unseen missingness distributions, underlining the importance of the generative approach to missing data. We do wish to emphasize that missing data is not typically found in most image data, nevertheless, experiments on images with missing data are very common, for both classification and inpainting tasks. Additionally, there is nothing about our method, nor the methods we compare it against, that is very specific to the image domain, and thus any conclusion drawn should not be limited to the chosen datasets, but be taken in the broader context of the missing data problem.\nThe problem of learning classifiers which are robust to unforeseen missingness distributions at test time was first proposed by Globerson and Roweis (2006). They suggested missing values could be denoted by values which were deleted, i.e. their values were changed to zero, and a robust classifier would have to assume that any of its zero-value inputs could be the result of such a deletion process, and must be treated as missing. Their solution was to train a linear classifier and formulate the optimization as a quadric program under the constraint that N of its features could be deleted. In Dekel and Shamir (2008), this solution was improved upon and generalized to other kinds of corruption beyond deletion as well as to an adversarial setting.\nWe follow the central experiment of these articles, conducted on binary classification of digits pairs from the MNIST dataset, where N non-zero pixels are deleted with uniform probability over the set of N non-zero pixel locations of the given image. We compare our method, using the deep GHT-\nModel, solely against the LP-based algorithm of Dekel and Shamir (2008), which is the previous state-of-the-art on this task. Due to the limited computational resources at the time, the original experiments were limited to training sets of just 50 images per digit. We have repeated their experiment, using the implementation kindly supplied to us by the authors, and increased the limit to 300 images per digit, which is the maximal amount possible with our current computational resources. Though it is possible to train our own models using much larger training sets, we have trained them under the same limitations. Despite the fact that missingness distribution of this experiment is of the MNAR type, which our method was not guarantied to be optimal under, the test results (see table 1) clearly show the large gap between our method and theirs. Additionally, whereas our method uses a single model trained once and with no prior knowledge on the missingness distribution, their method requires training special classifiers for each value of N , chosen through a cross-validation process, disqualifying it from being truly blind to the missingness distribution.\nWe continue to our main experiments on multi-class blind classification with missing data, where the missingness distribution is completely unknown during test time, and a single classifier must handle all possible distributions. We simulate two kinds of MAR missingness distributions: (i) an i.i.d. mask with a fixed probability p \u2208 [0, 1] of missing each pixel, and (ii) a mask composed of the union of N possibly overlapping rectangles of width and height equal to W , each with a randomly assigned position in the image, distributed uniformly. We evaluate both our shallow GCP-Model as well as the deep GHT-Model against the most widely used methods for blind classification with missing data. We repeat these experiments on the MNIST and NORB datasets, the results of which are presented in fig. 4.\nAs a baseline for our results, we use K-Nearest Neighbors (KNN) to vote on the most likely class of a given example. We extend KNN to missing data by comparing distances using only the observed entries, i.e. for a corrupted instance x m, and a clean image from the training set x\u0303, we compute: d(x\u0303,x m)=\u2211mij=1(x\u0303ij\u2212xij)2. Though it scores better than the majority of modern methods we have compared, in practice KNN is very inefficient, even more so for missing data, which prevents most common memory and runtime optimizations typically employed to reduce its inefficiency. Additionally, KNN does not generalize well for more complex datasets, as is evident by its poor performance on the clean test set of the NORB dataset.\nAs discusses in sec. 5, data-imputation is the most common method to handle missing data of unknown missingness distributions. Despite the popularity of this method, high quality data imputations are very hard to produce, amplified by the fact that classification algorithms are known to be highly sensitive to even a small noise applied to their inputs (?). Even if we assume the dataimputation step was done optimally, it would still not give optimal performance under all MAR missingness distributions, and under some settings could produce results which are only half as good as our method (see app. E for such a case). In our experiments, we have applied several data-imputations methods to complete the missing data, followed by classifying its outputs using a standard ConvNet fitted to the fully-observed training set. We first tested naive heuristics, filling missing values with zeros or the mean pixel value computed over all the images in the dataset. We then tested three generative models: GSN (Bengio et al., 2014), NICE (Dinh et al., 2014) and DPM (Sohl-Dickstein et al., 2015), which are known to work well for inpainting. GSN was omitted from the NORB experiments as we have not manage to properly train it on that dataset. Though the data-imputation methods are competitive when only few of the pixels are missing, they all fall far behind our models above a certain threshold, with more than 50 percentage points separating our GHT-model from the best data-imputation method under some of the cases. Additionally, all the generative models require very long runtimes, which prevents from using them in most real-world applications. While we tried to be as comprehensive as possible when choosing which inpainting methods to use, some of the most recent studies on the subject, e.g. the works of van den Oord et al. (2016) and Pathak et al. (2016), have either not yet published their code or only partially published it. We have also ruled out inpainting algorithms which are made specifically for images, as we did not want to limit the implications of these experiments solely to images.\nWe have also compared ourselves to the published results of the MPDBM model (Goodfellow et al., 2013). Unlike the previous generative models we tested, MPDBM is a generative classifier similar to our method. However, unlike our model, MPDBM does not posses the tractable marginalization nor the tractable inference properties, and uses approximations instead. Its lesser performance underlines the importance of these properties for achieving optimality under missing data. An additional factor might also be their training method, which includes randomly picking a subset of variables to act as missing, which might have introduced a bias to the specific missingness distribution used during their training.\nIn order to demonstrate the ineffectiveness of purely discriminative models, we trained ConvNets directly on randomly corrupted instances according to pre-selected missingness distributions on the MNIST dataset. Unlike the previous experiments, we do allow prior knowledge about the missingness distribution during training time. We found that the best results are achieved when replacing missing values with zeros, and adding as an extra input channel the mask of missing values (known as flag data-imputation). The results (see fig. 5) unequivocally show the effectiveness of this method when tested on the same distribution it was trained on, achieving a high accuracy even when only 10% of the pixels are visible. However, when tested on different distributions, whether on a completely different kind or even on the same kind but with different parameters, the accuracy drops by a large factor, at times by more than 35 percentage points. This illustrate the disadvantage of the discriminative method, as it necessarily incorporates bias towards the corruption process it had seen during training, which makes it fail on other distributions. One might wonder whether it is\npossible for a single network to be robust on more than a single distribution. We found out that the latter is true, and if we train a network on multiple different missingness distributions1, then the network will achieve good performance on all such distributions, though at some cases not reaching the optimal performance. However, though it is possible to train a network to be robust on more than one distribution, the type of missingness distributions are rarely known in advance, and there is no known method to train a neural network against all possible distributions, limiting the effectivity of this method in practice.\nUnlike all the above methods, our GHT-model, which is trained only once on the clean dataset, match or sometimes even surpass the performance of ConvNets that are trained and tested on the same distribution, showing it is achieving near optimal performance \u2013 as much as possible on any given distribution. Additionally, note that similar to ConvNets and according to the theory in app. C, the deep GHT-model is decidedly superior to the shallow GCP-model. Experimenting on more complex datasets is left for further research. Progress on optimization and regularization of networks based on product pooling (even in log-space) is required, and ways to incorporate larger b\u00d7b convolutional operations with overlaps would be useful before we venture into larger and complex datasets. Nevertheless, our preliminary results demonstrate an overwhelming advantage of our TMM models compared to competing methods, both in terms of robustness to different types of missing data, as well as in terms of raw performance, with very wide gaps in absolute accuracy than the next best method, at times as large as 50 percentage points more than the next best method.\n7 SUMMARY\nWe have introduced a new family of probabilistic models, which we call Tensorial Mixture Models. TMMs are based on a simple assumption on the data, which stems from known empirical results on natural images, that gives rise to mixture models with tensorial structure represented by the priors tensor. When the priors tensor is decomposed it gives rise to an arithmetic circuit which in turn transforms the TMM into a Convolutional Arithmetic Circuit (ConvAC). A ConvAC corresponds to a shallow (single hidden layer) network when the priors tensor is decomposed by a CP (sum of rank-1) approach and corresponds to a deep network when the decomposition follows the Hierarchical Tucker (HT) model.\nThe ConvAC representation of a TMM possesses several attractive properties. First, the inference is tractable and is implemented by a forward pass through a deep network. Second, the architectural design of the model follows the deep networks community design, i.e., the structure of TMMs is determined by just two easily understood factors: size of pooling windows and number of channels. Finally, we have demonstrated the effectiveness of our model when tackling the problem of classification with missing data, leveraging TMMs unique ability of tractable marginalization which leads to optimal classifiers regardless of the missingness distribution.\nThere are several avenues for future research on TMMs which we are currently looking at, including other problems which TMMs could solve (e.g. semi-supervised learning), experimenting with other ConvACs architectures (e.g. through different decompositions), and further progress on optimization and regularization of networks with product pooling.\nA BACKGROUND ON TENSOR DECOMPOSITIONS AND CONVOLUTIONAL ARITHMETIC CIRCUITS\nWe begin by establishing the minimal background in the field of tensor analysis required for following our work. A tensor is best thought of as a multi-dimensional array Ad1,...,dN \u2208 R, where \u2200i \u2208 [N ], di \u2208 [Mi]. The number of indexing entries in the array, which are also called modes, is referred to as the order of the tensor. The number of values an index of a particular mode can take is referred to as the dimension of the mode. The tensor A \u2208 RM1\u2297...\u2297MN mentioned above is thus of order N with dimension Mi in its i-th mode. For our purposes we typically assume that M1 = . . . = MN = M , and simply denote it as A \u2208 (RM )\u2297N .\nThe fundamental operator in tensor analysis is the tensor product. The tensor product operator, denoted by \u2297, is a generalization of outer product of vectors (1-ordered vectors) to any pair of tensors. Specifically, letA and B be tensors of order P and Q respectively, then the tensor product A\u2297 B results in a tensor of order P +Q, defined by: (A\u2297 B)d1,...,dP+Q = Ad1,...,dP \u00b7 BdP+1,...,dP+Q .\nThe main concept from tensor analysis we use in our work is that of tensor decompositions. The most straightforward and common tensor decomposition format is the rank-1 decomposition, also known as a CANDECOMP/PARAFAC decomposition, or in short, a CP decomposition. The CP decomposition is a natural extension of low-rank matrix decomposition to general tensors, both built upon the concept of a linear combination of rank-1 elements. Similarly to matrices, tensors of the form v(1) \u2297 \u00b7 \u00b7 \u00b7 \u2297 v(N), where v(i) \u2208 RMi are non-zero vectors, are regarded as N -ordered rank-1 tensors, thus the rank-Z CP decomposition of a tensor A is naturally defined by:\nA = Z\u2211 z=1 aza z,1 \u2297 \u00b7 \u00b7 \u00b7 \u2297 az,N\n\u21d2 Ad1,...,dN = Z\u2211 z=1 az N\u220f i=1 az,idi (7)\nwhere {az,i \u2208 RMi}N,Zi=1,z=1 and a \u2208 R Z are the parameters of the decomposition. As mentioned above, for N = 2 it is equivalent to low-order matrix factorization. It is simple to show that any tensor A can be represented by the CP decomposition for some Z, where the minimal such Z is known as its tensor rank.\nAnother decomposition we will use in this paper is of a hierarchical nature and known as the Hierarchical Tucker decomposition (Hackbusch and Ku\u0308hn, 2009), which we will refer to as HT decomposition. While the CP decomposition combines vectors into higher order tensors in a single step, the HT decomposition does that more gradually, combining vectors into matrices, these matrices into 4th ordered tensors and so on recursively in a hierarchically fashion. Specifically, the following describes the recursive formula of the HT decomposition2\n2 More precisely, we use a special case of the canonical HT decomposition as presented in Hackbusch and Ku\u0308hn (2009). In the terminology of the latter, the matrices Al,j,\u03b3 are diagonal and equal to diag(al,j,\u03b3) (using the notations from eq. 8).\nfor a tensor A \u2208 (RM )\u2297N where N = 2L, i.e. N is a power of two3:\n\u03c61,j,\u03b3 = r0\u2211 \u03b1=1 a1,j,\u03b3\u03b1 a 0,2j\u22121,\u03b1 \u2297 a0,2j,\u03b1\n\u00b7 \u00b7 \u00b7\n\u03c6l,j,\u03b3 = rl\u22121\u2211 \u03b1=1\nal,j,\u03b3\u03b1 \u03c6 l\u22121,2j\u22121,\u03b1\ufe38 \ufe37\ufe37 \ufe38 order 2l\u22121 \u2297\u03c6l\u22121,2j,\u03b1\ufe38 \ufe37\ufe37 \ufe38 order 2l\u22121\n\u00b7 \u00b7 \u00b7\n\u03c6L\u22121,j,\u03b3 = rL\u22122\u2211 \u03b1=1 aL\u22121,j,\u03b3\u03b1 \u03c6 L\u22122,2j\u22121,\u03b1\ufe38 \ufe37\ufe37 \ufe38\norder N 4\n\u2297\u03c6L\u22122,2j,\u03b1\ufe38 \ufe37\ufe37 \ufe38 order N\n4 A = rL\u22121\u2211 \u03b1=1\naL\u03b1 \u03c6 L\u22121,1,\u03b1\ufe38 \ufe37\ufe37 \ufe38 order N\n2\n\u2297\u03c6L\u22121,2,\u03b1\ufe38 \ufe37\ufe37 \ufe38 order N\n2\n(8)\nwhere the parameters of the decomposition are the vectors {al,j,\u03b3\u2208Rrl\u22121}l\u2208{0,...,L\u22121},j\u2208[N/2l],\u03b3\u2208[rl] and the top level vector aL \u2208 RrL\u22121 , and the scalars r0, . . . , rL\u22121 \u2208 N are referred to as the ranks of the decomposition. Similar to the CP decomposition, any tensor can be represented by an HT decomposition. Moreover, any given CP decomposition can be converted to an HT decomposition by only a polynomial increase in the number of parameters.\nThe relationship between tensor decomposition and networks arises from the simple observation that through decomposition one can tradeoff storage complexity with computation where the type of computation consists of sums and products. Specifically, tensor decompositions could be seen as a mapping, that takes a tensor of exponential size and converts it into a polynomially sized representation, coupled with a decoding algorithm of polynomial runtime complexity to retrieve the original entries of tensor \u2013 essentially trading off space complexity for computational complexity. Examining the decoding algorithms for the CP and HT decompositions, i.e. eq. 7 and eq. 8, respectively, reveal a shared framework for representing these algorithms via computation graphs of products and weighted sums, also known as Arithmetic Circuits (Shpilka and Yehudayoff, 2010) or Sum-Product Networks (Poon and Domingos, 2011). More specifically, these circuits take as inputN indicator vectors \u03b41, . . . , \u03b4N , representing the coordinates (d1, . . . , dN ), where \u03b4i = 1[j=di], and output the value of Ad1,...,dN . In the case of the CP decomposition, the matching decoding circuit is defined by eq. 9 below:\naz,idi = M\u2211 d=1 az,id \u03b4id \u21d2 Ad1,...,dN = Z\u2211 z=1 az N\u220f i=1 M\u2211 d=1 az,id \u03b4id (9)\nThe above formula is better represented by the network illustrated in fig. 6, beginning with an input layer of\u221a N \u00d7 \u221a N M -dimensional indicator vectors arranged in a 3D array, followed by a 1 \u00d7 1 conv operator, a global product pooling layer, and ends with a dense linear layer outputtingAd1,...,dN . The conv operator is not unlike the standard convolutional layer of ConvNets, with the sole difference being that it may operate without coefficient sharing, i.e. the filters that generate feature maps by sliding across the previous layer may have different coefficients at different spatial locations. This is often referred to in the deep learning community as a locally-connected operator (Taigman et al., 2014). Similarly to the CP decomposition, retrieving the entries of a tensor from its HT decomposition can be computed by the circuit represented in fig. 7, where instead of a single pair of conv and pooling layers there are log2 N such pairs, with pooling windows of size 2. Though the canonical HT decomposition dictates size 2 pooling windows, any pooling structure used in practice still results in a valid HT decomposition.\nArithmetic Circuits constructed from the above conv and product pooling layers are called Convolutional Arithmetic Circuits, or ConvACs for short, first suggested by Cohen et al. (2016a) as a theoretical framework for studying standard convolutional networks, sharing many of the defining traits of the latter, most noteworthy, the locality, sharing and pooling properties of ConvNets. Unlike general circuits, the structure of the network is determined solely by two parameters, the number of channels of each conv layer and the size of pooling windows, which indirectly controls the depth of the network.\n3The requirement for N to be a power of two is solely for simplifying the definition of the HT decomposition. More generally, instead of defining it through a complete binary tree describing the order of operations, the canonical decomposition can use any balanced binary tree.\nB THE UNIVERSALITY OF TENSORIAL MIXTURE MODELS\nIn this section we prove the universality property of TMMs, as discussed in sec. 3. We begin by taking note from functional analysis and define a new property called PDF total set, which is similar in concept to a total set, followed by proving that this property is invariant under the cartesian product of functions, which entails the universality of TMMs as a corollary. Definition 1. Let F be a set of PDFs over Rs. F is PDF total iff for any PDF h(x) over Rs and for all > 0 there exists M \u2208 N, {f1(x), . . . , fM (x)} \u2282 F and w \u2208 4M\u22121 s.t. \u2225\u2225\u2225h(x)\u2212\u2211Mi=1 wifi(x)\u2225\u2225\u2225 1 < . In other words, a set is a PDF total set if its convex span is a dense set under L1 norm.\nClaim 2. Let F be a set of PDFs over Rs and let F\u2297N = { \u220fN i=1 fi(x)|\u2200i, fi(x) \u2208 F} be a set of PDFs over the product space (Rs)N . If F is a PDF total set then F\u2297N is PDF total set.\nProof. If F is the set of Gaussian PDFs over Rs with diagonal covariance matrices, which is known to be a PDF total set, then F\u2297N is the set of Gaussian PDFs over (Rs)N with diagonal covariance matrices and the claim is trivially true.\nOtherwise, let h(x1, . . . ,xN ) be a PDF over (Rs)N and let > 0. From the above, there exists K \u2208 N, w \u2208 4M1\u22121 and a set of diagonal Gaussians {gij(x)}i\u2208[M1],j\u2208[N ] s.t.\u2225\u2225\u2225\u2225\u2225g(x)\u2212 M1\u2211 i=1 wi N\u220f j=1 gij(xj) \u2225\u2225\u2225\u2225\u2225 1 < 2 (10) Additionally, since F is a PDF total set then there exists M2 \u2208 N, {fk(x)}k\u2208[M2] \u2282 F and {wij \u2208 4M2\u22121}i\u2208[M1],j\u2208[N ] s.t. for all i \u2208 [M1], j \u2208 [N ] it holds that \u2225\u2225\u2225gij(x)\u2212\u2211M2k=1 wijkfk(x)\u2225\u2225\u2225 1 < 2N , from which it is trivially proven using a telescopic sum and the triangle inequality that:\u2225\u2225\u2225\u2225\u2225 M1\u2211 i=1 wi N\u220f j=1 gij(x)\u2212 M1\u2211 i=1 wi N\u220f j=1 M2\u2211 k=1 wijkfk(xj) \u2225\u2225\u2225\u2225\u2225 1 < 2 (11) From eq. 10, eq. 11 the triangle inequality it holds that:\u2225\u2225\u2225\u2225\u2225\u2225g(x)\u2212 M2\u2211\nk1,...,kN=1\nAk1,...,kN N\u220f j=1 fkj (xj) \u2225\u2225\u2225\u2225\u2225\u2225 1 <\nwhere Ak1,...,kN = \u2211M1 i=1 wi \u220fN j=1 wijkj which holds \u2211M2 k1,...,kN=1 Ak1,...,kN = 1. Taking M = M N 2 ,\n{ \u220fN j=1 fkj (xj)}k1\u2208[M2],...,kN\u2208[M2] \u2282 F \u2297N and w = vec(A) completes the proof.\nCorollary 2. Let F be a PDF total set of PDFs over Rs, then the family of TMMs with mixture components from F can approximate any PDF over (Rs)N arbitrarily well, given arbitrarily many components.\nC OVERVIEW ON THE EXPRESSIVE CAPACITY OF CONVOLUTIONAL ARITHMETIC CIRCUITS AND ITS AFFECT ON TENSORIAL MIXTURE MODELS\nThe expressiveness of ConvACs has been extensively studied, and specifically the non-generative variants of our models, named CP-model and HT-model respectively. In Cohen et al. (2016a) it was shown that ConvACs\nposses the property known as complete depth efficiency. Namely, almost all functions4 realized by an HT-model of polynomial size, for them to be realized (or approximated) by a CP-model, require it to be of exponential size. In other words, the expressiveness borne out of depth is exponentially stronger than a shallow network, almost always. It is worth noting that in the followup paper (Cohen and Shashua, 2016a), the authors have shown that the same result does not hold for standard ConvNets \u2013 while there are specific instances where depth efficiency holds, it is not complete, i.e. there is a non-zero probability that a function realized by a polynomially sized deep ConvNet can also be realized by a polynomially sized shallow ConvNet. Despite the additional simplex constraints put on the parameters, complete depth efficiency does hold for the generative ConvACs of our work, proof of which can be found in app. D, which shows the advantage of the deeper GHT-model over the shallow GCP-model. Additionally, this illustrates how the two factors controlling the architecture \u2013 number of channels and size of pooling windows \u2013 control the expressive capacity of the GHT-model. While the above shows why the deeper GHT-model is preferred over the shallow GCP-model, there is still the question of whether a polynomially sized GHT-model is sufficient for describing the complexities of natural data. Though a complete and definite answer is unknown as of yet, there are some strong theoretical evidence that it might. One aspect of being sufficient for modeling natural data is the ability of the model to describe the dependency structures typically found in the data. In Cohen and Shashua (2016b), the authors studied the separation rank \u2013 a measure of correlation, which for a given input partition, measures how far a function is from being separable \u2013 and found that a polynomially sized HT-model is capable of exponential separation rank for interleaved partitions, i.e. that it can model high correlations in local areas in the input. Additionally, for non-contiguous partitions, the separation rank can be at most polynomial, i.e. it can only model a limited correlation between far away areas in the input. These two results combined suggest that the HT-model, and thus also our GHT-model, is especially fit for modeling the type of correlations typically found in natural images and audio, even if it is only of polynomial size. Finally, from an empirical perspective, convolutional hierarchical structures have shown great success on multitude of different domains and tasks. Our models leverage these structures, taking them to a probabilistic setting, which leads us to believe that they will be able to effectively model distributions in practice \u2013 a belief we verify by experiments.\nD PROOF FOR THE DEPTH EFFICIENCY OF GENERATIVE CONVOLUTIONAL ARITHMETIC CIRCUITS\nIn this section we prove that the depth efficiency property of ConvACs proved in Cohen et al. (2016a) applies also to the Generative ConvACs we have introduced in sec. 3.2. More specifically, we prove the following theorem, which is the generative analog of theorem 1 from (Cohen et al., 2016a): Theorem 1. Let Ay be a tensor of order N and dimension M in each mode, generated by the recursive formulas in eq. 8, under the simplex constraints introduced in sec. 3.2. Define r := min{r0,M}, and consider the space of all possible configurations for the parameters of the decomposition \u2013 {al,j,\u03b3 \u2208 4rl\u22121\u22121}l,j,\u03b3 . In this space, the generated tensorAy will have CP-rank of at least rN/2 almost everywhere (w.r.t. the product measure of simplex spaces). Put differently, the configurations for which the CP-rank of Ay is less than rN/2 form a set of measure zero. The exact same result holds if we constrain the composition to be \u201cshared\u201d, i.e. set al,j,\u03b3 \u2261 al,\u03b3 and consider the space of {al,\u03b3 \u2208 4rl\u22121\u22121}l,\u03b3 configurations.\nThe only differences between ConvACs and their generative counter-parts are the simplex constraints applied to the parameters of the models, which necessitate a careful treatment to the measure theoretical arguments of the original proof. More specifically, while the k-dimensional simplex4k is a subset of the k+ 1-dimensional space Rk+1, it has a zero measure with respect to the Lebesgue measure over Rk+1. The standard method to define a measure over 4k is by the Lebesgue measure over Rk of its projection to that space, i.e. let \u03bb : Rk \u2192 R be the Lebesgue measure over Rk, p : Rk+1 \u2192 Rk, p(x) = (x1, . . . , xk)T be a projection, and A \u2282 4k be a subset of the simplex, then the latter\u2019s measure is defined as \u03bb(p(A)). Notice that p(4k) has a positive measure, and moreover that p is invertible over the set p(4k), and that its inverse is given by p\u22121(x1, . . . , xk) = (x1, . . . , xk, 1 \u2212 \u2211k i=1 xi). In our case, the parameter space is the cartesian product of several simplex spaces of different dimensions, for each of them the measure is defined as above, and the measure over their cartesian product is uniquely defined by the product measure. Though standard, the choice of the projection function p above could be seen as a limitation, however, the set of zero measure sets in 4k is identical for any reasonable choice of a projection \u03c0 (e.g. all polynomial mappings). More specifically, for any projection \u03c0 : Rk+1 \u2192 Rk that is invertible over \u03c0(4k), \u03c0\u22121 is differentiable, and the Jacobian of \u03c0\u22121 is bounded over \u03c0(4k), then a subset A \u2282 4k is of measure zero w.r.t. the projection \u03c0 iff it is of measure zero w.r.t. p (as defined above). This implies that if we sample the weights of the generative decomposition (eq. 8 with simplex constraints) by a continuous distribution, a property that holds with probability 1 under the standard parameterization (projection p), will hold with probability 1 under any reasonable parameterization.\n4\u201dAlmost all functions\u201d in this context means, that for any continuous distribution over the parameters of the HT-model, with probability one the following statement is true for a function realized by an HT-model with sampled parameters.\nWe now state and prove a lemma that will be needed for our proof of theorem 1.\nLemma 1. Let M,N,K \u2208 N, 1 \u2264 r \u2264 min{M,N} and a polynomial mapping A : RK \u2192 RM\u00d7N (i.e. for every i \u2208 [M ], j \u2208 [N ] then Aij : Rk \u2192 R is a polynomial function). If there exists a point x \u2208 RK s.t. rank (A(x)) \u2265 r, then the set {x \u2208 RK |rank (A(x)) < r} has zero measure.\nProof. Remember that rank (A(x)) \u2265 r iff there exits a non-zero r \u00d7 r minor of A(x), which is polynomial in the entries of A(x), and so it is polynomial in x as well. Let c = ( M r ) \u00b7 ( N r ) be the number of minors in A,\ndenote the minors by {fi(x)}ci=1, and define the polynomial function f(x) = \u2211c i=1 fi(x)\n2. It thus holds that f(x) = 0 iff for all i \u2208 [c] it holds that fi(x) = 0, i.e. f(x) = 0 iff rank (A(x)) < r.\nNow, f(x) is a polynomial in the entries of x, and so it either vanishes on a set of zero measure, or it is the zero polynomial (see Caron and Traynor (2005) for proof). Since we assumed that there exists x \u2208 RK s.t. rank(A(x)) \u2265 r, the latter option is not possible.\nFollowing the work of Cohen et al. (2016a), our main proof relies on following notations and facts:\n\u2022 We denote by [A] the matricization of an N -order tensor A (for simplicity, N is assumed to be even), where rows and columns correspond to odd and even modes, respectively. Specifically, if A \u2208 RM1\u00d7\u00b7\u00b7\u00b7MN , the matrix [A] has M1 \u00b7M3 \u00b7 . . . \u00b7MN\u22121 rows and M2 \u00b7M4 \u00b7 . . . \u00b7MN columns, rearranging the entries of the tensor such that Ad1,...,dN is stored in row index 1 + \u2211N/2 i=1(d2i\u22121 \u2212\n1) \u220fN/2 j=i+1 M2j\u22121 and column index 1 + \u2211N/2 i=1(d2i \u2212 1) \u220fN/2 j=i+1 M2j . Additionally, the matricization is a linear operator, i.e. for all scalars \u03b11, \u03b12 and tensors A1,A2 with the order and dimensions in every mode, it holds that [\u03b11A1 + \u03b12A2] = \u03b11[A1] + \u03b12[A2].\n\u2022 The relation between the Kronecker product (denoted by ) and the tensor product (denoted by \u2297) is given by [A\u2297 B] = [A] [B].\n\u2022 For any two matrices A and B, it holds that rank (A B) = rank (A) \u00b7 rank (B). \u2022 Let Z be the CP-rank of A, then it holds that rank ([A]) \u2264 Z (see (Cohen et al., 2016a) for proof).\nProof of theorem 1. Stemming from the above stated facts, to show that the CP-rank of Ay is at least rN/2, it is sufficient to examine its matricization [Ay] and prove that rank ([Ay]) \u2265 rN/2.\nNotice from the construction of [Ay], according to the recursive formula of the HT-decomposition, that its entires are polynomial in the parameters of the decomposition, its dimensions are MN/2 each and that 1 \u2264 rN/2 \u2264 MN/2. In accordance with the discussion on the measure of simplex spaces, for each vector parameter al,j,\u03b3 \u2208 4rl\u22121\u22121, we instead examine its projection a\u0303l,j,\u03b3 = p(al,j,\u03b3) \u2208 Rrl\u22121\u22121, and notice that p\u22121(a\u0303l,j,\u03b3) is a polynomial mapping5 w.r.t. a\u0303l,j,\u03b3 . Thus, [Ay] is a polynomial mapping w.r.t. the projected parameters {a\u0303l,j,\u03b3}l,j,\u03b3 , and using lemma 1 it is sufficient to show that there exists a set of parameters for which rank ([Ay]) \u2265 rN/2.\nDenoting for convenience \u03c6L,1,1 := Ay and rL = 1, we will construct by induction over l = 1, ..., L a set of parameters, {al,j,\u03b3}l,j,\u03b3 , for which the ranks of the matrices {[\u03c6l,j,\u03b3 ]}j\u2208[N/2l],\u03b3\u2208[rl] are at least r\n2l/2, while enforcing the simplex constraints on the parameters. More so, we\u2019ll construct these parameters s.t. al,j,\u03b3 = al,\u03b3 , thus proving both the \u201dunshared\u201d and \u201dshared\u201d cases.\nFor the case l = 1 we have:\n\u03c61,j,\u03b3 = r0\u2211 \u03b1=1 a1,j,\u03b3\u03b1 a 0,2j\u22121,\u03b1 \u2297 a0,2j,\u03b1\nand let a1,j,\u03b3\u03b1 = 1\u03b1\u2264r r and a0,j,\u03b1i = 1\u03b1=i for all i, j, \u03b3 and \u03b1 \u2264 M , and a 0,j,\u03b1 i = 1i=1 for all i and \u03b1 > M , and so\n[\u03c61,j,\u03b3 ]i,j = { 1/r i = j \u2227 i \u2264 r 0 Otherwise\nwhich means rank ( [\u03c61,j,\u03b3 ] ) = r, while preserving the simplex constraints, which proves our inductive hypothesis for l = 1.\n5As we mentioned earlier, p is invertible only over p(4k), for which its inverse is given by p\u22121(x1, . . . , xk) = (x1, . . . , xk, 1 \u2212 \u2211k i=1 xi). However, to simplified the proof and notations, we use p \u22121 as defined here over the entire range Rk\u22121, even where it does not serve as the inverse of p.\nAssume now that rank ( [\u03c6l\u22121,j \u2032,\u03b3\u2032 ] ) \u2265 r2\nl\u22121/2 for all j\u2032 \u2208 [N/2l\u22121] and \u03b3\u2032 \u2208 [rl\u22121]. For some specific choice of j \u2208 [N/2l] and \u03b3 \u2208 [rl] we have:\n\u03c6l,j,\u03b3 = rl\u22121\u2211 \u03b1=1 al,j,\u03b3\u03b1 \u03c6 l\u22121,2j\u22121,\u03b1 \u2297 \u03c6l\u22121,2j,\u03b1\n=\u21d2 [\u03c6l,j,\u03b3 ] = rl\u22121\u2211 \u03b1=1 al,j,\u03b3\u03b1 [\u03c6 l\u22121,2j\u22121,\u03b1] [\u03c6l\u22121,2j,\u03b1]\nDenote M\u03b1 := [\u03c6l\u22121,2j\u22121,\u03b1] [\u03c6l\u22121,2j,\u03b1] for \u03b1 = 1, ..., rl\u22121. By our inductive assumption, and by the general property rank (A B) = rank (A) \u00b7 rank (B), we have that the ranks of all matricesM\u03b1 are at least r 2l\u22121/2 \u00b7 r2 l\u22121/2 = r 2l/2. Writing [\u03c6l,j,\u03b3 ] = \u2211rl\u22121 \u03b1=1 a l,j,\u03b3 \u03b1 \u00b7M\u03b1, and noticing that {M\u03b1} do not depend on al,j,\u03b3 , we simply pick al,j,\u03b3\u03b1 = 1\u03b1=1, and thus \u03c6l,j,\u03b3 = M1, which is of rank r 2l/2. This completes the proof of the theorem.\nFrom the perspective of TMMs, theorem 1 leads to the following corollary:\nCorollary 3. Assume the mixing componentsM = {fi(x) \u2208 L2(R2)\u2229L1(Rs)}Mi=1 are square integrable6 probability density functions, which form a linearly independent set. Consider a deep GHT-model of polynomial size whose parameters are drawn at random by some continuous distribution. Then, with probability 1, the distribution realized by this network requires an exponential size in order to be realized (or approximated w.r.t. the L2 distance) by the shallow GCP-model. The claim holds regardless of whether the parameters of the deep GHT-model are shared or not.\nProof. Given a coefficient tensorA, the CP-rank ofA is a lower bound on the number of channels (denoted by Z in the body of the article) required to represent that tensor by the ConvAC following the CP decomposition as introduced in sec. 2. Additionally, since the mixing components are linearly independent, their products { \u220fN i=1 fi(xi)|fi \u2208 M} are linearly independent as well, which entails that any distribution representable by the TMM with mixing components M has a unique coefficient tensor A. From theorem 1, the set of parameters of a polynomial GHT-model with a coefficient tensor of a polynomial CP-rank, the requirement for a polynomial GCP-model realizing that distribution exactly, forms a set of measure zero.\nIt is left to prove, that not only is it impossible to exactly represent a distribution with an exponential coefficient tensor by a GCP-model, it is also impossible to approximate it. This follows directly from lemma 7 in appendix B of Cohen et al. (2016a), as our case meets the requirement of that lemma.\nE PROOF FOR THE OPTIMALITY OF MARGINALIZED BAYES PREDICTOR\nIn this section we give short proofs for the claims from sec. 5, on the optimality of the marginalized Bayes predictor under missing-at-random (MAR) distribution, when the missingness mechanism is unknown, as well as the general case when we do not add additional assumptions. In addition, we will also present a counter example proving data imputation results lead to suboptimal classification performance. We begin by introducing several notations that augment the notations already introduced in the body of the article.\nGiven a specific mask realization m \u2208 {0, 1}s, we use the following notations to denote partial assignments to the random vector X . For the observed indices of X , i.e. the indices for which mi = 1, we denote a partial assignment by X \\m = xo, where xo \u2208 Rdo is a vector of length do equal to the number of observed indices. Similarly, we denote by X \u2229 m = xm a partial assignment to the missing indices according to m, where xm \u2208 Rdm is a vector of length dm equal to the number of missing indices. As an example of the notation, for given realizations x \u2208 Rs and m \u2208 {0, 1}s, we defined in sec. 5 the event o(x,m), which using current notation is marked by the partial assignment X \\m = xo where xo matches the observed values of the vector x according to m.\nWith the above notations in place, we move on to prove claim 1, which describes the general solution to the optimal prediction rule given both the data and missingness distributions, and without adding any additional assumptions.\n6It is important to note that most commonly used distribution functions are square integrable, e.g. most members of the exponential family such as the Gaussian distribution.\nProof of claim 1. Fix an arbitrary prediction rule h. We will show that L(h\u2217) \u2264 L(h), where L is the expected 0-1 loss.\n1\u2212 L(h)=E(x,m,y)\u223c(X ,M,Y)[1h(x m)=y] = \u2211\nm\u2208{0,1}s \u2211 y\u2208[k] \u222b Rs P(M=m,X=x,Y=y)1h(x m)=ydx\n= \u2211\nm\u2208{0,1}s \u2211 y\u2208[k] \u222b Rdo \u222b Rdm P(M=m,X\\m=xo,X\u2229m=xm,Y=y)1h(x\u2297m)=ydxodxm\n=1 \u2211\nm\u2208{0,1}s \u2211 y\u2208[k] \u222b Rdo 1h(x m)=ydxo \u222b Rdm P(M=m,X\\m=xo,X\u2229m=xm,Y=y)dxm\n=2 \u2211\nm\u2208{0,1}s \u2211 y\u2208[k] \u222b Rdo 1h(x m)=yP(M=m,X\\m=xo,Y=y)dxo\n=3 \u2211\nm\u2208{0,1}s\n\u222b Rdo P(X\\m=xo) \u2211 y\u2208[k] 1h(x m)=yP(Y=y|X\\m=xo)P(M=m|X\\m=xo,Y=y)dxo\n\u22644 \u2211\nm\u2208{0,1}s\n\u222b Rdo P(X\\m=xo) \u2211 y\u2208[k] 1h\u2217(x m)=yP(Y=y|X\\m=xo)P(M=m|X\\m=xo,Y=y)dxo\n=1\u2212 L(h\u2217) Where (1) is because the output of h(x m) is independent of the missing values, (2) by marginalization, (3) by conditional probability definition and (4) because by definition h\u2217(x m) maximizes the expression P(Y=y|X\\m=xo)P(M=m|X\\m=xo,Y=y) w.r.t. the possible values of y for fixed vectors m and xo. Finally, by replacing integrals with sums, the proof holds exactly the same when instances (X ) are discrete.\nWe now continue and prove corollary 1, a direct implication of claim 1 which shows that in the MAR setting, the missingness distribution can be ignored, and the optimal prediction rule is given by the marginalized Bayes predictor.\nProof of corollary 1. Using the same notation as in the previous proof, and denoting by xo the partial vector containing the observed values of x m, the following holds:\nP(M=m|o(x,m),Y=y) := P(M=m|X\\m=xo,Y=y)\n= \u222b Rdm P(M=m,X \u2229m=xm|X\\m=xo,Y=y)dxm\n= \u222b Rdm P(X\u2229m=xm|X\\m=xo,Y=y) \u00b7 P(M=m|X\u2229m=xm,X\\m=xo,Y=y)dxm\n=1 \u222b Rdm P(X\u2229m=xm|X\\m=xo,Y=y) \u00b7 P(M=m|X\u2229m=xm,X\\m=xo)dxm\n=2 \u222b Rdm P(X\u2229m=xm|X\\m=xo,Y=y) \u00b7 P(M=m|X\\m=xo)dxm\n=P(M=m|X\\m=xo) \u222b Rdm P(X\u2229m=xm|X\\m=xo,Y=y)dxm\n=P(M=m|o(x,m)) Where (1) is due to the independence assumption of the events Y = y andM = m conditioned on X = x, while noting that (X \\m = xo) \u2227 (X \u2229m = xm) is a complete assignment of X . (2) is due to the MAR assumption, i.e. that for a given m and xo it holds for all xm \u2208 Rdm :\nP(M=m|X\\m=xo,X\u2229m=xm) = P(M=m|X\\m=xo) We have shown that P(M=m|o(x,m),Y = y) does not depend on y, and thus does not affect the optimal prediction rule in claim 1. It may therefore be dropped, and we obtain the marginalized Bayes predictor.\nHaving proved that in the MAR setting, classification through marginalization leads to optimal performance, we now move on to show that the same is not true for classification through data-imputation. Though there are many methods to perform data-imputation, i.e. to complete missing values given the observed ones, all of these methods can be seen as the solution of the following optimization problem, or more typically its approximation:\ng(x m) = argmax x\u2032\u2208Rs\u2227\u2200i:mi=1\u2192x\u2032i=xi\nP(X = x\u2032)\nWhere g(x m) is the most likely completion of x m. When data-imputation is carried out for classification purposes, one is often interested in data-imputation conditioned on a given class Y = y, i.e.:\ng(x m; y) = argmax x\u2032\u2208Rs\u2227\u2200i:mi=1\u2192x\u2032i=xi\nP(X = x\u2032|Y = y)\nGiven a classifier h : Rs \u2192 [K] and an instance x with missing values according to m, classification through data-imputation is simply the result of applying h on the output of g. When h is the optimal classifier for complete data, i.e. the Bayes predictor, we end up with one of the following prediction rules:\nUnconditional: h(x m) = argmax y P(Y = y|X = g(x m))\nConditional: h(x m) = argmax y P(Y = y|X = g(x m; y))\nClaim 3. There exists a data distribution D and MAR missingness distribution Q s.t. the accuracy of classification through data-imputation is almost half the accuracy of the optimal marginalized Bayes predictor, with an absolute gap of more than 33 percentage points.\nProof. For simplicity, we will give an example for a discrete distribution over the binary set X \u00d7Y = {0, 1}2 \u00d7 {0, 1}. Let 1> > 0 be some small positive number, and we defineD according to table 2, where each triplet (x1, x2, y) \u2208 X\u00d7Y is assigned a positive weight, which through normalization defines a distribution over X\u00d7Y . The missingness distribution Q is defined s.t. PQ(M1 = 1,M2 = 0|X = x) = 1 for all x \u2208 X , i.e. X1 is always observed andX2 is always missing, which is a trivial MAR distribution. Given the above data distribution D, we can easily calculate the exact accuracy of the optimal data-imputation classifier and the marginalized Bayes predictor under the missingness distribution Q, as well as the standard Bayes predictor under full-observability. First notice that whether we apply conditional or unconditional data-imputation, and whether X1 is equal to 0 or 1, the completion will always be X2 = 1 and the predicted class will always be Y = 1. Since the data-imputation classifiers always predict the same class Y = 1 regardless of their input, the probability of success is simply the probability P (Y = 1) = 1+\n3 (for = 10\u22124 it equals approximately\n33.337%). Similarly, the marginalized Bayes predictor always predicts Y = 0 regardless of its input, and so its probability of success is P (Y = 0) = 2\u2212\n3 (for = 10\u22124 it equals approximately 66.663%), which is\nalmost double the accuracy achieved by the data-imputation classifier. Additionally, notice that the marginalized Bayes predictor achieves almost the same accuracy as the Bayes predictor under full-observability, which equals exactly 2\n3 .\nF DETAILED DESCRIPTION OF THE EXPERIMENTS\nExperiments are meaningful only if they could be reproduced by other proficient individuals. Providing sufficient details to enable others to replicate our results is the goal of this section. We hope to accomplish this by making our code public, as well as documenting our experiments to a sufficient degree allowing for their reproduction from scratch. Our complete implementation of the models presented in this paper, as well as our modifications to other open-source projects and scripts used in the process of conducting our experiments, are available at our Github repository: https://github.com/HUJI-Deep/TMM. We additionally wish to invite readers to contact the authors, if they deem the following details insufficient in their process to reproduce our results.\nF.1 DESCRIPTION OF METHODS\nIn the following we give concise descriptions of each classification method we have used in our experiments. The results of the experiment on MP-DBM (Goodfellow et al., 2013) were taken directly from the paper and\nwere not conducted by us, hence we do not cover it in this section. We direct the reader to that article for exact details on how to reproduce their results.\nF.1.1 ROBUST LINEAR CLASSIFIER\nIn Dekel and Shamir (2008), binary linear classifiers were trained by formulating their optimization as a quadric program under the constraint that some of its features could be deleted, i.e. their original value was changed to zero. While the original source code was never published, the authors have kindly agreed to share with us their code, which we used to reproduced their results, but on larger datasets. The algorithm has only a couple hyper-parameters, which were chosen by a grid-search through a cross-validation process. For details on the exact protocol for testing binary classifiers on missing data, please see sec. F.2.1.\nF.1.2 K-NEAREST NEIGHBORS\nK-Nearest Neighbors (KNN) is a classical machine learning algorithm used for both regression and classification tasks. Its underlying mechanism is finding the k nearest examples (called neighbors) from the training set, (x1, y1), . . . , (xk, yk) \u2208 S, according to some metric function d(\u00b7, \u00b7) : X \u00d7 X \u2192 R+, after which a summarizing function f is applied to the targets of the k nearest neighbors to produce the output y\u2217 = f(y1, . . . , yk). When KNN is used for classification, f is typically the majority voting function, returning the class found in most of the k nearest neighbors.\nIn our experiments we use KNN for classification with missing data, where the training set consists of complete examples with no missing data, but at classification time the inputs have missing values. Given an input with missing values x m and an example x\u2032 from the training set, we use a modified Euclidean distance metric, where we compare the distance only against the non-missing coordinates of x, i.e. the metric is defined by d(x\u2032,x m) = \u2211 i:mi=1 (x\u2032i \u2212 xi) 2. Through a process of cross-validation we have chosen k = 5 for all of our experiments. Our implementation of KNN is based on the popular scikit-learn python library (Pedregosa et al., 2011).\nF.1.3 CONVOLUTIONAL NEURAL NETWORKS\nThe most widespread and successful discriminative method nowadays are Convolutional Neural Networks (ConvNets). Standard ConvNets are represented by a computational graph consisted of different kinds of nodes, called layers, with a convolutional-like operators applied to their inputs, followed by a non-linear point-wise activation function, e.g. max(0, x) known as ReLU.\nFor our experiments on MNIST, both with and without missing data, we have used the LeNeT ConvNet architecture (LeCun et al., 1998) that is bundled with Caffe (Jia et al., 2014), trained for 20,000 iterations using SGD with 0.9 momentum and 0.01 base learning rate, which remained constant for 10,000 iterations, followed by a linear decrease to 0.001 for another 5,000 iterations, followed by a linear decrease to 0 learning rate for the remaining 5,000 iterations. The model also used l2-regularization (also known as weight decay), which was chosen through cross-validation for each experiment separately. No other modifications were made to the model or its training procedure.\nFor our experiments on NORB, we have used an ensemble of 3 ConvNets, each using the following architecture: 5\u00d75 convolution with 128 output channels, 3\u00d73 max pooling with stride 2, ReLU activation, 5\u00d75 convolution with 128 output channels, ReLU activation, dropout layer with probability 0.5, 3\u00d73 average pooling with stride 2, 5\u00d75 convolution with 256 output channels, ReLU activation, dropout layer with probability 0.5, 3\u00d73 average pooling with stride 2, fully-connected layer with 768 output channels, ReLU activation, dropout layer with probability 0.5, and ends with fully-connected layer with 5 output channels. The stereo images were represented as a two-channel input image when fed to the network. During training we have used data augmentation consisting of randomly scaling and rotation transforms. The networks were trained for 40,000 iterations using SGD with 0.99 momentum and 0.001 base learning rate, which remained constant for 30,000 iterations, followed by a linear decrease to 0.0001 for 6000 iterations, followed by a linear decrease to 0 learning rate for the remaining 4,000 iterations. The model also used 0.0001 weight decay for additional regularization.\nWhen ConvNets were trained on images containing missing values, we passed the network the original image with missing values zeroed out, and an additional binary image as a separate channel, containing 1 for missing values at the same spatial position, and 0 otherwise \u2013 this missing data format is sometimes known as flag data imputation. Other formats for representing missing values were tested (e.g. just using zeros for missing values), however, the above scheme performed significantly better than other formats. In our experiments, we assumed that the training set was complete and missing values were only present in the test set. In order to design ConvNets that are robust against specific missingness distributions, we have simulated missing values during training, sampling a different mask of missing values for each image in each mini-batch. As covered in sec. 6, the results of training ConvNets directly on simulated missingness distributions resulted in classifiers\nwhich were biased towards the specific distribution used in training, and performed worse on other distributions compared to ConvNets trained on the same distribution.\nIn addition to training ConvNets directly on missing data, we have also used them as the classifier for testing different data imputation methods, as describe in the next section.\nF.1.4 CLASSIFICATION THROUGH DATA IMPUTATION\nThe most common method for handling missing data, while leveraging available discriminative classifiers, is through the application of data imputation \u2013 an algorithm for the completion of missing values \u2013 and then passing the results to a classifier trained on uncorrupted dataset. We have tested five different types of data imputation algorithms:\n\u2022 Zero data imputation: replacing every missing value by zero. \u2022 Mean data imputation: replacing every missing value by the mean value computed over the dataset. \u2022 Generative data imputation: training a generative model and using it to complete the missing values\nby finding the most likely instance that coincides with the observed values, i.e. solving the following\ng(x m) = argmax x\u2032\u2208Rs\u2227\u2200i,mi=1\u2192x\u2032i=xi\nP (X = x\u2032)\nWe have tested the following generative models:\n\u2013 Generative Stochastic Networks (GSN) (Bengio et al., 2014): We have used their original source code from https://github.com/yaoli/GSN, and trained their example model on MNIST for 1000 epochs. Whereas in the original article they have tested completing only the left or right side of a given image, we have modified their code to support general masks. Our modified implementation can be found at https://github.com/HUJI-Deep/GSN.\n\u2013 Non-linear Independent Components Estimation (NICE) (Dinh et al., 2014): We have used their original source code from https://github.com/laurent-dinh/nice, and trained it on MNIST using their example code without changes. Similarly to our modification to the GSN code, here too we have adapted their code to support general masks over the input. Additionally, their original inpainting code required 110,000 iterations, which we have reduced to just 8,000 iterations, since the effect on classification accuracy was marginal. For the NORB dataset, we have used their CIFAR10 example, with lower learning rate of 10\u22124. Our modified code can be found at https://github.com/HUJI-Deep/nice.\n\u2013 Diffusion Probabilistic Models (DPM) (Sohl-Dickstein et al., 2015): We have user their original source code from https://github.com/Sohl-Dickstein/ Diffusion-Probabilistic-Models, and trained it on MNIST using their example code without changes. Similarly to our modifications to GSN, we have add support for a general mask of missing values, but other than that kept the rest of the parameters for inpainting unchanged. For NORB we have used the same model as MNIST. We have tried using their CIFAR10 example for NORB, however, it produced exceptions during training. Our modified code can be found at https://github.com/HUJI-Deep/Diffusion-Probabilistic-Models.\nF.1.5 TENSORIAL MIXTURE MODELS\nFor a complete theoretical description of our model please see the body of the article. Our models were implemented by performing all intermediate computations in log-space, using numerically aware operations. In practiced, that meant our models were realized by the SimNets architecture (Cohen and Shashua, 2014; Cohen et al., 2016b), which consists of Similarity layers representing gaussian distributions, MEX layers representing weighted sums performed on log-space input and outputs, as well as standard pooling operations. The learned parameters of the MEX layers are called offsets, which represents the weights of the weighted sum, but saved in log-space. The parameters of the MEX layers can be optionally shared between spatial regions, or alternatively left with no parameter sharing at all. Additionally, when used to implement our generative models, the offsets are normalized to have a soft-max (i.e., log (\u2211 i exp(xi) ) ) of zero.\nThe network architectures we have tested in this article, consists of M different Gaussian mixture components with diagonal covariance matrices, over non-overlapping patches of the input of size 2\u00d7 2, which were implemented by a similarity layer as specified by the SimNets architecture, but with an added gaussian normalization term.\nWe first describe the architectures used for the MNIST dataset. For the GCP-model, we used M = 800, and following the similarity layer is a 1 \u00d7 1 MEX layer with no parameter sharing over spatial regions and 10 output channels. The model ends with a global sum pooling operation, followed by another 1 \u00d7 1 MEX layer\nwith 10 outputs, one for each class. The GHT-model starts with the similarity layer with M = 32, followed by a sequence of four pairs of 1 \u00d7 1 MEX layer followed by 2 \u00d7 2 sum pooling layer, and after the pairs and additional 1 \u00d7 1 MEX layer lowering the outputs of the model to 10 outputs as the number of classes. The number of output channels for each MEX layer are as follows 64-128-256-512-10. All the MEX layers in this network do not use parameter sharing, except the first MEX layer, which uses a repeated sharing pattern of 2 \u00d7 2 offsets, that analogous to a 2 \u00d7 2 convolution layer with stride 2. Both models were trained with the losses described in sec. 4, using the Adam SGD variant for optimizing the parameters, with a base learning rate of 0.03, and \u03b21 = \u03b22 = 0.9. The models were trained for 25,000 iterations, where the learning rate was dropped by 0.1 after 20,000 iterations.\nFor the NORB dataset, we have trained only the GHT-model with M = 128 for the similarity layer. The MEX layers use the same parameter sharing scheme as the one for MNIST, and the number of output channels for each MEX layer are as follows: 256-256-256-512-5. Training was identical to the MNIST models, with the exception of using 40,000 iterations instead of just 25,000. Additionally, we have used an ensemble of 4 models trained separately, each trained using a different generative loss weight (see below for more information). We have also used the same data augmentation methods (scaling and rotation) which were used in training the ConvNets for NORB used in this article.\nThe standard L2 weight regularization (sometimes known as weight decay) did not work well on our models, which lead us to adapt it to better fit to log-space weights, by minimizing \u03bb \u2211 i (exp (xi))\n2 instead of \u03bb||x||2 = \u03bb \u2211 i x 2 i , where the parameter \u03bb was chosen through cross-validation. Additionally, since even with large values of \u03bb our model was still overfitting, we have added another form of regularization in the form of random marginalization layers. A random marginalization layer, is similar in concept to dropout, but instead of zeroing activations completely in random, it choses spatial locations at random, and then zero out the activations at those locations for all the channels. Under our model, zeroing all the activations in a layer at a specific location, is equivalent to marginalizing over all the inputs for the receptive field for that respective location. We have used random marginalization layers in between all our layers during training, where the probability for zeroing out activations was chosen through cross-validation for each layer separately. Though it might raise concern that random marginalization layers could lead to biased results toward the missingness distributions we have tested it on, in practice the addition of those layers only helped improve our results under cases where only few pixels where missing.\nFinally, we wish to discuss a few optimization tricks which had a minor effects compared to the above, but were nevertheless very useful in achieving slightly better results. First, instead of optimizing directly the objective defined by eq. 4, we add smoothing parameter \u03b2 between the two terms, as follows:\n\u0398\u2217 = argmin \u0398 \u2212 |S|\u2211 i=1 log eN\u0398(X (i);Y (i))\u2211K y=1 e N\u0398(X (i);y) \u2212 \u03b2 |S|\u2211 i=1 log K\u2211 y=1 eN\u0398(X (i);y)\nsetting \u03b2 too low diminish the generative capabilities of our models, while setting it too high diminish the discriminative performance. Through cross-validation, we decided on the value \u03b2 = 0.01 for the models trained on MNIST, while for NORB we have used a different value of \u03b2 for each of the models, ranging in {0.01, 0.1, 0.5, 1}. Second, we found that performance increased if we normalized activations before applying the 1 \u00d7 1 MEX operations. Specifically, we calculate the soft-max over the channels for each spatial location which we call the activation norm, and then subtract it from every respective activation. After applying the MEX operation, we add back the activation norm. Though might not be obvious at first, subtracting a constant from the input of a MEX operation and adding it to its output is equivalent does not change the mathematical operation. However, it does resolve the numerical issue of adding very large activations to very small offsets, which might result in a loss of precision. Finally, we are applying our model in different translations of the input and then average the class predictions. Since our model can marginalize over inputs, we do not need to crop the original image, and instead mask the unknown parts after translation as missing. Applying a similar trick to standard ConvNets on MNIST does not seem to improve their results. We believe this method is especially fit to our model, is because it does not have a natural treatment of overlapping patches like ConvNets do, and because it is able to marginalize over missing pixels easily, not limiting it just to crop translation as is typically done.\nF.2 DESCRIPTION OF EXPERIMENTS\nIn this section we will give a detailed description of the protocol we have used during our experiments.\nF.2.1 BINARY DIGIT CLASSIFICATION WITH FEATURE DELETION MISSING DATA\nThis experiment focuses on the binary classification problem derived from MNIST, by limiting the number of classes to two different digits at a time. We use the same non-zero feature deletion distribution as suggested by Globerson and Roweis (2006), i.e. for a given image we uniformly sample a set of N non-zero pixels from the\nimage (if the image has less than N non-zero pixels then they are non-zero pixels are chosen), and replace their values with zeros. This type of missingness distribution falls under the MNAR type defined in sec.5.\nWe test values of N in {0, 25, 50, 75, 100, 125, 150}. For a given value of N , we train a separate classifier on each digit pair classifier on a randomly picked subset of the dataset containing 300 images per digit (600 total). During training we use a fixed validation set with 1000 images per digit. After picking the best classifier according to the validation set, the classifier is tested against a test set with a 1000 images per digits with a randomly chosen missing values according to the value of N . This experiment is repeated 10 times for each digit pair, each time using a different subset for the training set, and a new corrupted test set. After conducting all the different experiments, all the accuracies are averaged for each value of N , which are reported in table 1.\nF.2.2 MULTI-CLASS DIGIT CLASSIFICATION WITH MAR MISSING DATA\nThis experiment focuses on the complete multi-class digit classification of the MNIST dataset, in the presence of missing data according to different missingness distributions. Under this setting, only the test set contains missing values, whereas the training set does not. We test two kinds of missingness distributions, which both fall under the MAR type defined in sec.5. The first kind, which we call i.i.d. corruption, each pixel is missing with a fixed probability p. the second kind, which we call missing rectangles corruption, The positions of N rectangles of widthW or chosen uniformly in the picture, where the rectangles can overlap one another. During the training stage, the models to be tested are not to be biased toward the specific missingness distributions we have chosen, and during the test stage, the same classifier is tested against all types of missingness distributions, and without supplying it with the parameters or type of the missingness distribution it is tested against. This rule prevent the use of ConvNets trained on simulated missingness distributions. To demonstrate that the latter lead to biased classifiers, we have conducted a separate experiment just for ConvNets, where the previous rule is ignored, and we train a separate ConvNet classifier on each type and parameter of the missingness distributions we have used. We then tested each of those ConvNets on all other missingness distributions, the results of which are in fig. 5, which confirmed our hypothesis.\nG IMAGE GENERATION AND NETWORK VISUALIZATION\nFollowing the graphical model perspective of our models allows us to not only generate random instances from the distribution, but to also generate the most likely patches for each neuron in the network, effectively explaining its role in the classification process. We remind the reader that every neuron in the network corresponds to a possible assignment of a latent variable in the graphical model. By looking for the most likely assignments for each of its child nodes in the graphical tree model, we can generate a patch that describes that neuron. Unlike similar suggested methods to visualize neural networks (Zeiler and Fergus, 2014), often relying on brute-force search or on solving some optimization problem to find the most likely image, our method emerges naturally from the probabilistic interpretation of our model.\nIn fig. 8, we can see conditional samples generates for each digit, while in fig. 9 we can see a visualization of the top-level layers of network, where each small patch matches a different neuron in the network. The common wisdom of how ConvNets work is by assuming that simple low-level features are composed together to create more and more complex features, where each subsequent layer denotes features of higher abstraction \u2013 the visualization of our network clearly demonstrate this hypothesis to be true for our case, showing small strokes iteratively being composed into complete digits.\nH RAW RESULTS OF EXPERIMENTS\nFor both presentational and page layout reasons we have chosen to present most of results in the form of charts in the body or the article. Considering that exact results are important for both reproducibility as well as future comparisons to our work, we provide below the raw results of our experiments in the form of detailed tables. For completeness, some of the tables we did include in the body of the article are duplicated to here as well.\n", "cats": {"READ": 1.0, "DONTREAD": 0.0}, "reviews": [{"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "The authors have recently made several connections between deep learning and tensor algebra. While their earlier works dealt with supervised learning, the current work analyzes generative models through the lens of tensor algebra. \n The authors show propose a tensorial mixture model over local structures where the mixture components are expressed as tensor decompositions. They show that hierarchical tensor decomposition is exponentially more expressive compared to the shallow models. \n The paper makes original contributions in terms of establishing expressivity of deep generative models. The connections with tensor algebra could lead to further innovations, e.g. in training algorithms.\n However, the paper can be improved in two aspects: \n (1) It will be nice if the authors make a connection between the algebraic view presented here with the geometric view presented by: ", "OTHER_KEYS": "ICLR 2017 pcs"}, {"TITLE": "Interesting approach but ...", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "This paper proposes a generative model for mixtures of basic local structures where the dependency between local structures is a tensor. They use tensor decomposition and the result of their earlier paper on expressive power of CNNs along with hierarchical Tucker to provide an inference mechanism. However, this is conditioned on the existence of decomposition. The authors do not discuss how applicable their method is for a general case, what is the subspace where this decomposition exists/is efficient/has low approximation error. Their answer to this question is that in deep learning era these theoretical analysis is not needed. While this claim is subjective, I need to emphasize that the paper does not clarify this claim and does not mention the restrictions. Hence, from theoretical perspective, the paper has flaws and the claims are not justified completely. Some claims cannot be justified with the  current results in tensor literature as the authors also mentioned in the discussions. Therefore, they should have corrected their claims in the paper and made the clarifications that this approach is restricted to a clear subclass of tensors.\n\nIf we ignore the theoretical aspect and only consider the paper from empirical perspective, the experiments the appear in the paper are not enough to accept the paper. MNIST and CIFAR-10 are very simple baselines and more extensive experiments are required. Also, the experiments for missing data are not covering real cases and are too synthetic. Also, the paper lacks the extension beyond images. Since the authors repeatedly mention that their approach goes beyond images, and since the theory part is not complete, those experiments are essential for acceptance of this paper.\n\n", "IS_META_REVIEW": false, "RECOMMENDATION": 4, "DATE": "21 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "Interesting approach to generative models and missing data", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "This paper uses Tensors to build generative models.  The main idea is to divide the input into regions represented with mixture models, and represent the joint distribution of the mixture components with a tensor.  Then, by restricting themselves to tensors that have an efficient decomposition, they train convolutional arithmetic circuits to generate the probability of the input and class label, providing a generative model of the input and labels.  \n\nThis approach seems quite elegant.  It is not completely clear to me how the authors choose the specific architecture for their model, and how these choices relate to the class of joint distributions that they can represent, but even if these choices are somewhat heuristic, the overall framework provides a nice way of controlling the generality of the distributions that are represented.\n\nThe experiments are on simple, synthetic examples of missing data.  This is somewhat of a limitation, and the paper would be more convincing if it could include experiments on a real-world problem that contained missing data.  One issue here is that it must be known which elements of the input are missing, which somewhat limits applicability.  Could experiments be run on problems relating to the Netflix challenge, which is the classic example of a prediction problem with missing data?  In spite of these limitations, the experiments provide appropriate comparisons to prior work, and form a reasonable initial evaluation.\n\nI was a little confused about how the input of missing data is handled experimentally.  From the introductory discussion my impression was that the generative model was built over region patches in the image.  This led me to believe that they would marginalize over missing regions.  However, when the missing data consists of IID randomly missing pixels, it seems that every region will be missing some information.  Why is it appropriate to marginalize over missing pixels?  Specifically, $x_i$ in Equation 6 represents a local region, and the ensuing discussion shows how to marginalize over missing regions.  How is this done when only a subset of a region is missing?  It also seems like the summation in the equation following Equation 6 could be quite large.  What is the run time of this? \n\nThe paper is also a bit schizophrenic about the extent to which the results are applicable beyond images.  The motivation for the probabilistic model is mostly in terms of images.  But in the experiments, the authors state that they do not use state-of-the-art inpainting algorithms because their method is not limited to images and they want to compare to methods that are restricted to images.  This would be more convincing if there were experiments outside the image domain.\n\nIt was also not clear to me how, if at all, the proposed network makes use of translation invariance.  It is widely assumed that much of the success of CNNs comes from their encoding of translation invariance through weight sharing.   Is such invariance built into the authors\u2019 network?  If not, why would we expect it to work well in challenging image domains?\n\nAs a minor point, the paper is not carefully proofread.  To just give a few examples from the first page or so:\n\n\u201csignificantly lesser\u201d -> \u201csignificantly less\u201d\n\n\u201cthe the\u201d\n\n\u201cprovenly\u201d -> provably\n", "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 3}, {"TITLE": "The ideas are brilliant, but technical typos exist", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer4", "comments": "The paper provides an interesting use of generative models to address the classification with missing data problem. The tensorial mixture models proposed take into account the general problem of dependent samples. This is an nice extension of current mixture models where samples are usually considered as independent. Indeed the TMM model is reduced to the conventional latent variable models. As much as I love the ideas behind the paper, I feel pitiful about the sloppiness of the presentation (such as missing notations) and flaws in the technical derivations.  Before going into the technical details, my high level concerns are as follows:\n(1) The joint density over all samples is modeled as a tensorial mixture generative model. The interpretation of the CP decomposition or HT decomposition on the prior density tensor is not clear. The authors have an interpretation of TMM as product of mixture models when samples are independent, however their interpretation seems flawed to me, and I will elaborate on this in the detailed technical comments below. \n(2) The authors employ convolution operators to compute an inner product. It is realizable by zero padding, but the invariance structure, which is the advantage of CNN compared to feed-forward neural network, will be lost. However, I am not sure how much this would affect the performance in practice. \n(3) The author could comment in the paper a little bit on the sample complexity of this method given the complexity of the model. \n\nBecause I liked the ideas of the paper so much, and the ICLR paper submitted didn't present the technical details well due to sloppiness of notations, so I read the technical details in the arXiv version the authors pointed out. There are a few technical typos that I would like to point out (my reference to equations are to the ones in the arXiv paper). \n(1) The generative model as in figure (5) is flawed. P(x_i|d_i;\\theta_{d_i}) are vectors of length s, there the product of vectors is not well defined. It is obvious that the dimensions of the terms between two sides of the equation are not equal. In fact, this should be a tucker decomposition instead of multiplication. It should be P(X) = \\sum_{d1,\\ldots,d_N} P(d_1,\\ldots,d_N) (P(x_1|d_1;theta_{d_1},P(x_2|d_2;theta_{d_2},\\ldots,P(x_N|d_N;theta_{d_N}), which means a sum of multi-linear operation on tensor P(d_1,\\ldots,d_N), and each mode is projected onto P(x_i|d_i;theta_{d_i}. \n(2) I suspect the special case for diagonal Gaussian Mixture Models has some typos as I couldn't derive the third last equation on page 6. But it might be just I didn't understand this example. \n(3) The claim that TMM reduces to product of mixture model is not accurate. The first equation on page 7 is only right when \"sum of product\" operation is equal to \"product of sum\" operation. Similarly, in equation (6), the second equality doesn't hold unless in some special cases. However, this is not true. This might be just a typo, but it is good if the authors could fix this. I also suspect that if the authors correct this typo,the performance on MNIST might be improved.\n\nOverall, I like the ideas behind this paper very much. I suggest the authors fix the technical typos if the paper is accepted. ", "IS_META_REVIEW": false, "RECOMMENDATION": 5, "DATE": "15 Dec 2016", "REVIEWER_CONFIDENCE": 3}, {"DATE": "09 Dec 2016", "TITLE": "Missing convergence and error analysis", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3"}, {"DATE": "02 Dec 2016", "TITLE": "Exact equation explaining CP decomposition with a ConvAC", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer4"}, {"DATE": "30 Nov 2016", "TITLE": "Parameters and representation", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1"}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "The authors have recently made several connections between deep learning and tensor algebra. While their earlier works dealt with supervised learning, the current work analyzes generative models through the lens of tensor algebra. \n The authors show propose a tensorial mixture model over local structures where the mixture components are expressed as tensor decompositions. They show that hierarchical tensor decomposition is exponentially more expressive compared to the shallow models. \n The paper makes original contributions in terms of establishing expressivity of deep generative models. The connections with tensor algebra could lead to further innovations, e.g. in training algorithms.\n However, the paper can be improved in two aspects: \n (1) It will be nice if the authors make a connection between the algebraic view presented here with the geometric view presented by: ", "OTHER_KEYS": "ICLR 2017 pcs"}, {"TITLE": "Interesting approach but ...", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "This paper proposes a generative model for mixtures of basic local structures where the dependency between local structures is a tensor. They use tensor decomposition and the result of their earlier paper on expressive power of CNNs along with hierarchical Tucker to provide an inference mechanism. However, this is conditioned on the existence of decomposition. The authors do not discuss how applicable their method is for a general case, what is the subspace where this decomposition exists/is efficient/has low approximation error. Their answer to this question is that in deep learning era these theoretical analysis is not needed. While this claim is subjective, I need to emphasize that the paper does not clarify this claim and does not mention the restrictions. Hence, from theoretical perspective, the paper has flaws and the claims are not justified completely. Some claims cannot be justified with the  current results in tensor literature as the authors also mentioned in the discussions. Therefore, they should have corrected their claims in the paper and made the clarifications that this approach is restricted to a clear subclass of tensors.\n\nIf we ignore the theoretical aspect and only consider the paper from empirical perspective, the experiments the appear in the paper are not enough to accept the paper. MNIST and CIFAR-10 are very simple baselines and more extensive experiments are required. Also, the experiments for missing data are not covering real cases and are too synthetic. Also, the paper lacks the extension beyond images. Since the authors repeatedly mention that their approach goes beyond images, and since the theory part is not complete, those experiments are essential for acceptance of this paper.\n\n", "IS_META_REVIEW": false, "RECOMMENDATION": 4, "DATE": "21 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "Interesting approach to generative models and missing data", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "This paper uses Tensors to build generative models.  The main idea is to divide the input into regions represented with mixture models, and represent the joint distribution of the mixture components with a tensor.  Then, by restricting themselves to tensors that have an efficient decomposition, they train convolutional arithmetic circuits to generate the probability of the input and class label, providing a generative model of the input and labels.  \n\nThis approach seems quite elegant.  It is not completely clear to me how the authors choose the specific architecture for their model, and how these choices relate to the class of joint distributions that they can represent, but even if these choices are somewhat heuristic, the overall framework provides a nice way of controlling the generality of the distributions that are represented.\n\nThe experiments are on simple, synthetic examples of missing data.  This is somewhat of a limitation, and the paper would be more convincing if it could include experiments on a real-world problem that contained missing data.  One issue here is that it must be known which elements of the input are missing, which somewhat limits applicability.  Could experiments be run on problems relating to the Netflix challenge, which is the classic example of a prediction problem with missing data?  In spite of these limitations, the experiments provide appropriate comparisons to prior work, and form a reasonable initial evaluation.\n\nI was a little confused about how the input of missing data is handled experimentally.  From the introductory discussion my impression was that the generative model was built over region patches in the image.  This led me to believe that they would marginalize over missing regions.  However, when the missing data consists of IID randomly missing pixels, it seems that every region will be missing some information.  Why is it appropriate to marginalize over missing pixels?  Specifically, $x_i$ in Equation 6 represents a local region, and the ensuing discussion shows how to marginalize over missing regions.  How is this done when only a subset of a region is missing?  It also seems like the summation in the equation following Equation 6 could be quite large.  What is the run time of this? \n\nThe paper is also a bit schizophrenic about the extent to which the results are applicable beyond images.  The motivation for the probabilistic model is mostly in terms of images.  But in the experiments, the authors state that they do not use state-of-the-art inpainting algorithms because their method is not limited to images and they want to compare to methods that are restricted to images.  This would be more convincing if there were experiments outside the image domain.\n\nIt was also not clear to me how, if at all, the proposed network makes use of translation invariance.  It is widely assumed that much of the success of CNNs comes from their encoding of translation invariance through weight sharing.   Is such invariance built into the authors\u2019 network?  If not, why would we expect it to work well in challenging image domains?\n\nAs a minor point, the paper is not carefully proofread.  To just give a few examples from the first page or so:\n\n\u201csignificantly lesser\u201d -> \u201csignificantly less\u201d\n\n\u201cthe the\u201d\n\n\u201cprovenly\u201d -> provably\n", "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 3}, {"TITLE": "The ideas are brilliant, but technical typos exist", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer4", "comments": "The paper provides an interesting use of generative models to address the classification with missing data problem. The tensorial mixture models proposed take into account the general problem of dependent samples. This is an nice extension of current mixture models where samples are usually considered as independent. Indeed the TMM model is reduced to the conventional latent variable models. As much as I love the ideas behind the paper, I feel pitiful about the sloppiness of the presentation (such as missing notations) and flaws in the technical derivations.  Before going into the technical details, my high level concerns are as follows:\n(1) The joint density over all samples is modeled as a tensorial mixture generative model. The interpretation of the CP decomposition or HT decomposition on the prior density tensor is not clear. The authors have an interpretation of TMM as product of mixture models when samples are independent, however their interpretation seems flawed to me, and I will elaborate on this in the detailed technical comments below. \n(2) The authors employ convolution operators to compute an inner product. It is realizable by zero padding, but the invariance structure, which is the advantage of CNN compared to feed-forward neural network, will be lost. However, I am not sure how much this would affect the performance in practice. \n(3) The author could comment in the paper a little bit on the sample complexity of this method given the complexity of the model. \n\nBecause I liked the ideas of the paper so much, and the ICLR paper submitted didn't present the technical details well due to sloppiness of notations, so I read the technical details in the arXiv version the authors pointed out. There are a few technical typos that I would like to point out (my reference to equations are to the ones in the arXiv paper). \n(1) The generative model as in figure (5) is flawed. P(x_i|d_i;\\theta_{d_i}) are vectors of length s, there the product of vectors is not well defined. It is obvious that the dimensions of the terms between two sides of the equation are not equal. In fact, this should be a tucker decomposition instead of multiplication. It should be P(X) = \\sum_{d1,\\ldots,d_N} P(d_1,\\ldots,d_N) (P(x_1|d_1;theta_{d_1},P(x_2|d_2;theta_{d_2},\\ldots,P(x_N|d_N;theta_{d_N}), which means a sum of multi-linear operation on tensor P(d_1,\\ldots,d_N), and each mode is projected onto P(x_i|d_i;theta_{d_i}. \n(2) I suspect the special case for diagonal Gaussian Mixture Models has some typos as I couldn't derive the third last equation on page 6. But it might be just I didn't understand this example. \n(3) The claim that TMM reduces to product of mixture model is not accurate. The first equation on page 7 is only right when \"sum of product\" operation is equal to \"product of sum\" operation. Similarly, in equation (6), the second equality doesn't hold unless in some special cases. However, this is not true. This might be just a typo, but it is good if the authors could fix this. I also suspect that if the authors correct this typo,the performance on MNIST might be improved.\n\nOverall, I like the ideas behind this paper very much. I suggest the authors fix the technical typos if the paper is accepted. ", "IS_META_REVIEW": false, "RECOMMENDATION": 5, "DATE": "15 Dec 2016", "REVIEWER_CONFIDENCE": 3}, {"DATE": "09 Dec 2016", "TITLE": "Missing convergence and error analysis", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3"}, {"DATE": "02 Dec 2016", "TITLE": "Exact equation explaining CP decomposition with a ConvAC", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer4"}, {"DATE": "30 Nov 2016", "TITLE": "Parameters and representation", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1"}]}
{"text": "1 INTRODUCTION AND MOTIVATION\nTo go beyond the relatively simpler tasks of classification and regression, advancing our ability to learn good generative models of high-dimensional data appears essential. There are many scenarios where one needs to efficiently produce good high-dimensional outputs where output dimensions have unknown intricate statistical dependencies: from generating realistic images, segmentations, text, speech, keypoint or joint positions, etc..., possibly as an answer to the same, other, or multiple input modalities. These are typically cases where there is not just one right answer but a variety of equally valid ones following a non-trivial and unknown distribution. A fundamental ingredient for such scenarios is thus the ability to learn a good generative model from data, one from which we can subsequently efficiently generate varied samples of high quality.\nMany approaches for learning to generate high dimensional samples have been and are still actively being investigated. These approaches can be roughly classified under the following broad categories:\n\u2022 Ordered visible dimension sampling (van den Oord et al., 2016; Larochelle & Murray, 2011). In this type of auto-regressive approach, output dimensions (or groups of conditionally independent dimensions) are given an arbitrary fixed ordering, and each is sampled conditionally on the previous sampled ones. This strategy is often implemented using a recurrent network (LSTM or GRU). Desirable properties of this type of strategy are that the exact log likelihood can usually be computed tractably, and sampling is exact. Undesirable properties follow from the forced ordering, whose arbitrariness feels unsatisfactory especially for domains that do not have a natural ordering (e.g. images), and imposes for high-dimensional output a long sequential generation that can be slow. \u2022 Undirected graphical models with multiple layers of latent variables. These make inference, and thus learning, particularly hard and tend to be costly to sample from (Salakhutdinov & Hinton, 2009). \u2022 Directed graphical models trained as variational autoencoders (VAE) (Kingma & Welling, 2014; Rezende et al., 2014)\n\u2217Associate Fellow, Canadian Institute For Advanced Research (CIFAR)\n\u2022 Adversarially-trained generative networks. (GAN)(Goodfellow et al., 2014) \u2022 Stochastic neural networks, i.e. networks with stochastic neurons, trained by an adapted\nform of stochastic backpropagation \u2022 Generative uses of denoising autoencoders (Vincent et al., 2010) and their generalization\nas Generative Stochastic Networks (Alain et al., 2016) \u2022 Inverting a non-equilibrium thermodynamic slow diffusion process (Sohl-Dickstein et al.,\n2015) \u2022 Continuous transformation of a distribution by invertible functions (Dinh et al. (2014), also\nused for variational inference in Rezende & Mohamed (2015))\nSeveral of these approaches are based on maximizing an explicit or implicit model log-likelihood or a lower bound of its log-likelihood, but some successful ones are not e.g. GANs. The approach we propose here is based on the notion of \u201cdenoising\u201d and thus takes its root in denoising autoencoders and the GSN type of approaches. It is also highly related to the non-equilibrium thermodynamics inverse diffusion approach of Sohl-Dickstein et al. (2015). One key aspect that distinguishes these types of methods from others listed above is that sample generation is achieved thanks to a learned stochastic mapping from input space to input space, rather than from a latent-space to input-space.\nSpecifically, in the present work, we propose to learn to generate high quality samples through a process of progressive, stochastic, denoising, starting from a simple initial \u201cnoise\u201d sample generated in input space from a simple factorial distribution i.e. one that does not take into account any dependency or structure between dimensions. This, in effect, amounts to learning the transition operator of a Markov chain operating on input space. Starting from such an initial \u201cnoise\u201d input, and repeatedly applying the operator for a small fixed number T of steps, we aim to obtain a high quality resulting sample, effectively modeling the training data distribution. Our training procedure uses a novel \u201ctarget-infusion\u201d technique, designed to slightly bias model sampling to move towards a specific data point during training, and thus provide inputs to denoise which are likely under the model\u2019s sample generation paths. By contrast with Sohl-Dickstein et al. (2015) which consists in inverting a slow and fixed diffusion process, our infusion chains make a few large jumps and follow the model distribution as the learning progresses.\nThe rest of this paper is structured as follows: Section 2 formally defines the model and training procedure. Section 3 discusses and contrasts our approach with the most related methods from the literature. Section 4 presents experiments that validate the approach. Section 5 concludes and proposes future work directions.\n2 PROPOSED APPROACH\n2.1 SETUP\nWe are given a finite data set D containing n points in Rd, supposed drawn i.i.d from an unknown distribution q\u2217. The data set D is supposed split into training, validation and test subsets Dtrain, Dvalid, Dtest. We will denote q\u2217train the empirical distribution associated to the training set, and use x to denote observed samples from the data set. We are interested in learning the parameters of a generative model p conceived as a Markov Chain from which we can efficiently sample. Note that we are interested in learning an operator that will display fast \u201cburn-in\u201d from the initial factorial \u201cnoise\u201d distribution, but beyond the initial T steps we are not concerned about potential slow mixing or being stuck. We will first describe the sampling procedure used to sample from a trained model, before explaining our training procedure.\n2.2 GENERATIVE MODEL SAMPLING PROCEDURE\nThe generative model p is defined as the following sampling procedure:\n\u2022 Using a simple factorial distribution p(0)(z(0)), draw an initial sample z(0) \u223c p(0), where z(0) \u2208 Rd. Since p(0) is factorial, the d components of z(0) are independent: p0 cannot model any dependency structure. z(0) can be pictured as essentially unstructured random noise. \u2022 Repeatedly apply T times a stochastic transition operator p(t)(z(t)|z(t\u22121)), yielding a more \u201cdenoised\u201d sample z(t) \u223c p(t)(z(t)|z(t\u22121)), where all z(t) \u2208 Rd.\n\u2022 Output z(T ) as the final generated sample. Our generative model distribution is thus p(z(T )), the marginal associated to joint p(z(0), . . . , z(T )) = p(0)(z(0)) (\u220fT t=1 p (t)(z(t)|z(t\u22121)) ) .\nIn summary, samples from model p are generated, starting with an initial sample from a simple distribution p(0), by taking the T thsample along Markov chain z(0) \u2192 z(1) \u2192 z(2) \u2192 . . . \u2192 z(T ) whose transition operator is p(t)(z(t)|z(t\u22121)). We will call this chain the model sampling chain. Figure 1 illustrates this sampling procedure using a model (i.e. transition operator) that was trained on MNIST. Note that we impose no formal requirement that the chain converges to a stationary distribution, as we simply read-out z(T ) as the samples from our model p. The chain also needs not be time-homogeneous, as highlighted by notation p(t) for the transitions.\nThe set of parameters \u03b8 of model p comprise the parameters of p(0) and the parameters of transition operator p(t)(z(t)|z(t\u22121)). For tractability, learnability, and efficient sampling, these distributions will be chosen factorial, i.e. p(0)(z(0)) = \u220fd i=1 p (0) i (z (0) i ) and p\n(t)(z(t)|z(t\u22121)) =\u220fd i=1 p (t) i (z (t) i |z(t\u22121)). Note that the conditional distribution of an individual component i, p (t) i (z (t) i |z(t\u22121)) may however be multimodal, e.g. a mixture in which case p(t)(z(t)|z(t\u22121)) would be a product of independent mixtures (conditioned on z(t\u22121)), one per dimension. In our experiments, we will take the p(t)(z(t)|z(t\u22121)) to be simple diagonal Gaussian yielding a Deep Latent Gaussian Model (DLGM) as in Rezende et al. (2014).\n2.3 INFUSION TRAINING PROCEDURE\nWe want to train the parameters of model p such that samples from Dtrain are likely of being generated under the model sampling chain. Let \u03b8(0) be the parameters of p(0) and let \u03b8(t) be the parameters of p(t)(z(t)|z(t\u22121)). Note that parameters \u03b8(t) for t > 0 can straightforwardly be shared across time steps, which we will be doing in practice. Having committed to using (conditionally) factorial distributions for our p(0)(z(0)) and p(t)(z(t)|z(t\u22121)), that are both easy to learn and cheap to sample from, let us first consider the following greedy stagewise procedure. We can easily learn p(0)i (z\n(0)) to model the marginal distribution of each component xi of the input, by training it by gradient descent on a maximum likelihood objective, i.e.\n\u03b8(0) = argmax \u03b8 Ex\u223cq\u2217train [ log p(0)(x; \u03b8) ] (1)\nThis gives us a first, very crude unstructured (factorial) model of q\u2217.\nHaving learned this p(0), we might be tempted to then greedily learn the next stage p(1) of the chain in a similar fashion, after drawing samples z(0) \u223c p(0) in an attempt to learn to \u201cdenoise\u201d the sampled z(0) into x. Yet the corresponding following training objective \u03b8(1) = argmax\u03b8 Ex\u223cq\u2217train,z(0)\u223cp(0) [ log p(1)(x|z(0); \u03b8) ] makes no sense: x and z(0) are sampled independently of each other so z(0) contains no information about x, hence p(1)(x|z(0)) = p(1)(x). So maximizing this second objective becomes essentially the same as what we did when learning p(0). We would learn nothing more. It is essential, if we hope to learn a useful conditional distribution p(1)(x|z(0)) that it be trained on particular z(0) containing some information about x. In other words, we should not take our training inputs to be samples from p(0) but from a slightly different distribution, biased towards containing some information about x. Let us call it q(0)(z(0)|x). A natural choice for it, if it were possible, would be to take q(0)(z(0)|x) = p(z(0)|z(T ) = x) but this is an intractable inference, as all intermediate z(t) between z(0) and z(T ) are effectively latent states that we would need to marginalize over. Using a workaround such as a variational or MCMC approach would be a usual fallback. Instead, let us focus on our initial intent of guiding a progressive stochastic denoising, and think if we can come up with a different way to construct q(0)(z(0)|x) and similarly for the next steps q(t)i (z\u0303 (t) i |z\u0303(t\u22121),x).\nEventually, we expect a sequence of samples from Markov chain p to move from initial \u201cnoise\u201d towards a specific example x from the training set rather than another one, primarily if a sample along the chain \u201cresembles\u201d x to some degree. This means that the transition operator should learn to pick up a minor resemblance with an x in order to transition to something likely to be even more similar to x. In other words, we expect samples along a chain leading to x to both have high probability under the transition operator of the chain p(t)(z(t)|z(t\u22121)), and to have some form of at least partial \u201cresemblance\u201d with x likely to increase as we progress along the chain. One highly inefficient way to emulate such a chain of samples would be, for teach step t, to sample many candidate samples from the transition operator (a conditionally factorial distribution) until we generate one that has some minimal \u201cresemblance\u201d to x (e.g. for a discrete space, this resemblance measure could be based on their Hamming distance). A qualitatively similar result can be obtained at a negligible cost by sampling from a factorial distribution that is very close to the one given by the transition operator, but very slightly biased towards producing something closer to x. Specifically, we can \u201cinfuse\u201d a little of x into our sample by choosing for each input dimension, whether we sample it from the distribution given for that dimension by the transition operator, or whether, with a small probability, we take the value of that dimension from x. Samples from this biased chain, in which we slightly \u201cinfuse\u201d x, will provide us with the inputs of our input-target training pairs for the transition operator. The target part of the training pairs is simply x.\n2.3.1 THE INFUSION CHAIN\nFormally we define an infusion chain z\u0303(0) \u2192 z\u0303(1) \u2192 . . . \u2192 z\u0303(T\u22121) whose distribution q(z\u0303(0), . . . , z\u0303(T\u22121)|x) will be \u201cclose\u201d to the sampling chain z(0) \u2192 z(1) \u2192 z(2) \u2192 . . . \u2192 z(T\u22121) of model p in the sense that q(t)(z\u0303(t)|z\u0303(t\u22121),x) will be close to p(t)(z(t)|z(t\u22121)), but will at every step be slightly biased towards generating samples closer to target x, i.e. x gets progressively \u201cinfused\u201d into the chain. This is achieved by defining q(0)i (z\u0303 (0) i |x) as a mixture between p (0) i (with a large mixture weight) and \u03b4xi , a concentrated unimodal distribution around xi, such as a Gaussian with small variance (with a small mixture weight)1. Formally q(0)i (z\u0303 (0) i |x) = (1 \u2212 \u03b1(t))p(0)i (z\u0303 (0) i ) + \u03b1 (t)\u03b4xi(z\u0303 (0) i ), where 1 \u2212 \u03b1(t) and \u03b1(t) are the mixture weights 2. In other words, when sampling a value for z\u0303(0)i from q (0) i there will be a small probability \u03b1 (0) to pick value close to xi (as sampled from \u03b4xi ) rather than sampling the value from p (0) i . We call \u03b1(t) the infusion rate. We define the transition operator of the infusion chain similarly as: q (t) i (z\u0303 (t) i |z\u0303(t\u22121),x) = (1\u2212 \u03b1(t))p (t) i (z\u0303 (t) i |z\u0303(t\u22121)) + \u03b1(t)\u03b4xi(z\u0303 (t) i ).\n1Note that \u03b4xi does not denote a Dirac-Delta but a Gaussian with small sigma. 2In all experiments, we use an increasing schedule \u03b1(t) = \u03b1 (t\u22121) +\u03c9 with \u03b1 (0)\nand \u03c9 constant. This allows to build our chain such that in the first steps, we give little information about the target and in the last steps we give more informations about the target. This forces the network to have less confidence (greater incertitude) at the beginning of the chain and more confidence on the convergence point at the end of the chain.\n2.3.2 DENOISING-BASED INFUSION TRAINING PROCEDURE\nFor all x \u2208 Dtrain: \u2022 Sample from the infusion chain z\u0303 = (z\u0303(0), . . . , z\u0303(T\u22121)) \u223c q(z\u0303(0), . . . , z\u0303(T\u22121)|x).\nprecisely so: z\u03030 \u223c q(0)(z\u0303(0)|x) . . . z\u0303(t) \u223c q(t)(z\u0303(t)|z\u0303(t\u22121),x) . . . \u2022 Perform a gradient step so that p learns to \u201cdenoise\u201d every z\u0303(t) into x.\n\u03b8(t) \u2190 \u03b8(t) + \u03b7(t) \u2202 log p (t)(x|z\u0303(t\u22121); \u03b8(t)) \u2202\u03b8(t)\nwhere \u03b7(t) is a scalar learning rate. 3\nAs illustrated in Figure 2, the distribution of samples from the infusion chain evolves as training progresses, since this chain remains close to the model sampling chain.\n2.4 STOCHASTIC LOG LIKELIHOOD ESTIMATION\nThe exact log-likelihood of the generative model implied by our model p is intractable. The logprobability of an example x can however be expressed using proposal distribution q as:\nlog p(x) = logEq(z\u0303|x) [ p(z\u0303,x)\nq(z\u0303|x)\n] (2)\nUsing Jensen\u2019s inequality we can thus derive the following lower bound:\nlog p(x) \u2265 Eq(z\u0303|x) [log p(z\u0303,x)\u2212 log q(z\u0303|x)] (3)\nwhere log p(z\u0303,x) = log p(0)(z\u0303(0)) + (\u2211T\u22121\nt=1 log p (t)(z\u0303(t)|z\u0303(t\u22121))\n) + log p(T )(x|z\u0303(T\u22121)) and\nlog q(z\u0303|x) = log q(0)(z\u0303(0)|x) + \u2211T\u22121 t=1 log q\n(t)(z\u0303(t)|z\u0303(t\u22121),x). 3Since we will be sharing parameters between the p(t), in order for the expected larger error gradients on the earlier transitions not to dominate the parameter updates over the later transitions we used an increasing schedule \u03b7(t) = \u03b70 tT for t \u2208 {1, . . . , T}.\nA stochastic estimation can easily be obtained by replacing the expectation by an average using a few samples from q(z\u0303|x). We can thus compute a lower bound estimate of the average log likelihood over training, validation and test data.\nSimilarly in addition to the lower-bound based on Eq.3 we can use the same few samples from q(z\u0303|x) to get an importance-sampling estimate of the likelihood based on Eq. 24.\n2.4.1 LOWER-BOUND-BASED INFUSION TRAINING PROCEDURE\nSince we have derived a lower bound on the likelihood, we can alternatively choose to optimize this stochastic lower-bound directly during training. This alternative lower-bound based infusion training procedure differs only slightly from the denoising-based infusion training procedure by using z\u0303(t) as a training target at step t (performing a gradient step to increase log p(t)(z\u0303(t)|z\u0303(t\u22121); \u03b8(t))) whereas denoising training always uses x as its target (performing a gradient step to increase log p(t)(x|z\u0303(t\u22121); \u03b8(t))). Note that the same reparametrization trick as used in Variational Autoencoders (Kingma & Welling, 2014) can be used here to backpropagate through the chain\u2019s Gaussian sampling.\n3 RELATIONSHIP TO PREVIOUSLY PROPOSED APPROACHES\n3.1 MARKOV CHAIN MONTE CARLO FOR ENERGY-BASED MODELS\nGenerating samples as a repeated application of a Markov transition operator that operates on input space is at the heart of Markov Chain Monte Carlo (MCMC) methods. They allow sampling from an energy-model, where one can efficiently compute the energy or unnormalized negated log probability (or density) at any point. The transition operator is then derived from an explicit energy function such that the Markov chain prescribed by a specific MCMC method is guaranteed to converge to the distribution defined by that energy function, as the equilibrium distribution of the chain. MCMC techniques have thus been used to obtain samples from the energy model, in the process of learning to adjust its parameters.\nBy contrast here we do not learn an explicit energy function, but rather learn directly a parameterized transition operator, and define an implicit model distribution based on the result of running the Markov chain.\n3.2 VARIATIONAL AUTO-ENCODERS\nVariational auto-encoders (VAE) (Kingma & Welling, 2014; Rezende et al., 2014) also start from an unstructured (independent) noise sample and non-linearly transform this into a distribution that matches the training data. One difference with our approach is that the VAE typically maps from a lower-dimensional space to the observation space. By contrast we learn a stochastic transition operator from input space to input space that we repeat for T steps. Another key difference, is that the VAE learns a complex heavily parameterized approximate posterior proposal q whereas our infusion based q can be understood as a simple heuristic proposal distribution based on p. Importantly the specific heuristic we use to infuse x into q makes sense precisely because our operator is a map from input space to input space, and couldn\u2019t be readily applied otherwise. The generative network in Rezende et al. (2014) is a Deep Latent Gaussian Model (DLGM) just as ours. But their approximate posterior q is taken to be factorial, including across all layers of the DLGM, whereas our infusion based q involves an ordered sampling of the layers, as we sample from q(t)(z\u0303(t)|z\u0303(t\u22121),x). More recent proposals involve sophisticated approaches to sample from better approximate posteriors, as the work of Salimans et al. (2015) in which Hamiltonian Monte Carlo is combined with variational inference, which looks very promising, though computationally expensive, and Rezende & Mohamed (2015) that generalizes the use of normalizing flows to obtain a better approximate posterior.\n4Specifically, the two estimates (lower-bound and IS) start by collecting k samples from q(z\u0303|x) and computing for each the corresponding ` = log p(z\u0303,x) \u2212 log q(z\u0303|x). The lower-bound estimate is then obtained by averaging the resulting `1, . . . `k, whereas the IS estimate is obtained by taking the log of the averaged e`1 , . . . , e`k (in a numerical stable manner as logsumexp(`1, . . . , `k)\u2212 log k).\n3.3 SAMPLING FROM AUTOENCODERS AND GENERATIVE STOCHASTIC NETWORKS\nEarlier works that propose to directly learn a transition operator resulted from research to turn autoencoder variants that have a stochastic component, in particular denoising autoencoders (Vincent et al., 2010), into generative models that one can sample from. This development is natural, since a stochastic auto-encoder is a stochastic transition operator form input space to input space. Generative Stochastic Networks (GSN) (Alain et al., 2016) generalized insights from earlier stochastic autoencoder sampling heuristics (Rifai et al., 2012) into a more formal and general framework. These previous works on generative uses of autoencoders and GSNs attempt to learn a chain whose equilibrium distribution will fit the training data. Because autoencoders and the chain are typically started from or very close to training data points, they are concerned with the chain mixing quickly between modes. By contrast our model chain is always restarted from unstructured noise, and is not required to reach or even have an equilibrium distribution. Our concern is only what happens during the T \u201cburn-in\u201d initial steps, and to make sure that it transforms the initial factorial noise distribution into something that best fits the training data distribution. There are no mixing concerns beyond those T initial steps.\nA related aspect and limitation of previous denoising autoencoder and GSN approaches is that these were mainly \u201clocal\u201d around training samples: the stochastic operator explored space starting from and primarily centered around training examples, and learned based on inputs in these parts of space only. Spurious modes in the generated samples might result from large unexplored parts of space that one might encounter while running a long chain.\n3.4 REVERSING A DIFFUSION PROCESS IN NON-EQUILIBRIUM THERMODYNAMICS\nThe approach of Sohl-Dickstein et al. (2015) is probably the closest to the approach we develop here. Both share a similar model sampling chain that starts from unstructured factorial noise. Neither are concerned about an equilibrium distribution. They are however quite different in several key aspects: Sohl-Dickstein et al. (2015) proceed to invert an explicit diffusion process that starts from a training set example and very slowly destroys its structure to become this random noise, they then learn to reverse this process i.e. an inverse diffusion. To maintain the theoretical argument that the exact reverse process has the same distributional form (e.g. p(x(t\u22121)|x(t)) and p(x(t)|x(t\u22121)) both factorial Gaussians), the diffusion has to be infinitesimal by construction, hence the proposed approaches uses chains with thousands of tiny steps. Instead, our aim is to learn an operator that can yield a high quality sample efficiently using only a small number T of larger steps. Also our infusion training does not posit a fixed a priori diffusion process that we would learn to reverse. And while the distribution of diffusion chain samples of Sohl-Dickstein et al. (2015) is fixed and remains the same all along the training, the distribution of our infusion chain samples closely follow the model chain as our model learns. Our proposed infusion sampling technique thus adapts to the changing generative model distribution as the learning progresses.\nDrawing on both Sohl-Dickstein et al. (2015) and the walkback procedure introduced for GSN in Alain et al. (2016), a variational variant of the walkback algorithm was investigated by Goyal et al. (2017) at the same time as our work. It can be understood as a different approach to learning a Markov transition operator, in which a \u201cheating\u201d diffusion operator is seen as a variational approximate posterior to the forward \u201ccooling\u201d sampling operator with the exact same form and parameters, except for a different temperature.\n4 EXPERIMENTS\nWe trained models on several datasets with real-valued examples. We used as prior distribution p(0) a factorial Gaussian whose parameters were set to be the mean and variance for each pixel through the training set. Similarly, our models for the transition operators are factorial Gaussians. Their mean and elementwise variance is produced as the output of a neural network that receives the previous z(t\u22121) as its input, i.e. p(t)(z(t)i |z(t\u22121)) = N (\u00b5i(z(t\u22121)), \u03c32i (z(t\u22121))) where \u00b5 and \u03c32 are computed as output vectors of a neural network. We trained such a model using our infusion training procedure on MNIST (LeCun & Cortes, 1998), Toronto Face Database (Susskind et al., 2010), CIFAR-10 (Krizhevsky & Hinton, 2009), and CelebA (Liu et al., 2015). For all datasets, the only preprocessing we did was to scale the integer pixel values down to range [0,1]. The network\ntrained on MNIST and TFD is a MLP composed of two fully connected layers with 1200 units using batch-normalization (Ioffe & Szegedy, 2015) 5. The network trained on CIFAR-10 is based on the same generator as the GANs of Salimans et al. (2016), i.e. one fully connected layer followed by three transposed convolutions. CelebA was trained with the previous network where we added another transposed convolution. We use rectifier linear units (Glorot et al., 2011) on each layer inside the networks. Each of those networks have two distinct final layers with a number of units corresponding to the image size. They use sigmoid outputs, one that predict the mean and the second that predict a variance scaled by a scalar \u03b2 (In our case we chose \u03b2 = 0.1) and we add an epsilon = 1e \u2212 4 to avoid an excessively small variance. For each experiment, we trained the network on 15 steps of denoising with an increasing infusion rate of 1% (\u03c9 = 0.01, \u03b1 (0)\n= 0), except on CIFAR-10 where we use an increasing infusion rate of 2% (\u03c9 = 0.02, \u03b1 (0) = 0) on 20 steps.\n4.1 NUMERICAL RESULTS\nSince we can\u2019t compute the exact log-likelihood, the evaluation of our model is not straightforward. However we use the lower bound estimator derived in Section 2.4 to evaluate our model during training and prevent overfitting (see Figure 3). Since most previous published results on non-likelihood based models (such as GANs) used a Parzen-window-based estimator (Breuleux et al., 2011), we use it as our first comparison tool, even if it can be misleading (Lucas Theis & Bethge, 2016). Results are shown in Table 1, we use 10 000 generated samples and \u03c3 = 0.17 . To get a better estimate of the log-likelihood, we then computed both the stochastic lower bound and the importance sampling estimate (IS) given in Section 2.4. For the IS estimate in our MNIST-trained model, we used 20 000 intermediates samples. In Table 2 we compare our model with the recent Annealed Importance Sampling results (Wu et al., 2016). Note that following their procedure we add an uniform noise of 1/256 to the (scaled) test point before evaluation to avoid overevaluating models that might have overfitted on the 8 bit quantization of pixel values. Another comparison tool that we used is the Inception score as in Salimans et al. (2016) which was developed for natural images and is thus most relevant for CIFAR-10. Since Salimans et al. (2016) used a GAN trained in a semi-supervised way with some tricks, the comparison with our unsupervised trained model isn\u2019t straightforward. However, we can see in Table 3 that our model outperforms the traditional GAN trained without labeled data.\n4.2 SAMPLE GENERATION\nAnother common qualitative way to evaluate generative models is to look at the quality of the samples generated by the model. In Figure 4 we show various samples on each of the datasets we used. In order to get sharper images, we use at sampling time more denoising steps than in the training time (In the MNIST case we use 30 denoising steps for sampling with a model trained on 15 denoising steps). To make sure that our network didn\u2019t learn to copy the training set, we show in the last column the nearest training-set neighbor to the samples in the next-to last column. We can see that our training method allow to generate very sharp and accurate samples on various dataset.\n5We don\u2019t share batch norm parameters across the network, i.e for each time step we have different parameters and independent batch statistics.\n4.3 INPAINTING\nAnother method to evaluate a generative model is inpainting. It consists of providing only a partial image from the test set and letting the model generate the missing part. In one experiment, we provide only the top half of CelebA test set images and clamp that top half throughout the sampling chain. We restart sampling from our model several times, to see the variety in the distribution of the bottom part it generates. Figure 5 shows that the model is able to generate a varied set of bottom halves, all consistent with the same top half, displaying different type of smiles and expression. We also see that the generated bottom halves transfer some information about the provided top half of the images (such as pose and more or less coherent hair cut).\n5 CONCLUSION AND FUTURE WORK\nWe presented a new training procedure that allows a neural network to learn a transition operator of a Markov chain. Compared to the previously proposed method of Sohl-Dickstein et al. (2015) based on inverting a slow diffusion process, we showed empirically that infusion training requires far fewer denoising steps, and appears to provide more accurate models. Currently, many successful generative models, judged on sample quality, are based on GAN architectures. However these require to use two different networks, a generator and a discriminator, whose balance is reputed delicate to adjust, which can be source of instability during training. Our method avoids this problem by using only a single network and a simpler training objective.\nDenoising-based infusion training optimizes a heuristic surrogate loss for which we cannot (yet) provide theoretical guarantees, but we empirically verified that it results in increasing log-likelihood estimates. On the other hand the lower-bound-based infusion training procedure does maximize an explicit variational lower-bound on the log-likelihood. While we have run most of our experiments with the former, we obtained similar results on the few problems we tried with lower-bound-based infusion training.\nFuture work shall further investigate the relationship and quantify the compromises achieved with respect to other Markov Chain methods including Sohl-Dickstein et al. (2015); Salimans et al. (2015)\nand also to powerful inference methods such as Rezende & Mohamed (2015). As future work, we also plan to investigate the use of more sophisticated neural net generators, similar to DCGAN\u2019s (Radford et al., 2016) and to extend the approach to a conditional generator applicable to structured output problems.\nACKNOWLEDGMENTS\nWe would like to thank the developers of Theano (Theano Development Team, 2016) for making this library available to build on, Compute Canada and Nvidia for their computation resources, NSERC and Ubisoft for their financial support, and three ICLR anonymous reviewers for helping us improve our paper.\nA DETAILS ON THE EXPERIMENTS\nA.1 MNIST EXPERIMENTS\nWe show the impact of the infusion rate \u03b1(t) = \u03b1 (t\u22121)\n+ \u03c9 for different numbers of training steps on the lower bound estimate of log-likelihood on the Validation set of MNIST in Figure 6. We also show the quality of generated samples and the lower bound evaluated on the test set in Table 4. Each experiment in Table 4 uses the corresponding models of Figure 6 that obtained the best lower bound value on the validation set. We use the same network architecture as described in Section 4, i.e two fully connected layers with Relu activations composed of 1200 units followed by two distinct fully connected layers composed of 784 units, one that predicts the means, the other one that predicts the variances. Each mean and variance is associated with one pixel. All of the the parameters of the model are shared across different steps except for the batch norm parameters. During training, we use the batch statistics of the current mini-batch in order to evaluate our model on the train and validation sets. At test time (Table 4), we first compute the batch statistics over the entire train set for each step and then use the computed statistics to evaluate our model on the test test.\nWe did some experiments to evaluate the impact of \u03b1 or \u03c9 in \u03b1(t) = \u03b1 (t\u22121)\n+ \u03c9. Figure 6 shows that as the number of steps increases, the optimal value for infusion rate decreases. Therefore, if we want to use many steps, we should have a small infusion rate. These conclusions are valid for both increasing and constant infusion rate. For example, the optimal \u03b1 for a constant infusion rate, in Figure 6e with 10 steps is 0.08 and in Figure 6f with 15 steps is 0.06. If the number of steps is not enough or the infusion rate is too small, the network will not be able to learn the target distribution as shown in the first rows of all subsection in Table 4.\nIn order to show the impact of having a constant versus an increasing infusion rate, we show in Figure 7 the samples created by infused and sampling chains. We observe that having a small infusion rate over many steps ensures a slow blending of the model distribution into the target distribution.\nIn Table 4, we can see high lower bound values on the test set with few steps even if the model can\u2019t generate samples that are qualitatively satisfying. These results indicate that we can\u2019t rely on the lower bound as the only evaluation metric and this metric alone does not necessarily indicate the suitability of our model to generated good samples. However, it is still a useful tool to prevent overfitting (the networks in Figure 6e and 6f overfit when the infusion rate becomes too high). Concerning the samples quality, we observe that having a small infusion rate over an adequate number of steps leads to better samples.\nA.2 INFUSION AND MODEL SAMPLING CHAINS ON NATURAL IMAGES DATASETS\nIn order to show the behavior of our model trained by Infusion on more complex datasets, we show in Figure 8 chains on CIFAR-10 dataset and in Figure 9 chains on CelebA dataset. In each Figure, the first sub-figure shows the chains infused by some test examples and the second subfigure shows the model sampling chains. In the experiment on CIFAR-10, we use an increasing schedule \u03b1(t) = \u03b1 (t\u22121) + 0.02 with \u03b1(0) = 0 and 20 infusion steps (this corresponds to the training parameters). In the experiment on CelebA, we use an increasing schedule \u03b1(t) = \u03b1 (t\u22121)\n+0.01 with \u03b1(0) = 0 and 15 infusion steps.\n", "cats": {"READ": 1.0, "DONTREAD": 0.0}, "reviews": [{"IS_META_REVIEW": true, "comments": "Summary:\nThis paper introduces a heuristic approach for training a deep directed generative model, where similar to the transition operator of a Markov chain each layer samples from the same conditional distribution. Similar to optimizing a variational lower bound, the approach is to approximate the gradient by replacing the posterior over latents with an alternative distribution. However, the approximating distribution is not updated to improve the lower bound but heuristically constructed in each step. A further difference to variational optimization is that the conditional distributions are optimized greedily rather than following the gradient of the joint log-likelihood.\n\nReview:\nThe proposed approach is interesting and to me seems worth exploring more. Given that there are approaches for training the same class of models which are 1) theoretically more sound, 2) of similar computational complexity, and 3) work well in practice (e.g. Rezende & Mohamed, 2015), I am nevertheless not sure of its potential to generate impact. My bigger concern, however, is that the empirical evaluation is still quite limited.\n\nI appreciate the authors included proper estimates of the log-likelihood. This will enable and encourage future comparisons with this method on continuous MNIST. However, the authors should point out that the numbers taken from Wu et al. (2016) are not representative of the performance of a VAE. (From the paper: \u201cTherefore, the log-likelihood values we report should not be compared directly against networks which have a more flexible observation model.\u201d \u201cSuch observation models can easily achieve much higher log-likelihood scores, [\u2026].\u201d)\n\nComparisons with inpainting results using other methods would have been nice. How practical is the proposed approach compared to other approaches? Similar to the diffusion approach by Sohl-Dickstein et al. (2015), the proposed approach seems to be both efficient and effective for inpainting. Not making this a bigger point and performing the proper evaluations seems like a missed opportunity.\n\nMinor:\n\u2013\u00a0I am missing citations for \u201cordered visible dimension sampling\u201d\n\u2013\u00a0Typos and frequent incorrect use of \\citet and \\citep"}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "Infusion training is a new, somewhat heuristic, procedure for training deep generative models. It's an interesting novel idea and a good paper, which has also been improved after the authors have been responsive to reviewer feedback.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"DATE": "14 Jan 2017", "TITLE": "Change-log", "IS_META_REVIEW": false, "comments": "We updated the paper, the main changes are:\n\nAdded better log-likelihood estimates (one stochastic lower bound and one based on importance sampling)\n\nAdded curves showing that log-likelihood bound improves as infusion training progresses \n\nAdded references to related and relevant works: Rezende & Mohamed, 2015;  Salimans et al. 2015; Dinh et al. 2014; Wu et al. 2016.\n\nAdded results tables for likelihood comparison with models from the literature (Parzen estimates by Sohl-Dickstein et al. 2015 and AIS estimates by Wu et al. 2016)\n\nAdded further experimental details.\n\nAdded an Appendix containing details regarding the infusion rate schedule as well as examples of infused and sampled chains on cifar10 and celebA\n\nCorrected the typos mentioned by the reviewers\n", "OTHER_KEYS": "Pascal Vincent"}, {"IMPACT": 4, "SUBSTANCE": 3, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "TITLE": "interesting idea", "comments": "This paper trains a generative model which transforms noise into model samples by a gradual denoising process. It is similar to a generative model based on diffusion. Unlike the diffusion approach:\n- It uses only a small number of denoising steps, and is thus far more computationally efficient.\n- Rather than consisting of a reverse trajectory, the conditional chain for the approximate posterior jumps to q(z(0) | x), and then runs in the same direction as the generative model. This allows the inference chain to behave like a perturbation around the generative model, that pulls it towards the data. (This also seems somewhat related to ladder networks.)\n- There is no tractable variational bound on the log likelihood.\n\nI liked the idea, and found the visual sample quality given a short chain impressive. The inpainting results were particularly nice, since one shot inpainting is not possible under most generative modeling frameworks. It would be much more convincing to have a log likelihood comparison that doesn't depend on Parzen likelihoods.\n\nDetailed comments follow:\n\nSec. 2:\n\"theta(0) the\" -> \"theta(0) be the\"\n\"theta(t) the\" -> \"theta(t) be the\"\n\"what we will be using\" -> \"which we will be doing\"\nI like that you infer q(z^0|x), and then run inference in the same order as the generative chain. This reminds me slightly of ladder networks.\n\"q*. Having learned\" -> \"q*. [paragraph break] Having learned\"\nSec 3.3:\n\"learn to inverse\" -> \"learn to reverse\"\nSec. 4:\n\"For each experiments\" -> \"For each experiment\"\nHow sensitive are your results to infusion rate?\nSec. 5: \"appears to provide more accurate models\" I don't think you showed this -- there's no direct comparison to the Sohl-Dickstein paper.\nFig 4. -- neat!\n", "SOUNDNESS_CORRECTNESS": 5, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "RECOMMENDATION": 8, "DATE": "16 Dec 2016 (modified: 20 Jan 2017)", "APPROPRIATENESS": 3, "REVIEWER_CONFIDENCE": 5}, {"DATE": "16 Dec 2016", "TITLE": "-", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer4"}, {"TITLE": "Clearly written paper pursuing an interesting idea. Some shortcomings with respect to the evaluation and comparison to prior work", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer4", "comments": "The paper presents a method for training a generative model via an iterative denoising procedure. The denoising process is initialized with a random sample from a crude approximation to the data distribution and produces a high quality sample via multiple denoising steps. Training is performed by setting-up a Markov chain that slowly blends propositions from the current denoising model with a real example from the data distribution; using this chain the current denoising model is updated towards reproducing the changed, \"better\", samples from the blending process.\n\nThis is a clearly written paper that considers an interesting approach for training generative models. I was intrigued by the simplicity of the presented approach and really enjoyed reading the paper.\nThe proposed method is novel although it has clear ties to other recent work aiming to use denoising models for sampling from distributions such as the work by Sohl-Dickstein and the recent work on using DAEs as generative models.\nI think this general direction of research is important. The proposed procedure takes inspiration from the perspective of generating samples by minimizing an energy function via transitions along a Markov chain and, if successful, it can potentially sidestep many problems of current procedures for training directed generative models such as:\n- convergence and mode coverage problems as in generative adversarial networks\n- problems with modeling multi-modal distributions which can arise when a too restrictive approximate inference model is paired with a powerful generative model\n\nThat being said, another method that seems promising for addressing these issues that also has some superficially similarity to the presented work is the idea of combining Hamiltonian Monte Carlo inference with variational inference as in [1]. As such I am not entirely convinced that the method presented here will be able to perform better than the mentioned paper; although it might be simpler to train. Similarly, although I agree that using a MCMC chain to generate samples via a MC-EM like procedure is likely very costly I am not convinced such a procedure won't at least also work reasonably well for the simple MNIST example. In general a more direct comparison between different inference methods using an MCMC chain like procedure would be nice to have but I understand that this is perhaps out of the scope of this paper. One thing that I would have expected, however, is a direct comparison to the procedure from Sohl-Dickstein in terms of sampling steps and generation quality as it is so directly related.\n\nOther major points (good and bad):\n- Although in general the method is explained well some training details are missing. Most importantly it is never mentioned how alpha or omega are set (I am assuming omega is 0.01 as that is the increase mentioned in the experimental setup). It is also unclear how alpha affects the capabilities of the generator. While it intuitively seems reasonable to use a small alpha over many steps to ensure slow blending of the two distributions it is not clear how necessary this is or at what point the procedure would break (I assume alpha = 1 won't work as the generator then would have to magically denoise a sample from the relatively uninformative draw from p0 ?). The authors do mention in one of the figure captions that the denoising model does not produce good samples in only 1-2 steps but that might also be an artifact of training the model with small alpha (at least I see no a priori reason for this). More experiments should be carried out here.\n- No infusion chains or generating chains are shown for any of the more complicated data distributions, this is unfortunate as I feel these would be interesting to look at.\n- The paper does a good job at evaluating the model with respect to several different metrics. The bound on the log-likelihood is nice to have as well!\n- Unfortunately the current approach does not come with any theoretical guarantees. It is unclear for what choices of alpha the procedure will work and whether there is some deeper connection to MCMC sampling or energy based models. In my eyes this does not subtract from the value of the paper but would perhaps be worth a short sentence in the conclusion.\n\nMinor points:\n- The second reference seems broken\n- Figure 3 starts at 100 epochs and, as a result, contains little information. Perhaps it would be more useful to show the complete training procedure and put the x-axis on a log-scale ?\n- The explanation regarding the convolutional networks you use makes no sense to me. You write that you use the same structure as in the \"Improved GANs\" paper which, unlike your model, generates samples from a fixed length random input. I thus suppose you don't really use a generator with 1 fully connected network followed by up-convolutions but rather have several stages of convolutions followed by a fully connected layer and then up-convolutions ?\n- The choice of parametrizing the variance via a sigmoid output unit is somewhat unusual, was there a specific reason for this choice ?\n- footnote 1 contains errors: \"This allow to\" -> \"allows to\",  \"few informations\" -> \"little information\". \"This force the network\" -> \"forces\"\n- Page 1 error: etc...\n- Page 4 error: \"operator should to learn\"\n\n[1] Markov Chain Monte Carlo and Variational Inference: Bridging the Gap, Tim Salimans and Diedrik P. Kingma and Max Welling, ICML 2015\n\n\n>>> Update <<<<\nCopied here from my response below: \n\nI believe the response of the authors clarifies all open issues. I strongly believe the paper should be accepted to the conference. The only remaining issue I have with the paper is that, as the authors acknowledge the architecture of the generator is likely highly sub-optimal and might hamper the performance of the method in the evaluation. This however does not at all subtract from any of the main points of the paper.\n\nI am thus keeping my score as a clear accept. I want to emphasize that I believe the paper should be published (just in case the review process results in some form of cut-off threshold that is high due to overall \"inflated\" review scores).\n", "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "16 Dec 2016 (modified: 20 Jan 2017)", "REVIEWER_CONFIDENCE": 5}, {"SUBSTANCE": 4, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "TITLE": "Interesting idea with lacking theoretical motivation and limited empirical evaluation", "comments": "Summary:\nThis paper introduces a heuristic approach for training a deep directed generative model, where similar to the transition operator of a Markov chain each layer samples from the same conditional distribution. Similar to optimizing a variational lower bound, the approach is to approximate the gradient by replacing the posterior over latents with an alternative distribution. However, the approximating distribution is not updated to improve the lower bound but heuristically constructed in each step. A further difference to variational optimization is that the conditional distributions are optimized greedily rather than following the gradient of the joint log-likelihood.\n\nReview:\nThe proposed approach is interesting and to me seems worth exploring more. Given that there are approaches for training the same class of models which are 1) theoretically more sound, 2) of similar computational complexity, and 3) work well in practice (e.g. Rezende & Mohamed, 2015), I am nevertheless not sure of its potential to generate impact. My bigger concern, however, is that the empirical evaluation is still quite limited.\n\nI appreciate the authors included proper estimates of the log-likelihood. This will enable and encourage future comparisons with this method on continuous MNIST. However, the authors should point out that the numbers taken from Wu et al. (2016) are not representative of the performance of a VAE. (From the paper: \u201cTherefore, the log-likelihood values we report should not be compared directly against networks which have a more flexible observation model.\u201d \u201cSuch observation models can easily achieve much higher log-likelihood scores, [\u2026].\u201d)\n\nComparisons with inpainting results using other methods would have been nice. How practical is the proposed approach compared to other approaches? Similar to the diffusion approach by Sohl-Dickstein et al. (2015), the proposed approach seems to be both efficient and effective for inpainting. Not making this a bigger point and performing the proper evaluations seems like a missed opportunity.\n\nMinor:\n\u2013\u00a0I am missing citations for \u201cordered visible dimension sampling\u201d\n\u2013\u00a0Typos and frequent incorrect use of \\citet and \\citep", "ORIGINALITY": 4, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "14 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"SUBSTANCE": 4, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "TITLE": "Using IS to evaluate log-likelihood", "comments": "", "ORIGINALITY": 4, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "DATE": "02 Dec 2016"}, {"IMPACT": 4, "SUBSTANCE": 3, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "TITLE": "time dependence of infusion chain", "comments": "", "SOUNDNESS_CORRECTNESS": 5, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "DATE": "30 Nov 2016", "APPROPRIATENESS": 3}, {"IS_META_REVIEW": true, "comments": "Summary:\nThis paper introduces a heuristic approach for training a deep directed generative model, where similar to the transition operator of a Markov chain each layer samples from the same conditional distribution. Similar to optimizing a variational lower bound, the approach is to approximate the gradient by replacing the posterior over latents with an alternative distribution. However, the approximating distribution is not updated to improve the lower bound but heuristically constructed in each step. A further difference to variational optimization is that the conditional distributions are optimized greedily rather than following the gradient of the joint log-likelihood.\n\nReview:\nThe proposed approach is interesting and to me seems worth exploring more. Given that there are approaches for training the same class of models which are 1) theoretically more sound, 2) of similar computational complexity, and 3) work well in practice (e.g. Rezende & Mohamed, 2015), I am nevertheless not sure of its potential to generate impact. My bigger concern, however, is that the empirical evaluation is still quite limited.\n\nI appreciate the authors included proper estimates of the log-likelihood. This will enable and encourage future comparisons with this method on continuous MNIST. However, the authors should point out that the numbers taken from Wu et al. (2016) are not representative of the performance of a VAE. (From the paper: \u201cTherefore, the log-likelihood values we report should not be compared directly against networks which have a more flexible observation model.\u201d \u201cSuch observation models can easily achieve much higher log-likelihood scores, [\u2026].\u201d)\n\nComparisons with inpainting results using other methods would have been nice. How practical is the proposed approach compared to other approaches? Similar to the diffusion approach by Sohl-Dickstein et al. (2015), the proposed approach seems to be both efficient and effective for inpainting. Not making this a bigger point and performing the proper evaluations seems like a missed opportunity.\n\nMinor:\n\u2013\u00a0I am missing citations for \u201cordered visible dimension sampling\u201d\n\u2013\u00a0Typos and frequent incorrect use of \\citet and \\citep"}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "Infusion training is a new, somewhat heuristic, procedure for training deep generative models. It's an interesting novel idea and a good paper, which has also been improved after the authors have been responsive to reviewer feedback.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"DATE": "14 Jan 2017", "TITLE": "Change-log", "IS_META_REVIEW": false, "comments": "We updated the paper, the main changes are:\n\nAdded better log-likelihood estimates (one stochastic lower bound and one based on importance sampling)\n\nAdded curves showing that log-likelihood bound improves as infusion training progresses \n\nAdded references to related and relevant works: Rezende & Mohamed, 2015;  Salimans et al. 2015; Dinh et al. 2014; Wu et al. 2016.\n\nAdded results tables for likelihood comparison with models from the literature (Parzen estimates by Sohl-Dickstein et al. 2015 and AIS estimates by Wu et al. 2016)\n\nAdded further experimental details.\n\nAdded an Appendix containing details regarding the infusion rate schedule as well as examples of infused and sampled chains on cifar10 and celebA\n\nCorrected the typos mentioned by the reviewers\n", "OTHER_KEYS": "Pascal Vincent"}, {"IMPACT": 4, "SUBSTANCE": 3, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "TITLE": "interesting idea", "comments": "This paper trains a generative model which transforms noise into model samples by a gradual denoising process. It is similar to a generative model based on diffusion. Unlike the diffusion approach:\n- It uses only a small number of denoising steps, and is thus far more computationally efficient.\n- Rather than consisting of a reverse trajectory, the conditional chain for the approximate posterior jumps to q(z(0) | x), and then runs in the same direction as the generative model. This allows the inference chain to behave like a perturbation around the generative model, that pulls it towards the data. (This also seems somewhat related to ladder networks.)\n- There is no tractable variational bound on the log likelihood.\n\nI liked the idea, and found the visual sample quality given a short chain impressive. The inpainting results were particularly nice, since one shot inpainting is not possible under most generative modeling frameworks. It would be much more convincing to have a log likelihood comparison that doesn't depend on Parzen likelihoods.\n\nDetailed comments follow:\n\nSec. 2:\n\"theta(0) the\" -> \"theta(0) be the\"\n\"theta(t) the\" -> \"theta(t) be the\"\n\"what we will be using\" -> \"which we will be doing\"\nI like that you infer q(z^0|x), and then run inference in the same order as the generative chain. This reminds me slightly of ladder networks.\n\"q*. Having learned\" -> \"q*. [paragraph break] Having learned\"\nSec 3.3:\n\"learn to inverse\" -> \"learn to reverse\"\nSec. 4:\n\"For each experiments\" -> \"For each experiment\"\nHow sensitive are your results to infusion rate?\nSec. 5: \"appears to provide more accurate models\" I don't think you showed this -- there's no direct comparison to the Sohl-Dickstein paper.\nFig 4. -- neat!\n", "SOUNDNESS_CORRECTNESS": 5, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "RECOMMENDATION": 8, "DATE": "16 Dec 2016 (modified: 20 Jan 2017)", "APPROPRIATENESS": 3, "REVIEWER_CONFIDENCE": 5}, {"DATE": "16 Dec 2016", "TITLE": "-", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer4"}, {"TITLE": "Clearly written paper pursuing an interesting idea. Some shortcomings with respect to the evaluation and comparison to prior work", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer4", "comments": "The paper presents a method for training a generative model via an iterative denoising procedure. The denoising process is initialized with a random sample from a crude approximation to the data distribution and produces a high quality sample via multiple denoising steps. Training is performed by setting-up a Markov chain that slowly blends propositions from the current denoising model with a real example from the data distribution; using this chain the current denoising model is updated towards reproducing the changed, \"better\", samples from the blending process.\n\nThis is a clearly written paper that considers an interesting approach for training generative models. I was intrigued by the simplicity of the presented approach and really enjoyed reading the paper.\nThe proposed method is novel although it has clear ties to other recent work aiming to use denoising models for sampling from distributions such as the work by Sohl-Dickstein and the recent work on using DAEs as generative models.\nI think this general direction of research is important. The proposed procedure takes inspiration from the perspective of generating samples by minimizing an energy function via transitions along a Markov chain and, if successful, it can potentially sidestep many problems of current procedures for training directed generative models such as:\n- convergence and mode coverage problems as in generative adversarial networks\n- problems with modeling multi-modal distributions which can arise when a too restrictive approximate inference model is paired with a powerful generative model\n\nThat being said, another method that seems promising for addressing these issues that also has some superficially similarity to the presented work is the idea of combining Hamiltonian Monte Carlo inference with variational inference as in [1]. As such I am not entirely convinced that the method presented here will be able to perform better than the mentioned paper; although it might be simpler to train. Similarly, although I agree that using a MCMC chain to generate samples via a MC-EM like procedure is likely very costly I am not convinced such a procedure won't at least also work reasonably well for the simple MNIST example. In general a more direct comparison between different inference methods using an MCMC chain like procedure would be nice to have but I understand that this is perhaps out of the scope of this paper. One thing that I would have expected, however, is a direct comparison to the procedure from Sohl-Dickstein in terms of sampling steps and generation quality as it is so directly related.\n\nOther major points (good and bad):\n- Although in general the method is explained well some training details are missing. Most importantly it is never mentioned how alpha or omega are set (I am assuming omega is 0.01 as that is the increase mentioned in the experimental setup). It is also unclear how alpha affects the capabilities of the generator. While it intuitively seems reasonable to use a small alpha over many steps to ensure slow blending of the two distributions it is not clear how necessary this is or at what point the procedure would break (I assume alpha = 1 won't work as the generator then would have to magically denoise a sample from the relatively uninformative draw from p0 ?). The authors do mention in one of the figure captions that the denoising model does not produce good samples in only 1-2 steps but that might also be an artifact of training the model with small alpha (at least I see no a priori reason for this). More experiments should be carried out here.\n- No infusion chains or generating chains are shown for any of the more complicated data distributions, this is unfortunate as I feel these would be interesting to look at.\n- The paper does a good job at evaluating the model with respect to several different metrics. The bound on the log-likelihood is nice to have as well!\n- Unfortunately the current approach does not come with any theoretical guarantees. It is unclear for what choices of alpha the procedure will work and whether there is some deeper connection to MCMC sampling or energy based models. In my eyes this does not subtract from the value of the paper but would perhaps be worth a short sentence in the conclusion.\n\nMinor points:\n- The second reference seems broken\n- Figure 3 starts at 100 epochs and, as a result, contains little information. Perhaps it would be more useful to show the complete training procedure and put the x-axis on a log-scale ?\n- The explanation regarding the convolutional networks you use makes no sense to me. You write that you use the same structure as in the \"Improved GANs\" paper which, unlike your model, generates samples from a fixed length random input. I thus suppose you don't really use a generator with 1 fully connected network followed by up-convolutions but rather have several stages of convolutions followed by a fully connected layer and then up-convolutions ?\n- The choice of parametrizing the variance via a sigmoid output unit is somewhat unusual, was there a specific reason for this choice ?\n- footnote 1 contains errors: \"This allow to\" -> \"allows to\",  \"few informations\" -> \"little information\". \"This force the network\" -> \"forces\"\n- Page 1 error: etc...\n- Page 4 error: \"operator should to learn\"\n\n[1] Markov Chain Monte Carlo and Variational Inference: Bridging the Gap, Tim Salimans and Diedrik P. Kingma and Max Welling, ICML 2015\n\n\n>>> Update <<<<\nCopied here from my response below: \n\nI believe the response of the authors clarifies all open issues. I strongly believe the paper should be accepted to the conference. The only remaining issue I have with the paper is that, as the authors acknowledge the architecture of the generator is likely highly sub-optimal and might hamper the performance of the method in the evaluation. This however does not at all subtract from any of the main points of the paper.\n\nI am thus keeping my score as a clear accept. I want to emphasize that I believe the paper should be published (just in case the review process results in some form of cut-off threshold that is high due to overall \"inflated\" review scores).\n", "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "16 Dec 2016 (modified: 20 Jan 2017)", "REVIEWER_CONFIDENCE": 5}, {"SUBSTANCE": 4, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "TITLE": "Interesting idea with lacking theoretical motivation and limited empirical evaluation", "comments": "Summary:\nThis paper introduces a heuristic approach for training a deep directed generative model, where similar to the transition operator of a Markov chain each layer samples from the same conditional distribution. Similar to optimizing a variational lower bound, the approach is to approximate the gradient by replacing the posterior over latents with an alternative distribution. However, the approximating distribution is not updated to improve the lower bound but heuristically constructed in each step. A further difference to variational optimization is that the conditional distributions are optimized greedily rather than following the gradient of the joint log-likelihood.\n\nReview:\nThe proposed approach is interesting and to me seems worth exploring more. Given that there are approaches for training the same class of models which are 1) theoretically more sound, 2) of similar computational complexity, and 3) work well in practice (e.g. Rezende & Mohamed, 2015), I am nevertheless not sure of its potential to generate impact. My bigger concern, however, is that the empirical evaluation is still quite limited.\n\nI appreciate the authors included proper estimates of the log-likelihood. This will enable and encourage future comparisons with this method on continuous MNIST. However, the authors should point out that the numbers taken from Wu et al. (2016) are not representative of the performance of a VAE. (From the paper: \u201cTherefore, the log-likelihood values we report should not be compared directly against networks which have a more flexible observation model.\u201d \u201cSuch observation models can easily achieve much higher log-likelihood scores, [\u2026].\u201d)\n\nComparisons with inpainting results using other methods would have been nice. How practical is the proposed approach compared to other approaches? Similar to the diffusion approach by Sohl-Dickstein et al. (2015), the proposed approach seems to be both efficient and effective for inpainting. Not making this a bigger point and performing the proper evaluations seems like a missed opportunity.\n\nMinor:\n\u2013\u00a0I am missing citations for \u201cordered visible dimension sampling\u201d\n\u2013\u00a0Typos and frequent incorrect use of \\citet and \\citep", "ORIGINALITY": 4, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "14 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"SUBSTANCE": 4, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "TITLE": "Using IS to evaluate log-likelihood", "comments": "", "ORIGINALITY": 4, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "DATE": "02 Dec 2016"}, {"IMPACT": 4, "SUBSTANCE": 3, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "TITLE": "time dependence of infusion chain", "comments": "", "SOUNDNESS_CORRECTNESS": 5, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "DATE": "30 Nov 2016", "APPROPRIATENESS": 3}]}
{"text": "MULTI-TASK LEARNING WITH DEEP MODEL BASED REINFORCEMENT LEARNING\n1 INTRODUCTION\nRecently, there has been a lot of success in applying neural networks to reinforcement learning, achieving super-human performance in many ATARI games (Mnih et al. (2015); Mnih et al. (2016)). Most of these algorithms are based on Q-learning, which is a model free approach to reinforcement learning. This approaches learn which actions to perform in each situation, but do not learn an explicit model of the environment. Apart from that, learning to play multiple games simultaneously remains an open problem as these approaches heavily degrade when increasing the number of tasks to learn.\nIn contrast, we present a model based approach that can learn multiple tasks simultaneously. The idea of learning predictive models has been previously proposed (Schmidhuber (2015); Santana & Hotz (2016)), but all of them focus on learning the predictive models in an unsupervised way. We propose using the reward as a means to learn a representation that captures only that which is important for the game. This also allows us to do the training in a fully supervised way. In the experiments, we show that our approach can surpass human performance simultaneously on three different games. In fact, we show that transfer learning occurs and it benefits from learning multiple tasks simultaneously.\nIn this paper, we first discuss why Q-learning fails to learn multiple tasks and what are its drawbacks. Then, we present our approach, Predictive Reinforcement Learning, as an alternative to overcome those weaknesses. In order to implement our model, we present a recurrent neural network architecture based on residual nets that is specially well suited for our task. Finally, we discuss our experimental results on several ATARI games.\n2 PREVIOUS WORK: DEEP Q-LEARNING\nIn recent years, approaches that use Deep Q-learning have achieved great success, making an important breakthrough when Mnih et al. (2015) presented a neural network architecture that was able to achieve human performance on many different ATARI games, using just the pixels in the screen as input.\nAs the name indicates, this approach revolves around the Q-function. Given a state s and an action a, Q(s, a) returns the expected future reward we will get if we perform action a in state s. Formally, the Q-function is defined in equation 1.\nQ(s, a) = Es\u2032 [ r + \u03b3max\na\u2032 Q(s\u2032, a\u2032)|s, a\n] (1)\nFor the rest of this subsection, we assume the reader is already familiar with Deep Q-learning and we discuss its main problems. Otherwise, we recommend skipping to the next section directly as none of the ideas discussed here are necessary to understand our model.\nAs the true value of the Q-function is not known, the idea of Deep Q-learning is iteratively approximating this function using a neural network1 which introduces several problems.\nFirst, the Q-values depend on the strategy the network is playing. Thus, the target output for the network given a state-action pair is not constant, since it changes as the network learns. This means that apart from learning an strategy, the network also needs to remember which strategy it is playing. This is one of the main problems when learning multiple tasks, as the networks needs to remember how it is acting on each of the different tasks. Rusu et al. (2015) and Parisotto et al. (2015) have managed to successfully learn multiple tasks using Q-learning. Both approaches follow a similar idea: an expert network learns to play a single game, while a multi-tasking network learns to copy the behavior of an expert for each different game. This means that the multi-tasking network does not iteratively approximate the Q-function, it just learns to copy the function that the single-task expert has approximated. That is why their approach works, they manage to avoid the problem of simultaneously approximating all the Q-functions, as this is done by each single task expert.\nApart from that, the network has to change the strategy very slightly at each update as drastically changing the strategy would change the Q-values a lot and cause the approximation process to diverge/slow-down. This forces the model to interact many times with the environment in order to find good strategies. This is not problematic in simulated environments like ATARI games where the simulation can easily be speed up using more computing power. Still, in real world environments, like for example robotics, this is not the case and data efficiency can be an important issue.\n3 PREDICTIVE REINFORCEMENT LEARNING\nIn order to avoid the drawbacks of Deep Q-learning, we present Predictive Reinforcement Learning (PRL). In our approach, we separate the understanding of the environment from the strategy. This has the advantage of being able to learn from different strategies simultaneously while also being able to play strategies that are completely different to the ones that it learns from. We will also argue that this approach makes generalization easier. But before we present it, we need to define what we want to solve.\n3.1 PREDICTION PROBLEM\nThe problem we want to solve is the following: given the current state of the environment and the actions we will make in the future, how is our score going to change through time?\nTo formalize this problem we introduce the following notation:\n\u2022 ai: The observation of the environment at time i. In the case of ATARI games, this corresponds to the pixels of the screen.\n\u2022 ri: The total accumulated reward at time i. In the case of ATARI games, this corresponds to the in-game score.\n\u2022 ci: The control that was performed at time i. In the case of ATARI games, this corresponds to the inputs of the ATARI controller: up, right, shoot, etc.\n1We do not explain the process, but Mnih et al. (2015) give a good explanation on how this is done.\nThen, we want to solve the following problem: For a given time i and a positive integer k, let the input to our model be an observation ai and a set of future controls ci+1, . . . ci+k. Then, we want to predict the change in score for the next k time steps, i.e. (ri+1 \u2212 ri), . . . , (ri+k \u2212 ri). Figure 1 illustrates this with an example.\nObserve that, unlike in Q-learning, our predictions do not depend on the strategy being played. The outputs only depend on the environment we are trying to predict. So, the output for a given state-actions pair is always the same or, in the case of non-deterministic environments, it comes from the same distribution.\n3.2 MODEL\nWe have defined what we want to solve but we still need to specify how to implement a model that will do it. We will use neural networks for this and we will divide it into three different networks as follows:\n\u2022 Perception: This network reads a state ai and converts it to a lower dimensional vector h0 that is used by the Prediction.\n\u2022 Prediction: For each j \u2208 {1, . . . , k}, this network reads the vector hj\u22121 and the corresponding control ci+j and generates a vector hj that will be used in the next steps of the Prediction and Valuation. Observe that this is actually a recurrent neural network.\n\u2022 Valuation: For each j \u2208 {1, . . . , k}, this network reads the current vector hj of the Prediction and predicts the difference in score between the initial time and the current one, i.e, ri+j \u2212 ri.\nFigure 2 illustrates the model. Observe that what we actually want to solve is a supervised learning problem. Thus, the whole model can be jointly trained with simple backpropagation. We will now proceed to explain each of the components in more detail.\n3.2.1 PERCEPTION\nThe Perception has to be tailored for the kind of observations the environment returns. For now, we will focus only on vision based Perception. As we said before, the idea of this network is to convert the high dimensional input to a low dimensional vector that contains only the necessary information for predicting the score. In the case of video games, it is easy to see that such vector exists. The input will consists of thousands of pixels but all we care about is the position of a few key objects, like for example, the main character or the enemies. This information can easily be\nencoded using very few neurons. In our experiments, we convert an input consisting of 28K pixels into a vector of just 100 real values.\nIn order to do this, we use deep convolutional networks. These networks have recently achieved super-human performance in very complex image recognition tasks (He et al., 2015). In fact, it has been observed that the upper layers in these models learn lower dimensional abstract representations of the input (Yosinski et al. (2015), Karpathy & Li (2015)). Given this, it seems reasonable to believe that if we use any of the successful architectures for vision, our model will be able to learn a useful representation that can be used by the Prediction.\n3.2.2 PREDICTION\nFor the Prediction network, we present a new kind of recurrent network based on residual neural networks (He et al., 2015), which is specially well suited for our task and it achieved better results than an LSTM (Hochreiter & Schmidhuber, 1997) with a similar number of parameters in our initial tests.\nResidual Recurrent Neural Network (RRNN) We define the RRNN in Figure 3 using the following notation: LN is the layer normalization function (Ba et al., 2016) which normalizes the activations to have a median of 0 and standard deviation of 1. \u201d\u00b7\u201d is the concatenation of two vectors. f can be any parameterizable and differentiable function, e.g., a multilayer perceptron.\nAs in residual networks, instead of calculating what the new state of the network should be, we calculate how it should change (ri). As shown by He et al. (2015) this prevents vanishing gradients or optimization difficulties. LN outputs a vector with mean 0 and standard deviation 1. As we\nproof2 in Observation 1, this prevents internal exploding values that may arise from repeatedly adding r to h. It also avoids the problem of vanishing gradients in saturating functions like sigmoid or hyperbolic tangent.\nObservation 1. Let x \u2208 Rn be a vector with median 0 and standard deviation 1. Then, for all 1 \u2264 i \u2264 n, we get that xi \u2264 \u221a n.\nProof. Taking into account that the median is 0 and the standard deviation is 1, simply substituting the values in the formula for the standard deviation shows the observation.\n\u03c3 = \u221a\u221a\u221a\u221a 1 n n\u2211 j=1 (xj \u2212 \u00b5)2 (4)\n1 = \u221a\u221a\u221a\u221a 1 n n\u2211 j=1 x2j (5)\n\u221a n = \u221a\u221a\u221a\u221a n\u2211 j=1 x2j (6) \u221a n \u2265 xi (7)\nThe idea behind this network is mimicking how a video game\u2019s logic works. A game has some variables (like positions or speeds of different objects) that are slightly modified at each step. Our intuition is that the network can learn a representation of these variables (h), while f learns how they are transformed at each frame. Apart from that, this model decouples memory from computation allowing to increase the complexity of f without having to increase the number of neurons in h. This is specially useful as the number of real valued neurons needed to represent the state of a game is quite small. Still, the function to move from one frame to the next can be quite complex, as it has to model all the interactions between the objects such as collisions, movements, etc.\nEven if this method looks like it may be just tailored for video games, it should work equally well for real world environments. After all, physics simulations that model the real world work in the same way, with some variables that represent the current state of the system and some equations that define how that system evolves over time.\n3.2.3 VALUATION\nThe Valuation network reads the h vector at time i + j and outputs the change in reward for that time step, i.e. ri+j \u2212 rj . Still, it is a key part of our model as it allows to decouple the representation learned by the Prediction from the reward function. For example, consider a robot in a real world environment. If the Perception learns to capture the physical properties of all surrounding objects (shape, mass, speed, etc.) and the Prediction learns to make a physical simulation of the environment, this model can be used for any possible task in that environment, only the Valuation would need to be changed.\n3.3 STRATEGY\nAs we previously said, finding an optimal strategy is a very hard problem and this part is the most complicated. So, in order to test our model in the experiments, we opted for hard-coding a strategy. There, we generate a set of future controls uniformly at random and then we pick the one that would maximize our reward, given that the probability of dying is low enough. Because of this, the games we have tried have been carefully selected such that they do not need very sophisticated and long-term strategies.\n2The bound is not tight but it is sufficient for our purposes and straightforward to prove.\nStill, our approach learns a predictive model that is independent of any strategy and this can be beneficial in two ways. First, the model can play a strategy that is completely different to the ones it learns from. Apart from that, learning a predictive model is a very hard task to over-fit. Consider a game with 10 possible control inputs and a training set where we consider the next 25 time steps. Then, there are 1025 possible control sequences. This means that every sequence we train on is unique and this forces the model to generalize. Unfortunately, there is also a downside. Our approach is not able to learn from good strategies because we test our model with many different ones in order to pick the best. Some of these strategies will be quite bad and thus, the model needs to learn what makes the difference between a good and a bad set of moves.\n4 EXPERIMENTS\n4.1 ENVIRONMENT\nOur experiments have been performed on a computer with a GeForce GTX 980 GPU and an Intel Xeon E5-2630 CPU. For the neural network, we have used the Torch7 framework and for the ATARI simulations, we have used Alewrap, which is a Lua wrapper for the Arcade Learning Environment (Bellemare et al., 2015).\n4.2 MODEL\nFor the Perception, we used a network inspired in deep residual networks (He et al., 2015). Figure 4 shows the architecture. The reason for this, is that even if the Perception is relatively shallow, when unfolding the Prediction network over time, the depth of the resulting model is over 50 layers deep.\nFor the Prediction, we use a Residual Recurrent Neural Network. Table 1 describes the network used for the f function. Finally, Table 2 illustrates the Valuation network.\n4.3 SETUP\nIn our experiments, we have trained on three different ATARI games simultaneously: Breakout, Pong and Demon Attack.\nWe preprocess the images following the same technique of Mnih et al. (2015). We take the maximum from the last 2 frames to get a single 84 \u00d7 84 black and white image for the current observation. The input to the Perception is a 4\u00d784\u00d784 tensor containing the last 4 observations. This is necessary to be able to use a feed-forward network for the Perception. If we observed a single frame,\nit would not be possible to infer the speed and direction of a moving object. Not doing this would force us to use a recurrent network on the Perception, making the training of the whole model much slower.\nIn order to train the Prediction, we unfold the network over time (25 time steps) and treat the model as a feed-forward network with shared weights. This corresponds to approximately 1.7 seconds.\nFor our Valuation, network we output two values. First, the probability that our score is higher than in the initial time step. Second, we output the probability of dying. This is trained using cross entropy loss.\nTo train the model, we use an off-line learning approach for simplicity. During training we alternate between two steps. First, generate and store data and then, train the model off-line on that data.\n4.4 GENERATING DATA\nIn order to generate the data, we store tuples (ai, C = {ci+1, . . . ci+25}, R = {ri+1 \u2212 ri, . . . ri+25 \u2212 ri}) as we are playing the game. That is, for each time i, we store the following:\n\u2022 ai: A 4\u00d7 84\u00d7 84 tensor, containing 4 consecutive black and white frames of size 84\u00d7 84 each.\n\u2022 C: For j \u2208 {i+ 1, . . . , i+ 25}, each cj is a 3 dimensional vector that encodes the control action performed at time j. The first dimension corresponds to the shoot action, the second to horizontal actions and the third to vertical actions. For example, [1,\u22121, 0] represent pressing shoot and left.\n\u2022 R: For j \u2208 {i+1, . . . , i+25}, we store a 2 dimensional binary vector rj . rj1 is 1 if we die between time i and j. rj2 is 1 if we have not lost a life and we also earn a point between time i and j.\nInitially, we have an untrained model, so at each time step, we pick an action uniformly at random and perform it. For the next iterations, we pick a k and do the following to play the game:\n1. Run the Perception network on the last 4 frames to obtain the initial vector.\n2. Generate k \u2212 1 sequences of 25 actions uniformly at random. Apart from that, take the best sequence from the previous time step and also consider it. This gives a total of k sequences. Then, for each sequence, run the Prediction and Valuation networks with the vector obtained in Step 1.\n3. Finally, pick a sequence of actions as follows. Consider only the moves that have a low enough probability of dying. From those, pick the one that has the highest probability of earning a point. If none has a high enough probability, just pick the one with the lowest probability of dying.\nWe start with k = 25 and increase it every few iterations up to k = 200. For the full details check Appendix A. In order to accelerate training, we run several games in parallel. This allows to run the Perception, Prediction and Valuation networks together with the ATARI simulation in parallel, which heavily speeds up the generation of data without any drawback.\n4.5 TRAINING\nIn the beginning, we generate 400K training cases for each of the games by playing randomly, which gives us a total of 1.2M training cases. Then, for the subsequent iterations, we generate 200K additional training cases per game (600K in total) and train again on the whole dataset. That is, at first we have 1.2M training cases, afterwards 1.8M , then 2.4M and so on.\nThe training is done in a supervised way as depicted in Figure 2b. ai and C are given as input to the network and R as target. We minimize the cross-entropy loss using mini-batch gradient descent. For the full details on the learning schedule check Appendix A.\nIn order to accelerate the process, instead of training a new network in each iteration, we keep training the model from the previous iteration. This has the effect that we would train much more on the initial training cases while the most recent ones would have an ever smaller effect as the training set grows. To avoid this, we assign a weight to each iteration and sample according to these weights during training. Every three iterations, we multiply by three the weights we assign to them. By doing this, we manage to focus on recent training cases, while still preserving the whole training set.\nObserve that we never tell our network which game it is playing, but it learns to infer it from the observation ai. Also, at each iteration, we add cases that are generated using a different neural network. So our training set contains instances generated using many different strategies.\n4.6 RESULTS\nWe have trained a model on the three games for a total of 19 iterations, which correspond to 4M time steps per game (74 hours of play at 60 Hz). Each iteration takes around two hours on our hardware. We have also trained an individual model for each game for 4M time steps. In the individual models, we reduced the length of the training such that the number of parameter updates per game is the same as in the multi-task case. Unless some kind of transfer learning occurs, one would expect some degradation in performance in the multi-task model. Figure 5 shows that not only there is no degradation in Pong and Demon Attack, but also that there is a considerable improvement in Breakout. This confirms our initial belief that our approach is specially well suited for multi-task learning.\nWe have also argued that our model can potentially play a very different strategy from the one it has observed. Table 3 shows that this is actually the case. A model that has learned only from random play is able to play at least 7 times better.\nDemon Attack\u2019s plot in Figure 5c shows a potential problem we mentioned earlier which also happens in the other two games to a lesser extent. Once the strategy is good enough, the agent dies very rarely. This causes the model to \u201dforget\u201d which actions lead to a death and makes the score oscillate.\n5 DISCUSSION\nWe have presented a novel model based approach to deep reinforcement learning. Despite not achieving state of the art results, this papers opens new lines of research showing that a model based approach can work in environments as complex as ATARI. We have also shown that it can beat human performance in three different tasks simultaneously and that it can benefit from learning multiple tasks.\nStill, the model has two areas that can be addressed in future work: long-term dependencies and the instability during training. The first, can potentially be solved by combining our approach with Q-learning based techniques. For the instability, balancing the training set or oversampling hard training cases could alleviate the problem.\nFinally, we have also presented a new kind of recurrent network which can be very useful for problems were little memory and a lot of computation is needed.\nACKNOWLEDGMENTS\nI thank Angelika Steger and Florian Meier for their hardware support in the final experiments and comments on previous versions of the paper.\n", "cats": {"READ": 1.0, "DONTREAD": 0.0}, "reviews": [{"IS_META_REVIEW": true, "comments": "This paper proposes a new approach to model based reinforcement learning and\nevaluates it on 3 ATARI games. The approach involves training a model that\npredicts a sequence of rewards and probabilities of losing a life given a\ncontext of frames and a sequence of actions. The controller samples random\nsequences of actions and executes the one that balances the probabilities of\nearning a point and losing a life given some thresholds. The proposed system\nlearns to play 3 Atari games both individually and when trained on all 3 in a\nmulti-task setup at super-human level.\n\nThe results presented in the paper are very encouraging but there are many\nad-hoc design choices in the design of the system. The paper also provides\nlittle insight into the importance of the different components of the system.\n\nMain concerns:\n- The way predicted rewards and life loss probabilities are combined is very ad-hoc.\n  The natural way to do this would be by learning a Q-value, instead different\n  rules are devised for different games.\n- Is a model actually being learned and improved? It would be good to see\n  predictions for several actions sequences from some carefully chosen start\n  states. This would be good to see both on a game where the approach works and\n  on a game where it fails. The learning progress could also be measured by\n  plotting the training loss on a fixed holdout set of sequences.\n- How important is the proposed RRNN architecture? Would it still work without\n  the residual connections? Would a standard LSTM also work?\n\nMinor points:\n- Intro, paragraph 2 - There is a lot of much earlier work on using models in\n  RL. For example, see Dyna and \"Memory approaches to reinforcement learning in\n  non-Markovian domains\" by Lin and Mitchell to name just two.\n- Section 3.1 - Minor point, but using a_i to represent the observation is\n  unusual.  Why not use o_i for observations and a_i for actions?\n- Section 3.2.2 - Notation again, r_i was used earlier to represent the\n  reward at time i but it is being used again for something else.\n- Observation 1 seems somewhat out of place. Citing the layer normalization\n  paper for the motivation is enough.\n- Section 3.2.2, second last paragraph - How is memory decoupled from\n  computation here? Models like neural turning machines accomplish this by using\n  an external memory, but this looks like an RNN with skip connections.\n- Section 3.3, second paragraph - Whether the model overfits or not depends on\n  the data. The approach doesn't work with demonstrations precisely because it\n  would overfit.\n- Figure 4 - The reference for Batch Normalization should be Ioffe and Szegedy\n  instead of Morimoto et al.\n\nOverall I think the paper has some really promising ideas and encouraging\nresults but is missing a few exploratory/ablation experiments and some polish."}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "The authors have proposed a new method for deep RL that uses model-based evaluation of states and actions and reward/life loss predictions. The evaluation, on just 3 ATari games with no comparisons to state of the art methods, is insufficient, and the method seems ad-hoc and unclear. Design choices are not clearly described or justified. The paper gives no insight as to how the different aspects of the approach relate or contribute to the overall results.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"TITLE": "Interesting ideas but investigated too superficially", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "This paper proposes a model-based reinforcement learning approach focusing on predicting future rewards given a current state and future actions. This is achieved with a \"residual recurrent neural network\", that outputs the expected reward increase at various time steps in the future. To demonstrate the usefulness of this approach, experiments are conducted on Atari games, with a simple playing strategy that consists in evaluating random sequences of moves and picking the one with highest expected reward (and low enough chance of dying). Interestingly, out of the 3 games tested, one of them exhibits better performance when the agent is trained in a multitask setting (i.e. learning all games simultaneously), hinting that transfer learning is occurring.\n\nThis submission is easy enough to read, and the reward prediction architecture looks like an original and sound idea. There are however several points that I believe prevent this work from reaching the ICLR bar, as detailed below.\n\nThe first issue is the discrepancy between the algorithm proposed in Section 3 vs its actual implementation in Section 4 (experiments): in Section 3 the output is supposed to be the expected accumulated reward in future time steps (as a single scalar), while in experiments it is instead two numbers, one which is the probability of dying and another one which is the probability of having a higher score without dying. This might work better, but it also means the idea as presented in the main body of the paper is not actually evaluated (and I guess it would not work well, as otherwise why implement it differently?)\n\nIn addition, the experimental results are quite limited: only on 3 games that were hand-picked to be easy enough, and no comparison to other RL techniques (DQN & friends). I realize that the main focus of the paper is not about exhibiting state-of-the-art results, since the policy being used is only a simple heuristic to show that the model predictions can ne used to drive decisions. That being said, I think experiments should have tried to demonstrate how to use this model to obtain better reinforcement learning algorithms: there is actually no reinforcement learning done here, since the model is a supervised algorithm, used in a manually-defined hardcoded policy. Another question that could have been addressed (but was not) in the experiments is how good these predictions are (e.g. classification error on dying probability, MSE on future rewards, ...), compared to simpler baselines.\n\nFinally, the paper's \"previous work\" section is too limited, focusing only on DQN and in particular saying very little on the topic of model-based RL. I think a paper like for instance \"Action-Conditional Video Prediction using Deep Networks in Atari Games\" should have been an obvious \"must cite\".\n\nMinor comments:\n- Notations are unusual, with \"a\" denoting a state rather than an action, this is potentially confusing and I see no reason to stray away from standard RL notations\n- Using a dot for tensor concatenation is not a great choice either, since the dot usually indicates a dot product\n- The r_i in 3.2.2 is a residual that has nothing to do with r_i the reward\n- c_i is defined as \"The control that was performed at time i\", but instead it seems to be the control performed at time i-1\n- There is a recurrent confusion between mean and median in 3.2.2\n- x should not be used in Observation 1 since the x from Fig. 3 does not go through layer normalization\n- The inequality in Observation 1 should be about |x_i|, not x_i\n- Observation 1 (with its proof) takes too much space for such a simple result\n- In 3.2.3 the first r_j should be r_i\n- The probability of dying comes out of nowhere in 3.3, since we do not know yet it will be an output of the model\n- \"Our approach is not able to learn from good strategies\" => did you mean \"*only* from good strategies\"?\n- Please say that in Fig. 4 \"fc\" means \"fully connected\"\n- It would be nice also to say how the architecture of Fig. 4 differs from the classical DQN architecture from Mnih et al (2015)\n- Please clarify r_j2 as per your answer in OpenReview comments\n- Table 3 says \"After one iteration\" but has \"PRL Iteration 2\" in it, which is confusing\n- \"Figure 5 shows that not only there is no degradation in Pong and Demon Attack\"=> to me it seems to be a bit worse, actually\n- \"A model that has learned only from random play is able to play at least 7 times better.\" => not clear where this 7 comes from\n- \"Demon Attack's plot in Figure 5c shows a potential problem we mentioned earlier\" => where was it mentioned?\n", "IS_META_REVIEW": false, "RECOMMENDATION": 4, "DATE": "17 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "Final Review", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "The term strategy is a bit ambiguous. Could you please explain more in formal terms what is strategy?\nIs r the discounted Return at time t, or the reward at time t?\nCould the author compare the method to TD learning?\nThe paper is vague and using many RL terms with different meanings without clarifying those diversions.\n\"So, the output for a given state-actions pair is always same\". Q function by definition is the value of (state, action). So as long as the policy is deterministic the output would be always same too. How's this different from Q learning?\nThe model description doesn't specify what is the policy, and it's only being mentioned in data generation part.\nWhy is it a model based approach?\nThe learning curves are only for 19 iterations, which does not give any useful information. The final results are clearly nothing comparable to previous works. The model is only being tested on three games.\n\nThe paper is vague and using informal language or sometimes misusing the common RL terms. The experiments are very small scale and even in that scenario performing very bad. It's not clear, why it's a model-based approach. ", "IS_META_REVIEW": false, "RECOMMENDATION": 2, "DATE": "16 Dec 2016 (modified: 23 Jan 2017)", "REVIEWER_CONFIDENCE": 5}, {"TITLE": "Encouraging results but the approach is too ad-hoc", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "This paper proposes a new approach to model based reinforcement learning and\nevaluates it on 3 ATARI games. The approach involves training a model that\npredicts a sequence of rewards and probabilities of losing a life given a\ncontext of frames and a sequence of actions. The controller samples random\nsequences of actions and executes the one that balances the probabilities of\nearning a point and losing a life given some thresholds. The proposed system\nlearns to play 3 Atari games both individually and when trained on all 3 in a\nmulti-task setup at super-human level.\n\nThe results presented in the paper are very encouraging but there are many\nad-hoc design choices in the design of the system. The paper also provides\nlittle insight into the importance of the different components of the system.\n\nMain concerns:\n- The way predicted rewards and life loss probabilities are combined is very ad-hoc.\n  The natural way to do this would be by learning a Q-value, instead different\n  rules are devised for different games.\n- Is a model actually being learned and improved? It would be good to see\n  predictions for several actions sequences from some carefully chosen start\n  states. This would be good to see both on a game where the approach works and\n  on a game where it fails. The learning progress could also be measured by\n  plotting the training loss on a fixed holdout set of sequences.\n- How important is the proposed RRNN architecture? Would it still work without\n  the residual connections? Would a standard LSTM also work?\n\nMinor points:\n- Intro, paragraph 2 - There is a lot of much earlier work on using models in\n  RL. For example, see Dyna and \"Memory approaches to reinforcement learning in\n  non-Markovian domains\" by Lin and Mitchell to name just two.\n- Section 3.1 - Minor point, but using a_i to represent the observation is\n  unusual.  Why not use o_i for observations and a_i for actions?\n- Section 3.2.2 - Notation again, r_i was used earlier to represent the\n  reward at time i but it is being used again for something else.\n- Observation 1 seems somewhat out of place. Citing the layer normalization\n  paper for the motivation is enough.\n- Section 3.2.2, second last paragraph - How is memory decoupled from\n  computation here? Models like neural turning machines accomplish this by using\n  an external memory, but this looks like an RNN with skip connections.\n- Section 3.3, second paragraph - Whether the model overfits or not depends on\n  the data. The approach doesn't work with demonstrations precisely because it\n  would overfit.\n- Figure 4 - The reference for Batch Normalization should be Ioffe and Szegedy\n  instead of Morimoto et al.\n\nOverall I think the paper has some really promising ideas and encouraging\nresults but is missing a few exploratory/ablation experiments and some polish.", "IS_META_REVIEW": false, "RECOMMENDATION": 4, "DATE": "15 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"DATE": "15 Dec 2016", "TITLE": "Experimental details", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2"}, {"DATE": "02 Dec 2016", "TITLE": "Where are the final results?", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1"}, {"DATE": "30 Nov 2016", "TITLE": "Pre-review questions", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3"}, {"IS_META_REVIEW": true, "comments": "This paper proposes a new approach to model based reinforcement learning and\nevaluates it on 3 ATARI games. The approach involves training a model that\npredicts a sequence of rewards and probabilities of losing a life given a\ncontext of frames and a sequence of actions. The controller samples random\nsequences of actions and executes the one that balances the probabilities of\nearning a point and losing a life given some thresholds. The proposed system\nlearns to play 3 Atari games both individually and when trained on all 3 in a\nmulti-task setup at super-human level.\n\nThe results presented in the paper are very encouraging but there are many\nad-hoc design choices in the design of the system. The paper also provides\nlittle insight into the importance of the different components of the system.\n\nMain concerns:\n- The way predicted rewards and life loss probabilities are combined is very ad-hoc.\n  The natural way to do this would be by learning a Q-value, instead different\n  rules are devised for different games.\n- Is a model actually being learned and improved? It would be good to see\n  predictions for several actions sequences from some carefully chosen start\n  states. This would be good to see both on a game where the approach works and\n  on a game where it fails. The learning progress could also be measured by\n  plotting the training loss on a fixed holdout set of sequences.\n- How important is the proposed RRNN architecture? Would it still work without\n  the residual connections? Would a standard LSTM also work?\n\nMinor points:\n- Intro, paragraph 2 - There is a lot of much earlier work on using models in\n  RL. For example, see Dyna and \"Memory approaches to reinforcement learning in\n  non-Markovian domains\" by Lin and Mitchell to name just two.\n- Section 3.1 - Minor point, but using a_i to represent the observation is\n  unusual.  Why not use o_i for observations and a_i for actions?\n- Section 3.2.2 - Notation again, r_i was used earlier to represent the\n  reward at time i but it is being used again for something else.\n- Observation 1 seems somewhat out of place. Citing the layer normalization\n  paper for the motivation is enough.\n- Section 3.2.2, second last paragraph - How is memory decoupled from\n  computation here? Models like neural turning machines accomplish this by using\n  an external memory, but this looks like an RNN with skip connections.\n- Section 3.3, second paragraph - Whether the model overfits or not depends on\n  the data. The approach doesn't work with demonstrations precisely because it\n  would overfit.\n- Figure 4 - The reference for Batch Normalization should be Ioffe and Szegedy\n  instead of Morimoto et al.\n\nOverall I think the paper has some really promising ideas and encouraging\nresults but is missing a few exploratory/ablation experiments and some polish."}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "The authors have proposed a new method for deep RL that uses model-based evaluation of states and actions and reward/life loss predictions. The evaluation, on just 3 ATari games with no comparisons to state of the art methods, is insufficient, and the method seems ad-hoc and unclear. Design choices are not clearly described or justified. The paper gives no insight as to how the different aspects of the approach relate or contribute to the overall results.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"TITLE": "Interesting ideas but investigated too superficially", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "This paper proposes a model-based reinforcement learning approach focusing on predicting future rewards given a current state and future actions. This is achieved with a \"residual recurrent neural network\", that outputs the expected reward increase at various time steps in the future. To demonstrate the usefulness of this approach, experiments are conducted on Atari games, with a simple playing strategy that consists in evaluating random sequences of moves and picking the one with highest expected reward (and low enough chance of dying). Interestingly, out of the 3 games tested, one of them exhibits better performance when the agent is trained in a multitask setting (i.e. learning all games simultaneously), hinting that transfer learning is occurring.\n\nThis submission is easy enough to read, and the reward prediction architecture looks like an original and sound idea. There are however several points that I believe prevent this work from reaching the ICLR bar, as detailed below.\n\nThe first issue is the discrepancy between the algorithm proposed in Section 3 vs its actual implementation in Section 4 (experiments): in Section 3 the output is supposed to be the expected accumulated reward in future time steps (as a single scalar), while in experiments it is instead two numbers, one which is the probability of dying and another one which is the probability of having a higher score without dying. This might work better, but it also means the idea as presented in the main body of the paper is not actually evaluated (and I guess it would not work well, as otherwise why implement it differently?)\n\nIn addition, the experimental results are quite limited: only on 3 games that were hand-picked to be easy enough, and no comparison to other RL techniques (DQN & friends). I realize that the main focus of the paper is not about exhibiting state-of-the-art results, since the policy being used is only a simple heuristic to show that the model predictions can ne used to drive decisions. That being said, I think experiments should have tried to demonstrate how to use this model to obtain better reinforcement learning algorithms: there is actually no reinforcement learning done here, since the model is a supervised algorithm, used in a manually-defined hardcoded policy. Another question that could have been addressed (but was not) in the experiments is how good these predictions are (e.g. classification error on dying probability, MSE on future rewards, ...), compared to simpler baselines.\n\nFinally, the paper's \"previous work\" section is too limited, focusing only on DQN and in particular saying very little on the topic of model-based RL. I think a paper like for instance \"Action-Conditional Video Prediction using Deep Networks in Atari Games\" should have been an obvious \"must cite\".\n\nMinor comments:\n- Notations are unusual, with \"a\" denoting a state rather than an action, this is potentially confusing and I see no reason to stray away from standard RL notations\n- Using a dot for tensor concatenation is not a great choice either, since the dot usually indicates a dot product\n- The r_i in 3.2.2 is a residual that has nothing to do with r_i the reward\n- c_i is defined as \"The control that was performed at time i\", but instead it seems to be the control performed at time i-1\n- There is a recurrent confusion between mean and median in 3.2.2\n- x should not be used in Observation 1 since the x from Fig. 3 does not go through layer normalization\n- The inequality in Observation 1 should be about |x_i|, not x_i\n- Observation 1 (with its proof) takes too much space for such a simple result\n- In 3.2.3 the first r_j should be r_i\n- The probability of dying comes out of nowhere in 3.3, since we do not know yet it will be an output of the model\n- \"Our approach is not able to learn from good strategies\" => did you mean \"*only* from good strategies\"?\n- Please say that in Fig. 4 \"fc\" means \"fully connected\"\n- It would be nice also to say how the architecture of Fig. 4 differs from the classical DQN architecture from Mnih et al (2015)\n- Please clarify r_j2 as per your answer in OpenReview comments\n- Table 3 says \"After one iteration\" but has \"PRL Iteration 2\" in it, which is confusing\n- \"Figure 5 shows that not only there is no degradation in Pong and Demon Attack\"=> to me it seems to be a bit worse, actually\n- \"A model that has learned only from random play is able to play at least 7 times better.\" => not clear where this 7 comes from\n- \"Demon Attack's plot in Figure 5c shows a potential problem we mentioned earlier\" => where was it mentioned?\n", "IS_META_REVIEW": false, "RECOMMENDATION": 4, "DATE": "17 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "Final Review", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "The term strategy is a bit ambiguous. Could you please explain more in formal terms what is strategy?\nIs r the discounted Return at time t, or the reward at time t?\nCould the author compare the method to TD learning?\nThe paper is vague and using many RL terms with different meanings without clarifying those diversions.\n\"So, the output for a given state-actions pair is always same\". Q function by definition is the value of (state, action). So as long as the policy is deterministic the output would be always same too. How's this different from Q learning?\nThe model description doesn't specify what is the policy, and it's only being mentioned in data generation part.\nWhy is it a model based approach?\nThe learning curves are only for 19 iterations, which does not give any useful information. The final results are clearly nothing comparable to previous works. The model is only being tested on three games.\n\nThe paper is vague and using informal language or sometimes misusing the common RL terms. The experiments are very small scale and even in that scenario performing very bad. It's not clear, why it's a model-based approach. ", "IS_META_REVIEW": false, "RECOMMENDATION": 2, "DATE": "16 Dec 2016 (modified: 23 Jan 2017)", "REVIEWER_CONFIDENCE": 5}, {"TITLE": "Encouraging results but the approach is too ad-hoc", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "This paper proposes a new approach to model based reinforcement learning and\nevaluates it on 3 ATARI games. The approach involves training a model that\npredicts a sequence of rewards and probabilities of losing a life given a\ncontext of frames and a sequence of actions. The controller samples random\nsequences of actions and executes the one that balances the probabilities of\nearning a point and losing a life given some thresholds. The proposed system\nlearns to play 3 Atari games both individually and when trained on all 3 in a\nmulti-task setup at super-human level.\n\nThe results presented in the paper are very encouraging but there are many\nad-hoc design choices in the design of the system. The paper also provides\nlittle insight into the importance of the different components of the system.\n\nMain concerns:\n- The way predicted rewards and life loss probabilities are combined is very ad-hoc.\n  The natural way to do this would be by learning a Q-value, instead different\n  rules are devised for different games.\n- Is a model actually being learned and improved? It would be good to see\n  predictions for several actions sequences from some carefully chosen start\n  states. This would be good to see both on a game where the approach works and\n  on a game where it fails. The learning progress could also be measured by\n  plotting the training loss on a fixed holdout set of sequences.\n- How important is the proposed RRNN architecture? Would it still work without\n  the residual connections? Would a standard LSTM also work?\n\nMinor points:\n- Intro, paragraph 2 - There is a lot of much earlier work on using models in\n  RL. For example, see Dyna and \"Memory approaches to reinforcement learning in\n  non-Markovian domains\" by Lin and Mitchell to name just two.\n- Section 3.1 - Minor point, but using a_i to represent the observation is\n  unusual.  Why not use o_i for observations and a_i for actions?\n- Section 3.2.2 - Notation again, r_i was used earlier to represent the\n  reward at time i but it is being used again for something else.\n- Observation 1 seems somewhat out of place. Citing the layer normalization\n  paper for the motivation is enough.\n- Section 3.2.2, second last paragraph - How is memory decoupled from\n  computation here? Models like neural turning machines accomplish this by using\n  an external memory, but this looks like an RNN with skip connections.\n- Section 3.3, second paragraph - Whether the model overfits or not depends on\n  the data. The approach doesn't work with demonstrations precisely because it\n  would overfit.\n- Figure 4 - The reference for Batch Normalization should be Ioffe and Szegedy\n  instead of Morimoto et al.\n\nOverall I think the paper has some really promising ideas and encouraging\nresults but is missing a few exploratory/ablation experiments and some polish.", "IS_META_REVIEW": false, "RECOMMENDATION": 4, "DATE": "15 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"DATE": "15 Dec 2016", "TITLE": "Experimental details", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2"}, {"DATE": "02 Dec 2016", "TITLE": "Where are the final results?", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1"}, {"DATE": "30 Nov 2016", "TITLE": "Pre-review questions", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3"}]}
{"text": "ON DETECTING ADVERSARIAL PERTURBATIONS\n1 INTRODUCTION\nIn the last years, machine learning and in particular deep learning methods have led to impressive performance on various challenging perceptual tasks, such as image classification (Russakovsky et al., 2015; He et al., 2016) and speech recognition (Amodei et al., 2016). Despite these advances, perceptual systems of humans and machines still differ significantly. As Szegedy et al. (2014) have shown, small but carefully directed perturbations of images can lead to incorrect classification with high confidence on artificial systems. Yet, for humans these perturbations are often visually imperceptible and do not stir any doubt about the correct classification. In fact, so called adversarial examples are crucially characterized by requiring minimal perturbations that are quasi-imperceptible to a human observer. For computer vision tasks, multiple techniques to create such adversarial examples have been developed recently. Perhaps most strikingly, adversarial examples have been shown to transfer between different network architectures, and networks trained on disjoint subsets of data (Szegedy et al., 2014). Adversarial examples have also been shown to translate to the real world (Kurakin et al., 2016), e.g., adversarial images can remain adversarial even after being printed and recaptured with a cell phone camera. Moreover, Papernot et al. (2016a) have shown that a potential attacker can construct adversarial examples for a network of unknown architecture by training an auxiliary network on similar data and exploiting the transferability of adversarial inputs.\nThe vulnerability to adversarial inputs can be problematic and even prevent the application of deep learning methods in safety- and security-critical applications. The problem is particularly severe when human safety is involved, for example in the case of perceptual tasks for autonomous driving. Methods to increase robustness against adversarial attacks have been proposed and range from augmenting the training data (Goodfellow et al., 2015) over applying JPEG compression to the input (Dziugaite et al., 2016) to distilling a hardened network from the original classifier network (Papernot et al., 2016b). However, for some recently published attacks (Carlini & Wagner, 2016), no effective counter-measures are known yet.\nIn this paper, we propose to train a binary detector network, which obtains inputs from intermediate feature representations of a classifier, to discriminate between samples from the original data set and adversarial examples. Being able to detect adversarial perturbations might help in safety- and security-critical semi-autonomous systems as it would allow disabling autonomous operation and\nrequesting human intervention (along with a warning that someone might be manipulating the system). However, it might intuitively seem very difficult to train such a detector since adversarial inputs are generated by tiny, sometimes visually imperceptible, perturbations of genuine examples. Despite this intuition, our results on CIFAR10 and a 10-class subset of ImageNet show that a detector network that achieves high accuracy in detection of adversarial inputs can be trained successfully. Moreover, while we train a detector network to detect perturbations of a specific adversary, our experiments show that detectors generalize to similar and weaker adversaries. An obvious attack against our approach would be to develop adversaries that take into account both networks, the classification and the adversarial detection network. We present one such adversary and show that we can harden the detector against such an adversary using a novel training procedure.\n2 BACKGROUND\nSince their discovery by Szegedy et al. (2014), several methods to generate adversarial examples have been proposed. Most of these methods generate adversarial examples by optimizing an image w.r.t. the linearized classification cost function of the classification network by maximizing the probability for all but the true class or minimizing the probability of the true class (e.g., (Goodfellow et al., 2015), (Kurakin et al., 2016)). The method introduced by Moosavi-Dezfooli et al. (2016b) estimates a linearization of decision boundaries between classes in image space and iteratively shifts an image towards the closest of these linearized boundaries. For more details about these methods, please refer to Section 3.1.\nSeveral approaches exist to increase a model\u2019s robustness against adversarial attacks. Goodfellow et al. (2015) propose to augment the training set with adversarial examples. At training time, they minimize the loss for real and adversarial examples, while adversarial examples are chosen to fool the current version of the model. In contrast, Zheng et al. (2016) propose to append a stability term to the objective function, which forces the model to have similar outputs for samples of the training set and their perturbed versions. This differs from data augmentation since it encourages smoothness of the model output between original and distorted samples instead of minimizing the original objective on the adversarial examples directly. Another defense-measure against certain adversarial attack methods is defensive distillation (Papernot et al., 2016b), a special form of network distillation, to train a network that becomes almost completely resistant against attacks such as the L-BFGS attack (Szegedy et al., 2014) and the fast gradient sign attack (Goodfellow et al., 2015). However, Carlini & Wagner (2016) recently introduced a novel method for constructing adversarial examples that manages to (very successfully) break many defense methods, including defensive distillation. In fact, the authors find that previous attacks were very fragile and could easily fail to find adversarial examples even when they existed. An experiment on the cross-model adversarial portability (Rozsa et al., 2016) has shown that models with higher accuracies tend to be more robust against adversarial examples, while examples that fool them are more portable to less accurate models.\nEven though the existence of adversarial examples has been demonstrated several times on many different classification tasks, the question of why adversarial examples exist in the first place and whether they are sufficiently regular to be detectable, which is studied in this paper, has remained open. Szegedy et al. (2014) speculated that the data-manifold is filled with \u201cpockets\u201d of adversarial inputs that occur with very low probability and thus are almost never observed in the test set. Yet, these pockets are dense and so an adversarial example is found virtually near every test case. The authors further speculated that the high non-linearity of deep networks might be the cause for the existence of these low-probability pockets. Later, Goodfellow et al. (2015) introduced the linear explanation: Given an input and some adversarial noise \u03b7 (subject to: ||\u03b7||\u221e < ), the dot product between a weight vector w and an adversarial input xadv = x+ \u03b7 is given by wTxadv = wTx+ wT\u03b7. The adversarial noise \u03b7 causes a neuron\u2019s activation to grow by wT\u03b7. The max-norm constraint on \u03b7 does not allow for large values in one dimension, but if x and thus \u03b7 are high-dimensional, many small changes in each dimension of \u03b7 can accumulate to a large change in a neuron\u2019s activation. The conclusion was that \u201clinear behavior in high-dimensional spaces is sufficient to cause adversarial examples\u201d.\nTanay & Griffin (2016) challenged the linear-explanation hypothesis by constructing classes of images that do not suffer from adversarial examples under a linear classifier. They also point out that if the change in activation wT\u03b7 grows linearly with the dimensionality of the problem, so does the activation\nwTx. Instead of the linear explanation, Tanay et al. provide a different explanation for the existence of adversarial examples, including a strict condition for the non-existence of adversarial inputs, a novel measure for the strength of adversarial examples and a taxonomy of different classes of adversarial inputs. Their main argument is that if a learned class boundary lies close to the data manifold, but the boundary is (slightly) tilted with respect to the manifold1, then adversarial examples can be found by perturbing points from the data manifold towards the classification boundary until the perturbed input crosses the boundary. If the boundary is only slightly tilted, the distance required by the perturbation to cross the decision-boundary is very small, leading to strong adversarial examples that are visually almost imperceptibly close to the data. Tanay et. al further argue that such situations are particularly likely to occur along directions of low variance in the data and thus speculate that adversarial examples can be considered an effect of an over-fitting phenomenon that could be alleviated by proper regularization, though it is completely unclear how to regularize neural networks accordingly.\nRecently, Moosavi-Dezfooli et al. (2016a) demonstrated that there even exist universal, imageagnostic perturbations which, when added to all data points, fool deep nets on a large fraction of ImageNet validation images. Moreover, they showed that these universal perturbations are to a certain extent also transferable between different network architectures. While this observation raises interesting questions about geometric properties and correlations of different parts of the decision boundary of deep nets, potential regularities in adversarial perturbations may also help detecting them. However, the existence of universal perturbations does not necessarily imply that the adversarial examples generated by data-dependent adversaries will be regular. Actually, Moosavi-Dezfooli et al. (2016a) show that universal perturbations are not unique and that there even exist many different universal perturbations which have little in common. This paper studies if data-dependent adversarial perturbations can nevertheless be detected reliably and answers this question affirmatively.\n3 METHODS\nIn this section, we introduce the adversarial attacks used in the experiments, propose an approach for detecting adversarial perturbations, introduce a novel adversary that aims at fooling both the classification network and the detector, and propose a training method for the detector that aims at counteracting this novel adversary.\n3.1 GENERATING ADVERSARIAL EXAMPLES\nLet x be an input image x \u2208 R3\u00d7width\u00d7height, ytrue(x) be a one-hot encoding of the true class of image x, and Jcls(x, y(x)) be the cost function of the classifier (typically cross-entropy). We briefly introduce different adversarial attacks used in the remainder of the paper.\nFast method: One simple approach to compute adversarial examples was described by Goodfellow et al. (2015). The applied perturbation is the direction in image space which yields the highest increase of the linearized cost function under `\u221e-norm. This can be achieved by performing one step in the direction of the gradient\u2019s sign with step-width \u03b5:\nxadv = x+ \u03b5 sgn(\u2207xJcls(x, ytrue(x)))\nHere, \u03b5 is a hyper-parameter governing the distance between adversarial and original image. As suggested in Kurakin et al. (2016) we also refer to this as the fast method due to its non-iterative and hence fast computation.\nBasic Iterative method (`\u221e and `2): As an extension, Kurakin et al. (2016) introduced an iterative version of the fast method, by applying it several times with a smaller step size \u03b1 and clipping all pixels after each iteration to ensure results stay in the \u03b5-neighborhood of the original image:\nxadv0 = x, x adv n+1 = Clip \u03b5 x { xadvn + \u03b1 sgn(\u2207xJcls(xadvn , ytrue(x))) } 1It is easier to imagine a linear decision-boundary - for neural networks this argument must be translated into\na non-linear equivalent of boundary tilting.\nFollowing Kurakin et al. (2016), we refer to this method as the basic iterative method and use \u03b1 = 1, i.e., we change each pixel maximally by 1. The number of iterations is set to 10. In addition to this method, which is based on the `\u221e-norm, we propose an analogous method based on the `2-norm: in each step this method moves in the direction of the (normalized) gradient and projects the adversarial examples back on the \u03b5-ball around x (points with `2 distance \u03b5 to x) if the `2 distance exceeds \u03b5:\nxadv0 = x, x adv n+1 = Project \u03b5 x { xadvn + \u03b1\n\u2207xJcls(xadvn , ytrue(x)) ||\u2207xJcls(xadvn , ytrue(x))||2 } DeepFool method: Moosavi-Dezfooli et al. (2016b) introduced the DeepFool adversary which iteratively perturbs an image xadv0 . Therefore, in each step the classifier is linearized around x adv n and the closest class boundary is determined. The minimal step according to the `p distance from xadvn to traverse this class boundary is determined and the resulting point is used as xadvn+1. The algorithm stops once xadvn+1 changes the class of the actual (not linearized) classifier. Arbitrary `p-norms can be used within DeepFool, and here we focus on the `2- and `\u221e-norm. The technical details can be found in (Moosavi-Dezfooli et al., 2016b). We would like to note that we use the variant of DeepFool presented in the first version of the paper (https://arxiv.org/abs/1511.04599v1) since we found it to be more stable compared to the variant reported in the final version.\n3.2 DETECTING ADVERSARIAL EXAMPLES\nWe augment classification networks by (relatively small) subnetworks, which branch off the main network at some layer and produce an output padv \u2208 [0, 1] which is interpreted as the probability of the input being adversarial. We call this subnetwork \u201cadversary detection network\u201d (or \u201cdetector\u201d for short) and train it to classify network inputs into being regular examples or examples generated by a specific adversary. For this, we first train the classification networks on the regular (non-adversarial) dataset as usual and subsequently generate adversarial examples for each data point of the train set using one of the methods discussed in Section 3.1. We thus obtain a balanced, binary classification dataset of twice the size of the original dataset consisting of the original data (label zero) and the corresponding adversarial examples (label one). Thereupon, we freeze the weights of the classification network and train the detector such that it minimizes the cross-entropy of padv and the labels. The details of the adversary detection subnetwork and how it is attached to the classification network are specific for datasets and classification networks. Thus, evaluation and discussion of various design choices of the detector network are provided in the respective section of the experimental results.\n3.3 DYNAMIC ADVERSARIES AND DETECTORS\nIn the worst case, an adversary might not only have access to the classification network and its gradient but also to the adversary detector and its gradient2. In this case, the adversary might potentially generate inputs to the network that fool both the classifier (i.e., get classified wrongly) and fool the detector (i.e., look innocuous). In principle, this can be achieved by replacing the cost Jcls(x, ytrue(x)) by (1 \u2212 \u03c3)Jcls(x, ytrue(x)) + \u03c3Jdet(x, 1), where \u03c3 \u2208 [0, 1] is a hyperparameter and Jdet(x, 1) is the cost (cross-entropy) of the detector for the generated x and the label one, i.e., being adversarial. An adversary maximizing this cost would thus aim at letting the classifier mis-label the input x and making the detectors output padv as small as possible. The parameter \u03c3 allows trading off these two objectives. For generating x, we propose the following extension of the basic iterative (`\u221e) method:\nxadv0 = x; x adv n+1 = Clip \u03b5 x { xadvn + \u03b1 [ (1\u2212 \u03c3) sgn(\u2207xJcls(xadvn , ytrue(x))) + \u03c3 sgn(\u2207xJdet(xadvn , 1)) ]} Note that we found a smaller \u03b1 to be essential for this method to work; more specifically, we use \u03b1 = 0.25. Since such an adversary can adapt to the detector, we call it a dynamic adversary. To\n2We would like to emphasize that is a stronger assumption than granting the adversary access to only the original classifier\u2019s predictions and gradients since the classifier\u2019s predictions need often be presented to a user (and thus also to an adversary). The same is typically not true for the predictions of the adversary detector as they will only be used internally.\ncounteract dynamic adversaries, we propose dynamic adversary training, a method for hardening detectors against dynamic adversaries. Based on the approach proposed by Goodfellow et al. (2015), instead of precomputing a dataset of adversarial examples, we compute the adversarial examples on-the-fly for each mini-batch and let the adversary modify each data point with probability 0.5. Note that a dynamic adversary will modify a data point differently every time it encounters the data point since it depends on the detector\u2019s gradient and the detector changes over time. We extend this approach to dynamic adversaries by employing a dynamic adversary, whose parameter \u03c3 is selected uniform randomly from [0, 1], for generating the adversarial data points during training. By training the detector in this way, we implicitly train it to resist dynamic adversaries for various values of \u03c3. In principle, this approach bears the risk of oscillation and unlearning for \u03c3 > 0 since both, the detector and adversary, adapt to each other (i.e., there is no fixed data distribution). In practice, however, we found this approach to converge stably without requiring careful tuning of hyperparameters.\n4 EXPERIMENTAL RESULTS\nIn this section, we present results on the detectability of adversarial perturbations on the CIFAR10 dataset (Krizhevsky, 2009), both for static and dynamic adversaries. Moreover, we investigate whether adversarial perturbations are also detectable in higher-resolution images based on a subset of the ImageNet dataset (Russakovsky et al., 2015).\n4.1 CIFAR10\nWe use a 32-layer Residual Network (He et al., 2016, ResNet) as classifier. The structure of the network is shown in Figure 1. The network has been trained for 100 epochs with stochastic gradient descent and momentum on 45000 data points from the train set. The momentum term was set to 0.9 and the initial learning rate was set to 0.1, reduced to 0.01 after 41 epochs, and further reduced to 0.001 after 61 epochs. After each epoch, the network\u2019s performance on the validation data (the remaining 5000 data points from the train set) was determined. The network with maximal performance on the validation data was used in the subsequent experiments (with all tunable weights being fixed). This network\u2019s accuracy on non-adversarial test data is 91.3%. We attach an adversary detection subnetwork (called \u201cdetector\u201d below) to the ResNet. The detector is a convolutional neural network using batch normalization (Ioffe & Szegedy, 2015) and rectified linear units. In the experiments, we investigate different positions where the detector can be attached (see also Figure 1).\n4.1.1 STATIC ADVERSARIES\nIn this subsection, we investigate a static adversary, i.e., an adversary that only has access to the classification network but not to the detector. The detector was trained for 20 epochs on 45000 data points from the train set and their corresponding adversarial examples using the Adam optimizer (Kingma & Ba, 2015) with a learning rate of 0.0001 and \u03b21 = 0.99, \u03b22 = 0.999. The remaining 5000 data points from the CIFAR10 train set are used as validation data and used for model selection. The detector was attached to position AD(2) (see Figure 1) except for the DeepFool-based adversaries where the detector was attached to AD(4); see below for a discussion. For the \u201cFast\u201d and \u201cIterative\u201d adversaries, the parameter \u03b5 from Section 3.1 was chosen from [1, 2, 3, 4] for `\u221e-based methods and from [20, 40, 60, 80] for `2-based methods; larger values of \u03b5 generally result in reduced accuracy of the classifier but increased detectability. For the \u201cIterative\u201d method with `2-norm, we used \u03b1 = 20, i.e., in each iteration we make a step of `2 distance 20. Please note that these values of \u03b5 are based on assuming a range of [0, 255] per color channel of the input.\nFigure 2 (left) compares the detectability3 of different adversaries. In general, points in the lower left of the plot correspond to stronger adversaries because their adversarial examples are harder to detect and at the same time fool the classifier on most of the images. Detecting adversarial examples works surprisingly well given that no differences are perceivable to humans for all shown settings: the detectability is above 80% for all adversaries which decrease classification accuracy below 30% and above 90% for adversaries which decrease classification accuracy below 10%. Comparing the different adversaries, the \u201cFast\u201d adversary can generally be considered as a weak adversary, the DeepFool based methods as relatively strong adversaries, and the \u201cIterative\u201d method being somewhere in-between. Moreover, the methods based on the `2-norm are generally slightly stronger than their `\u221e-norm counter-parts.\nFigure 2 (right) compares the detectability of different adversaries for detectors attached at different points to the classification network. \u03b5 was chosen minimal under the constraint that the classification accuracy is below 30%. For the \u201cFast\u201d and \u201cIterative\u201d adversaries, the attachment position AD(2) works best, i.e., attaching to a middle layer where more abstract features are already extracted but still the full spatial resolution is maintained. For the DeepFool methods, the general pattern is similar except for AD(4), which works best for these adversaries.\nFigure 3 illustrates the generalizability of trained detectors for the same adversary with different choices of \u03b5: while a detector trained for large \u03b5 does not generalize well to small \u03b5, the other direction works reasonably well. Figure 4 shows the generalizability of detectors trained for one adversary when tested on data from other adversaries (\u03b5 was chosen again minimal under the constraint that the\n3Detectability refers to the accuracy of the detector. The detectability on the test data is calculated as follows: for every test sample, a corresponding adversarial example is generated. The original and the corresponding adversarial examples form a joint test set (twice the size of the original test set). This test set is shuffled and the detector is evaluated on this dataset. Original and corresponding adversarial example are thus processed independently.\nclassification accuracy is below 30%): we can see that detectors generalize well between `\u221e- and `2-norm based variants of the same approach. Moreover, detectors trained on the stronger \u201cIterative\u201d adversary generalize well to the weaker \u201cFast\u201d adversary but not vice versa. Detectors trained for the DeepFool-based methods do not generalize well to other adversaries; however, detectors trained for the \u201cIterative\u201d adversaries generalize relatively well to the DeepFool adversaries.\n4.1.2 DYNAMIC ADVERSARIES\nIn this section, we evaluate the robustness of detector networks to dynamic adversaries (see Section 3.3). For this, we evaluate the detectability of dynamic adversaries for \u03c3 \u2208 {0.0, 0.1, . . . , 1.0}. We use the same optimizer and detector network as in Section 4.1.1. When evaluating the detectability of dynamic adversaries with \u03c3 close to 1, we need to take into account that the adversary might choose to solely focus on fooling the detector, which is trivially achieved by leaving the input unmodified. Thus, we ignore adversarial examples that do not cause a misclassification in the evaluation of the detector and evaluate the detector\u2019s accuracy on regular data versus the successful adversarial examples. Figure 5 shows the results of a dynamic adversary with \u03b5 = 1 against a static detector, which was trained to only detect static adversaries, and a dynamic detector, which was explicitly trained to resist dynamic adversaries. As can be seen, the static detector is not robust to dynamic adversaries since for certain values of \u03c3, namely \u03c3 = 0.3 and \u03c3 = 0.4, the detectability is close to\nchance level while the predictive performance of the classifier is severely reduced to less than 30% accuracy. A dynamic detector is considerably more robust and achieves a detectability of more than 70% for any choice of \u03c3.\n4.2 10-CLASS IMAGENET\nIn this section, we report results for static adversaries on a subset of ImageNet consisting of all data from ten randomly selected classes4. The motivation for this section is to investigate whether adversarial perturbations can be detected in higher-resolution images and for other network architectures than residual networks. We limit the experiment to ten classes in order to keep the computational resources required for computing the adversarial examples small and avoid having too similar classes which would oversimplify the task for the adversary. We use a pretrained VGG16 (Simonyan & Zisserman, 2015) as classification network and add a layer before the softmax which selects only the 10 relevant class entries from the logits vector. Based on preliminary experiments, we attach the detector network after the fourth max-pooling layer. The detector network consists of a sequence of five 3x3 convolutions with 196 feature maps each using batch-normalization and rectified linear units, followed by a 1x1 convolution which maps onto the 10 classes, global-average pooling, and a softmax layer. An additional 2x2 max-pooling layer is added after the first convolution. Note that we did not tune the specific details of the detector network; other topologies might perform better than the results reported below. When applicable, we vary \u03b5 \u2208 [2, 4, 6] for `\u221e-based methods and \u03b5 \u2208 [400, 800, 1200] for `2. Moreover, we limit changes of the DeepFool adversaries to an `\u221e distance of 6 since the adversary would otherwise sometimes generate distortions which are clearly perceptible. We train the detector for 500 epochs using the Adam optimizer with a learning rate of 0.0001 and \u03b21 = 0.99, \u03b22 = 0.999.\nFigure 6 compares the detectability of different static adversaries. All adversaries fail to decrease predictive accuracy of the classifier below the chance level of 0.1 (note that predictive accuracy refers to the accuracy on the 10-class problem not on the full 1000-class problem) for the given values of \u03b5. Nevertheless, detectability is 85% percent or more with the exception of the \u201cIterative\u201d `2-based adversary with \u03b5 = 400. For this adversary, the detector only reaches chance level. Other choices of the detector\u2019s attachment depth, internal structure, or hyperparameters of the optimizer might achieve\n4The synsets of the selected classes are: palace; joystick; bee; dugong, Dugong dugon; cardigan; modem; confectionery, confectionary, candy store; valley, vale; Persian cat; stone wall. Classes were selected by randomly drawing 10 ILSVRC2012 Synset-IDs (i.e. integers from [1, 1000]), using the randint function of the python-package numpy after initializing numpy\u2019s random number generator seed with 0. This results in a train set of 10000 images, a validation set of 2848 images, and a test set (from ImageNet\u2019s validation data) of 500 images.\nbetter results; however, this failure case emphasizes that the detector has to detect very subtle patterns and the optimizer might get stuck in bad local optima or plateaus.\nFigure 7 illustrates the transferability of the detector between different values of \u03b5. The results are roughly analogous to the results on CIFAR10 in Section 4.1.1: detectors trained for an adversary for a small value of \u03b5 work well for the same adversary with larger \u03b5 but not vice versa. Note that a detector trained for the \u201cIterative\u201d `2-based adversary with \u03b5 = 1200 can detect the changes of the same adversary with \u03b5 = 400 with 78% accuracy; this emphasizes that this adversary is not principally undetectable but that rather the optimization of a detector for this setting is difficult. Figure 8 shows the transferability between adversaries: transferring the detector works well between similar adversaries such as between the two DeepFool adversaries and between the Fast and Iterative adversary based on the `\u221e distance. Moreover, detectors trained for DeepFool adversaries work well on all other adversaries. In summary, transferability is not symmetric and typically works best between similar adversaries and from stronger to weaker adversary.\n5 DISCUSSION\nWhy can tiny adversarial perturbations be detected that well? Adopting the boundary tilting perspective of Tanay & Griffin (2016), strong adversarial examples occur in situations in which classification boundaries are tilted against the data manifold such that they lie close and nearly parallel to the data manifold. A detector could (potentially) identify adversarial examples by detecting inputs which are slightly off the data manifold\u2019s center in the direction of a nearby class boundary. Thus, the detector can focus on detecting inputs which move away from the data manifold in a certain direction, namely one of the directions to a nearby class boundary (the detector does not have explicit\nknowledge of class boundaries but it might learn about their direction implicitly from the adversarial training data). However, training a detector which captures these directions in a model with small capacity and generalizes to unseen data requires certain regularities in adversarial perturbations. The results of Moosavi-Dezfooli et al. (2016a) suggest that there may exist regularities in the adversarial perturbations since universal perturbations exist. However, these perturbations are not unique and data-dependent adversaries might potentially choose among many different possible perturbations in a non-regular way, which would be hard to detect. Our positive results on detectability suggest that this is not the case for the tested adversaries. Thus, our results are somewhat complementary to Moosavi-Dezfooli et al. (2016a): while they show that universal, image-agnostic perturbations exist, we show that image-dependent perturbations are sufficiently regular to be detectable. Whether a detector generalizes over different adversaries depends mainly on whether the adversaries choose among many different possible perturbations in a consistent way.\nWhy is the joint classifier/detector system harder to fool? For a static detector, there might be areas which are adversarial to both classifier and detector; however, this will be a (small) subset of the areas which are adversarial to the classifier alone. Nevertheless, results in Section 4.1.2 show that such a static detector can be fooled along with the classifier. However, a dynamic detector is considerably harder to fool: on the one hand, it might further reduce the number of areas which are both adversarial to classifier and detector. On the other hand, the areas which are adversarial to the detector might become increasingly non-regular and difficult to find by gradient descent-based adversaries.\n6 CONCLUSION AND OUTLOOK\nIn this paper, we have shown empirically that adversarial examples can be detected surprisingly well using a detector subnetwork attached to the main classification network. While this does not directly allow classifying adversarial examples correctly, it allows mitigating adversarial attacks against machine learning systems by resorting to fallback solutions, e.g., a face recognition might request human intervention when verifying a person\u2019s identity and detecting a potential adversarial attack. Moreover, being able to detect adversarial perturbations may in the future enable a better understanding of adversarial examples by applying network introspection to the detector network. Furthermore, the gradient propagated back through the detector may be used as a source of regularization of the classifier against adversarial examples. We leave this to future work. Additional future work will be developing stronger adversaries that are harder to detect by adding effective randomization which would make selection of adversarial perturbations less regular. Finally, developing methods for training detectors explicitly such that they can detect many different kinds of attacks reliably at the same time would be essential for safety- and security-related applications.\nACKNOWLEDGMENTS\nWe would like to thank Michael Herman and Michael Pfeiffer for helpful discussions and their feedback on drafts of this article. Moreover, we would like to thank the developers of Theano (The Theano Development Team, 2016), keras (https://keras.io), and seaborn (http:// seaborn.pydata.org/).\n", "cats": {"READ": 1.0, "DONTREAD": 0.0}, "reviews": [{"IS_META_REVIEW": true, "comments": "I reviewed the manuscript on December 5th.\n\nSummary:\nThe authors investigate the phenomenon of adversarial perturbations and ask whether one may build a system to independently detect an adversarial data point -- if one could detect an adversarial example, then might prevent a machine from automatically processing it. Importantly, the authors investigate whether it is possible to build an adversarial detector which is resilient to adversarial examples built against *both* the classifier and the detector. Their results suggest that training a detector in this more difficult setting still yields gains but does not entirely resolve the problem of detecting adversarial examples.\n\nMajor comments:\n\nThe authors describe a novel approach for dealing with adversarial examples from a security standpoint -- namely, build an independent system to detect the adversary so a human might intervene in those cases. \n\nA potential confound of this approach is that an adversary might respond by constructing adversarial examples to fool *both* the original classifier and the new detector. If that were possible, then this approach is moot since an attacker could always outwit the original system. To their credit, the authors show that building a 'dynamic' detector to detect adversarial examples but also be resilient to an adversary mitigates this potential escalation (worse case from 55% to 70% detection rate). Even though the 'dynamic' detector  demonstrates positive gains, I am concerned about overall scores. Detecting adversarial examples at this rate would not be a reliable security procedure.\n\nMy second comment is about 'model transferability'. My definition of 'model transferability' is different then the one used in the paper. My definition means that one constructs an adversarial example on one network and measures how well the adversarial examples attack a second trained model -- where the second model has been trained with different initial conditions. (The author's definition of 'transferability' is based on seeing how well the detector generalizes across training methods). 'Model transferability' (per my definition) is quite important because it measures how general an adversarial example is across all models -- and not specific to a given trained model. Different methods have different levels of 'model transferability' (Kurakin et al, 2016) and I am concerned how well the detector they built would be able to detect adversarial examples across *all models* and not just the trained model in question. In other words, a good detector would be able to detect adversarial examples from any network and not just one particularly trained network. This question seems largely unaddressed in this paper but perhaps I missed some subtle point in their descriptions.\n\nMinor comments:\n\nIf there were any points in the bottom-left of the Figure 2 left, then this would be very important to see -- perhaps move the legend to highlight if the area contains no points.\n\n- X-axis label is wrong in Figure 2 right.\n\nMeasure the transferability of the detector?\n\n- How is \\sigma labeled on Figure 5?\n\n- Whenever an image is constructed to be an 'adversary', has the image actually been tested to see if it is adversarial? In other words, does the adversarial image actually result in a misclassification by the original network?"}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "The paper explores the automatic detection of adversarial examples by training a classifier to recognize them. This is an interesting direction, even though they are obviously concerns about training an adversary to circumvent this model. Nonetheless, the experimental results presented in the paper are of interest to the ICLR audience. Many of the initial reviewer comments appear to be appropriately addressed in the revision of the paper.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"DATE": "10 Jan 2017", "TITLE": "New revision of paper", "IS_META_REVIEW": false, "comments": "We have uploaded a new revision of the paper in which we have tried to address the reviewer comments. Here is a more detailed changelog:\n\n* Fixed a bug in the ImageNet experiment: we originally applied the softmax operator twice (once before and once after selecting the ten target classes). This did not affect the accuracy of the classification network but made the network harder to fool by adversaries for similar reasons as in the ``defensive distillation'' approach. We have corrected the issue in the updated version of the paper by applying softmax only after selection the ten target classes. To briefly summarize the corrected results: adversaries remain detectable with an accuracy of at least 85% (with the same exception as before, the basic iterative l2-based adversary for epsilon=400). More details are contained in the updated Section 4.2. Sorry for this error in the first revision.\n* Fixed wrong resolution in Figure 1 (16x16 instead of 8x8). Thanks to AnonReviewer3 for noting this.\n* Input range specified to be [0, 255] (Section 4.1.1). Thanks to AnonReviewer1 for requesting clarification on this.\n* Clarified computation of adversarial detectability (footnote in Section 4.1.1).\n* We discuss briefly that dynamic adversaries are based on stronger assumptions than static adversaries (footnote in Section 3.3)\n* Clarified that we did use version 1 of DeepFool (Section 3.1)\n* Fixed x-axis label in Figure 2 (right). Thanks to AnonReviewer2 for noting this.\n* Moved legend in Figure 2 (left) to upper right corner based on suggestion of AnonReviewer2.\n* Clarified choices of \\sigma in Figure 5\n* Adding more details about the dynamic adversary training method. \n", "OTHER_KEYS": "Jan Hendrik Metzen"}, {"TITLE": "Good paper with significant novelty", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "This paper proposes a new idea to help defending adversarial examples by training a complementary classifier to detect them. The results of the paper show that adversarial examples in fact can be easily detected. Moreover, such detector generalizes well to other similar or weaker adversarial examples. The idea of this paper is simple but non-trivial. While no final scheme is proposed in the paper how this idea can help in building defensive systems, it actually provides a potential new direction. Based on its novelty, I suggest an acceptance.\n\nMy main concern of this paper is about its completeness. No effective method is reported in the paper to defend the dynamic adversaries. It could be difficult to do so, but rather the paper doesn\u2019t seem to put much effort to investigate this part. How difficult it is to defend the dynamic adversaries is an important and interesting question following the conclusions of this paper. Such investigation may essentially help improve our understanding of adversarial examples.\nThat being said, the novelty of this paper is still significant.\n\nMinor comment:\nThe paper needs to improve its clarity. Some important details are skipped in the paper. For example, the paper should provide more details about the dynamic adversaries and the dynamic adversary training method. \n\n", "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "17 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "Nicely written experimental paper, making the next step in the adversarial contest.", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "This paper explores an important angle to adversarial examples: the detection of adversarial images and their utilization for trainig more robust networks.\n\nThis takes the competition between adversaries and models to a new level. The paper presents appealing evidence for the feasibility of robustifying networks by employing the a detector subnetwork that is trained particularly for the purpose of detecting the adversaries in a terget manner rather than just making the networks themselves robust to adversarial examples.\n\nThe jointly trained primary/detector system is evaluated in various scenarios including the cases when the adversary generator has access to the model and those where they are generated in a generic way.\n\nThe results of the paper show good improvements with the approach and present well motived thorough analyses to back the main message. The writing is clear and concise.\n", "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "15 Dec 2016 (modified: 16 Dec 2016)", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "Official review", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "I reviewed the manuscript on December 5th.\n\nSummary:\nThe authors investigate the phenomenon of adversarial perturbations and ask whether one may build a system to independently detect an adversarial data point -- if one could detect an adversarial example, then might prevent a machine from automatically processing it. Importantly, the authors investigate whether it is possible to build an adversarial detector which is resilient to adversarial examples built against *both* the classifier and the detector. Their results suggest that training a detector in this more difficult setting still yields gains but does not entirely resolve the problem of detecting adversarial examples.\n\nMajor comments:\n\nThe authors describe a novel approach for dealing with adversarial examples from a security standpoint -- namely, build an independent system to detect the adversary so a human might intervene in those cases. \n\nA potential confound of this approach is that an adversary might respond by constructing adversarial examples to fool *both* the original classifier and the new detector. If that were possible, then this approach is moot since an attacker could always outwit the original system. To their credit, the authors show that building a 'dynamic' detector to detect adversarial examples but also be resilient to an adversary mitigates this potential escalation (worse case from 55% to 70% detection rate). Even though the 'dynamic' detector  demonstrates positive gains, I am concerned about overall scores. Detecting adversarial examples at this rate would not be a reliable security procedure.\n\nMy second comment is about 'model transferability'. My definition of 'model transferability' is different then the one used in the paper. My definition means that one constructs an adversarial example on one network and measures how well the adversarial examples attack a second trained model -- where the second model has been trained with different initial conditions. (The author's definition of 'transferability' is based on seeing how well the detector generalizes across training methods). 'Model transferability' (per my definition) is quite important because it measures how general an adversarial example is across all models -- and not specific to a given trained model. Different methods have different levels of 'model transferability' (Kurakin et al, 2016) and I am concerned how well the detector they built would be able to detect adversarial examples across *all models* and not just the trained model in question. In other words, a good detector would be able to detect adversarial examples from any network and not just one particularly trained network. This question seems largely unaddressed in this paper but perhaps I missed some subtle point in their descriptions.\n\nMinor comments:\n\nIf there were any points in the bottom-left of the Figure 2 left, then this would be very important to see -- perhaps move the legend to highlight if the area contains no points.\n\n- X-axis label is wrong in Figure 2 right.\n\nMeasure the transferability of the detector?\n\n- How is \\sigma labeled on Figure 5?\n\n- Whenever an image is constructed to be an 'adversary', has the image actually been tested to see if it is adversarial? In other words, does the adversarial image actually result in a misclassification by the original network?", "IS_META_REVIEW": false, "RECOMMENDATION": 5, "DATE": "13 Dec 2016", "REVIEWER_CONFIDENCE": 3}, {"DATE": "03 Dec 2016", "TITLE": "Question on network architecture", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3"}, {"DATE": "02 Dec 2016", "TITLE": "The test accuracy of the detector, and a problme about the magnitude of perturbation", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1"}, {"DATE": "30 Nov 2016", "TITLE": "Adversary to the Adversary-Detector", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2"}, {"IS_META_REVIEW": true, "comments": "I reviewed the manuscript on December 5th.\n\nSummary:\nThe authors investigate the phenomenon of adversarial perturbations and ask whether one may build a system to independently detect an adversarial data point -- if one could detect an adversarial example, then might prevent a machine from automatically processing it. Importantly, the authors investigate whether it is possible to build an adversarial detector which is resilient to adversarial examples built against *both* the classifier and the detector. Their results suggest that training a detector in this more difficult setting still yields gains but does not entirely resolve the problem of detecting adversarial examples.\n\nMajor comments:\n\nThe authors describe a novel approach for dealing with adversarial examples from a security standpoint -- namely, build an independent system to detect the adversary so a human might intervene in those cases. \n\nA potential confound of this approach is that an adversary might respond by constructing adversarial examples to fool *both* the original classifier and the new detector. If that were possible, then this approach is moot since an attacker could always outwit the original system. To their credit, the authors show that building a 'dynamic' detector to detect adversarial examples but also be resilient to an adversary mitigates this potential escalation (worse case from 55% to 70% detection rate). Even though the 'dynamic' detector  demonstrates positive gains, I am concerned about overall scores. Detecting adversarial examples at this rate would not be a reliable security procedure.\n\nMy second comment is about 'model transferability'. My definition of 'model transferability' is different then the one used in the paper. My definition means that one constructs an adversarial example on one network and measures how well the adversarial examples attack a second trained model -- where the second model has been trained with different initial conditions. (The author's definition of 'transferability' is based on seeing how well the detector generalizes across training methods). 'Model transferability' (per my definition) is quite important because it measures how general an adversarial example is across all models -- and not specific to a given trained model. Different methods have different levels of 'model transferability' (Kurakin et al, 2016) and I am concerned how well the detector they built would be able to detect adversarial examples across *all models* and not just the trained model in question. In other words, a good detector would be able to detect adversarial examples from any network and not just one particularly trained network. This question seems largely unaddressed in this paper but perhaps I missed some subtle point in their descriptions.\n\nMinor comments:\n\nIf there were any points in the bottom-left of the Figure 2 left, then this would be very important to see -- perhaps move the legend to highlight if the area contains no points.\n\n- X-axis label is wrong in Figure 2 right.\n\nMeasure the transferability of the detector?\n\n- How is \\sigma labeled on Figure 5?\n\n- Whenever an image is constructed to be an 'adversary', has the image actually been tested to see if it is adversarial? In other words, does the adversarial image actually result in a misclassification by the original network?"}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "The paper explores the automatic detection of adversarial examples by training a classifier to recognize them. This is an interesting direction, even though they are obviously concerns about training an adversary to circumvent this model. Nonetheless, the experimental results presented in the paper are of interest to the ICLR audience. Many of the initial reviewer comments appear to be appropriately addressed in the revision of the paper.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"DATE": "10 Jan 2017", "TITLE": "New revision of paper", "IS_META_REVIEW": false, "comments": "We have uploaded a new revision of the paper in which we have tried to address the reviewer comments. Here is a more detailed changelog:\n\n* Fixed a bug in the ImageNet experiment: we originally applied the softmax operator twice (once before and once after selecting the ten target classes). This did not affect the accuracy of the classification network but made the network harder to fool by adversaries for similar reasons as in the ``defensive distillation'' approach. We have corrected the issue in the updated version of the paper by applying softmax only after selection the ten target classes. To briefly summarize the corrected results: adversaries remain detectable with an accuracy of at least 85% (with the same exception as before, the basic iterative l2-based adversary for epsilon=400). More details are contained in the updated Section 4.2. Sorry for this error in the first revision.\n* Fixed wrong resolution in Figure 1 (16x16 instead of 8x8). Thanks to AnonReviewer3 for noting this.\n* Input range specified to be [0, 255] (Section 4.1.1). Thanks to AnonReviewer1 for requesting clarification on this.\n* Clarified computation of adversarial detectability (footnote in Section 4.1.1).\n* We discuss briefly that dynamic adversaries are based on stronger assumptions than static adversaries (footnote in Section 3.3)\n* Clarified that we did use version 1 of DeepFool (Section 3.1)\n* Fixed x-axis label in Figure 2 (right). Thanks to AnonReviewer2 for noting this.\n* Moved legend in Figure 2 (left) to upper right corner based on suggestion of AnonReviewer2.\n* Clarified choices of \\sigma in Figure 5\n* Adding more details about the dynamic adversary training method. \n", "OTHER_KEYS": "Jan Hendrik Metzen"}, {"TITLE": "Good paper with significant novelty", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "This paper proposes a new idea to help defending adversarial examples by training a complementary classifier to detect them. The results of the paper show that adversarial examples in fact can be easily detected. Moreover, such detector generalizes well to other similar or weaker adversarial examples. The idea of this paper is simple but non-trivial. While no final scheme is proposed in the paper how this idea can help in building defensive systems, it actually provides a potential new direction. Based on its novelty, I suggest an acceptance.\n\nMy main concern of this paper is about its completeness. No effective method is reported in the paper to defend the dynamic adversaries. It could be difficult to do so, but rather the paper doesn\u2019t seem to put much effort to investigate this part. How difficult it is to defend the dynamic adversaries is an important and interesting question following the conclusions of this paper. Such investigation may essentially help improve our understanding of adversarial examples.\nThat being said, the novelty of this paper is still significant.\n\nMinor comment:\nThe paper needs to improve its clarity. Some important details are skipped in the paper. For example, the paper should provide more details about the dynamic adversaries and the dynamic adversary training method. \n\n", "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "17 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "Nicely written experimental paper, making the next step in the adversarial contest.", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "This paper explores an important angle to adversarial examples: the detection of adversarial images and their utilization for trainig more robust networks.\n\nThis takes the competition between adversaries and models to a new level. The paper presents appealing evidence for the feasibility of robustifying networks by employing the a detector subnetwork that is trained particularly for the purpose of detecting the adversaries in a terget manner rather than just making the networks themselves robust to adversarial examples.\n\nThe jointly trained primary/detector system is evaluated in various scenarios including the cases when the adversary generator has access to the model and those where they are generated in a generic way.\n\nThe results of the paper show good improvements with the approach and present well motived thorough analyses to back the main message. The writing is clear and concise.\n", "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "15 Dec 2016 (modified: 16 Dec 2016)", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "Official review", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "I reviewed the manuscript on December 5th.\n\nSummary:\nThe authors investigate the phenomenon of adversarial perturbations and ask whether one may build a system to independently detect an adversarial data point -- if one could detect an adversarial example, then might prevent a machine from automatically processing it. Importantly, the authors investigate whether it is possible to build an adversarial detector which is resilient to adversarial examples built against *both* the classifier and the detector. Their results suggest that training a detector in this more difficult setting still yields gains but does not entirely resolve the problem of detecting adversarial examples.\n\nMajor comments:\n\nThe authors describe a novel approach for dealing with adversarial examples from a security standpoint -- namely, build an independent system to detect the adversary so a human might intervene in those cases. \n\nA potential confound of this approach is that an adversary might respond by constructing adversarial examples to fool *both* the original classifier and the new detector. If that were possible, then this approach is moot since an attacker could always outwit the original system. To their credit, the authors show that building a 'dynamic' detector to detect adversarial examples but also be resilient to an adversary mitigates this potential escalation (worse case from 55% to 70% detection rate). Even though the 'dynamic' detector  demonstrates positive gains, I am concerned about overall scores. Detecting adversarial examples at this rate would not be a reliable security procedure.\n\nMy second comment is about 'model transferability'. My definition of 'model transferability' is different then the one used in the paper. My definition means that one constructs an adversarial example on one network and measures how well the adversarial examples attack a second trained model -- where the second model has been trained with different initial conditions. (The author's definition of 'transferability' is based on seeing how well the detector generalizes across training methods). 'Model transferability' (per my definition) is quite important because it measures how general an adversarial example is across all models -- and not specific to a given trained model. Different methods have different levels of 'model transferability' (Kurakin et al, 2016) and I am concerned how well the detector they built would be able to detect adversarial examples across *all models* and not just the trained model in question. In other words, a good detector would be able to detect adversarial examples from any network and not just one particularly trained network. This question seems largely unaddressed in this paper but perhaps I missed some subtle point in their descriptions.\n\nMinor comments:\n\nIf there were any points in the bottom-left of the Figure 2 left, then this would be very important to see -- perhaps move the legend to highlight if the area contains no points.\n\n- X-axis label is wrong in Figure 2 right.\n\nMeasure the transferability of the detector?\n\n- How is \\sigma labeled on Figure 5?\n\n- Whenever an image is constructed to be an 'adversary', has the image actually been tested to see if it is adversarial? In other words, does the adversarial image actually result in a misclassification by the original network?", "IS_META_REVIEW": false, "RECOMMENDATION": 5, "DATE": "13 Dec 2016", "REVIEWER_CONFIDENCE": 3}, {"DATE": "03 Dec 2016", "TITLE": "Question on network architecture", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3"}, {"DATE": "02 Dec 2016", "TITLE": "The test accuracy of the detector, and a problme about the magnitude of perturbation", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1"}, {"DATE": "30 Nov 2016", "TITLE": "Adversary to the Adversary-Detector", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2"}]}
{"text": "RECURRENT COEVOLUTIONARY FEATURE EMBEDDING PROCESSES FOR RECOMMENDATION\n1 INTRODUCTION\nE-commerce platforms and social service websites, such as Reddit, Amazon, and Netflix, attracts thousands of users every second. Effectively recommending the appropriate service items to users is a fundamentally important task for these online services. It can significantly boost the user activities on these sites and leads to increased product purchases and advertisement clicks.\nThe interactions between users and items play a critical role in driving the evolution of user interests and item features. For example, for music streaming services, a long-time fan of Rock music listens to an interesting Blues one day, and starts to listen to more Blues instead of Rock music. Similarly, a single music may also serve different audiences at different times,e.g., a music initially targeted for an older generation may become popular among the young, and the features of this music need to be updated. Furthermore, as users interact with different items, users\u2019 interests and items\u2019 features can also co-evolve over time, i.e., their features are intertwined and can influence each other:\n\u2022 User \u2192 item. In online discussion forums such as Reddit, although a group (item) is initially created for statistics topics, users with very different interest profiles can join this group. Hence, the participants can shape the features of the group through their postings. It is likely that this group can finally become one about deep learning because most users concern about deep learning. \u2022 Item\u2192 user. As the group is evolving towards topics on deep learning, some users may become more interested in deep learning topics, and they may participate in other specialized groups on deep learning. On the opposite side, some users may gradually gain interests in pure math groups, lose interests in statistics and become inactive in this group.\nSuch co-evolutionary nature of user-item interactions raises very important questions on how to learn them from the increasingly available data. However, existing methods either treat the temporal user-item interactions data as a static graph or use epoch based methods such as tensor factorization to learn the latent features (Chi & Kolda, 2012; Koren, 2009; Yang et al., 2011). These methods are not able to capture the fine grained temporal dynamics of user-item interactions. Recent point process based models treat time as a random variable and improves over the traditional methods significantly (Du et al., 2015; Wang et al., 2016b). However, these works make strong assumptions\n\u2217Authors have equal contributions.\nabout the function form of the generative processes, which may not reflect the reality or accurate enough to capture the complex and nonlinear user-item influence in real world.\nIn this paper, we propose a recurrent coevolutionary feature embedding process framework. It combines recurrent neural network (RNN) with point process models, and efficiently captures the co-evolution of user-item features. Our model can automatically find an efficient representation of the underlying user and item latent feature without assuming a fixed parametric forms in advance. Figure 1 summarizes our framework. In particular, our work makes the following contributions:\n\u2022 Novel model. We propose a novel model that captures the nonlinear co-evolution nature of users\u2019 and items\u2019 embeddings. It assigns an evolving feature embedding process for each user and item, and the co-evolution of these latent feature processes is modeled with two parallel components: (i) item\u2192 user component, a user\u2019s latent feature is determined by the nonlinear embedding of latent features of the items he interacted with; and (ii) user\u2192 item component, an item\u2019s latent features are also determined by the latent features of the users who interact with the item. \u2022 Technical Challenges. We use RNN to parametrize the interdependent and intertwined user and item embeddings. The increased flexibility and generality further introduces technical challenges on how to train RNN on the co-evolving graphs. The co-evolution nature of the model makes the samples inter-dependent and not identically distributed, which is contrary to the assumptions in the traditional setting and significantly more challenging. We are the first to propose an efficient stochastic training algorithm that makes the BTPP tractable in the co-evolving graph. \u2022 Strong performance. We evaluate our method over multiple datasets, verifying that our method can lead to significant improvements in user behavior prediction compared to previous state-of-thearts. Precise time prediction is especially novel and not possible by most prior work.\n2 RELATED WORK\nRecent work predominantly fix the latent features assigned to each user and item (Salakhutdinov & Mnih, 2008; Chen et al., 2009; Agarwal & Chen, 2009; Ekstrand et al., 2011; Koren & Sill, 2011; Yang et al., 2011; Yi et al., 2014; Wang & Pal, 2015). In more sophisticated methods, the time is divided into epochs, and static latent feature models are applied to each epoch to capture some temporal aspects of the data (Koren, 2009; Karatzoglou et al., 2010; Xiong et al., 2010; Karatzoglou et al., 2010; Xiong et al., 2010; Chi & Kolda, 2012; Gultekin & Paisley, 2014; Charlin et al., 2015; Preeti Bhargava, 2015; Gopalan et al., 2015; Hidasi & Tikk, 2015; Wang et al., 2016a). For such methods, it is not clear how to choose the epoch length parameter. First, different users may have very different timescale when they interact with those service items, making it difficult to choose a unified epoch length. Second, it is not easy for these methods to answer time-sensitive queries such as when a user will return to the service item. The predictions are only in the resolution of the chosen epoch length. Recently, (Du et al., 2015) proposed a low-rank point process based model for time-sensitive recommendations from recurrent user activities. However, it fails to capture the heterogeneous coevolutionary properties of user-item interactions. Wang et al. (2016b) models the co-evolutionary property, but uses a simple linear representation of the users\u2019 and items\u2019 latent features, which might not be expressive enough to capture the real world patterns. As demonstrated in Du et al. (2016),\nthe nonlinear RNN is quite flexible to approximate many point process models. Also we will show that, our model only has O(#user + #item) regardless of RNN related parameters, and can also be potentially applied to online setting.\nIn the deep learning community, (Wang et al., 2015a) proposed a hierarchical Bayesian model that jointly performs learning for the content features and collaborative filtering for the ratings matrix. (Hidasi et al., 2016) applied RNN and adopt item-to-item recommendation approach with session based data. (Tan et al., 2016) improved this model with techniques like data augmentation, temporal change adaptation. (Ko et al., 2016) proposed collaborative RNN that extends collaborative filtering method to capture history of user behavior. Specifically, they used static global latent factors for items and assign separate latent factors for users that are dependent on their past history. (Song et al., 2016) extended the deep semantic structured model to capture multi-granularity temporal preference of users. They use separate RNN for each temporal granularity and combine them with feed forward network which models users\u2019 and items\u2019 long term static features. However, none of these works model the coevolution of users\u2019 and items\u2019 latent features and are still extensions of epoch based methods. Our work is unique since we explicitly treat time as a random variable and captures the coevolution of users\u2019 and items\u2019 latent features using temporal point processes. Finally, our work is inspired from the recurrent marked temporal point process model (Du et al., 2016). However, this work only focuses on learning a one-dimension point process. Our work is significantly different since we focus on the recommendation system setting with the novel idea of feature coevolution and we use multi-dimensional point processes to capture user-item interactions.\n3 BACKGROUND ON TEMPORAL POINT PROCESSES\nA temporal point process (Cox & Isham, 1980; Cox & Lewis, 2006; Aalen et al., 2008) is a random process whose realization consists of a list of discrete events localized in time, {ti} with ti \u2208 R+. Equivalently, a given temporal point process can be represented as a counting process, N(t), which records the number of events before time t. An important way to characterize temporal point processes is via the conditional intensity function \u03bb(t), a stochastic model for the time of the next event given all the previous events. Formally, \u03bb(t)dt is the conditional probability of observing an event in a small window [t, t+dt) given the historyH(t) up to t and that the event has not happen before t, i.e.,\n\u03bb(t)dt := P {event in [t, t+ dt)|H(t)} = E[dN(t)|H(t)], where one typically assumes that only one event can happen in a small window of size dt, i.e., dN(t) \u2208 {0, 1}. Then, given a time t > 0, we can also characterize the conditional probability that no event happens during [0, t) as: S(t) = exp ( \u2212 \u222b t 0 \u03bb(\u03c4) d\u03c4 ) and the conditional density that an event occurs at time t is defined as f(t) = \u03bb(t)S(t) (1)\nThe function form of the intensity \u03bb(t) is often designed to capture the phenomena of interests. Some commonly used form includes:\n\u2022 Hawkes processes (Hawkes, 1971; Wang et al., 2016c), whose intensity models the mutual excitation between events, i.e., \u03bb(t) = \u00b5 + \u03b1 \u2211 ti\u2208H(t) \u03ba\u03c9(t \u2212 ti), where \u03ba\u03c9(t) := exp(\u2212\u03c9t)\nis an exponential triggering kernel, \u00b5 > 0 is a baseline intensity. Here, the occurrence of each historical event increases the intensity by a certain amount determined by the kernel \u03ba\u03c9 and the weight \u03b1 > 0, making the intensity history dependent and a stochastic process by itself. \u2022 Rayleigh process, whose intensity function is \u03bb(t) = \u03b1t, where \u03b1 > 0 is the weight parameter.\n4 RECURRENT COEVOLUTIONARY FEATURE EMBEDDING PROCESSES\nIn this section, we present the generative framework for modeling the temporal dynamics of user-item interactions. We first use RNN to explicitly capture the co-evolving nature of users\u2019 and items\u2019 latent feature. Then, based on the compatibility between the users\u2019 and items\u2019 latent feature, we model the user-item interactions by a multi-dimensional temporal point process. We further parametrize the intensity function by the compatibility between users\u2019 and items\u2019 latent features.\n4.1 EVENT REPRESENTATION\nGiven m users and n items, we denote the ordered list of N observed events as O = {ej = (uj , ij , tj , qj)}Nj=1 on time window [0, T ], where uj \u2208 {1, . . . ,m}, ij \u2208 {1, . . . , n}, tj \u2208 R+, 0 6 t1 6 t2 . . . 6 T . This represents the interaction between user uj , item ij at time tj , with the interaction context qj \u2208 Rd. Here qj can be a high dimension vector such as the text review, or\nsimply the embedding of static user/item features such as user\u2019s profile and item\u2019s categorical features. For notation simplicity, we defineOu = {euj = (iuj , tuj , quj )} as the ordered listed of all events related to user u, and Oi = {eij = (uij , tij , qij)} as the ordered list of all events related to item i. We also set ti0 = t u 0 = 0 for all the users and items. tk\u2212 denotes the time point just before time tk.\n4.2 RECURRENT FEATURE EMBEDDING PROCESSES\nWe associate feature embeddings uu(t) \u2208 Rk with each user u and ii(t) \u2208 Rk with each item i. These features represent the subtle properties which cannot be directly observed, such as the interests of a user and the semantic topics of an item. Specifically, we model the drift, evolution, and co-evolution of uu(t) and ii(t) as a piecewise constant function of time that has jumps only at event times. Specifically, we define:\nUser latent feature embedding process. For each user u, the corresponding embedding after user u\u2019s k-th event euk = (i u k , t u k , q u k ) can be formulated as:\nuu(t u k) = \u03c3 ( W1(t\nu k \u2212 tuk\u22121)\ufe38 \ufe37\ufe37 \ufe38\ntemporal drift\n+W2uu(t u k\u22121)\ufe38 \ufe37\ufe37 \ufe38\nself evolution\n+ W3iik(t u k\u2212)\ufe38 \ufe37\ufe37 \ufe38\nco-evolution: item feature\n+ W4q u,ik k\ufe38 \ufe37\ufe37 \ufe38\ninteraction feature\n) (2)\nItem latent feature embedding process. For each item i, we specify ii(t) at time tik as:\nii(t i k) = \u03c3 ( V1(t\ni k \u2212 tik\u22121)\ufe38 \ufe37\ufe37 \ufe38\ntemporal drift\n+V2ii(t i k\u22121)\ufe38 \ufe37\ufe37 \ufe38\nself evolution\n+ V3uuk(t i k\u2212)\ufe38 \ufe37\ufe37 \ufe38\nco-evolution: item feature\n+ V4q i,uk k\ufe38 \ufe37\ufe37 \ufe38\ninteraction feature\n) (3)\nwhere t\u2212 means the time point just before time t, W4,V4 \u2208 Rk\u00d7d are the embedding matrices mapping from the explicit high-dimensional feature space into the low-rank latent feature space and W1,V1 \u2208 Rk, W2,V2,W3,V3 \u2208 Rk\u00d7k are weights parameters. \u03c3(\u00b7) is the nonlinear activation function, such as commonly used Tanh or Sigmoid for RNN. For simplicity, we use basic recurrent neural network to formulate the recurrence, but it is also straightforward to extend it using GRU or LSTM to gain more expressive power. Figure 1 summarizes the basic setting of our model.\nHere both the user and item\u2019s feature embedding processes are piecewise constant functions of time and only updated if an interaction event happens. A user\u2019s attribute changes only when he has a new interaction with some item. For example, a user\u2019s taste for music changes only when he listens to some new or old musics. Also, an item\u2019s attribute changes only when some user interacts with it. Different from Chen et al. (2013) who also models the time change with piecewise constant function, but their work has no coevolve modeling, and is not capable of predicting the future time point.\nNext we discuss the rationale of each term in detail:\n\u2022 Temporal drift. The first term is defined based on the time difference between consecutive events of specific user or item. It allows the basic features of users (e.g., a user\u2019s self-crafted interests) and items (e.g., textual categories and descriptions) to smoothly drift through time. Such changes of basic features normally are caused by external influences. \u2022 Self evolution. The current user feature should also be influenced by its feature at the earlier time. This captures the intrinsic evolution of user/item features. For example, a user\u2019s current taste should be more or less similar to his/her tastes two days ago. \u2022 User-item coevolution. Users\u2019 and items\u2019 latent features can mutually influence each other. This term captures the two parallel processes. First, a user\u2019s embedding is determined by the latent features of the items he interacted with. At each time tk, the latent item feature is iik(t u k\u2212).\nWe capture both the temporal influence and feature of each history item as a latent embedding. Conversely, an item\u2019s embedding is determined by the feature embedding of the user who just interacts with the item. \u2022 Evolution with interaction features. Users\u2019 and items\u2019 features can evolve and be influenced by the characteristics of their interactions. For instance, the genre changes of movies indicate the changing tastes of users. The theme of a chatting-group can be easily shifted to certain topics of the involved discussions. In consequence, this term captures the influence of the current interaction features to the changes of the latent user (item) features. \u2022 Interaction feature. This is the additional information happened in the user-item interactions. For example, in online discussion forums such as Reddit, the interaction features are the posts and comments. In online review sites such as Yelp, it is the reviews of the businesses.\nTo summarize, each feature embedding process evolves according to the respective base temporal user (item) features and also are mutually dependent on each other due to the endogenous influences from the interaction features and the entangled latent features.\n4.3 USER-ITEM INTERACTIONS AS TEMPORAL POINT PROCESSES\nFor each user, we model the recurrent occurrences of all users interaction with all items as a multidimensional temporal point process, with each user-item pair as one dimension. In particular, the intensity function in the (u, i)-th dimension (user u and item i) is modeled as a Rayleigh process:\n\u03bbu,i(t|t\u2032) = exp ( uu(t \u2032)>ii(t \u2032) )\ufe38 \ufe37\ufe37 \ufe38\nuser-item compatibility\n\u00b7 (t\u2212 t\u2032)\ufe38 \ufe37\ufe37 \ufe38 time lapse\n(4)\nwhere t > t\u2032, and t\u2032 is the last time point where either user u\u2019s embedding or item i\u2019s embedding changes before time t. The rationale behind this formulation is three-fold:\n\u2022 Time as a random variable. Instead of discretizing the time into epochs as traditional methods (Charlin et al., 2015; Preeti Bhargava, 2015; Gopalan et al., 2015; Hidasi & Tikk, 2015; Wang et al., 2016a), we explicitly model the timing of each interaction event as a random variable, which naturally captures the heterogeneity of the temporal interactions between users and items. \u2022 Short term preference. The probability for user u to interact with item i depends on the compatibility of their instantaneous embeddings, which is evaluated through the inner product at the last event time t\u2032. Because uu(t) and ii(t) co-evolve through time, their inner-product measures a general representation of the cumulative influence from the past interactions to the occurrence of the current event. The exp(\u00b7) function ensures the intensity is positive and well defined. \u2022 Rayleigh time distribution. The user and item embeddings are piecewise constant, and we use the time lapse term to make the intensity piecewise linear. This form leads to a Rayleigh distribution for the time intervals between consecutive events in each dimension. It is well-adapted to modeling fads, where the event-happening likelihood f(\u00b7) in (1) rises to a peak and then drops extremely rapidly. Furthermore, it is computationally easy to obtain an analytic form of f(\u00b7). One can then use f(\u00b7) to make item recommendation by finding the dimension that f(\u00b7) reaches the peak.\nWith the parameterized intensity function, we can further estimate the parameters using maximum likelihood estimation of all events. The joint negative log-likelihood is (Daley & Vere-Jones, 2007):\n` = \u2212 N\u2211 j=1 log ( \u03bbuj ,ij (tj |t\u2032j) ) \ufe38 \ufe37\ufe37 \ufe38\nintensity of interaction event\n+ m\u2211 u=1 n\u2211 i=1 \u222b T 0\n\u03bbu,i(\u03c4 |\u03c4 \u2032) d\u03c4\ufe38 \ufe37\ufe37 \ufe38 survival probability of event not happened\n(5)\nThe rationale of the objective two-fold: (i) the negative intensity summation term ensures the probability of all interaction events is maximized; (ii) the second survival probability term penalizes the non-presence of an interaction between all possible user-item pairs on the observation window. Hence, our framework not only explains why an event happens, but also why an event did not happen.\n5 PARAMETER LEARNING\nIn this section, we propose an efficient algorithm to learn the parameters {Vi}4i=1 and {Wi} 4 i=1. The batch objective function is presented in (5). The Back Propagation Through Time (BPTT) is the standard way to train a RNN. To make the back propagation tractable, one typically needs to do truncation during training. However, due to the novel co-evolutionary nature of our model, all the events are related to each other by the user-item bipartite graph (Figure 2), which makes it hard to decompose.\nHence, in sharp contrast to works (Hidasi et al., 2016; Du et al., 2016) in sequential data where one can easily break the sequences into multiple segments to make the BPTT trackable, it is a challenging task to design BPTT in our case. To efficiently solve this problem, we first order all the events globally and then do mini-batch training in a sliding window fashion. Each time when conducting feed forward and back propagation, we take the consecutive events within current sliding window to build the computational graph. Thus in our case the truncation is on the global timeline, instead over individual independent sequences as in prior works.\nNext, we explain our procedure in detail. Given a mini-batch of M ordered events O\u0303 = {ej}Mj=1, we set the time span to be [T0 = t1, T = tM ]. Below we show how to compute the intensity and survival probability term in the objective function (5) respectively.\nembedding is shown. (b) Survival probability for a user-item pair (u, i). The integral\n\u222b T\n0 \u03bbu,i(\u03c4 |\u03c4 \u2032)d\u03c4 is decomposed into 4 inter-event intervals separated by {t0, \u00b7 \u00b7 \u00b7 , t3}, with close form on each interval.\nComputing the intensity function. Each time when a new event ej happens between uj and ij , their corresponding feature embeddings will evolve according to a computational graph, as illustrated in Figure 2a. Due to the change of feature embedding, all the dimensions related to uj or ij will be influenced and the intensity function for that dimension will change consequently. Such crossdimension influence dependency is shown in Figure 2b. In our implementation, we first compute the corresponding intensity \u03bbuj ,ij (tj |t\u2032j) according to (4), and then update the embedding of uj and ij . This operation takes O(M) complexity, and is independent to the number of users or items.\nComputing the survival function. To compute the survival probability \u2212 \u222b T T0 \u03bbu,i(\u03c4 |\u03c4 \u2032)d\u03c4 for each pair (u, i), we first collect all the time stamps {tk} that have events related to either u or i. For notation simplicity, let |{tk}| = nu,i and t1 = T0, tnu,i = T . Since the embeddings are piecewise constant, the corresponding intensity function is piecewise linear, according to (4). Thus, the integration is decomposed into each time interval where the intensity is constant, i.e.,\u222b T\nT0 \u03bbu,i(\u03c4 |\u03c4 \u2032)d\u03c4 = nu,i\u22121\u2211 k=1 \u222b tk+1 tk \u03bbu,i(\u03c4 |\u03c4 \u2032)d\u03c4 = nu,i\u22121\u2211 k=1 (t2k+1 \u2212 t2k) exp ( uu(tk) >ii(tk) ) (6)\nFigure 3 visualizes the computation. Although the survival probability term exists in close form, we still need to solve two challenges. First, it is still expensive to compute it for each user item pair. Moreover, since the user-item interaction bipartite graph is very sparse, it is not necessary to monitor each dimension in the stochastic training setting. To speed up the computation, we propose a novel random-sampling scheme as follows.\nNote that the intensity term in the objective function (5) tries to maximize the inner product between user and item that has interaction event, while the survival term penalize over all other pairs of inner\nproducts. We observe that this is similar to Softmax computing for classification problem. Hence, inspired by the noise-contrastive estimation method (Gutmann & Hyva\u0308rinen, 2012) that is widely used in language models (Mnih & Kavukcuoglu, 2013), we keep the dimensions that have events on them, while randomly sample dimensions without events in current mini-batch.\nThe second challenge lies in the fact that the user-item interactions vary a lot across mini-batches, hence the corresponding computational graph also changes greatly. To make the learning efficient, we use the graph embedding framework (Dai et al., 2016) which allows training deep learning models where each term in the objective has a different computational graphs but with shared parameters. The Adam Optimizer (Kingma & Ba, 2014) together with gradient clip is used in our experiment.\n6 EXPERIMENTS\nWe evaluate our model on real-world datasets. For each sequence of user activities, we use all the events up to time T \u00b7 p as the training data, and the rest events as the testing data, where T is the observation window. We tune the latent rank of other baselines using 5-fold cross validation with grid search. We vary the proportion p \u2208 {0.7, 0.72, 0.74, 0.76, 0.78} and report the averaged results over five runs on two tasks (we will release code and data once published):\n\u2022 Item prediction. At each test time t, we predict the item that the user u will interact with. We rank all the items in the descending order of the conditional density fu,i(t) = \u03bbu,i(t)Su,i(t). We report the Mean Average Rank (MAR) of each test item at the test time. Ideally, the item associated with the test time t should rank one, hence smaller value indicates better predictive performance. \u2022 Time prediction. We predict the expected time when a testing event will occur between a given user-item pair. Using Rayleigh distribution, it is given by Et\u223cfu,i(t)(t) = \u221a \u03c0\n2 exp(uu(t\u2212)>ii(t\u2212)) .\nWe report the Mean Absolute Error (MAE) between the predicted and true time.\n6.1 COMPETITORS\nWe compared our DEEPCOEVOLVE with the following methods. Table 1 summarizes the differences.\n\u2022 LowRankHawkes (Du et al., 2015): This is a low rank Hawkes process model which assumes user-item interactions to be independent of each other and does not capture the co-evolution of user and item features. \u2022 Coevolving (Wang et al., 2016b): This is a multi-dimensional point process model which uses a simple linear embedding to model the co-evolution of user and item features. \u2022 PoissonTensor (Chi & Kolda, 2012): Poisson Tensor Factorization has been shown to perform better than factorization methods based on squared loss (Karatzoglou et al., 2010; Xiong et al., 2010; Wang et al., 2015b) on recommendation tasks. The performance for this baseline is reported using the average of the parameters fitted over all time intervals. \u2022 TimeSVD++ (Koren, 2009) and FIP (Yang et al., 2011): These two methods are only designed for explicit ratings, the implicit user feedbacks (in the form of a series of interaction events) are converted into the explicit ratings by the respective frequency of interactions with users. \u2022 STIC (Kapoor et al., 2015): it fits a semi-hidden markov model (HMM) to each observed user-item pair and is only designed for time prediction.\n6.2 DATASETS\nWe use three real world datasets as follows.\n\u2022 IPTV. It contains 7,100 users\u2019 watching history of 385 TV programs in 11 months (Jan 1 - Nov 30 2012), with around 2M events, and 1,420 movie features (including 1,073 actors, 312 directors, 22 genres, 8 countries and 5 years). \u2022 Yelp. This data was available in Yelp Dataset challenge Round 7. It contains reviews for various businesses from October, 2004 to December, 2015. The dataset we used here contains 1,005 users and 47,924 businesses, with totally 291,716 reviews.\n\u2022 Reddit. We collected discussion related data on different subreddits (groups) for the month of January 2014. We filtered all bot users\u2019 and their posts from this dataset. Furthermore, we randomly selected 1,000 users, 1,403 groups, and 10,000 discussion events.\n6.3 PREDICTION RESULTS\nFigure 4 shows that DEEPCOEVOLVE significantly outperforms both epoch-based baselines and state-of-arts point process based methods. LOWRANKHAWKES has good performance on item prediction but not on time prediction, while COEVOLVING has good performance on time prediction but not on item prediction. We discuss the performance regarding the two metrics below.\nItem prediction. Note that the best possible MAR one can achieve is 1, and our method gets quite accurate results: with the value of 1.7 on IPTV and 1.9 on Reddit. Note LOWRANKHAWKES achieves comparable item prediction performance, but not as good on the time prediction task. We think the reason is as follows. Since one only need the rank of conditional density f(\u00b7) in (1) to conduct item prediction, LOWRANKHAWKES may still be good at differentiating the conditional density function, but could not learn its actual value accurately, as shown in the time prediction task where the value of the conditional density function is needed for precise prediction.\nTime prediction. The second row of Figure 4 shows that DEEPCOEVOLVE outperforms other methods. Compared with LOWRANKHAWKES that achieves comparable time predication performance, 6\u00d7 improvement on Reddit, it has 10\u00d7 improvement on Yelp, and 30\u00d7 improvement on IPTV. The time unit is hour. Hence it has 2 weeks accuracy improvement on IPTV and 2 days on Reddit. This is important for online merchants to make time sensitive recommendations. An intuitive explanation is that our method accurately captures the nonlinear pattern between user and item interactions. The competitor LOWRANKHAWKES assumes specific parametric forms of the user-item interaction process, hence may not be accurate or expressive enough to capture real world temporal patterns. Furthermore, it models each user-item interaction dimension independently, which may lose the important affection from user\u2019s interaction with other items while predicting the current item\u2019s reoccurrence time. Our work also outperforms COEVOLVING, e.g., with around 3\u00d7MAE improve on IPTV. Moreover, the item prediction performance is also much better than COEVOLVING. It shows the importance of using RNN to capture the nonlinear embedding of user and item latent features, instead of the simple parametrized linear embedding in COEVOLVING.\n6.4 INSIGHT OF RESULTS\nWe will look deeper and provide rationale behind the prediction results in the following two subsections. First, to understand the difficulty of conducting prediction tasks in each dataset, we study their different sparsity properties. For the multidimensional point process models, the fewer events we observe in each dimension, the more sparse the dataset is. Our approach alleviates the sparsity problem via the modeling of dependencies among dimensions, thus is consistently doing better than other baseline algorithms.\nNext, we fix one dataset and evaluate how different levels of sparsity in training data influences each algorithm\u2019s performance.\n6.4.1 UNDERSTANDING THE DATASETS\nWe visualize the three datasets in Figure 5 according to (i) the number of events per user, and (ii) the user-item interaction graph.\nSparsity in terms of the number of events per user. Typically, the more user history data we have, the better results we will obtain in the prediction tasks. We can see in IPTV dataset, users typically have longer length of history than the users in Reddit and Yelp datasets. Thus our algorithm and all other baseline methods have their best performance on this dataset. However, the Reddit dataset and Yelp dataset are hard to tell the performance based only on the distribution of history length, thus we do a more detailed visualization.\nSparsity in terms of diversity of items to recommend. From the bipartite graph, it is easy to see that Yelp dataset has higher density than the other two datasets. The density of the interaction graph reflects the variety of history per each user. For example, the users in IPTV only has 385 programs to watch, but they can have 47,924 businesses to choose in Yelp dataset. Also, the Yelp dataset has 9 times more items than IPTV and Reddit dataset in the bipartite graph. This means the users in Yelp dataset has more diverse tastes than users in other two datasets. This is because if users has similar tastes, the distinct number of items in the union of their history should be small.\nBased on the above two facts, we can see Yelp dataset is the most sparse, since it has shorter length of history per user, and much more diversity of the items, it is not surprising that this dataset is much harder than the other IPTV and Reddit dataset.\n6.4.2 ROBUSTNESS OF THE ALGORITHM\nWith the case study on the most challenging Yelp dataset, we further evaluate how each algorithm performs with lower level of sparsity as compared to the one used in Figure 4 (c).We use this to demonstrate that our work is most robust and performs well across different levels of sparsity.\nWe first create Yelp100, a more dense dataset, by filtering the original Yelp dataset to keep the top 100 users. Each user would have at least 200 events. Figure 6 (a) shows the statistics of this dataset. On average the users have more history events than the original Yelp dataset in Figure 5(c).\nOn this dense dataset, Figure 6 (b) and (c) show that all the algorithms\u2019 performances improve with more history events, comparing to the performance in original Yelp dataset. For example, LOWRANKHAWKES has similar rank prediction results as our DEEPCOEVOLVE on this dense dataset. However, as the dataset becomes sparse, the performance of LOWRANKHAWKES drops significantly, as shown in Figure 4(c). For example, the rank prediction error goes from 90 to 2128, and the\ntime error goes from 724 to 11043.5. We think it is because this model relies more on the history information per each user-item pair.\nOn the contrary, our DEEPCOEVOLVE still has superior performance with such high level of sparsity. The rank error only changes from 87 to 107, and the time error changes from 72 to 884 as the data becomes sparse. It shows that our work is the most robust to the sparsity in the data. We think it is because our work accurately captures the nonlinear multidimensional dependencies between users and items latent features.\n7 CONCLUSION\nWe have proposed an efficient framework to model the nonlinear co-evolution nature of users\u2019 and items\u2019 latent features. Moreover, the user and item\u2019s evolving and co-evolving processes are captured by the RNN. It is based on temporal point processes and models time as a random variable. Hence it is in sharp contrast to prior epoch based works. We demonstrate the superior performance of our method on both the time and item prediction task, which is not possible by most prior work. Future work includes extending to other social applications, such as group dynamics in message services.\nA DETAILS ON GRADIENT COMPUTATION\nComputing gradient. For illustration purpose, we here use Sigmoid as the nonlinear activation function \u03c3. In order to get gradient with respect to parameter W \u2019s, we first compute gradients with respect to each varying points of embeddings. For user u\u2019s embedding after his k-th event, the corresponding partial derivatives are computed by:\n\u2202`\n\u2202uu(tuk) = \u2212iiuk\ufe38\ufe37\ufe37\ufe38\nfrom intensity\n+ n\u2211 i=1\n\u2202 \u222b tuk+1 tuk\n\u03bbu,i(\u03c4 |\u03c4 \u2032)d\u03c4 \u2202uu(tuk)\ufe38 \ufe37\ufe37 \ufe38\nfrom survival\n+ \u2202`\n\u2202uu(tuk+1) (1\u2212 uu(tuk+1)) uu(tuk+1)W2\ufe38 \ufe37\ufe37 \ufe38 from user u\u2019s next embedding\n+ \u2202`\n\u2202iiuk+1(t u k+1)\n(1\u2212 iiuk+1(t u k+1)) iiuk+1(t u k+1)\ufe38 \ufe37\ufe37 \ufe38\nfrom user u\u2019s next item embedding\nwhere denotes element-wise multiplication. The gradient coming from the second term (i.e., the survival term) is also easy to compute, since the Rayleigh distribution has closed form of survival function. For a certain item i, if its feature doesn\u2019t changed between time interval [tuk , t u k+1], then we have\n\u2202 \u222b tuk+1 tuk\n\u03bbu,i(\u03c4 |\u03c4 \u2032)d\u03c4 \u2202uu(tuk) = (tuk+1 \u2212 tuk)2 2 exp\n( uu(t u k) >ii(t u k)ii(t u k) )\n(7)\nOn the other hand, if the embedding of item i changes during this time interval, then we should break this interval into segments and compute the summation of gradients in each segment in a way similar to (7). Thus, we are able to compute the gradients with respect to Wi, i \u2208 {1, 2, 3, 4} as follows.\n\u2202`\n\u2202W1 = m\u2211 u=1 \u2211 k\n\u2202`\n\u2202uu(tuk) (i\u2212 uu(tuk)) uu(tuk)(tuk \u2212 tuk\u22121)\n\u2202`\n\u2202W2 = m\u2211 u=1 \u2211 k ( \u2202` \u2202uu(tuk) (i\u2212 uu(tuk)) uu(tuk) ) uu(t u k\u22121) >\n\u2202`\n\u2202W3 = m\u2211 u=1 \u2211 k ( \u2202` \u2202uu(tuk) (i\u2212 uu(tuk)) uu(tuk) ) iik(t u k\u2212)>\n\u2202`\n\u2202W4 = m\u2211 u=1 \u2211 k ( \u2202` \u2202uu(tuk) (i\u2212 uu(tuk)) uu(tuk) ) qu,ikk\nSince the items are treated symmetrically as users, the corresponding derivatives can be obtained in a similar way.\n", "cats": {"READ": 1.0, "DONTREAD": 0.0}, "reviews": [{"IS_META_REVIEW": true, "comments": "The paper introduces a time dependent recommender system based on point processes parametrized by time dependent user and item latent representations. The later are modeled as coupled \u2013 autoregressive processes \u2013 i.e. the representation of a user/item changes when he interacts with an item/user, and is a function of both the user and the item representations before time t. This is called coevolution here and the autoregressive process is called recurrent NN. The model may also incorporate heterogeneous inputs. Experiments are performed on several datasets, and the model is compared with different baselines.\nThere are several contributions in the paper: 1) modeling recommendation via parametrized point processes where the parameter dynamics are modeled by latent user/item representations, 2) an optimization algorithm for maximizing the likelihood of this process, with different technical tricks that seem to break its intrinsic complexity, 3) evaluation experiments for time dependent recommendation.\nThe paper by the same authors (NIPS 2016) describes a similar model of continuous time coevolution, and a similar evaluation. The difference lies in the details of the model: the point process model is not the same and of the latent factor dynamic model is slightly different, but the modeling approach and the arguments are exactly the same. By the end, one does not know what makes this model perform better than the one proposed in NIPS, is it the choice for the process, the new parametrization? Both are quite similar. There is no justification on the choice of the specific form of the point process in the two papers. Did the authors tried other forms as well? The same remark applies for the form of the dynamical process: the non-linearity used for the modeling of the latent user/item vectors here is limited to a sigmoid function, which probably does not change much w.r.t. a linear model, but there is no evidence of the role of this non linearity in the paper. Note that there are some inconsistencies between the results in the two papers.\nConcerning the evaluation, the authors introduce two criteria. I did not get exactly how they evaluate the item recommendation: it is mentioned that at each time t, the model predicts the item the user will interact with. Do you mean, the next item the user will interact with after time t? For the time prediction, why is it a relevant metric for recommendation?\nA comparison of the complexity, or execution time of the different methods would be helpful. The complexity of your method is apparently proportional to #items*#users, what are the complexity limits of your methods.\nOverall, the paper is quite nice and looks technically sound, albeit many details are missing. On the other hand, I have a mixed feeling because of the similarity with NIPS paper. The authors should have make a better work at convincing us that this is not a marginal extension of previous work by the authors.  I was not convinced either by the evaluation criteria and there is no evidence that the model can be used for large datasets."}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "A nice paper, with sufficient experimental validation, and the idea of incorporating a form of change point detection is good. However, the technical contribution relative to the NIPS paper by the same authors is not significant, in that it primarily involves using an RNN instead of a Hawkes process to model the temporal dynamics. The results are significantly better than this earlier paper -- the authors should explore if this is due only to the RNN, or to the optimization method.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"DATE": "13 Jan 2017", "TITLE": "Paper Revision", "IS_META_REVIEW": false, "comments": "Dear reviewers, we have revised our paper according to your insightful suggestions and comments.\n\n1) We highlight the importance and difficulty of modeling the nonlinearity in the point process models in the introduction part. \n\n2) We added the discussion with Chen et.al ICML 2013 and a detailed comparison with Wang et.al NIPS 2016.\n \n3) We added the experiment on the large Yelp dataset, which contains 1,005 users and 47,924 items. We run our algorithm and all the baselines on this one. The result of our algorithm is consistently better than alternatives. \n\n4) We added the section 6.4.1 explaining the quantitative results we get in the experiment. Specifically, we studied the performance of different history length of users, and how the diversity of tastes of users affect the results by visualizing the user-item interaction graph. \n\n5) We also added the section 6.4.2 that quantitatively compare the effect of different history length on Yelp dataset. \n", "OTHER_KEYS": "Hanjun Dai"}, {"TITLE": "No Title", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "The paper introduces a time dependent recommender system based on point processes parametrized by time dependent user and item latent representations. The later are modeled as coupled \u2013 autoregressive processes \u2013 i.e. the representation of a user/item changes when he interacts with an item/user, and is a function of both the user and the item representations before time t. This is called coevolution here and the autoregressive process is called recurrent NN. The model may also incorporate heterogeneous inputs. Experiments are performed on several datasets, and the model is compared with different baselines.\nThere are several contributions in the paper: 1) modeling recommendation via parametrized point processes where the parameter dynamics are modeled by latent user/item representations, 2) an optimization algorithm for maximizing the likelihood of this process, with different technical tricks that seem to break its intrinsic complexity, 3) evaluation experiments for time dependent recommendation.\nThe paper by the same authors (NIPS 2016) describes a similar model of continuous time coevolution, and a similar evaluation. The difference lies in the details of the model: the point process model is not the same and of the latent factor dynamic model is slightly different, but the modeling approach and the arguments are exactly the same. By the end, one does not know what makes this model perform better than the one proposed in NIPS, is it the choice for the process, the new parametrization? Both are quite similar. There is no justification on the choice of the specific form of the point process in the two papers. Did the authors tried other forms as well? The same remark applies for the form of the dynamical process: the non-linearity used for the modeling of the latent user/item vectors here is limited to a sigmoid function, which probably does not change much w.r.t. a linear model, but there is no evidence of the role of this non linearity in the paper. Note that there are some inconsistencies between the results in the two papers.\nConcerning the evaluation, the authors introduce two criteria. I did not get exactly how they evaluate the item recommendation: it is mentioned that at each time t, the model predicts the item the user will interact with. Do you mean, the next item the user will interact with after time t? For the time prediction, why is it a relevant metric for recommendation?\nA comparison of the complexity, or execution time of the different methods would be helpful. The complexity of your method is apparently proportional to #items*#users, what are the complexity limits of your methods.\nOverall, the paper is quite nice and looks technically sound, albeit many details are missing. On the other hand, I have a mixed feeling because of the similarity with NIPS paper. The authors should have make a better work at convincing us that this is not a marginal extension of previous work by the authors.  I was not convinced either by the evaluation criteria and there is no evidence that the model can be used for large datasets.\n", "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "19 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "review for Recurrent Coevolutionary Feature Embedding Processes for Recommendation", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "This paper proposes a method to model time changing dynamics in collaborative filtering.\nComments:\n1) The main idea of the paper is build upon similar to a previous work by the same group of author (Wang et.al KDD), the major difference appears to be change some of the latent factors to be RNN\n2) The author describes a BPTT technique to train the model  \n3) The author introduced time prediction as a new metric to evaluate the effectiveness of time dependent model. However, this need to be condition on a given user-item pair.\n4) It would be interesting to consider other metrics, for example\n- The switching time where a user changes his/her to another item \n- Jointly predict the next item and switching time. \nIn summary, this is a paper that improves over an existing work on time dynamics model in recommender system. The time prediction metric is interesting and opens up interesting discussion on how we should evaluate recommender systems when time is involved (see also comments).\n\n \n", "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "16 Dec 2016 (modified: 17 Dec 2016)", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "review", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "The paper seeks to predict user events (interactions with items at a particular point in time). Roughly speaking the contributions are as follows:\n(a) the paper models the co-evolutionary process of users' preferences toward items\n(b) the paper is able to incorporate external sources of information, such as user and item features\n(c) the process proposed is generative, so is able to estimate specific time-points at which events occur\n(d) the model is able to account for non-linearities in the above\n\nFollowing the pre-review questions, I understand that it is the combination of (a) and (c) that is the most novel aspect of the paper. A fully generative process which can be sampled is certainly nice (though of course, non-generative processes like regular old regression can estimate specific time points and such too, so not sure in practice how relevant this distinction is).\n\nOther than that the above parts have all appeared in some combination in previous work, though the combination of parts here certainly passes the novelty bar.\n\nI hadn't quite followed the issue mentioned in the pre-review discussion that the model requires multiple interactions per userXitem pair in order to fit the model (e.g. a user interacts with the same business multiple times). This is a slightly unusual setting compared to most temporal recommender systems work. I question to some extent whether this problem setting isn't a bit restrictive. That being said I take the point about why the authors had to subsample the Yelp data, but keeping only users with \"hundreds\" of events means that you're left with a very biased sample of the user base.\n\nOther than the above issues, the paper is technically nice, and the experiments include strong baselines and reports good performance.\n", "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "15 Dec 2016", "REVIEWER_CONFIDENCE": 3}, {"DATE": "09 Dec 2016", "TITLE": "Already published?", "IS_META_REVIEW": false, "comments": "Nice work, but is seems to me that it was already published at DLRS 2016 in September (", "OTHER_KEYS": "(anonymous)"}, {"DATE": "08 Dec 2016", "TITLE": "relation with general function matrix factorization", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3"}, {"DATE": "03 Dec 2016", "TITLE": "pre-review questions", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1"}, {"IS_META_REVIEW": true, "comments": "The paper introduces a time dependent recommender system based on point processes parametrized by time dependent user and item latent representations. The later are modeled as coupled \u2013 autoregressive processes \u2013 i.e. the representation of a user/item changes when he interacts with an item/user, and is a function of both the user and the item representations before time t. This is called coevolution here and the autoregressive process is called recurrent NN. The model may also incorporate heterogeneous inputs. Experiments are performed on several datasets, and the model is compared with different baselines.\nThere are several contributions in the paper: 1) modeling recommendation via parametrized point processes where the parameter dynamics are modeled by latent user/item representations, 2) an optimization algorithm for maximizing the likelihood of this process, with different technical tricks that seem to break its intrinsic complexity, 3) evaluation experiments for time dependent recommendation.\nThe paper by the same authors (NIPS 2016) describes a similar model of continuous time coevolution, and a similar evaluation. The difference lies in the details of the model: the point process model is not the same and of the latent factor dynamic model is slightly different, but the modeling approach and the arguments are exactly the same. By the end, one does not know what makes this model perform better than the one proposed in NIPS, is it the choice for the process, the new parametrization? Both are quite similar. There is no justification on the choice of the specific form of the point process in the two papers. Did the authors tried other forms as well? The same remark applies for the form of the dynamical process: the non-linearity used for the modeling of the latent user/item vectors here is limited to a sigmoid function, which probably does not change much w.r.t. a linear model, but there is no evidence of the role of this non linearity in the paper. Note that there are some inconsistencies between the results in the two papers.\nConcerning the evaluation, the authors introduce two criteria. I did not get exactly how they evaluate the item recommendation: it is mentioned that at each time t, the model predicts the item the user will interact with. Do you mean, the next item the user will interact with after time t? For the time prediction, why is it a relevant metric for recommendation?\nA comparison of the complexity, or execution time of the different methods would be helpful. The complexity of your method is apparently proportional to #items*#users, what are the complexity limits of your methods.\nOverall, the paper is quite nice and looks technically sound, albeit many details are missing. On the other hand, I have a mixed feeling because of the similarity with NIPS paper. The authors should have make a better work at convincing us that this is not a marginal extension of previous work by the authors.  I was not convinced either by the evaluation criteria and there is no evidence that the model can be used for large datasets."}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "A nice paper, with sufficient experimental validation, and the idea of incorporating a form of change point detection is good. However, the technical contribution relative to the NIPS paper by the same authors is not significant, in that it primarily involves using an RNN instead of a Hawkes process to model the temporal dynamics. The results are significantly better than this earlier paper -- the authors should explore if this is due only to the RNN, or to the optimization method.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"DATE": "13 Jan 2017", "TITLE": "Paper Revision", "IS_META_REVIEW": false, "comments": "Dear reviewers, we have revised our paper according to your insightful suggestions and comments.\n\n1) We highlight the importance and difficulty of modeling the nonlinearity in the point process models in the introduction part. \n\n2) We added the discussion with Chen et.al ICML 2013 and a detailed comparison with Wang et.al NIPS 2016.\n \n3) We added the experiment on the large Yelp dataset, which contains 1,005 users and 47,924 items. We run our algorithm and all the baselines on this one. The result of our algorithm is consistently better than alternatives. \n\n4) We added the section 6.4.1 explaining the quantitative results we get in the experiment. Specifically, we studied the performance of different history length of users, and how the diversity of tastes of users affect the results by visualizing the user-item interaction graph. \n\n5) We also added the section 6.4.2 that quantitatively compare the effect of different history length on Yelp dataset. \n", "OTHER_KEYS": "Hanjun Dai"}, {"TITLE": "No Title", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "The paper introduces a time dependent recommender system based on point processes parametrized by time dependent user and item latent representations. The later are modeled as coupled \u2013 autoregressive processes \u2013 i.e. the representation of a user/item changes when he interacts with an item/user, and is a function of both the user and the item representations before time t. This is called coevolution here and the autoregressive process is called recurrent NN. The model may also incorporate heterogeneous inputs. Experiments are performed on several datasets, and the model is compared with different baselines.\nThere are several contributions in the paper: 1) modeling recommendation via parametrized point processes where the parameter dynamics are modeled by latent user/item representations, 2) an optimization algorithm for maximizing the likelihood of this process, with different technical tricks that seem to break its intrinsic complexity, 3) evaluation experiments for time dependent recommendation.\nThe paper by the same authors (NIPS 2016) describes a similar model of continuous time coevolution, and a similar evaluation. The difference lies in the details of the model: the point process model is not the same and of the latent factor dynamic model is slightly different, but the modeling approach and the arguments are exactly the same. By the end, one does not know what makes this model perform better than the one proposed in NIPS, is it the choice for the process, the new parametrization? Both are quite similar. There is no justification on the choice of the specific form of the point process in the two papers. Did the authors tried other forms as well? The same remark applies for the form of the dynamical process: the non-linearity used for the modeling of the latent user/item vectors here is limited to a sigmoid function, which probably does not change much w.r.t. a linear model, but there is no evidence of the role of this non linearity in the paper. Note that there are some inconsistencies between the results in the two papers.\nConcerning the evaluation, the authors introduce two criteria. I did not get exactly how they evaluate the item recommendation: it is mentioned that at each time t, the model predicts the item the user will interact with. Do you mean, the next item the user will interact with after time t? For the time prediction, why is it a relevant metric for recommendation?\nA comparison of the complexity, or execution time of the different methods would be helpful. The complexity of your method is apparently proportional to #items*#users, what are the complexity limits of your methods.\nOverall, the paper is quite nice and looks technically sound, albeit many details are missing. On the other hand, I have a mixed feeling because of the similarity with NIPS paper. The authors should have make a better work at convincing us that this is not a marginal extension of previous work by the authors.  I was not convinced either by the evaluation criteria and there is no evidence that the model can be used for large datasets.\n", "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "19 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "review for Recurrent Coevolutionary Feature Embedding Processes for Recommendation", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "This paper proposes a method to model time changing dynamics in collaborative filtering.\nComments:\n1) The main idea of the paper is build upon similar to a previous work by the same group of author (Wang et.al KDD), the major difference appears to be change some of the latent factors to be RNN\n2) The author describes a BPTT technique to train the model  \n3) The author introduced time prediction as a new metric to evaluate the effectiveness of time dependent model. However, this need to be condition on a given user-item pair.\n4) It would be interesting to consider other metrics, for example\n- The switching time where a user changes his/her to another item \n- Jointly predict the next item and switching time. \nIn summary, this is a paper that improves over an existing work on time dynamics model in recommender system. The time prediction metric is interesting and opens up interesting discussion on how we should evaluate recommender systems when time is involved (see also comments).\n\n \n", "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "16 Dec 2016 (modified: 17 Dec 2016)", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "review", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "The paper seeks to predict user events (interactions with items at a particular point in time). Roughly speaking the contributions are as follows:\n(a) the paper models the co-evolutionary process of users' preferences toward items\n(b) the paper is able to incorporate external sources of information, such as user and item features\n(c) the process proposed is generative, so is able to estimate specific time-points at which events occur\n(d) the model is able to account for non-linearities in the above\n\nFollowing the pre-review questions, I understand that it is the combination of (a) and (c) that is the most novel aspect of the paper. A fully generative process which can be sampled is certainly nice (though of course, non-generative processes like regular old regression can estimate specific time points and such too, so not sure in practice how relevant this distinction is).\n\nOther than that the above parts have all appeared in some combination in previous work, though the combination of parts here certainly passes the novelty bar.\n\nI hadn't quite followed the issue mentioned in the pre-review discussion that the model requires multiple interactions per userXitem pair in order to fit the model (e.g. a user interacts with the same business multiple times). This is a slightly unusual setting compared to most temporal recommender systems work. I question to some extent whether this problem setting isn't a bit restrictive. That being said I take the point about why the authors had to subsample the Yelp data, but keeping only users with \"hundreds\" of events means that you're left with a very biased sample of the user base.\n\nOther than the above issues, the paper is technically nice, and the experiments include strong baselines and reports good performance.\n", "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "15 Dec 2016", "REVIEWER_CONFIDENCE": 3}, {"DATE": "09 Dec 2016", "TITLE": "Already published?", "IS_META_REVIEW": false, "comments": "Nice work, but is seems to me that it was already published at DLRS 2016 in September (", "OTHER_KEYS": "(anonymous)"}, {"DATE": "08 Dec 2016", "TITLE": "relation with general function matrix factorization", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3"}, {"DATE": "03 Dec 2016", "TITLE": "pre-review questions", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1"}]}
{"text": "1 INTRODUCTION\nReading comprehension-based question answering (RCQA) is the task of answering a question with a chunk of text taken from related document(s). A variety of neural models have been proposed recently either for extracting a single entity or a single token as an answer from a given text (Hermann et al., 2015; Kadlec et al., 2016; Trischler et al., 2016b; Dhingra et al., 2016; Chen et al., 2016; Sordoni et al., 2016; Cui et al., 2016a); or for selecting the correct answer by ranking a small set of human-provided candidates (Yin et al., 2016; Trischler et al., 2016a). In both cases, an answer boundary is either easy to determine or already given.\nDifferent from the above two assumptions for RCQA, in the real-world QA scenario, people may ask questions about both entities (factoid) and non-entities such as explanations and reasons (nonfactoid) (see Table 1 for examples).\nIn this regard, RCQA has the potential to complement other QA approaches that leverage structured data (e.g., knowledge bases) for both the above question types. This is because RCQA can exploit the textual evidences to ensure increased answer coverage, which is particularly helpful for nonfactoid answers. However, it is also challenging for RCQA to identify answer in arbitrary position in the passage with arbitrary length, especially for non-factoid answers which might be clauses or sentences.\nAs a result, apart from a few exceptions (Rajpurkar et al., 2016; Wang & Jiang, 2016), this research direction has not been fully explored yet.\nCompared to the relatively easier RC task of predicting single tokens/entities1, predicting answers of arbitrary lengths and positions significantly increase the search space complexity:\nthe number of possible candidates to consider is in the order of O(n2), where n is the number of passage words. In contrast, for previous works in which answers are single tokens/entities or from candidate lists, the complexity is in O(n) or the size of candidate lists l (usually l \u22645), respectively. To address the above complexity, Rajpurkar et al. (Rajpurkar et al., 2016) used a two-step chunkand-rank approach that employs a rule-based algorithm to extract answer candidates from a passage,\n\u2217Both authors contribute equally 1State-of-the-art RC models have a decent accuracy of \u223c70% on the widely used CNN/DailyMail dataset\n(Hermann et al., 2015).\nfollowed by a ranking approach with hand-crafted features to select the best answer. The rule-based chunking approach suffered from low coverage (\u2248 70% recall of answer chunks) that cannot be improved during training; and candidate ranking performance depends greatly on the quality of the hand-crafted features. More recently, Wang and Jiang (Wang & Jiang, 2016) proposed two end-toend neural network models, one of which chunks a candidate answer by predicting the answer\u2019s two boundary indices and the other classifies each passage word into answer/not-answer. Both models improved significantly over the method proposed by Rajpurkar et al. (Rajpurkar et al., 2016).\nOur proposed model, called dynamic chunk reader (DCR), not only significantly differs from both the above systems in the way that answer candidates are generated and ranked, but also shares merits with both works. First, our model uses deep networks to learn better representations for candidate answer chunks, instead of using fixed feature representations as in (Rajpurkar et al., 2016). Second, it represents answer candidates as chunks, as in (Rajpurkar et al., 2016), instead of wordlevel representations (Wang & Jiang, 2016), to make the model aware of the subtle differences among candidates (importantly, overlapping candidates).\nThe contributions of this paper are three-fold. (1) We propose a novel neural network model for joint candidate answer chunking and ranking, where the candidate answer chunks are dynamically constructed and ranked in an end-to-end manner. (2) we propose a new question-attention mechanism to enhance passage word representation, which is subsequently used to construct chunk representations. (3) We also propose several simple but effective features to strengthen the attention mechanism, which fundamentally improves candidate ranking, with the by-product of higher exact boundary match accuracy.\nThe experiments on the Stanford Question Answering Dataset (SQuAD) (Rajpurkar et al., 2016), which contains a variety of human-generated factoid and non-factoid questions, have shown the effectiveness of above three contributions.\nOur paper is organized as follows. We formally define the RCQA problem first. Next, we describe our baseline with a neural network component. We present the end-to-end dynamic chunk reader model next. Finally, we analyze our experimental results and discuss the related work. In appendix, we show formal equations and details of the model.\n2 PROBLEM DEFINITION\nTable 1 shows an example of our RC setting where the goal is to answer a question Qi, factoid (Q1) or non-factoid (Q2 and Q3), based on a supporting passage Pi, by selecting a continuous sequence of text Ai \u2286 Pi as answer. Qi, Pi, and Ai are all word sequences, where each word is drawn from a vocabulary, V . The i-th instance in the training set is a triple in the form of (Pi, Qi, Ai), where Pi = (pi1, . . . , pi|Pi|), Qi = (qi1, . . . , qi|Qi|), and Ai = (ai1, . . . , ai|Ai|) (pi\u00b7, qi\u00b7, ai\u00b7 \u2208 V ). Owing to the disagreement among annotators, there could be more than one correct answer for the same question; and the k-th answer to Qi is denoted by Aki = {aki1, . . . , aki|Aki |}. An answer candidate for the i-th training example is defined as cm,ni , a sub-sequence in Pi, that spans from position m to n (1 \u2264 m \u2264 n \u2264 |Pi|). The ground truth answer Ai could be included in the set of all candidates\nCi = {cm,ni |\u2200m,n \u2208 N+, subj(m,n, Pi) and 1 \u2264 m \u2264 n \u2264 |Pi|}, where subj(m,n, Pi) is the constraint put on the candidate chunk for Pi, such as, \u201cc m,n i can have at most 10 tokens\u201d, or \u201ccm,ni must have a pre-defined POS pattern\u201d. To evaluate a system\u2019s performance, its top answer to a question is matched against the corresponding gold standard answer(s).\nRemark: Categories of RC Tasks Other simpler variants of the aforementioned RC task were explored in the past. For example, quiz-style datasets (e.g., MCTest (Richardson et al., 2013), MovieQA (Tapaswi et al., 2015)) have multiple-choice questions with answer options. Cloze-style datesets(Hermann et al., 2015; Hill et al., 2015; Onishi et al., 2016), usually automatically generated, have factoid \u201cquestion\u201ds created by replacing the answer in a sentence from the text with blank. For the answer selection task this paper focuses on, several datasets exist, e.g. TREC-QA for factoid answer extraction from multiple given passages, bAbI (Weston et al., 2014) designed for inference purpose, and the SQuAD dataset (Rajpurkar et al., 2016) used in this paper. To the best of our knowledge, the SQuAD dataset is the only one for both factoid and non-factoid answer extraction with a question distribution more close to real-world applications.\n3 BASELINE: CHUNK-AND-RANK PIPELINE WITH NEURAL RC\nIn this section we modified a state-of-the-art RC system for cloze-style tasks for our answer extraction purpose, to see how much gap we have for the two type of tasks, and to inspire our end-to-end system in the next section. In order to make the cloze-style RC system to make chunk-level decision, we use the RC model to generate features for chunks, which are further used in a feature-based ranker like in (Rajpurkar et al., 2016). As a result, this baseline can be viewed as a deep learning based counterpart of the system in (Rajpurkar et al., 2016). It has two main components: 1) a standalone answer chunker, which is trained to produce overlapping candidate chunks, and 2) a neural RC model, which is used to score each word in a given passage to be used thereafter for generating chunk scores.\nAnswer Chunking To reduce the errors generated by the rule-based chunker in (Rajpurkar et al., 2016), first, we capture the part-of-speech (POS) pattern of all answer sub-sequences in the training dataset to form a POS pattern trie tree, and then apply the answer POS patterns to passage Pi to acquire a collection of all subsequences (chunk candidates) Ci whose POS patterns can be matched to the POS pattern trie. This is equivalent to putting an constraint subj(m,n, Pi) to candidate answer chunk generation process that only choose the chunk with a POS pattern seen for answers in the training data. Then the sub-sequences Ci are used as answer candidates for Pi. Note that overlapping chunks could be generated for a passage, and we rely on the ranker to choose the best candidate based on features from the cloze-style RC system. Experiments showed that for > 90% of the questions on the development set, the ground truth answer is included in the candidate set constructed in such manner.\nFeature Extraction and Ranking For chunk ranking, we (1) use neural RCQA model to annotate each pij in passage Pi to get score sij , then (2) for every chunk c m,n i in passage i, collect scores (sim, . . . , sin) for all the (pim, ..., pin) contained within c m,n i , and (3) extract features on the sequence of scores (sim, . . . , sin) to characterize its scale and distribution information, which serves as the feature representation of cm,ni . In step (1) to acquire sij we train and apply a word-level single-layer Gated Attention Reader 2 (Dhingra et al., 2016), which has state-of-the-art performance on CNN/DailyMail cloze-style RC task. In step (3) for chunk cm,ni , we designed 5 features, including 4 statistics on (sim, . . . , sin): maximum, minimum, average and sum; as well as the count of matched POS pattern within the chunk, which serves as an answer prior. We use these 5 features in a state-of-the-art ranker (Ganjisaffar et al., 2011).\n4 DYNAMIC CHUNK READER\nThe dynamic chunk reader (DCR) model is presented in Figure 1. Inspired by the baseline we built, DCR is deemed to be superior to the baseline for 3 reasons. First, each chunk has a representation constructed dynamically, instead of having a set of pre-defined feature values. Second, each passage\n2We tried using more than one layers in Gated Attention Reader, but no improvement was observed.\nword\u2019s representation is enhanced by word-by-word attention that evaluates the relevance of the passage word to the question. Third, these components are all within a single, end-to-end model that can be trained in a joint manner.\nDCR works in four steps. First, the encoder layer encodes passage and question separately, by using bidirectional recurrent neural networks (RNN).\nSecond, the attention layer calculates the relevance of each passage word to the question.\nThird, the convolution layer generates unigram, bigram and trigram representation for each word. bigram and trigram of a word ends with the same word, and proper padding is applied on the first word to make sure the output is the same length as input to CNN layer.\nFourth, the chunk representation layer dynamically extracts the candidate chunks from the given passage, and create chunk representation that encodes the contextual information of each chunk.\nFifth, the ranker layer scores the relevance between the representations of a chunk and the given question, and ranks all candidate chunks using a softmax layer.\nWe describe each step below.\nEncoder Layer We use bi-directional RNN encoder to encode Pi and Qi of example i, and get hidden state for each word position pij and qik.3 As RNN input, a word is represented by a row vector x \u2208 Rn. x can be the concatenation of word embedding and word features (see Fig. 1). The word vector for the t-th word is xt. A word sequence is processed using an RNN encoder with gated recurrent units (GRU) (Cho et al., 2014), which was proved to be effective in RC and neural machine translation tasks (Bahdanau et al., 2015; Kadlec et al., 2016; Dhingra et al., 2016). For each position t, GRU computes ht with input xt and previous state ht\u22121, as:\n3We can have separated parameters for question and passage encoders but a single shared encoder for both works better in the experiments.\nrt = \u03c3(Wrxt + Urht\u22121) (1) ut = \u03c3(Wuxt + Uuht\u22121) (2) h\u0304t = tanh(Wxt + U(rt ht\u22121)) (3) ht = (1\u2212 ut) \u00b7 ht\u22121 + ut \u00b7 h\u0304t (4)\nwhere ht, rt, and ut \u2208 Rd are d-dimensional hidden state, reset gate, and update gate, respectively; W{r,u}, W \u2208 Rn\u00d7d and U{r,u}, U \u2208 Rd\u00d7d are the parameters of the GRU; \u03c3 is the sigmoid function, and denotes element-wise production. For a word at t, we use the hidden state \u2212\u2192h t from the forward RNN as a representation of the preceding context, and the\u2190\u2212h t from a backward RNN that encodes text reversely, to incorporate the context after t. Next, ht = [ \u2212\u2192 ht ; \u2190\u2212 ht ], the bi-directional contextual encoding of xt, is formed. [\u00b7; \u00b7] is the concatenation operator. To distinguish hidden states from different sources, we denote the hj of j-th word in P and the hk of k-th word in Q as h p j and hqk respectively.\nAttention Layer Attention mechanism in previous RC tasks (Kadlec et al., 2016; Hermann et al., 2015; Sordoni et al., 2016; Dhingra et al., 2016; Cui et al., 2016a;b) enables question-aware passage representations. We propose a novel attention mechanism inspired by word-by-word style attention methods (Rockta\u0308schel et al., 2015; Wang & Jiang, 2015; Santos et al., 2016). For each pj , a questionattended representation vj is computed as follows (example index i is omitted for simplicity):\n\u03b1jk = h p j \u00b7 h q k, (5)\n\u03b2j = |Q|\u2211 k=1 \u03b1jkh q k (6) vj = [h p j ;\u03b2j ] (7)\nwhere hpj and h q k are hidden states from the bi-directional RNN encoders (see Figure 1). An inner product, \u03b1jk, is calculated between h p j and every question word h q k. It indicates how well the passage word pj matches with every question word qk. \u03b2j is a weighted pooling of |Q| question hidden states, which serves as a pj-aware question representation. The concatenation of h p j and \u03b2j leads to a passage-question joint representation, vj \u2208 R4d.4 Next, we apply a second bi-GRU layer taking the vjs as inputs, and obtain forward and backward representations \u2212\u2192\u03b3j and\u2190\u2212\u03b3j \u2208 Rd, and in turn their concatenation, \u03b3j = [\u2212\u2192\u03b3j ;\u2190\u2212\u03b3j ]. Convolution Layer Every word is encoded with complete passage context through attention layer RNN. We would like to model more complex representation of the words, by introducing unigram, bigram and trigram representations. There are two benefits for this enhanced representation: 1) each word could be enhanced with local context information to help identify the boundary of the answer chunk. Using previous words has been a common feature used in POS tagging and Named entity recognition; and 2) The information brought in by the ngram into the word representation could enhance the semantic match between the answer chunk internal and the question. Imagine scenario of a three word candidate, where the last word representation includes the two previous words through the convolution layer. Matching to the last word could also lead to the match to the semantics of the internal of the chunk. Specifically, we create for every word position j three representations, by using ngrams ending with the hidden state j:\n\u03b3\u0303j1 = \u03b3j \u00b7Wc1 (8) \u03b3\u0303j2 = [\u03b3j\u22121; \u03b3j ] \u00b7Wc2 (9) \u03b3\u0303j3 = [\u03b3j\u22122; \u03b3j\u22121; \u03b3j ] \u00b7Wc3 (10)\n4We tried another word-by-word attention methods as in (Santos et al., 2016), which has similar passage representation input to question side. However, this does not lead to improvement due to the confusion caused by long passages in RC. Consequently, we used the proposed simplified version of word-by-word attention on passage side only.\nThe details shown in equations above. We used three different convolution kernels for different n-grams.\nChunk Representation Layer A candidate answer chunk representation is dynamically created given convolution layer output. We first decide the text boundary for the candidate chunk, and then form a chunk representation using all or part of those \u03b3j outputs inside the chunk. To decide a candidate chunk (boundary): we tried two ways: (1) adopt the POS trie-based approach used in our baseline, and (2) enumerate all possible chunks up to a maximum number of tokens. For (2), we create up to N (max chunk length) chunks starting from any position j in Pj . Approach (1) can generate candidates with arbitrary lengths, but fails to recall candidates whose POS pattern is unseen in training set; whereas approach (2) considers all possible candidates within a window and is more flexible, but over-generates invalid candidates.\nFor a candidate answer chunk cm,n spanning from position m to n inclusively, we construct chunk representation \u03b3lm,n \u2208 R2d using every \u03b3\u0303jl within range [m,n], with a function g(\u00b7), and l \u2208 {1, 2, 3}. Formally,\n\u03b3lm,n = g(\u03b3\u0303ml, . . . , \u03b3\u0303nl)\nEach \u03b3\u0303jl is a convolution output over concatenated forward and backward RNN hidden states from attention layer. So the first half in \u03b3\u0303jl encodes information in forward RNN hidden states and the second half encodes information in backward RNN hidden states. We experimented with several pooling functions (e.g., max, average) for g(\u00b7), and found out that, instead of pooling, the best g(\u00b7) function is to concatenate the first half of convolution output of the chunk\u2019s first word and the second half of convolution output of the chunk\u2019s last word. Formally,\n\u03b3lm,n = g(\u03b3\u0303ml, . . . , \u03b3\u0303nl) = [ \u2212\u2192 \u03b3\u0303ml; \u2190\u2212 \u03b3\u0303nl] (11)\nwhere \u2212\u2192\u03b3\u0303ml is half of the hidden state for l-gram word representation corresponding to forward attention RNN output. We hypothesize that the hidden states at that two ends can better represent the chunk\u2019s contexts, which is critical for this task, than the states within the chunk. This observation also agrees with (Kobayashi et al., 2016).\nRanker Layer A score slm,n for each l-gram chunk representation \u03b3lm,n denoting the probability of that chunk to be the true answer is calculated by dot product with question representation. The question representation is the concatenation of the last hidden state in forward RNN and the first hidden state in backward RNN. Formally for the chunk cm,ni we have\nsl(cm,ni |Pi, Qi) = \u03b3 l m,n \u00b7 [ \u2212\u2212\u2192 hQi|Qi|; \u2190\u2212\u2212 hQi1 ] (12)\nwhere sl denotes the score generated from l-gram representation. \u2212\u2212\u2192 hQik or \u2190\u2212\u2212 hQik is the k-th hidden state output from question Qi\u2019s forward and backward RNN encoder, respectively.\nAfter that, the final score for cm,ni is evaluated as the linear combination of three scores, followed by a softmax:\ns(cm,ni |Pi, Qi) = softmax(W \u00b7 [s 1; s2; s3]) (13)\nwhere sl is the shorthand notation for sl(cm,ni |Pi, Qi); W \u2208 R3. In runtime, the chunk with the highest probability is taken as the answer. In training, the following negative log likelihood is minimized:\nL = \u2212 N\u2211 i=1 logP(Ai|Pi, Qi) (14)\nNote that the i-th training instance is only used when Ai is included in the corresponding candidate chunk set Ci, i.e. \u2203m,nAi = cm,ni . The softmax in the final layer serves as the list-wise ranking module similar in spirit to (Cao et al., 2007).\n5 EXPERIMENTS\nDataset We used the Stanford Question Answering Dataset (SQuAD) (Rajpurkar et al., 2016) for the experiment. SQuAD came into our sight because it is a mix of factoid and non-factoid\nquestions, a real-world data (crowd-sourced), and of large scale (over 100K question-answer pairs collected from 536 Wikipedia articles). Answers range from single words to long, variable-length phrase/clauses. It is a relaxation of assumptions by the cloze-style and quiz-style RC datasets in the Problem Definition section.\nFeatures The input vector representation of each word w to encoder RNNs has six parts including a pre-trained 300-dimensional GloVe embedding (Pennington et al., 2014) and five features (see Figure 1): (1) a one-hot encoding (46 dimensions) for the part-of-speech (POS) tag of w; (2) a one-hot encoding (14 dimensions) for named entity (NE) tag of w; (3) a binary value indicating whether w\u2019s surface form is the same to any word in the quesiton; (4) if the lemma form of w is the same to any word in the question; and (5) if w is caplitalized. Feature (3) and (4) are designed to help the model align the passage text with question. Note that some types of questions (e.g., \u201cwho\u201d, \u201cwhen\u201d questions) have answers that have a specific POS/NE tag pattern. For instance, \u201cwho\u201d questions mostly have proper nouns/persons as answers and \u201cwhen\u201d questions may frequently have numbers/dates (e.g., a year) as answers. Thus, we believe that the model could exploit the co-relation between question types and answer POS/NE patterns easier with POS and NE tag features. Implementation Details We pre-processed the SQuAD dataset using Stanford CoreNLP tool5 (Manning et al., 2014) with its default setting to tokenize the text and obtain the POS and NE annotations. To train our model, we used stochastic gradient descent with the ADAM optimizer (Kingma & Ba, 2014), with an initial learning rate of 0.001. All GRU weights were initialized from a uniform distribution between (-0.01, 0.01). The hidden state size, d, was set to 300 for all GRUs. The question bi-GRU shared parameters with the passage bi-GRU, while the attention-based passage bi-GRU had its own parameters. We shuffled all training examples at the beginning of each epoch and adopted a curriculum learning approach (Bengio et al., 2009), by sorting training instances by length in every 10 batches, to enable the model start learning from relatively easier instances and to harder ones. We also applied dropout of rate 0.2 to the embedding layer of input bi-GRU encoder, and gradient clipping when the norm of gradients exceeded 10. We trained in mini-batch style (mini-batch size is 180) and applied zero-padding to the passage and question inputs in each batch. We also set the maximum passage length to be 300 tokens, and pruned all the tokens after the 300-th token in the training set to save memory and speed up the training process. This step reduced the training set size by about 1.6%. During test, we test on the full length of passage, so that we don\u2019t prune out the potential candidates. We trained the model for at most 30 epochs, and in case the accuracy did not improve for 10 epochs, we stopped training.\nFor the feature ranking-based system, we used jforest ranker (Ganjisaffar et al., 2011) with LambdaMART-RegressionTree algorithm and the ranking metric was NDCG@10. For the Gated Attention Reader in baseline system, we replicated the method and use the same configurations as in (Dhingra et al., 2016).\nResults\nTable 2 shows our main results on the SQuAD dataset. Compared to the scores reported in (Wang & Jiang, 2016), our exact match (EM) and F1 on the development set and EM score on the test set are better, and F1 on the test set is comparable. We also studied how each component in our model contributes to the overall performance. Table 3 shows the details as well as the results of the baseline ranker. As the first row of Table 3 shows, our baseline system improves 10% (EM) over Rajpurkar et al. (Rajpurkar et al., 2016) (Table 2, row 1), the feature-based ranking system. However when compared to our DCR model (Table 3, row 2), the baseline (row 1) is more than 12% (EM) behind\n5 stanfordnlp.github.io/CoreNLP/\nWe also did ablation tests on our DCR model. First, replacing the word-by-word attention with Attentive Reader style attention (Hermann et al., 2015) decreases the EM score by about 4.5%, showing the strength of our proposed attention mechanism.\nSecond, we remove the features in input to see the contribution of each feature. The result shows that POS feature (1) and question-word feature (3) are the two most important features.\nFinally, combining the DCR model with the proposed POS-trie constraints yields a score similar to the one obtained using the DCR model with all possible n-gram chunks. The result shows that (1) our chunk representations are powerful enough to differentiate even a huge amount of chunks when no constraints are applied; and (2) the proposed POS-trie reduces the search space at the cost of a small drop in performance.\nAnalysis To better understand our system, we calculated the accuracy of the attention mechanism of the gated attention reader used in our deep learning-based baseline. We found that it is 72% accurate i.e., 72% of the times a word with the highest attention score is inside the correct answer span. This means that, if we could accurately detect the boundary around the word with the highest attention score to form the answer span, we could achieve an accuracy close to 72%. In addition, we checked the answer recall of our candidate chunking approach. When we use a window size of 10, 92% of the time, the ground truth answer will be included in the extracted Candidate chunk set. Thus the upper bound of the exact match score of our baseline system is around 66% (92% (the answer recall) \u00d7 72%). From the results, we see our DCR system\u2019s exact match score is at 62%. This shows that DCR is proficient at differentiating answer spans dynamically.\nTo further analyze the system\u2019s performance while predicting answers of different lengths, we show the exact match (EM) and F1 scores for answers with lengths up to 10 tokens in Figure 2(a). From the graph, we can see that, with the increase of answer length, both EM and F1 drops, but in different speed. The gap between F1 and exact match also widens as answer length increases. However, the model still yields a decent accuracy when the answer is longer than a single word. Additionally, Figure 2(b) shows that the system is better at \u201cwhen\u201d and \u201cwho\u201d questions, but performs poorly\non \u201cwhy\u201d questions. The large gap between exact match and F1 on \u201cwhy\u201d questions means that perfectly identifying the span is harder than locating the core of the answer span.\nSince \u201cwhat\u201d, \u201cwhich\u201d, and \u201chow\u201d questions contain a broad range of question types, we split them further based on the bigram a question starts with, and Figure 3 shows the breakdown for \u201cwhat\u201d questions. We can see that \u201cwhat\u201d questions asking for explanations such as \u201cwhat happens\u201d and \u201cwhat happened\u201d have lower EM and F1 scores. In contrast, \u201cwhat\u201d questions asking for year and numbers have much higher scores and, for these questions, exact match scores are close to F1 scores, which means chunking for these questions are easier for DCR.\n6 RELATED WORK\nAttentive Reader was the first neural model for factoid RCQA (Hermann et al., 2015). It uses Bidirectional RNN (Cho et al., 2014; Chung et al.,2014) to encode document and query respectively, and use query representation to match with every token from the document. Attention Sum Reader (Kadlec et al., 2016) simplifies the model to just predicting positions of correct answer in the document and the training speed and test accuracy are both greatly improved on the CNN/Daily Mail dataset. (Chen et al., 2016) also simplified Attentive Reader and reported higher accuracy. Windowbased Memory Networks (MemN2N) is introduced along with the CBT dataset (Hill et al., 2015), which does not use RNN encoders, but embeds contexts as memory and matches questions with embedded contexts. Those models\u2019 mechanism is to learn the match between answer context with question/query representation. In contrast, memory enhanced neural networks like Neural Turing Machines (Graves et al., 2014) and its variants (Zhang et al., 2015; Gulcehre et al., 2016; Zaremba & Sutskever, 2015; Chandar et al., 2016; Grefenstette et al., 2015) were also potential candidates for the task, and Gulcehre et al. (Gulcehre et al., 2016) reported results on the bAbI task, which is worse than memory networks. Similarly, sequence-to-sequence models were also used (Yu et al., 2015; Hermann et al., 2015), but they did not yield better results either.\nRecently, several models have been proposed to enable more complex inference for RC task. For instance, gated attention model (Dhingra et al., 2016) employs a multi-layer architecture, where each layer encodes the same document, but the attention is updated from layer to layer. EpiReader (Trischler et al., 2016b) adopted a joint training model for answer extractor and reasoner, where the extractor proposes top candidates, and the reasoner weighs each candidate by examining entailment relationship between question-answer representation and the document. An iterative alternating attention mechanism and gating strategies were proposed in (Sordoni et al., 2016) to optimize the attention through several hops. In contrast, Cui et al. (Cui et al., 2016a;b) introduced fine-grained document attention from each question word and then aggregated those attentions from each question token by summation with or without weights. This system achieved the state-of-the-art score on the CNN dataset. Those different variations all result in roughly 3-5% improvement over attention sum reader, but none of those could achieve higher than that. Other methods include using dynamic entity representation with max-pooling (Kobayashi et al., 2016) that aims to change entity representation with context, and Weissenborn\u2019s (Weissenborn, 2016) system, which tries to separate entity from the context and then matches the question to context, scoring an accuracy around 70% on the CNN dataset.\nHowever, all of those models assume that the answers are single tokens. This limits the type of questions the models can answer. Wang and Jiang (Wang & Jiang, 2016) proposed a match-lstm and achieved good results on SQuAD. However, this approach predicts a chunk boundary or whether a word is part of a chunk or not. In contrast, our approach explicitly constructs the chunk representations and similar chunks are compared directly to determine correct answer boundaries.\n7 CONCLUSION\nIn this paper we proposed a novel neural reading comprehension model for question answering. Different from the previously proposed models for factoid RCQA, the proposed model, dynamic chunk reader, is not restricted to predicting a single named entity as an answer or selecting an answer from a small, pre-defined candidate list. Instead, it is capable of answering both factoid and nonfactoid questions as it learns to select answer chunks that are suitable for an input question. DCR achieves this goal with a joint deep learning model enhanced with a novel attention mechanism and five simple yet effective features. Error analysis shows that the DCR model achieves good performance, but still needs to improve on predicting longer answers, which are usually non-factoid in nature.\n", "cats": {"READ": 1.0, "DONTREAD": 0.0}, "reviews": [{"IS_META_REVIEW": true, "comments": "SYNOPSIS: The paper proposes a new neural network-based model for reading comprehension (reading a passage of text and answering questions based on the passage). It is similar in spirit to several other recent models, with the main exception that it is able to predict answers of different lengths, as opposed to single words/tokens/entities. The authors compare their model on the Stanford Question Answering Dataset (SQuAD), and show improvements over the baselines, while apparently lagging quite far behind the current state of the art reported on the SQuAD leaderboard.\n\nTHOUGHTS: The main novelty of the method is to be able to identify phrases of different lengths as possible answers to the question. However, both approaches considered -- using a POS pattern trie tree to filter out word sequences with POS tags matching those of answers in the training set, and brute-force enumeration of all phrases up to length N -- seem somewhat orthogonal to the idea of \"learning end-to-end \" an answer chunk extraction model. Furthermore, as other reviews have pointed out, it seems that the linguistic features actually contribute a lot to the final accuracy (Table 3). One could argue that these are easy to obtain using standard taggers, but it takes away even more from the idea of an \"end-to-end trained\" system.\n\nThe paper is generally well written, but there are several crucial sections in parts describing the model where it was really hard for me to follow the descriptions. In particular, the attention mechanism seems fairly standard to me in a seq2seq sense (i.e. there is nothing architecturally novel about it, as is for instance the case with the Gated Attentive Reader). I may be missing something, but even after the clarification round I still don't understand how it is novel compared to standard attention used in for instance seq2seq models.\n\nFinally, although the method is shown to outperform the baseline method reported in the original paper introducing the SQuAD dataset, it currently seems to be 12th (out of 15 systems) on the leaderboard ("}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "The program committee appreciates the authors' response to the clarification questions and one review. Unfortunately, reviews are not leaning sufficiently towards acceptance. Reviewers have concerns about the novelty of the approach, its effectiveness in terms of empirical performance, and lack of analysis that would help determine the main contributions of the proposed approach. Authors are strongly encouraged to incorporate reviewer feedback in future iterations of the work.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"TITLE": "Review", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer4", "comments": "SYNOPSIS: The paper proposes a new neural network-based model for reading comprehension (reading a passage of text and answering questions based on the passage). It is similar in spirit to several other recent models, with the main exception that it is able to predict answers of different lengths, as opposed to single words/tokens/entities. The authors compare their model on the Stanford Question Answering Dataset (SQuAD), and show improvements over the baselines, while apparently lagging quite far behind the current state of the art reported on the SQuAD leaderboard.\n\nTHOUGHTS: The main novelty of the method is to be able to identify phrases of different lengths as possible answers to the question. However, both approaches considered -- using a POS pattern trie tree to filter out word sequences with POS tags matching those of answers in the training set, and brute-force enumeration of all phrases up to length N -- seem somewhat orthogonal to the idea of \"learning end-to-end \" an answer chunk extraction model. Furthermore, as other reviews have pointed out, it seems that the linguistic features actually contribute a lot to the final accuracy (Table 3). One could argue that these are easy to obtain using standard taggers, but it takes away even more from the idea of an \"end-to-end trained\" system.\n\nThe paper is generally well written, but there are several crucial sections in parts describing the model where it was really hard for me to follow the descriptions. In particular, the attention mechanism seems fairly standard to me in a seq2seq sense (i.e. there is nothing architecturally novel about it, as is for instance the case with the Gated Attentive Reader). I may be missing something, but even after the clarification round I still don't understand how it is novel compared to standard attention used in for instance seq2seq models.\n\nFinally, although the method is shown to outperform the baseline method reported in the original paper introducing the SQuAD dataset, it currently seems to be 12th (out of 15 systems) on the leaderboard (", "IS_META_REVIEW": false, "RECOMMENDATION": 4, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 3}, {"TITLE": "Review", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "The paper proposed an end-to-end machine learning model called dynamic reader for the machine reading comprehension task. Compared to earlier systems, the proposed model is able to extract and rank a set of answer candidates from a given document.\n\nThere are many recent models focusing on building good question answering systems by extracting phrases from a given article. It seems that there are two different aspects that are unique in this work:\n\n1.\tThe use of convolution model, and\n2.\tDynamic chunking\n\nConvolution network is often only used for modeling character-based word embeddings so I am curious about its effectiveness on representing phrases. Therefore, I wish there could be more analysis on how effective it is, as the authors do not compare the convolution framework to other alternative approaches such as LSTM. The comparisons are important, as the authors uses uni-gram, bi-gram and tri-gram information in the convolution network, and it is not clear to me that if tri-gram information is still needed for LSTM models.\n\nThe dynamic chunking is a good idea, and a very similar idea is proposed in some of the recent papers such as [Kenton et al, 16], which also targets at the same dataset. However, I would like to see more analysis on the dynamic chunking. Why this approach is a good approach for representing answer chunks? Given the representation of the chunk is constructed by the first and the end word representations generated by a convolution network, I am not sure about the ability of this representation to capture the long answer phrases.\n\nThe authors do not use character base embedding but use some of the previous trained NLP models. It would be interesting if the authors could show what are the advantages and disadvantages of using linguistic features compared to character embeddings.\n\nIn short, there are several good ideas proposed in the paper, but the lack of proper analysis make it difficult to judge how important the proposed techniques are.\n", "IS_META_REVIEW": false, "RECOMMENDATION": 5, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 3}, {"TITLE": "No Title", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "SUMMARY.\nThe paper propose a reading-comprehension question answering system for the recent QA task where answers of a question can be either single tokens or spans in the given text passage.\nThe model first encodes the passage and the query using a recurrent neural network.\nWith an attention mechanism the model calculates the importance of each word on the passage with respect to each word in the question.\nThe encoded words in the passage are concatenated with the attention; the resulting vector is re-encoded with a further RNN.\nThree convolutional neural networks with different filter size (1,2,3-gram) are used to further capture local features.\nCandidate answers are selected either matching POS patterns of answers in the training set or choosing all possible text span until a certain length.\nEach candidate answer has three representations, one for each n-gram representation. The compatibility of these representation with the question representation is then calculated.\nThe scores are combined linearly and used for calculating the probability of the candidate answer being the right answer for the question.\n\nThe method is tested on the SQUAD dataset and outperforms the proposed baselines.\n\n----------\n\nOVERALL JUDGMENT\nThe method presented in this paper is interesting but not very motivated in some points.\nFor example, it is not explained why in the attention mechanism it is beneficial to concatenate the original passage encoding with the attention-weighted ones.\nThe contributions of the paper are moderately novel proposing mainly the attention mechanism and the convolutional re-encoding.\nIn fact, combining questions and passages and score their compatibility has became a fairly standard procedure in all QA models.\n\n----------\n\nDETAILED COMMENTS\n\nEquation (13) i should be s, not s^l.\n\nI still do not understand the sentence \" the best function is to concatenate the hidden stat of the fist word in a chunk in forward RNN and that of the last word in backward RNN\". The RNN is over what all the words in the chunk? in the passage? \nThe answer the authors gave in the response does not clarify this point.", "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "15 Dec 2016", "REVIEWER_CONFIDENCE": 3}, {"DATE": "04 Dec 2016", "TITLE": "More clarifications", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer4"}, {"DATE": "02 Dec 2016", "TITLE": "Pre-review Questions", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3"}, {"DATE": "30 Nov 2016", "TITLE": "Cnn questions", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2"}, {"IS_META_REVIEW": true, "comments": "SYNOPSIS: The paper proposes a new neural network-based model for reading comprehension (reading a passage of text and answering questions based on the passage). It is similar in spirit to several other recent models, with the main exception that it is able to predict answers of different lengths, as opposed to single words/tokens/entities. The authors compare their model on the Stanford Question Answering Dataset (SQuAD), and show improvements over the baselines, while apparently lagging quite far behind the current state of the art reported on the SQuAD leaderboard.\n\nTHOUGHTS: The main novelty of the method is to be able to identify phrases of different lengths as possible answers to the question. However, both approaches considered -- using a POS pattern trie tree to filter out word sequences with POS tags matching those of answers in the training set, and brute-force enumeration of all phrases up to length N -- seem somewhat orthogonal to the idea of \"learning end-to-end \" an answer chunk extraction model. Furthermore, as other reviews have pointed out, it seems that the linguistic features actually contribute a lot to the final accuracy (Table 3). One could argue that these are easy to obtain using standard taggers, but it takes away even more from the idea of an \"end-to-end trained\" system.\n\nThe paper is generally well written, but there are several crucial sections in parts describing the model where it was really hard for me to follow the descriptions. In particular, the attention mechanism seems fairly standard to me in a seq2seq sense (i.e. there is nothing architecturally novel about it, as is for instance the case with the Gated Attentive Reader). I may be missing something, but even after the clarification round I still don't understand how it is novel compared to standard attention used in for instance seq2seq models.\n\nFinally, although the method is shown to outperform the baseline method reported in the original paper introducing the SQuAD dataset, it currently seems to be 12th (out of 15 systems) on the leaderboard ("}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "The program committee appreciates the authors' response to the clarification questions and one review. Unfortunately, reviews are not leaning sufficiently towards acceptance. Reviewers have concerns about the novelty of the approach, its effectiveness in terms of empirical performance, and lack of analysis that would help determine the main contributions of the proposed approach. Authors are strongly encouraged to incorporate reviewer feedback in future iterations of the work.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"TITLE": "Review", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer4", "comments": "SYNOPSIS: The paper proposes a new neural network-based model for reading comprehension (reading a passage of text and answering questions based on the passage). It is similar in spirit to several other recent models, with the main exception that it is able to predict answers of different lengths, as opposed to single words/tokens/entities. The authors compare their model on the Stanford Question Answering Dataset (SQuAD), and show improvements over the baselines, while apparently lagging quite far behind the current state of the art reported on the SQuAD leaderboard.\n\nTHOUGHTS: The main novelty of the method is to be able to identify phrases of different lengths as possible answers to the question. However, both approaches considered -- using a POS pattern trie tree to filter out word sequences with POS tags matching those of answers in the training set, and brute-force enumeration of all phrases up to length N -- seem somewhat orthogonal to the idea of \"learning end-to-end \" an answer chunk extraction model. Furthermore, as other reviews have pointed out, it seems that the linguistic features actually contribute a lot to the final accuracy (Table 3). One could argue that these are easy to obtain using standard taggers, but it takes away even more from the idea of an \"end-to-end trained\" system.\n\nThe paper is generally well written, but there are several crucial sections in parts describing the model where it was really hard for me to follow the descriptions. In particular, the attention mechanism seems fairly standard to me in a seq2seq sense (i.e. there is nothing architecturally novel about it, as is for instance the case with the Gated Attentive Reader). I may be missing something, but even after the clarification round I still don't understand how it is novel compared to standard attention used in for instance seq2seq models.\n\nFinally, although the method is shown to outperform the baseline method reported in the original paper introducing the SQuAD dataset, it currently seems to be 12th (out of 15 systems) on the leaderboard (", "IS_META_REVIEW": false, "RECOMMENDATION": 4, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 3}, {"TITLE": "Review", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "The paper proposed an end-to-end machine learning model called dynamic reader for the machine reading comprehension task. Compared to earlier systems, the proposed model is able to extract and rank a set of answer candidates from a given document.\n\nThere are many recent models focusing on building good question answering systems by extracting phrases from a given article. It seems that there are two different aspects that are unique in this work:\n\n1.\tThe use of convolution model, and\n2.\tDynamic chunking\n\nConvolution network is often only used for modeling character-based word embeddings so I am curious about its effectiveness on representing phrases. Therefore, I wish there could be more analysis on how effective it is, as the authors do not compare the convolution framework to other alternative approaches such as LSTM. The comparisons are important, as the authors uses uni-gram, bi-gram and tri-gram information in the convolution network, and it is not clear to me that if tri-gram information is still needed for LSTM models.\n\nThe dynamic chunking is a good idea, and a very similar idea is proposed in some of the recent papers such as [Kenton et al, 16], which also targets at the same dataset. However, I would like to see more analysis on the dynamic chunking. Why this approach is a good approach for representing answer chunks? Given the representation of the chunk is constructed by the first and the end word representations generated by a convolution network, I am not sure about the ability of this representation to capture the long answer phrases.\n\nThe authors do not use character base embedding but use some of the previous trained NLP models. It would be interesting if the authors could show what are the advantages and disadvantages of using linguistic features compared to character embeddings.\n\nIn short, there are several good ideas proposed in the paper, but the lack of proper analysis make it difficult to judge how important the proposed techniques are.\n", "IS_META_REVIEW": false, "RECOMMENDATION": 5, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 3}, {"TITLE": "No Title", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "SUMMARY.\nThe paper propose a reading-comprehension question answering system for the recent QA task where answers of a question can be either single tokens or spans in the given text passage.\nThe model first encodes the passage and the query using a recurrent neural network.\nWith an attention mechanism the model calculates the importance of each word on the passage with respect to each word in the question.\nThe encoded words in the passage are concatenated with the attention; the resulting vector is re-encoded with a further RNN.\nThree convolutional neural networks with different filter size (1,2,3-gram) are used to further capture local features.\nCandidate answers are selected either matching POS patterns of answers in the training set or choosing all possible text span until a certain length.\nEach candidate answer has three representations, one for each n-gram representation. The compatibility of these representation with the question representation is then calculated.\nThe scores are combined linearly and used for calculating the probability of the candidate answer being the right answer for the question.\n\nThe method is tested on the SQUAD dataset and outperforms the proposed baselines.\n\n----------\n\nOVERALL JUDGMENT\nThe method presented in this paper is interesting but not very motivated in some points.\nFor example, it is not explained why in the attention mechanism it is beneficial to concatenate the original passage encoding with the attention-weighted ones.\nThe contributions of the paper are moderately novel proposing mainly the attention mechanism and the convolutional re-encoding.\nIn fact, combining questions and passages and score their compatibility has became a fairly standard procedure in all QA models.\n\n----------\n\nDETAILED COMMENTS\n\nEquation (13) i should be s, not s^l.\n\nI still do not understand the sentence \" the best function is to concatenate the hidden stat of the fist word in a chunk in forward RNN and that of the last word in backward RNN\". The RNN is over what all the words in the chunk? in the passage? \nThe answer the authors gave in the response does not clarify this point.", "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "15 Dec 2016", "REVIEWER_CONFIDENCE": 3}, {"DATE": "04 Dec 2016", "TITLE": "More clarifications", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer4"}, {"DATE": "02 Dec 2016", "TITLE": "Pre-review Questions", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3"}, {"DATE": "30 Nov 2016", "TITLE": "Cnn questions", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2"}]}
{"text": "LEARNING TO DISCOVER SPARSE GRAPHICAL MODELS\n1 INTRODUCTION\nProbabilistic graphical models provide a powerful framework for describing the dependencies between a set of variables. Many applications infer the structure of a probabilistic graphical model from data to elucidate the relationships between variables. These relationships are often represented by an undirected graphical model also known as a Markov Random Field (MRF). We focus on a common MRF model, Gaussian graphical models (GGMs). GGMs are used in structure-discovery settings for rich data such as neuroimaging, genetics, or finance (Friedman et al., 2008; Ryali et al, 2012; Mohan et al., 2012; Belilovsky et al., 2016). Although multivariate Gaussian distributions are well-behaved, determining likely structures from few examples is a complex task when the data is high dimensional. It requires strong priors, typically a sparsity assumption, or other restrictions on the structure of the graph, which now make the distribution difficult to express analytically and use.\nA standard approach to estimating structure with GGMs in high dimensions is based on the classic result that the zeros of a precision matrix correspond to zero partial correlation, a necessary and sufficient condition for conditional independence (Lauritzen, 1996). Assuming only a few conditional dependencies corresponds to a sparsity constraint on the entries of the precision matrix, leading to a combinatorial problem. Many popular approaches to learning GGMs can be seen as leveraging the\n`1-norm to create convex surrogates to this problem. Meinshausen & B\u00fchlmann (2006) use nodewise `1 penalized regressions. Other estimators penalize the precision matrix directly (Cai et al., 2011; Friedman et al., 2008; Ravikumar et al., 2011). The most popular being the graphical lasso\nfglasso(\u03a3\u0302) = arg min \u0398 0 \u2212 log |\u0398|+ Tr (\u03a3\u0302\u0398) + \u03bb\u2016\u0398\u20161, (1)\nwhich can be seen as a penalized maximum-likelihood estimator. Here \u0398 and \u03a3\u0302 are the precision and sample covariance matrices, respectively. A large variety of alternative regularization penalties extend the priors of the graphical lasso (Danaher et al., 2014; Ryali et al, 2012; Varoquaux et al., 2010). However, several problems arise in this approach. Constructing novel surrogates for structured-sparsity assumptions on MRF structures is challenging, as a prior needs to be formulated and incorporated into a penalized maximum likelihood objective which then needs an efficient optimization algorithm to be developed, often within a separate research effort. Furthermore, model selection in a penalized maximum likelihood setting is difficult as regularization parameters are often unintuitive.\nWe propose to learn the estimator. Rather than manually designing a specific graph-estimation procedure, we frame this estimator-engineering problem as a learning problem, selecting a function from a large flexible function class by risk minimization. This allows us to construct a loss function that explicitly aims to recover the edge structure. Indeed, sampling from a distribution of graphs and empirical covariances with desired properties is often possible, even when this distribution is not analytically tractable. As such we can perform empirical risk minimization to select an appropriate function for edge estimation. Such a framework gives more easy control on the assumed level of sparsity (as opposed to graph lasso) and can impose structure on the sampling to shape the expected distribution, while optimizing a desired performance metric.\nFor particular cases we show that the problem of interest can be solved with a polynomial function, which is learnable with a neural network (Andoni et al., 2014). Motivated by this fact, as well as theoretical and empricial results on learning smooth functions approximating solutions to combinatorial problems (Cohen et al., 2016; Vinyals et al., 2015), we propose to use a particular convolutional neural network as the function class. We train it by sampling small datasets, generated from graphs with the prescribed properties, with a primary focus on sparse graphical models. We estimate from this data small-sample covariance matrices (n < p), where n is the number of samples and p is the dimensionality of the data. Then we use them as training data for the neural network (Figure 2) where target labels are indicators of present and absent edges in the underlying GGM. The learned network can then be employed in various real-world structure discovery problems.\nIn Section 1.1 we review the related work. In Section 2 we formulate the risk minimization view of graph-structure inference and describe how it applies to sparse GGMs. Section 2.3 describes and motivates the deep-learning architecture we chose to use for the sparse GGM problem in this work. In Section 3 we describe the details of how we train an edge estimator for sparse GGMs. We then evaluate its properties extensively on simulation data. Finally, we show that this edge estimator trained only on synthetic data can obtain state of the art performance at inference time on real neuroimaging and genetics problems, while being much faster to execute than other methods.\n1.1 RELATED WORK\nLopez-Paz et al. (2015) analyze learning functions to identify the structure of directed graphical models in causal inference using estimates of kernel-mean embeddings. As in our work, they demonstrate the use of simulations for training while testing on real data. Unlike our work, they primarily focus on finding the causal direction in two node graphs with many observations.\nOur learning architecture is motivated by the recent literature on deep networks. Vinyals et al. (2015) have shown that neural networks can learn approximate solutions to NP-hard combinatorial problems, and the problem of optimal edge recovery in MRFs can be seen as a combinatorial optimization problem. Several recent works have been proposed which show neural architectures for graph input data (Henaff et al., 2015; Duvenaud et al, 2015; Li et al., 2016). These are based on multi layer convolutional networks, as in our work, or multi-step recurrent neural networks. The input in our approach can be viewed as a complete graph, while the ouput a sparse graph, thus none of these are directly applicable. A related use of deep networks to approximate a posterior distribution can be found in Balan et al. (2015). Finally, Gregor & LeCun (2010); Xin et al. (2016) use deep networks to approximate steps of a known sparse recovery algorithm.\nBayesian approaches to structure learning rely on priors on the graph combined with sampling techniques to estimate the posterior of the graph structure. Some approaches make assumptions on the decomposability of the graph (Moghaddam et al., 2009). The G-Wishart distribution is a popular distribution which forms part of a framework for structure inference, and advances have been recently made in efficient sampling (Mohammadi & Wit, 2015). These methods can still be rather slow compared to competing methods, and in the setting of p > n we find they are less powerful.\n2 METHODS\n2.1 LEARNING AN APPROXIMATE EDGE ESTIMATION PROCEDURE\nWe consider MRF edge estimation as a learnable function. Let X \u2208 Rn\u00d7p be a matrix whose n rows are i.i.d. samples x \u223c P (x) of dimension p. Let G = (V,E) be an undirected and unweighted graph associated with the set of variables in x. Let L = {0, 1} and Ne = p(p\u22121)2 the maximum possible edges in E. Let Y \u2208 LNe indicate the presence or absence of edges in the edge set E of G, namely\nY ij = { 0 xi \u22a5 xj |xV \\i,j 1 xi 6\u22a5 xj |xV \\i,j\n(2)\nWe define an approximate structure discovery method gw(X), which produces a prediction of the edge structure, Y\u0302 = gw(X), given a set of data X . We focus on X drawn from a Gaussian distribution. In this case, the empirical covariance matrix, \u03a3\u0302, is a sufficient statistic of the population covariance and therefore of the conditional dependency structure. We thus express our structure-recovery problem as a function of \u03a3\u0302: gw(X) := fw(\u03a3\u0302). fw is parametrized by w and belongs to the function class F . We note that the graphical lasso in Equation (1) is an fw for an appropriate choice of F . This view on the edge estimator now allows us to bring the selection of fw from the domain of human design to the domain of empirical risk minimization over F . Defining a distribution P on Rp\u00d7p \u00d7 LNe such that (\u03a3\u0302, Y ) \u223c P, we would like our estimator, fw, to minimize the expected risk R(f) = E(\u03a3\u0302,Y )\u223cP[l(f(\u03a3\u0302), Y )] (3) Here l : LNe\u00d7LNe \u2192 R+ is the loss function. For graphical model selection the 0/1 loss function is the natural error metric to consider (Wang et al., 2010). The estimator with minimum risk is generally not possible to compute as a closed form expression for most interesting choices of P, such as those arising from sparse graphs. In this setting, Eq. (1) achieves the information theoretic optimal recovery rate up to a constant for certain P corresponding to uniformly sparse graphs with a maximum degree, but only when the optimal \u03bb is used and the non-zero precision matrix values are bounded away from zero (Wang et al., 2010; Ravikumar et al., 2011).\nThe design of the estimator in Equation (1) is not explicitly minimizing this risk functional. Thus modifying the estimator to fit a different class of graphs (e.g. small-world networks) while minimizing R(f) is not obvious. Furthermore, in practical settings the optimal \u03bb is unknown and precision matrix entries can be very small. We would prefer to directly minimize the risk functional. Desired structural assumptions on samples from P on the underlying graph, such as sparsity, may imply that the distribution is not tractable for analytic solutions. Meanwhile, we can often devise a sampling procedure for P allowing us to select an appropriate function via empirical risk minimization. Thus it is sufficient to define a rich enough F over which we can minimize the empirical risk over the samples generated, giving us a learning objective over N samples {Yk,\u03a3k}Nk=1 drawn from P: min w 1 N \u2211N k=1 l(fw(\u03a3\u0302k), Yk). To maintain tractability, we use the standard cross-entropy loss as a convex surrogate, l\u0302 : RNe \u00d7 LNe , given by: l\u0302(fw(\u03a3\u0302), Y ) =\n\u2211 i 6=j ( Y ij log(f ijw (\u03a3\u0302)) + (1\u2212 Y ij) log(1\u2212 f ijw (\u03a3\u0302)) ) . (4)\nWe now need to select a sufficiently rich function class for fw and a method to produce appropriate (Y, \u03a3\u0302) which model our desired data priors. This will allow us to learn a fw that explicitly attempts to minimize errors in edge discovery.\n2.2 DISCOVERING SPARSE GAUSSIAN GRAPHICAL MODELS AND BEYOND\nWe discuss how the described approach can be applied to recover sparse Gaussian graphical models. A typical assumption in many modalities is that the number of edges is sparse. A convenient property of these GGMs is that the precision matrix has a zero value in the (i, j)th entry precisely when variables i and j are independent conditioned on all others. Additionally, the precision matrix and partial correlation matrix have the same sparsity pattern, while the partial correlation matrix has normalized entries.\nAlgorithm 1 Training a GGM edge estimator for i \u2208 {1, .., N} do\nSample Gi \u223c P(G) Sample \u03a3i \u223c P(\u03a3|G = Gi) Xi \u2190 {xj \u223c N(0,\u03a3i)}nj=1 Construct (Yi, \u03a3\u0302i) pair from (Gi,Xi)\nend for Select Function Class F (e.g. CNN) Optimize: min\nf\u2208F 1 N\n\u2211N k=1 l\u0302(f(\u03a3\u0302k), Yk)) We propose to simulate our a priori assumptions of sparsity and Gaussianity to learn fw(\u03a3\u0302), which can then produce predictions of edges from the input data. We model P (x|G) as arising from a sparse prior on the graph G and correspondingly the entries of the precision matrix \u0398. To obtain a single sample of X corresponds to n i.i.d. samples from N (0,\u0398\u22121). We can now train fw(\u03a3\u0302) by generating sample pairs (\u03a3\u0302, Y ). At execution time we standardize the input data and compute the covariance matrix before evaluating fw(\u03a3\u0302). The process of learning fw for the sparse GGM is given in Algorithm 1. A weakly-informative sparsity prior is one where each edge is equally likely with small probability, versus structured sparsity where edges have specific configurations. For obtaining the training samples (\u03a3\u0302, Y ) in this case we would like to create a sparse precision matrix, \u0398, with the desired number of zero entries distributed uniformly. One strategy to do this and assure the precision matrices lie in the positive definite cone is to first construct an upper triangular sparse matrix and then multiply it by its transpose. This process is described in detail in the experimental section. Alternatively, an MCMC based G-Wishart distribution sampler can be employed if specific structures of the graph are desired (Lenkoski, 2013).\nThe sparsity patterns in real data are often not uniformly distributed. Many real world networks have a small-world structure: graphs that are sparse and yet have a comparatively short average distance between nodes. These transport properties often hinge on a small number of high-degree nodes called hubs. Normally, such structural patterns require sophisticated adaptation when applying estimators like Eq. (1). Indeed, high-degree nodes break the small-sample, sparse-recovery properties of `1-penalized estimators (Ravikumar et al., 2011). In our framework such structural assumptions appear as a prior that can be learned offline during training of the prediction function. Similarly priors on other distributions such as general exponential families can be more easily integrated. As the structure discovery model can be trained offline, even a slow sampling procedure may suffice.\n2.3 NEURAL NETWORK GRAPH ESTIMATOR\nIn this work we propose to use a neural network as our function fw. To motivate this let us consider the extreme case when n p. In this case \u03a3\u0302 \u2248 \u03a3 and thus entries of \u03a3\u0302\u22121 or the partial correlation that are almost equal to zero can give the edge structure. Definition 1 (P-consistency). A function class F is P-consistent if \u2203f \u2208 F such that\nE(\u03a3\u0302,Y )\u223cP[l(f(\u03a3\u0302), Y )]\u2192 0 as n\u2192\u221e with high probability.\nProposition 1 (Existence of P-consistent neural network graph estimator). There exists a feed forward neural network function class F that is P-consistent. Proof. If the data is standardized, each entry of \u03a3 corresponds to the correlation \u03c1i,j . The partial correlation of edge (i, j) conditioned on nodes Z, is given recursively as\n\u03c1i,j|Z = (\u03c1i,j|Z\\zo \u2212 \u03c1i,zo|Z\\zo\u03c1j,zo|Z\\zo) 1\nD . (5)\nWe may ignore the denominator, D, as we are interested in I(\u03c1i,j|Z = 0). Thus we are left with a recursive formula that yields a high degree polynomial. From Andoni et al. (2014, Theorem 3.1) using gradient descent, a neural network with only two layers can learn a polynomial function of degree d to arbitrary precision given sufficient hidden units.\nRemark 1. Na\u00efvely the polynomial from the recursive definition of partial correlation is of degree bounded by 2p\u22122. In the worst case, this would seem to imply that we would need an exponentially\ngrowing number of hidden nodes to approximate it. However, this problem has a great deal of structure that can allow efficient approximation. Firstly, higher order monomials will go to zero quickly with a uniform prior on \u03c1i,j , which takes values between 0 and 1, suggesting that in many cases a concentration bound exists that guarantees non-exponential growth. Furthermore, the existence result is shown already for a shallow network, and we expect a logarithmic decrease in the number of parameters to peform function estimation with a deep network (Cohen et al., 2016).\nMoreover, there are a great deal of redundant computations in Eq. (5) and an efficient dynamic programming implementation can yield polynomial computation time and require only low order polynomial computations with appropriate storage of previous computation. Similarly we would like to design a network that would have capacity to re-use computations across edges and approximate low order polynomials. We also observe that the conditional independence of nodes i, j given Z can be computed equivalently in many ways by considering many paths through the nodes Z. Thus we can choose any valid ordering for traversing the nodes starting from a given edge.\nWe propose a series of shared operations at each edge. We consider a feedforward network where each edge i, j is associated with a fixed sized vector, oki,j , of dimensionality d at each layer, k > 0. o0i,j is initialized to the covariance entries at k = 0. For each edge we start with a neighborhood of the 6 adjacent nodes, i, j, i-1, i+1, j-1, j+1 for which we take all corresponding edge values from the covariance matrix illustrated in Figure 1. We proceed at each layer to increase the nodes considered for each edge, the output at each layer progressively increasing the receptive field making sure all values associated with the considered nodes are present. The receptive field here refers to the original covariance entries which are accessible by a given, oki,j (Luo et al., 2010). The equations defining the process are shown in Figure 1. Here a neural network fwk is applied at each edge at each layer and a dilation sequence dk is used. We call a network of this topology a D-Net of depth l. We use dilation here to allow the receptive field to grow fast, so the network does not need a great deal of layers. We make the following observations: Proposition 2. For general P it is a necessary condition for P-consistency that the receptive field of D-Net covers all entries of the covariance, \u03a3\u0302, at any edge it is applied. Proof. Consider nodes i and j and a chain graph such that i and j are adjacent to each other in the matrix but are at the terminal nodes of the chain graph. One would need to consider all other variables to be able to explain away the correlation. Alternatively we can see this directly from expanding Eq. (5).\nProposition 3. A p\u00d7 p matrix \u03a3\u0302 will be covered by the receptive field for a D-Net of depth log2(p) and dk = 2k\u22121 Proof. The receptive field of a D-Net with dilation sequence dk = 2k\u22121 of depth l is O(2l). We can see this as oki,j will receive input from o k\u22121 a,b at the edge of it\u2019s receptive field, effectively doubling it. It now follows that we need at least log2(p) layers to cover the receptive field.\nIntuitively adjacent edges have a high overlap in their receptive fields and can easily share information about the non-overlapping components. This is analogous to a parametrized message passing. For example if edge (i, j) is explained by node k, as k enters the receptive field of edge (i, j \u2212 1),\nthe path through (i, j) can already be discounted. In terms of Eq. 5 this can correspond to storing computations that can be used by neighbor edges from lower levels in the recursion.\nHere fwk is shared amongst all nodes and thus we can implement this as a special kind of convolutional network. We make sure that to have considered all edges relevant to the current set of nodes in the receptive field which requires us to add values from filters applied at the diagonal to all edges. In Figure 1 we illustrate the nodes and receptive field considered with respect to the covariance matrix. This also motivates a straightforward implementation using 2D convolutions (adding separate convolutions at i, i and j, j to each i, j at each layer to achieve the specific input pattern described) shown in (Figure 2).\nUltimately our choice of architecture that has shared computations and multiple layers is highly scalable as compared with a naive fully connected approach and allows leveraging existing optimized 2-D convolutions. In preliminary work we have also considered fully connected layers but this proved to be much less efficient in terms of storage and scalibility than using deep convolutional networks.\nConsidering the general n p case is illustrative. However, the main advantages of making the computations differentiable and learned from data is that we can take advantage of the sparsity and structure assumptions on the target function to obtain more efficient results than naive computation of partial correlation or matrix inversion. As n decreases our estimate of \u03c1\u0302i,j becomes inexact and here a data driven model which can take advantage of the assumptions on the underlying distribution can more accurately recover the graph structure.\nThe convolution structure is dependent on the order of the variables used to build the covariance matrix, which is arbitrary. Permuting the input data we can obtain another estimate of the output. In the experiments, we leverage these various estimate in an ensembling approach, averaging the results of several permutations of input. We observe that this generally yields a modest increase in accuracy, but that even a single node ordering can show substantially improved performance over competing methods in the literature.\n3 EXPERIMENTS\nOur experimental evaluations focus on the challenging high dimensional settings in which p > n and consider both synthetic data and real data from genetics and neuroimaging. In our experiments we explore how well networks trained on parametric samples generalize, both to unseen synthetic data and to several real world problems. In order to highlight the generality of the learned networks, we apply the same network to multiple domains. We train networks taking in 39, 50, and 500 node graphs. The former sizes are chosen based on the real data we consider in subsequent sections. We refer to these networks as DeepGraph-39, 50, and 500. In all cases we have 50 feature maps of 3\u00d7 3 kernels. The 39 and 50 node network with 6 convolutional layers and dk = k + 1. For the 500 node network with 8 convolutional layers and dk = 2k+1. We use ReLU activations. The last layer has 1\u00d7 1 convolution and a sigmoid outputing a value of 0 to 1 for each edge. We sample P (X|G) with a sparse prior on P (G) as follows. We first construct a lower diagonal matrix, L, where each entry has \u03b1 probability of being zero. Non-zero entries are set uniformly between \u2212c and c. Multiplying LLT gives a sparse positive definite precision matrix, \u0398. This gives us our P (\u0398|G) with a sparse prior on P (G). We sample from the Gaussian N (0,\u0398\u22121) to obtain\nsamples of X . Here \u03b1 corresponds approximately to a specific sparsity level in the final precision matrix, which we set to produce matrices 92\u2212 96% sparse and c chosen so that partial correlations range 0 to 1.\nEach network is trained continously with new samples generated until the validation error saturates. For a given precision matrix we generate 5 possible X samples to be used as training data, with a total of approximately 100K training samples used for each network. The networks are optimized using ADAM (Kingma & Ba, 2015) coupled with cross-entropy loss as the objective function (cf. Sec. 2.1). We use batch normalization at each layer. Additionally, we found that using the absolute value of the true partial correlations as labels, instead of hard binary labels, improves results.\nSynthetic Data Evaluation To understand the properties of our learned networks, we evaluated them on different synthetic data than the ones they were trained on. More specifically, we used a completely different third party sampler so as to avoid any contamination. We use DeepGraph-39 on a variety of settings. The same trained network is utilized in the subsequent neuroimaging evaluations as well. DeepGraph-500 is also used to evaluate larger graphs.\nWe used the BDGraph R-package to produce sparse precision matrices based on the G-Wishart distribution (Mohammadi & Wit, 2015) as well as the R-package rags2ridges (Peeters et al., 2015) to generate data from small-world networks corresponding to the Watts\u2013Strogatz model (Watts & Strogatz, 1998). We compared our learned estimator against the scikit-learn (Pedregosa et al, 2011) implementation of Graphical Lasso with regularizer chosen by cross-validation as well as the Birth-Death Rate MCMC (BDMCMC) method from Mohammadi & Wit (2015).\nFor each scenario we repeat the experiment for 100 different graphs and small sample observations showing the average area under the ROC curve (AUC), precision@k corresponding to 5% of possible edges, and calibration error (CE) (Mohammadi & Wit, 2015).\nFor graphical lasso we use the partial correlations to indicate confidence in edges; BDGraph automatically returns posterior probabilities as does our method. Finally to understand the effect of the regularization parameter we additionally report the result of graphical lasso under optimal regularizer setting on the testing data.\nOur method dominates all other approaches in all cases with p > n (which also corresponds to the training regime). For the case of random Gaussian graphs with n=35 (as in our training data), and graph sparsity of 95%, we have superior performance and can further improve on this by averaging permutations. Next we apply the method to a less straightforward synthetic data, with distributions typical of many applications. We found that, compared to baseline methods, our network performs particularly well with high-degree nodes and when the distribution becomes non-normal. In particular our method performs well on the relevant metrics with small-world networks, a very common family of graphs in real-world data, obtaining superior precision at the primary levels of interest. Figure 3 shows examples of random and Watts-Strogatz small-world graphs used in these experiments.\nTraining a new network for each number of samples can pose difficulties with our proposed method. Thus we evaluted how robust the network DeepGraph-39 is to input covariances obtained from fewer or more samples. We find that overall the performance is quite good even when lowering the number of samples to n = 15, we obtain superior performance to the other approaches (Table 1). We also applied DeepGraph-39 on data from a multivariate generalization of the Laplace distribution (G\u00f3mez et al., 1998). As in other experiments precision matrices were sampled from the G-Wishart at a sparsity of 95%. G\u00f3mez et al. (1998, Proposition 3.1) was applied to produce samples. We find that DeepGraph-39 performs competitively, despite the discrepancy between train and test distributions. Experiments with variable sparsity are considered in the supplementary material, which find that for very sparse graphs, the networks remain robust in performance, while for increased density performance degrades but remains competitive.\nUsing the small-world network data generator (Peeters et al., 2015), we demonstrate that we can update the generic sparse prior to a structured one. We re-train DeepGraph-39 using only 1000 examples of small-world graphs mixed with 1000 examples from the original uniform sparsity model. We perform just one epoch of training and observe markedly improved performance on this test case as seen in the last row of Table 1.\nFor our final scenario we consider the very challenging setting with 500 nodes and only n = 50 samples. We note that the MCMC based method fails to converge at this scale, while graphical lasso is very slow as seen in the timing performance and barely performs better than chance. Our method convincingly outperforms graphical lasso in this scenario. Here we additionally report precision at just the first 0.05% of edges since competitors perform nearly at chance at the 5% level.\nCancer Genome Data We perform experiments on a gene expression dataset described in Honorio et al. (2012). The data come from a cancer genome atlas from 2360 subjects for various types of cancer. We used the first 50 genes from Honorio et al. (2012, Appendix C.2) of commonly regulated genes in cancer. We evaluated on two groups of subjects, one with breast invasive carcinoma (BRCA) consisting of 590 subjects and the other colon adenocarcinoma (CODA) consisting of 174 subjects.\nEvaluating edge selection in real-world data is challenging. We use the following methodology: for each method we select the top-k ranked edges, recomputing the maximum likelihood precision matrix with support given by the corresponding edge selection method. We then evaluate the likelihood on a held-out set of data. We repeat this procedure for a range of k. We rely on Algorithm 0 in Hara & Takemura (2010) to compute the maximum likelihood precision given a support. The experiment is repeated for each of CODA and BRCA subject groups 150 times. Results are shown in Figure 4. In all cases we use 40 samples for edge selection and precision estimation. We compare with graphical lasso as well as the Ledoit-Wolf shrinkage estimator (Ledoit & Wolf, 2004). We additionally consider the MCMC based approach described in previous section. For graphical lasso and Ledoit-Wolf, edge selection is based on thresholding partial correlation (Balmand & Dalalyan, 2016).\nAdditionally, we evaluate the stability of the solutions provided by the various methods. In several applications a low variance on the estimate of the edge set is important. On Table 3, we report\nSpearman correlations between pairs of solutions, as it is a measure of a monotone link between two variables. DeepGraph has far better stability in the genome experiments and is competitive in the fMRI data.\nResting State Functional Connectivity We evaluate our graph discovery method to study brain functional connectivity in resting-state fMRI data. Correlations in brain activity measured via fMRI reveal functional interactions between remote brain regions. These are an important measure to study psychiatric diseases that have no known anatomical support. Typical connectome analysis describes each subject or group by a GGM measuring functional connectivity between a set of regions (Varoquaux & Craddock, 2013). We use the ABIDE dataset (Di Martino et al, 2014), a large scale resting state fMRI dataset. It gathers brain scans from 539 individuals suffering from autism spectrum disorder and 573 controls over 16 sites.1 For our experiments we use an atlas with 39 regions of interest derived in Varoquaux et al. (2011).\nWe use the network DeepGraph-39, the same network and parameters from synthetic experiments, using the same evaluation protocol as used in the genomic data. For both control and autism patients we use time series from 35 random subjects to estimate edges and corresponding precision matrices. We find that for both the Autism and Control group we can obtain edge selection comparable to graph lasso for very few selected edges. When the number of selected edges is in the range above 25 we begin to perform significantly better in edge selection as seen in Fig. 4. We evaluated stability of the results as shown in Tab. 3. DeepGraph outperformed the other methods across the board.\nABIDE has high variability across sites and subjects. As a result, to resolve differences between approaches, we needed to perform 1000 folds to obtain well-separated error bars. We found that the birth-death MCMC method took very long to converge on this data, moreover the need for many folds to obtain significant results amongst the methods made this approach prohibitively slow to evaluate.\n1http://preprocessed-connectomes-project.github.io/abide/\nWe show the edges returned by Graph Lasso and DeepGraph for a sample from 35 subjects (Fig. 5) in the control group. We also show the result of a large-sample result based on 368 subjects from graphical lasso. In visual evaluation of the edges returned by DeepGraph we find that they closely align with results from a large-sample estimation procedure. Furthermore we can see several edges in the subsample which were particularly strongly activated in both methods.\n4 DISCUSSION AND CONCLUSIONS\nOur method was competitive with strong baselines. Even in cases that deviate from standard GGM sparsity assumptions (e.g. Laplacians, small-world) it performed substantially better. When finetuning on the target distribution performance further improves. Most importantly the learned estimator generalizes well to real data finding relevant stable edges. We also observed that the learned estimators generalize to variations not seen at training time (e.g. different n or sparsity), which points to this potentialy learning generic computations. This also shows potential to more easily scale the method to different graph sizes. One could consider transfer learning, where a network for one size of data is used as a starting point to learn a network working on larger dimension data.\nPenalized maximum likelihood can provide performance guarantees under restrictive assumptions on the form of the distribution and not considering the regularization path. In the proposed method one could obtain empirical bounds under the prescribed data distribution. Additionally, at execution time the speed of the approach can allow for re-sampling based uncertainty estimates and efficient model selection (e.g. cross-validation) amongst several trained estimators.\nWe have introduced the concept of learning an estimator for determining the structure of an undirected graphical model. A network architecture and sampling procedure for learning such an estimator for the case of sparse GGMs was proposed. We obtained competitive results on synthetic data with various underlying distributions, as well as on challenging real-world data. Empirical results show that our method works particularly well compared to other approaches for small-world networks, an important class of graphs common in real-world domains. We have shown that neural networks can obtain improved results over various statistical methods on real datasets, despite being trained with samples from parametric distributions. Our approach enables straightforward specifications of new priors and opens new directions in efficient graphical structure discovery from few examples.\nACKNOWLEDGEMENTS\nThis work is partially funded by Internal Funds KU Leuven, FP7-MC-CIG 334380, DIGITEO 2013- 0788D - SOPRANO, and ANR-11-BINF-0004 NiConnect. We thank Jean Honorio for providing pre-processed Cancer Genome Data.\nA SUPPLEMENTARY EXPERIMENTS\nA.1 PREDICTING COVARIANCE MATRICES\nUsing our framework it is possible to attempt to directly predict an accurate covariance matrix given a noisy one constructed from few observations. This is a more challenging task than predicting the edges. In this section we show preliminay experiments which given an empirical covariance matrix from few observations attempts to predict a more accurate covariance matrix that takes into account underlying sparse data dependency structure.\nOne challenge is that outputs of our covariance predictor must be on the positive semidefinite cone, thus we choose to instead predict on the cholesky decompositions, which allows us to always produce positive definite covariances. We train a similar structure to DeepGraph-39 structure modifying the last layer to be fully connected linear layer that predicts on the cholesky decomposition of the true covariance matrices generated by our model with a squared loss.\nWe evaluate this network using the ABIDE dataset described in Section 3. The ABIDE data has a large number of samples allowing us to obtain a large sample estimate of the covariance and compare it to our estimator as well as graphical lasso and empirical covariance estimators. Using the large sample ABIDE empirical covariance matrix. We find that we can obtain competitive `2 and `\u221e norm using few samples. We use 403 subjects from the ABIDE Control group each with a recording of 150 \u2212 200 samples to construct covariance matrix, totaling 77 330 samples (some correlated). This acts as our very approximate estimate of the population \u03a3. We then evaluate covariance estimation on 35 samples using the empirical covariance estimator, graphical lasso, and DeepGraph trained to output covariance matrices. We repeat the experiment for 50 different subsamples of the data. We see in 5 that the prediction approach can obtain competitive results. In terms of `2 graphical lasso performs better, however our estimate is better than empirical covariance estimation and much faster then graphical lasso. In some applications such as robust estimation a fast estimate of the covariance matrix (automatically embedding sparsity assumptions) can be of great use. For `\u221e error we see the empirical covariance estimation outperforms graphical lasso and DeepGraph for this dataset, while DeepGraph performs better in terms of this metric.\nWe note these results are preliminary, as the covariance predicting networks were not heavily optimized, moreover the ABIDE dataset is very noisy even when pre-processed and thus even the large sample covariance estimate may not be accurate. We believe this is an interesting alternate application of our paper.\nA.2 ADDITIONAL SYNTHETIC RESULTS ON SPARSITY\nWe investigate the affect of sparsity on DeepGraph-39 which has been trained with input that has sparsity 96% \u2212 92% sparse. We find that DeepGraph performs well at the 2% sparsity level despite not seeing this at training time. At the same time performance begins to degrade for 15% but is still competitive in several categories. The results are shown in Table 6. Future investigation can consider how alternate variation of sparsity at training time will affect these results.\nA.3 APPLICATION OF LARGER NETWORK ON SMALLER INPUT\nWe perform preliminary investigation of application of a network trained for a larger number of nodes to a smaller set of nodes. Specifically, we consider the breast invasive carcinoma groups gene data. We now take all 175 valid genes from Appendix C.2 of Honorio et al. (2012). We take the network trained on 500 nodes in the synthetic experiments section. We use the same experimental setup as in the gene experiments. The 175\u00d7 175\ncovariance matrix from 40 samples and padded to the appropriate size. We observe that DeepGraph has similar performance to graph lasso while permuting the input and ensembling the result gives substantial improvement.\nA.4 PERMUTATION AS ENSEMBLE METHOD\nAs discussed in Section 2.3, permuting the input and averaging several permutations can produce an improved result empirically. We interpret this as a typical ensembling method. This can be an advantage of the proposed architecture as we are able to easily use standard ensemble techniques. We perform an experiment to further verify that indeed the permutation of the input (and subsequent inverse permutation) allows us to produce separate classifiers that have uncorrelated errors.\nWe use the setup from the synthetic experiments with DeepGraph-39 in Section 3 with n = 35 and p = 39. We construct 20 permutation matrices as in the experimental section. Treating each as a separate classifier we compute the correlation coefficient of the errors on 50 synthetic input examples. We find that the average correlation coefficient of the errors of two classifiers is 0.028\u00b1 0.002, suggesting they are uncorrelated. Finally we note the individual errors are relatively small, as can already be inferred from our extensive experimental results in Section 3. We however compute the average absolute error of all the outputs across each permutation for this set of inputs as 0.03, notably the range of outputs is 0 to 1. Thus since prediction error differ at each permutation but are accurate we can average and yield a lower total prediction error.\nFinally we note that our method is extremely efficient computationally thus averaging the results of several permutations is practical even as the graph becomes large.\n", "cats": {"READ": 1.0, "DONTREAD": 0.0}, "reviews": [{"IS_META_REVIEW": true, "comments": "The paper proposes a novel algorithm to estimate graph structures by using a convolutional neural network to approximate the function that maps from empirical covariance matrix to the sparsity pattern of the graph. Compared with existing approaches, the new algorithm can adapt to different network structures, e.g. small-world networks, better under the same empirical risk minimization framework. Experiments on synthetic and real-world datasets show promising results compared with baselines.\n\nIn general, I think it is an interesting and novel paper. The idea of framing structure estimation as a learning problem is especially interesting and may inspire further research on related topics. The advantage of such an approach is that it allows easier adaptation to different network structure properties without designing specific regularization terms as in graph lasso.\n\nThe experiment results are also promising. In both synthetic and real-world datasets, the proposed algorithm outperforms other baselines in the small sample region. \n\nHowever, the paper can be made clearer in describing the network architectures. For example, in page 5, each o^k_{i,j} is said be a d-dimensional vector. But from the context, it seems o^k_{i,j} is a scalar (from o^0_{i,j} = p_{i,j}). It is not clear what o^k_{i,j} is exactly and what d is. Is it the number of channels for the convolutional filters?\n\nFigure 1 is also quite confusing. Why in (b) the table is 16 x 16 whereas in (a) there are only six nodes? And from the figure, it seems there is only one channel in each layer? What do the black squares represent and why are there three blocks of them. There are some descriptions in the text, but it is still not clear what they mean exactly.\n\nFor real-world data, how are the training data (Y, Sigma) generated? Are they generated in the same way as in the synthetic experiments where the entries are uniformly sparse? This is also related to the more general question of how to sample from the distribution P, in the case of real-world data."}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "The authors provide a modern twist to the classical problem of graphical model selection. Traditionally, the sparsity priors to encourage selection of specific structures is hand-engineered. Instead, the authors propose using a neural network to train for these priors. Since graphical models are useful in the small-sample regime, using neural networks directly on the training data is not effective. Instead, the authors propose generating data based on the desired graph structures to train the neural network. \n \n While this is a nice idea, the paper is not clear and convincing enough to be accepted to the conference, and instead, recommend it to the workshop track.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"DATE": "12 Jan 2017", "TITLE": "General Comments", "IS_META_REVIEW": false, "comments": "We thank the reviewers for their comments and for providing us with excellent feedback. We have updated the paper with clarifications in Section 2.3 as well as an Appendix (A.4) with some additional experiments which analyze permuted inputs. We have also made an early release of our code available at ", "OTHER_KEYS": "Eugene Belilovsky"}, {"TITLE": "Review", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "I sincerely apologize for the late-arriving review. \n\nThis paper proposes to frame the problem of structure estimation as a supervised classification problem. The input is an empirical covariance matrix of the observed data, the output the binary decision whether or not two variables share a link. The paper is sufficiently clear, the goals are clear and everything is well described. \n\nThe main interesting point is the empirical results of the experimental section. The approach is simple and performs better than previous non-learning based methods. This observation is interesting and will be of interest in structure discovery problems. \n\nI rate the specific construction of the supervised learning method as a reasonable attempt attempt to approach this problem. There is not very much technical novelty in this part. E.g., an algorithmic contribution would have been a method that is invariant to data permutation could have been a possible target for a technical contribution. The paper makes no claims on this technical part, as said, the method is well constructed and well executed. \n\nIt is good to precisely state the theoretical parts of a paper, the authors do this well. All results are rather straight-forward, I like that the claims are written down, but there is little surprise in the statements. \n\nIn summary, the paper makes a very interesting observation. Graph estimation can be posed as a supervised learning problem and training data from a separate source is sufficient to learn structure in novel and unseen test data from a new source. Practically this may be relevant, on one hand the empirical results are stronger with this method, on the other hand a practitioner who is interested in structural discovery may have side constraints about interpretability of the deriving method. From the Discussion and Conclusion I understand that the authors consider this as future work. It is a good first step, it could be stronger but also stands on its own already.", "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "20 Dec 2016", "REVIEWER_CONFIDENCE": 3}, {"TITLE": "Interesting algorithm to estimate sparse graph structure", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "The paper proposes a novel algorithm to estimate graph structures by using a convolutional neural network to approximate the function that maps from empirical covariance matrix to the sparsity pattern of the graph. Compared with existing approaches, the new algorithm can adapt to different network structures, e.g. small-world networks, better under the same empirical risk minimization framework. Experiments on synthetic and real-world datasets show promising results compared with baselines.\n\nIn general, I think it is an interesting and novel paper. The idea of framing structure estimation as a learning problem is especially interesting and may inspire further research on related topics. The advantage of such an approach is that it allows easier adaptation to different network structure properties without designing specific regularization terms as in graph lasso.\n\nThe experiment results are also promising. In both synthetic and real-world datasets, the proposed algorithm outperforms other baselines in the small sample region. \n\nHowever, the paper can be made clearer in describing the network architectures. For example, in page 5, each o^k_{i,j} is said be a d-dimensional vector. But from the context, it seems o^k_{i,j} is a scalar (from o^0_{i,j} = p_{i,j}). It is not clear what o^k_{i,j} is exactly and what d is. Is it the number of channels for the convolutional filters?\n\nFigure 1 is also quite confusing. Why in (b) the table is 16 x 16 whereas in (a) there are only six nodes? And from the figure, it seems there is only one channel in each layer? What do the black squares represent and why are there three blocks of them. There are some descriptions in the text, but it is still not clear what they mean exactly.\n\nFor real-world data, how are the training data (Y, Sigma) generated? Are they generated in the same way as in the synthetic experiments where the entries are uniformly sparse? This is also related to the more general question of how to sample from the distribution P, in the case of real-world data.", "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 3}, {"TITLE": "Advantage of the proposed method", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "This paper proposes a new method for learning graphical models. Combined with a neural network architecture, some sparse edge structure is estimated via sampling methods. In introduction, the authors say that a problem in graphical lasso is model selection. However, the proposed method still implicitly includes model selection. In the proposed method, $P(G)$ is a sparse prior, and should include some hyper-parameters. How do you tune the hyper-parameters? Is this tuning an equivalent problem to model section? Therefore, I do not understand real advantage of this method over previous methods. What is the advantage of the proposed method?\n\nAnother concern is that this paper is unorganized. In Algorithm 1, first, G_i and \\Sigma_i are sampled, and then x_j is sampled from N(0, \\Sigma). Here, what is \\Sigma? Is it different from \\Sigma_i? Furthermore, how do you construct (Y_i, \\hat{\\Sigma}_i) from (G_i, X_i )? Finally, I have a simple question: Where is input data X (not sampled data) is used in Algorithm 1?\n\nWhat is the definition of the receptive field in Proposition 2 and Proposition 3?\n", "IS_META_REVIEW": false, "RECOMMENDATION": 5, "DATE": "14 Dec 2016", "REVIEWER_CONFIDENCE": 2}, {"IS_META_REVIEW": true, "comments": "The paper proposes a novel algorithm to estimate graph structures by using a convolutional neural network to approximate the function that maps from empirical covariance matrix to the sparsity pattern of the graph. Compared with existing approaches, the new algorithm can adapt to different network structures, e.g. small-world networks, better under the same empirical risk minimization framework. Experiments on synthetic and real-world datasets show promising results compared with baselines.\n\nIn general, I think it is an interesting and novel paper. The idea of framing structure estimation as a learning problem is especially interesting and may inspire further research on related topics. The advantage of such an approach is that it allows easier adaptation to different network structure properties without designing specific regularization terms as in graph lasso.\n\nThe experiment results are also promising. In both synthetic and real-world datasets, the proposed algorithm outperforms other baselines in the small sample region. \n\nHowever, the paper can be made clearer in describing the network architectures. For example, in page 5, each o^k_{i,j} is said be a d-dimensional vector. But from the context, it seems o^k_{i,j} is a scalar (from o^0_{i,j} = p_{i,j}). It is not clear what o^k_{i,j} is exactly and what d is. Is it the number of channels for the convolutional filters?\n\nFigure 1 is also quite confusing. Why in (b) the table is 16 x 16 whereas in (a) there are only six nodes? And from the figure, it seems there is only one channel in each layer? What do the black squares represent and why are there three blocks of them. There are some descriptions in the text, but it is still not clear what they mean exactly.\n\nFor real-world data, how are the training data (Y, Sigma) generated? Are they generated in the same way as in the synthetic experiments where the entries are uniformly sparse? This is also related to the more general question of how to sample from the distribution P, in the case of real-world data."}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "The authors provide a modern twist to the classical problem of graphical model selection. Traditionally, the sparsity priors to encourage selection of specific structures is hand-engineered. Instead, the authors propose using a neural network to train for these priors. Since graphical models are useful in the small-sample regime, using neural networks directly on the training data is not effective. Instead, the authors propose generating data based on the desired graph structures to train the neural network. \n \n While this is a nice idea, the paper is not clear and convincing enough to be accepted to the conference, and instead, recommend it to the workshop track.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"DATE": "12 Jan 2017", "TITLE": "General Comments", "IS_META_REVIEW": false, "comments": "We thank the reviewers for their comments and for providing us with excellent feedback. We have updated the paper with clarifications in Section 2.3 as well as an Appendix (A.4) with some additional experiments which analyze permuted inputs. We have also made an early release of our code available at ", "OTHER_KEYS": "Eugene Belilovsky"}, {"TITLE": "Review", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "I sincerely apologize for the late-arriving review. \n\nThis paper proposes to frame the problem of structure estimation as a supervised classification problem. The input is an empirical covariance matrix of the observed data, the output the binary decision whether or not two variables share a link. The paper is sufficiently clear, the goals are clear and everything is well described. \n\nThe main interesting point is the empirical results of the experimental section. The approach is simple and performs better than previous non-learning based methods. This observation is interesting and will be of interest in structure discovery problems. \n\nI rate the specific construction of the supervised learning method as a reasonable attempt attempt to approach this problem. There is not very much technical novelty in this part. E.g., an algorithmic contribution would have been a method that is invariant to data permutation could have been a possible target for a technical contribution. The paper makes no claims on this technical part, as said, the method is well constructed and well executed. \n\nIt is good to precisely state the theoretical parts of a paper, the authors do this well. All results are rather straight-forward, I like that the claims are written down, but there is little surprise in the statements. \n\nIn summary, the paper makes a very interesting observation. Graph estimation can be posed as a supervised learning problem and training data from a separate source is sufficient to learn structure in novel and unseen test data from a new source. Practically this may be relevant, on one hand the empirical results are stronger with this method, on the other hand a practitioner who is interested in structural discovery may have side constraints about interpretability of the deriving method. From the Discussion and Conclusion I understand that the authors consider this as future work. It is a good first step, it could be stronger but also stands on its own already.", "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "20 Dec 2016", "REVIEWER_CONFIDENCE": 3}, {"TITLE": "Interesting algorithm to estimate sparse graph structure", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "The paper proposes a novel algorithm to estimate graph structures by using a convolutional neural network to approximate the function that maps from empirical covariance matrix to the sparsity pattern of the graph. Compared with existing approaches, the new algorithm can adapt to different network structures, e.g. small-world networks, better under the same empirical risk minimization framework. Experiments on synthetic and real-world datasets show promising results compared with baselines.\n\nIn general, I think it is an interesting and novel paper. The idea of framing structure estimation as a learning problem is especially interesting and may inspire further research on related topics. The advantage of such an approach is that it allows easier adaptation to different network structure properties without designing specific regularization terms as in graph lasso.\n\nThe experiment results are also promising. In both synthetic and real-world datasets, the proposed algorithm outperforms other baselines in the small sample region. \n\nHowever, the paper can be made clearer in describing the network architectures. For example, in page 5, each o^k_{i,j} is said be a d-dimensional vector. But from the context, it seems o^k_{i,j} is a scalar (from o^0_{i,j} = p_{i,j}). It is not clear what o^k_{i,j} is exactly and what d is. Is it the number of channels for the convolutional filters?\n\nFigure 1 is also quite confusing. Why in (b) the table is 16 x 16 whereas in (a) there are only six nodes? And from the figure, it seems there is only one channel in each layer? What do the black squares represent and why are there three blocks of them. There are some descriptions in the text, but it is still not clear what they mean exactly.\n\nFor real-world data, how are the training data (Y, Sigma) generated? Are they generated in the same way as in the synthetic experiments where the entries are uniformly sparse? This is also related to the more general question of how to sample from the distribution P, in the case of real-world data.", "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 3}, {"TITLE": "Advantage of the proposed method", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "This paper proposes a new method for learning graphical models. Combined with a neural network architecture, some sparse edge structure is estimated via sampling methods. In introduction, the authors say that a problem in graphical lasso is model selection. However, the proposed method still implicitly includes model selection. In the proposed method, $P(G)$ is a sparse prior, and should include some hyper-parameters. How do you tune the hyper-parameters? Is this tuning an equivalent problem to model section? Therefore, I do not understand real advantage of this method over previous methods. What is the advantage of the proposed method?\n\nAnother concern is that this paper is unorganized. In Algorithm 1, first, G_i and \\Sigma_i are sampled, and then x_j is sampled from N(0, \\Sigma). Here, what is \\Sigma? Is it different from \\Sigma_i? Furthermore, how do you construct (Y_i, \\hat{\\Sigma}_i) from (G_i, X_i )? Finally, I have a simple question: Where is input data X (not sampled data) is used in Algorithm 1?\n\nWhat is the definition of the receptive field in Proposition 2 and Proposition 3?\n", "IS_META_REVIEW": false, "RECOMMENDATION": 5, "DATE": "14 Dec 2016", "REVIEWER_CONFIDENCE": 2}]}
{"text": "1 INTRODUCTION\nText classification is an important problem in Natural Language Processing (NLP). Real world usecases include spam filtering or e-mail categorization. It is a core component in more complex systems such as search and ranking. Recently, deep learning techniques based on neural networks have achieved state of the art results in various NLP applications. One of the main successes of deep learning is due to the effectiveness of recurrent networks for language modeling and their application to speech recognition and machine translation (Mikolov, 2012). However, in other cases including several text classification problems, it has been shown that deep networks do not convincingly beat the prior state of the art techniques (Wang & Manning, 2012; Joulin et al., 2016).\nIn spite of being (typically) orders of magnitude slower to train than traditional techniques based on n-grams, neural networks are often regarded as a promising alternative due to compact model sizes, in particular for character based models. This is important for applications that need to run on systems with limited memory such as smartphones.\nThis paper specifically addresses the compromise between classification accuracy and the model size. We extend our previous work implemented in the fastText library1. It is based on n-gram features, dimensionality reduction, and a fast approximation of the softmax classifier (Joulin et al., 2016). We show that a few key ingredients, namely feature pruning, quantization, hashing, and retraining, allow us to produce text classification models with tiny size, often less than 100kB when trained on several popular datasets, without noticeably sacrificing accuracy or speed.\nWe plan to publish the code and scripts required to reproduce our results as an extension of the fastText library, thereby providing strong reproducible baselines for text classifiers that optimize the compromise between the model size and accuracy. We hope that this will help the engineering community to improve existing applications by using more efficient models.\nThis paper is organized as follows. Section 2 introduces related work, Section 3 describes our text classification model and explains how we drastically reduce the model size. Section 4 shows the effectiveness of our approach in experiments on multiple text classification benchmarks.\n1https://github.com/facebookresearch/fastText\n2 RELATED WORK\nModels for text classification. Text classification is a problem that has its roots in many applications such as web search, information retrieval and document classification (Deerwester et al., 1990; Pang & Lee, 2008). Linear classifiers often obtain state-of-the-art performance while being scalable (Agarwal et al., 2014; Joachims, 1998; Joulin et al., 2016; McCallum & Nigam, 1998). They are particularly interesting when associated with the right features (Wang & Manning, 2012). They usually require storing embeddings for words and n-grams, which makes them memory inefficient.\nCompression of language models. Our work is related to compression of statistical language models. Classical approaches include feature pruning based on entropy (Stolcke, 2000) and quantization. Pruning aims to keep only the most important n-grams in the model, leaving out those with probability lower than a specified threshold. Further, the individual n-grams can be compressed by quantizing the probability value, and by storing the n-gram itself more efficiently than as a sequence of characters. Various strategies have been developed, for example using tree structures or hash functions, and are discussed in (Talbot & Brants, 2008).\nCompression for similarity estimation and search. There is a large body of literature on how to compress a set of vectors into compact codes, such that the comparison of two codes approximates a target similarity in the original space. The typical use-case of these methods considers an indexed dataset of compressed vectors, and a query for which we want to find the nearest neighbors in the indexed set. One of the most popular is Locality-sensitive hashing (LSH) by Charikar (2002), which is a binarization technique based on random projections that approximates the cosine similarity between two vectors through a monotonous function of the Hamming distance between the two corresponding binary codes. In our paper, LSH refers to this binarization strategy2. Many subsequent works have improved this initial binarization technique, such as spectal hashing (Weiss et al., 2009), or Iterative Quantization (ITQ) (Gong & Lazebnik, 2011), which learns a rotation matrix minimizing the quantization loss of the binarization. We refer the reader to two recent surveys by Wang et al. (2014) and Wang et al. (2015) for an overview of the binary hashing literature.\nBeyond these binarization strategies, more general quantization techniques derived from Jegou et al. (2011) offer better trade-offs between memory and the approximation of a distance estimator. The Product Quantization (PQ) method approximates the distances by calculating, in the compressed domain, the distance between their quantized approximations. This method is statistically guaranteed to preserve the Euclidean distance between the vectors within an error bound directly related to the quantization error. The original PQ has been concurrently improved by Ge et al. (2013) and Norouzi & Fleet (2013), who learn an orthogonal transform minimizing the overall quantization loss. In our paper, we will consider the Optimized Product Quantization (OPQ) variant (Ge et al., 2013).\nSoftmax approximation The aforementioned works approximate either the Euclidean distance or the cosine similarity (both being equivalent in the case of unit-norm vectors). However, in the context of fastText, we are specifically interested in approximating the maximum inner product involved in a softmax layer. Several approaches derived from LSH have been recently proposed to achieve this goal, such as Asymmetric LSH by Shrivastava & Li (2014), subsequently discussed by Neyshabur & Srebro (2015). In our work, since we are not constrained to purely binary codes, we resort a more traditional encoding by employing a magnitude/direction parametrization of our vectors. Therefore we only need to encode/compress an unitary d-dimensional vector, which fits the aforementioned LSH and PQ methods well.\nNeural network compression models. Recently, several research efforts have been conducted to compress the parameters of architectures involved in computer vision, namely for state-of-theart Convolutional Neural Networks (CNNs) (Han et al., 2016; Lin et al., 2015). Some use vector quantization (Gong et al., 2014) while others binarize the network (Courbariaux et al., 2016). Denil et al. (2013) show that such classification models are easily compressed because they are overparametrized, which concurs with early observations by LeCun et al. (1990).\n2In the literature, LSH refers to multiple distinct strategies related to the Johnson-Lindenstrauss lemma. For instance, LSH sometimes refers to a partitioning technique with random projections allowing for sublinear search via cell probes, see for instance the E2LSH variant of Datar et al. (2004).\nSome of these works both aim at reducing the model size and the speed. In our case, since the fastText classifier on which our proposal is built upon is already very efficient, we are primilarly interested in reducing the size of the model while keeping a comparable classification efficiency.\n3 PROPOSED APPROACH\n3.1 TEXT CLASSIFICATION\nIn the context of text classification, linear classifiers (Joulin et al., 2016) remain competitive with more sophisticated, deeper models, and are much faster to train. On top of standard tricks commonly used in linear text classification (Agarwal et al., 2014; Wang & Manning, 2012; Weinberger et al., 2009), Joulin et al. (2016) use a low rank constraint to reduce the computation burden while sharing information between different classes. This is especially useful in the case of a large output space, where rare classes may have only a few training examples. In this paper, we focus on a similar model, that is, which minimizes the softmax loss ` over N documents:\nN\u2211 n=1 `(yn, BAxn), (1)\nwhere xn is a bag of one-hot vectors and yn the label of the n-th document. In the case of a large vocabulary and a large output space, the matrices A and B are big and can require gigabytes of memory. Below, we describe how we reduce this memory usage.\n3.2 BOTTOM-UP PRODUCT QUANTIZATION\nProduct quantization is a popular method for compressed-domain approximate nearest neighbor search (Jegou et al., 2011). As a compression technique, it approximates a real-valued vector by finding the closest vector in a pre-defined structured set of centroids, referred to as a codebook. This codebook is not enumerated, since it is extremely large. Instead it is implicitly defined by its structure: a d-dimensional vector x \u2208 Rd is approximated as\nx\u0302 = k\u2211 i=1 qi(x), (2)\nwhere the different subquantizers qi : x 7\u2192 qi(x) are complementary in the sense that their respective centroids lie in distinct orthogonal subspaces, i.e., \u2200i 6= j, \u2200x, y, \u3008qi(x)|qj(y)\u3009 = 0. In the original PQ, the subspaces are aligned with the natural axis, while OPQ learns a rotation, which amounts to alleviating this constraint and to not depend on the original coordinate system. Another way to see this is to consider that PQ splits a given vector x into k subvectors xi, i = 1 . . . k, each of dimension d/k: x = [x1 . . . xi . . . xk], and quantizes each sub-vector using a distinct k-means quantizer. Each subvector xi is thus mapped to the closest centroid amongst 2b centroids, where b is the number of bits required to store the quantization index of the subquantizer, typically b = 8. The reconstructed vector can take 2kb distinct reproduction values, and is stored in kb bits.\nPQ estimates the inner product in the compressed domain as\nx>y \u2248 x\u0302>y = k\u2211\ni=1\nqi(x i)>yi. (3)\nThis is a straightforward extension of the square L2 distance estimation of Jegou et al. (2011). In practice, the vector estimate x\u0302 is trivially reconstructed from the codes, i.e., from the quantization indexes, by concatenating these centroids.\nThe two parameters involved in PQ, namely the number of subquantizers k and the number of bits b per quantization index, are typically set to k \u2208 [2, d/2], and b = 8 to ensure byte-alignment.\nDiscussion. PQ offers several interesting properties in our context of text classification. Firstly, the training is very fast because the subquantizers have a small number of centroids, i.e., 256 centroids for b = 8. Secondly, at test time it allows the reconstruction of the vectors with almost no\ncomputational and memory overhead. Thirdly, it has been successfully applied in computer vision, offering much better performance than binary codes, which makes it a natural candidate to compress relatively shallow models. As observed by Sa\u0301nchez & Perronnin (2011), using PQ just before the last layer incurs a very limited loss in accuracy when combined with a support vector machine.\nIn the context of text classification, the norms of the vectors are widely spread, typically with a ratio of 1000 between the max and the min. Therefore kmeans performs poorly because it optimizes an absolute error objective, so it maps all low-norm vectors to 0. A simple solution is to separate the norm and the angle of the vectors and to quantize them separately. This allows a quantization with no loss of performance, yet requires an extra b bits per vector.\nBottom-up strategy: re-training. The first works aiming at compressing CNN models like the one proposed by (Gong et al., 2014) used the reconstruction from off-the-shelf PQ, i.e., without any re-training. However, as observed in Sablayrolles et al. (2016), when using quantization methods like PQ, it is better to re-train the layers occurring after the quantization, so that the network can re-adjust itself to the quantization. There is a strong argument arguing for this re-training strategy: the square magnitude of vectors is reduced, on average, by the average quantization error for any quantizer satisfying the Lloyd conditions; see Jegou et al. (2011) for details.\nThis suggests a bottom-up learning strategy where we first quantize the input matrix, then retrain and quantize the output matrix (the input matrix being frozen). Experiments in section 4 show that it is worth adopting this strategy.\nMemory savings with PQ. In practice, the bottom-up PQ strategy offers a compression factor of 10 without any noticeable loss of performance. Without re-training, we notice a drop in accuracy between 0.1% and 0.5%, depending on the dataset and setting; see Section 4 and the appendix.\n3.3 FURTHER TEXT SPECIFIC TRICKS\nThe memory usage strongly depends on the size of the vocabulary, which can be large in many text classification tasks. While it is clear that a large part of the vocabulary is useless or redundant, directly reducing the vocabulary to the most frequent words is not satisfactory: most of the frequent words, like \u201cthe\u201d or \u201cis\u201d are not discriminative, in contrast to some rare words, e.g., in the context of tag prediction. In this section, we discuss a few heuristics to reduce the space taken by the dictionary. They lead to major memory reduction, in extreme cases by a factor 100. We experimentally show that this drastic reduction is complementary with the PQ compression method, meaning that the combination of both strategies reduces the model size by a factor up to \u00d71000 for some datasets.\nPruning the vocabulary. Discovering which word or n-gram must be kept to preserve the overall performance is a feature selection problem. While many approaches have been proposed to select groups of variables during training (Bach et al., 2012; Meier et al., 2008), we are interested in selecting a fixed subset of K words and ngrams from a pre-trained model. This can be achieved by selecting the K embeddings that preserve as much of the model as possible, which can be reduced to selecting the K words and ngrams associated with the highest norms.\nWhile this approach offers major memory savings, it has one drawback occurring in some particular cases: some documents may not contained any of the K best features, leading to a significant drop in performance. It is thus important to keep the K best features under the condition that they cover the whole training set. More formally, the problem is to find a subset S in the feature set V that maximizes the sum of their norms ws under the constraint that all the documents in the training set D are covered:\nmax S\u2286V \u2211 s\u2208S ws s.t. |S| \u2264 K, P1S \u2265 1D,\nwhere P is a matrix such that Pds = 1 if the s-th feature is in the d-th document, and 0 otherwise. This problem is directly related to set covering problems that are NP-hard (Feige, 1998). Standard greedy approaches require the storing of an inverted index or to do multiple passes over the dataset, which is prohibitive on very large dataset (Chierichetti et al., 2010). This problem can be cast as an instance of online submodular maximization with a rank constraint (Badanidiyuru et al., 2014;\nBateni et al., 2010). In our case, we use a simple online parallelizable greedy approach: For each document, we verify if it is already covered by a retained feature and, if not, we add the feature with the highest norm to our set of retained features. If the number of features is below k, we add the features with the highest norm that have not yet been picked.\nHashing trick & Bloom filter. On small models, the dictionary can take a significant portion of the memory. Instead of saving it, we extend the hashing trick used in Joulin et al. (2016) to both words and n-grams. This strategy is also used in Vowpal Wabbit (Agarwal et al., 2014) in the context of online training. This allows us to save around 1-2Mb with almost no overhead at test time (just the cost of computing the hashing function).\nPruning the vocabulary while using the hashing trick requires keeping a list of the indices of the K remaining buckets. At test time, a binary search over the list of indices is required. It has a complexity of O(log(K)) and a memory overhead of a few hundreds of kilobytes. Using Bloom filters instead reduces the complexityO(1) at test time and saves a few hundred kilobytes. However, in practice, it degrades performance.\n4 EXPERIMENTS\nThis section evaluates the quality of our model compression pipeline and compare it to other compression methods on different text classification problems, and to other compact text classifiers.\nEvaluation protocol and datasets. Our experimental pipeline is as follows: we train a model using fastText with the default setting unless specified otherwise. That is 2M buckets, a learning rate of 0.1 and 10 training epochs. The dimensionality d of the embeddings is set to powers of 2 to avoid border effects that could make the interpretation of the results more difficult. As baselines, we use Locality-Sensitive Hashing (LSH) (Charikar, 2002), PQ (Jegou et al., 2011) and OPQ (Ge et al., 2013) (the non-parametric variant). Note that we use an improved version of LSH where random orthogonal matrices are used instead of random matrix projection Je\u0301gou et al. (2008). In a first series of experiments, we use the 8 datasets and evaluation protocol of Zhang et al. (2015). These datasets contain few million documents and have at most 10 classes. We also explore the limit of quantization on a dataset with an extremely large output space, that is a tag dataset extracted from the YFCC100M collection (Thomee et al., 2016)3, referred to as FlickrTag in the rest of this paper.\n4.1 SMALL DATASETS\nCompression techniques. We compare three popular methods used for similarity estimation with compact codes: LSH, PQ and OPQ on the datasets released by Zhang et al. (2015). Figure 1 shows the accuracy as a function of the number of bytes used per embedding, which corresponds to the number k of subvectors in the case of PQ and OPQ. See more results in the appendix. As discussed in Section 2, LSH reproduces the cosine similarity and is therefore not adapted to un-normalized data. Therefore we only report results with normalization. Once normalized, PQ and OPQ are almost lossless even when using only k = 4 subquantizers per embedding (equivalently, bytes). We observe in practice that using k = d/2, i.e., half of the components of the embeddings, works well in practice. In the rest of the paper and if not stated otherwise, we focus on this setting. The difference between the normalized versions of PQ and OPQ is limited and depends on the dataset. Therefore we adopt the normalized PQ (NPQ) for the rest of this study, since it is faster to train.\n3Data available at https://research.facebook.com/research/fasttext/\nPruning. Figure 2 shows the performance of our model with different sizes. We fix k = d/2 and use different pruning thresholds. NPQ offers a compression rate of\u00d710 compared to the full model. As the pruning becomes more agressive, the overall compression can increase up up to \u00d71, 000 with little drop of performance and no additional overhead at test time. In fact, using a smaller dictionary makes the model faster at test time. We also compare with character-level Convolutional Neural Networks (CNN) (Zhang et al., 2015; Xiao & Cho, 2016). They are attractive models for text classification because they achieve similar performance with less memory usage than linear models (Xiao & Cho, 2016). Even though fastText with the default setting uses more memory, NPQ is already on par with CNNs\u2019 memory usage. Note that CNNs are not quantized, and it would be worth seeing how much they can be quantized with no drop of performance. Such a study is beyond the scope of this paper. Our pruning is based on the norm of the embeddings according to the guidelines of Section 3.3. Table 1 compares the ranking obtained with norms to the ranking obtained using entropy, which is commonly used in unsupervised settings Stolcke (2000).\nExtreme compression. Finally, in Table 2, we explore the limit of quantized model by looking at the performance obtained for models under 64KiB. Surprisingly, even at 64KiB and 32KiB, the drop of performance is only around 0.8% and 1.7% despite a compression rate of \u00d71, 000\u2212 4, 000.\n4.2 LARGE DATASET: FLICKRTAG\nIn this section, we explore the limit of compression algorithms on very large datasets. Similar to Joulin et al. (2016), we consider a hashtag prediction dataset containing 312, 116 labels. We set the minimum count for words at 10, leading to a dictionary of 1, 427, 667 words. We take 10M buckets for n-grams and a hierarchical softmax. We refer to this dataset as FlickrTag.\nOutput encoding. We are interested in understanding how the performance degrades if the classifier is also quantized (i.e., the matrix B in Eq. 1) and when the pruning is at the limit of the minimum number of features required to cover the full dataset.\nTable 3 shows that quantizing both the \u201cinput\u201d matrix (i.e., A in Eq. 1) and the \u201coutput\u201d matrix (i.e., B) does not degrade the performance compared to the full model. We use embeddings with d = 256 dimensions and use k = d/2 subquantizers. We do not use any text specific tricks, which leads to a compression factor of 8. Note that even if the output matrix is not retrained over the embeddings, the performance is only 0.2% away from the full model. As shown in the Appendix, using less subquantizers significantly decreases the performance for a small memory gain.\nPruning. Table 4 shows how the performance evolves with pruning. We measure this effect on top of a fully quantized model. The full model misses 11.6% of the test set because of missing words (some documents are either only composed of hashtags or have only rare words). There are 312, 116 labels and thus it seems reasonable to keep embeddings in the order of the million. A naive pruning with 1M features misses about 30\u221240% of the test set, leading to a significant drop of performance. On the other hand, even though the max-coverage pruning approach was set on the train set, it does not suffer from any coverage loss on the test set. This leads to a smaller drop of performance. If the pruning is too aggressive, however, the coverage decreases significantly.\n5 FUTURE WORK\nIt may be possible to obtain further reduction of the model size in the future. One idea is to condition the size of the vectors (both for the input features and the labels) based on their frequency (Chen et al., 2015; Grave et al., 2016). For example, it is probably not worth representing the rare labels by full 256-dimensional vectors in the case of the FlickrTag dataset. Thus, conditioning the vector size on the frequency and norm seems like an interesting direction to explore in the future.\nWe may also consider combining the entropy and norm pruning criteria: instead of keeping the features in the model based just on the frequency or the norm, we can use both to keep a good set of features. This could help to keep features that are both frequent and discriminative, and thereby to reduce the coverage problem that we have observed.\nAdditionally, instead of pruning out the less useful features, we can decompose them into smaller units (Mikolov et al., 2012). For example, this can be achieved by splitting every non-discriminative word into a sequence of character trigrams. This could help in cases where training and test examples are very short (for example just a single word).\n6 CONCLUSION\nIn this paper, we have presented several simple techniques to reduce, by several orders of magnitude, the memory complexity of certain text classifiers without sacrificing accuracy nor speed. This is achieved by applying discriminative pruning which aims to keep only important features in the trained model, and by performing quantization of the weight matrices and hashing of the dictionary.\nWe will publish the code as an extension of the fastText library. We hope that our work will serve as a baseline to the research community, where there is an increasing interest for comparing the performance of various deep learning text classifiers for a given number of parameters. Overall, compared to recent work based on convolutional neural networks, fastText.zip is often more accurate, while requiring several orders of magnitude less time to train on common CPUs, and incurring a fraction of the memory complexity.\n", "cats": {"READ": 1.0, "DONTREAD": 0.0}, "reviews": [{"IS_META_REVIEW": true, "comments": "This paper describes how to approximate the FastText approach such that its memory footprint is reduced by several orders of magnitude, while preserving its classification accuracy. The original FastText approach was based on a linear classifier on top of bag-of-words embeddings. This type of method is extremely fast to train and test, but the model size can be quite large.\n\nThis paper focuses on approximating the original approach with lossy compression techniques. Namely, the embeddings and classifier matrices A and B are compressed with Product Quantization, and an aggressive dictionary pruning is carried out. Experiments on various datasets (either with small or large number of classes) are conducted to tune the parameters and demonstrate the effectiveness of the approach. With a negligible loss in classification accuracy, an important reduction in term of model size (memory footprint) can be achieved, in the order of 100~1000 folds compared to the original size.\n\nThe paper is well written overall. The goal is clearly defined and well carried out, as well as the experiments. Different options for compressing the model data are evaluated and compared (e.g. PQ vs LSH), which is also interesting. Nevertheless the paper does not propose by itself any novel idea for text classification. It just focuses on adapting existing lossy compression techniques, which is not necessarily a problem. Specifically, it introduces:\n  - a straightforward variant of PQ for unnormalized vectors,\n  - dictionary pruning is cast as a set covering problem (which is NP-hard), but a greedy approach is shown to yield excellent results nonetheless,\n  - hashing tricks and bloom filter are simply borrowed from previous papers.\n\nThese techniques are quite generic and could as well be used in other works. \n\n\nHere are some minor problems with the paper:\n\n  - it is not made clear how the full model size is computed. What is exactly in the model? Which proportion of the full size do the A and B matrices, the dictionary, and the rest, account for? It is hard to follow where is the size bottleneck, which also seems to depend on the target application (i.e. small or large number of test classes). It would have been nice to provide a formula to calculate the total model size as a function of all parameters (k,b for PQ and K for dictionary, number of classes).\n  \n  - some parts lack clarity. For instance, the greedy approach to prune the dictionary is exposed in less than 4 lines (top of page 5), though it is far from being straightforward. Likewise, it is not clear why the binary search used for the hashing trick would introduce an overhead of a few hundreds of KB.\n  \n\nOverall this looks like a solid work, but with potentially limited impact research-wise."}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "The submission describes a method for compressing shallow and wide text classification models. The paper is well written, but the proposed method is not particularly novel, as it's comprised of existing model compression techniques. Overall, the contributions are incremental and the potential impact seems rather limited.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"TITLE": "Review", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "The paper presents a few tricks to compress a wide and shallow text classification model based on n-gram features. These tricks include (1) using (optimized) product quantization to compress embedding weights (2) pruning some of the vocabulary elements (3) hashing to reduce the storage of the vocabulary (this is a minor component of the paper). The paper focuses on models with very large vocabularies and shows a reduction in the size of the models at a relatively minor reduction of the accuracy.\n\nThe problem of compressing neural models is important and interesting. The methods section of the paper is well written with good high level comments and references. However, the machine learning contributions of the paper are marginal to me. The experiments are not too convincing mainly focusing on benchmarks that are not commonly used. The implications of the paper on the state-of-the-art RNN text classification models is unclear.\n\nThe use of (optimized) product quantization for approximating inner product is not particularly novel. Previous work also considered doing this. Most of the reduction in the model sizes comes from pruning vocabulary elements. The method proposed for pruning vocabulary elements is simply based on the assumption that embeddings with larger L2 norm are more important. A coverage heuristic is taken into account too. From a machine learning point of view, the proper baseline to solve this problem is to have a set of (relaxed) binary coefficients for each embedding vector and learn the coefficients jointly with the weights. An L1 regularizer on the coefficients can be used to encourage sparsity. From a practical point of view, I believe an important baseline is missing: what if one simply uses fewer vocabulary elements (e.g based on subword units - see ", "IS_META_REVIEW": false, "RECOMMENDATION": 5, "DATE": "23 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "Effective if simple combination of existing techniques for text-classifier compression", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "The paper proposes a series of tricks for compressing fast (linear) text classification models. The paper is clearly written, and the results are quite strong. The main compression is achieved via product quantization, a technique which has been explored in other applications within the neural network model compression literature. In addition to the Gong et al. work which was cited, it would be worth mentioning Quantized Convolutional Neural Networks for Mobile Devices (CVPR 2016, ", "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "19 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "Lossy compression techniques applied to FastText with nice results", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "This paper describes how to approximate the FastText approach such that its memory footprint is reduced by several orders of magnitude, while preserving its classification accuracy. The original FastText approach was based on a linear classifier on top of bag-of-words embeddings. This type of method is extremely fast to train and test, but the model size can be quite large.\n\nThis paper focuses on approximating the original approach with lossy compression techniques. Namely, the embeddings and classifier matrices A and B are compressed with Product Quantization, and an aggressive dictionary pruning is carried out. Experiments on various datasets (either with small or large number of classes) are conducted to tune the parameters and demonstrate the effectiveness of the approach. With a negligible loss in classification accuracy, an important reduction in term of model size (memory footprint) can be achieved, in the order of 100~1000 folds compared to the original size.\n\nThe paper is well written overall. The goal is clearly defined and well carried out, as well as the experiments. Different options for compressing the model data are evaluated and compared (e.g. PQ vs LSH), which is also interesting. Nevertheless the paper does not propose by itself any novel idea for text classification. It just focuses on adapting existing lossy compression techniques, which is not necessarily a problem. Specifically, it introduces:\n  - a straightforward variant of PQ for unnormalized vectors,\n  - dictionary pruning is cast as a set covering problem (which is NP-hard), but a greedy approach is shown to yield excellent results nonetheless,\n  - hashing tricks and bloom filter are simply borrowed from previous papers.\n\nThese techniques are quite generic and could as well be used in other works. \n\n\nHere are some minor problems with the paper:\n\n  - it is not made clear how the full model size is computed. What is exactly in the model? Which proportion of the full size do the A and B matrices, the dictionary, and the rest, account for? It is hard to follow where is the size bottleneck, which also seems to depend on the target application (i.e. small or large number of test classes). It would have been nice to provide a formula to calculate the total model size as a function of all parameters (k,b for PQ and K for dictionary, number of classes).\n  \n  - some parts lack clarity. For instance, the greedy approach to prune the dictionary is exposed in less than 4 lines (top of page 5), though it is far from being straightforward. Likewise, it is not clear why the binary search used for the hashing trick would introduce an overhead of a few hundreds of KB.\n  \n\nOverall this looks like a solid work, but with potentially limited impact research-wise.\n", "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "15 Dec 2016", "REVIEWER_CONFIDENCE": 3}, {"DATE": "08 Dec 2016", "TITLE": "comparison to non-deep baselines", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2"}, {"DATE": "03 Dec 2016", "TITLE": "Question", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1"}, {"IS_META_REVIEW": true, "comments": "This paper describes how to approximate the FastText approach such that its memory footprint is reduced by several orders of magnitude, while preserving its classification accuracy. The original FastText approach was based on a linear classifier on top of bag-of-words embeddings. This type of method is extremely fast to train and test, but the model size can be quite large.\n\nThis paper focuses on approximating the original approach with lossy compression techniques. Namely, the embeddings and classifier matrices A and B are compressed with Product Quantization, and an aggressive dictionary pruning is carried out. Experiments on various datasets (either with small or large number of classes) are conducted to tune the parameters and demonstrate the effectiveness of the approach. With a negligible loss in classification accuracy, an important reduction in term of model size (memory footprint) can be achieved, in the order of 100~1000 folds compared to the original size.\n\nThe paper is well written overall. The goal is clearly defined and well carried out, as well as the experiments. Different options for compressing the model data are evaluated and compared (e.g. PQ vs LSH), which is also interesting. Nevertheless the paper does not propose by itself any novel idea for text classification. It just focuses on adapting existing lossy compression techniques, which is not necessarily a problem. Specifically, it introduces:\n  - a straightforward variant of PQ for unnormalized vectors,\n  - dictionary pruning is cast as a set covering problem (which is NP-hard), but a greedy approach is shown to yield excellent results nonetheless,\n  - hashing tricks and bloom filter are simply borrowed from previous papers.\n\nThese techniques are quite generic and could as well be used in other works. \n\n\nHere are some minor problems with the paper:\n\n  - it is not made clear how the full model size is computed. What is exactly in the model? Which proportion of the full size do the A and B matrices, the dictionary, and the rest, account for? It is hard to follow where is the size bottleneck, which also seems to depend on the target application (i.e. small or large number of test classes). It would have been nice to provide a formula to calculate the total model size as a function of all parameters (k,b for PQ and K for dictionary, number of classes).\n  \n  - some parts lack clarity. For instance, the greedy approach to prune the dictionary is exposed in less than 4 lines (top of page 5), though it is far from being straightforward. Likewise, it is not clear why the binary search used for the hashing trick would introduce an overhead of a few hundreds of KB.\n  \n\nOverall this looks like a solid work, but with potentially limited impact research-wise."}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "The submission describes a method for compressing shallow and wide text classification models. The paper is well written, but the proposed method is not particularly novel, as it's comprised of existing model compression techniques. Overall, the contributions are incremental and the potential impact seems rather limited.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"TITLE": "Review", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "The paper presents a few tricks to compress a wide and shallow text classification model based on n-gram features. These tricks include (1) using (optimized) product quantization to compress embedding weights (2) pruning some of the vocabulary elements (3) hashing to reduce the storage of the vocabulary (this is a minor component of the paper). The paper focuses on models with very large vocabularies and shows a reduction in the size of the models at a relatively minor reduction of the accuracy.\n\nThe problem of compressing neural models is important and interesting. The methods section of the paper is well written with good high level comments and references. However, the machine learning contributions of the paper are marginal to me. The experiments are not too convincing mainly focusing on benchmarks that are not commonly used. The implications of the paper on the state-of-the-art RNN text classification models is unclear.\n\nThe use of (optimized) product quantization for approximating inner product is not particularly novel. Previous work also considered doing this. Most of the reduction in the model sizes comes from pruning vocabulary elements. The method proposed for pruning vocabulary elements is simply based on the assumption that embeddings with larger L2 norm are more important. A coverage heuristic is taken into account too. From a machine learning point of view, the proper baseline to solve this problem is to have a set of (relaxed) binary coefficients for each embedding vector and learn the coefficients jointly with the weights. An L1 regularizer on the coefficients can be used to encourage sparsity. From a practical point of view, I believe an important baseline is missing: what if one simply uses fewer vocabulary elements (e.g based on subword units - see ", "IS_META_REVIEW": false, "RECOMMENDATION": 5, "DATE": "23 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "Effective if simple combination of existing techniques for text-classifier compression", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "The paper proposes a series of tricks for compressing fast (linear) text classification models. The paper is clearly written, and the results are quite strong. The main compression is achieved via product quantization, a technique which has been explored in other applications within the neural network model compression literature. In addition to the Gong et al. work which was cited, it would be worth mentioning Quantized Convolutional Neural Networks for Mobile Devices (CVPR 2016, ", "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "19 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "Lossy compression techniques applied to FastText with nice results", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "This paper describes how to approximate the FastText approach such that its memory footprint is reduced by several orders of magnitude, while preserving its classification accuracy. The original FastText approach was based on a linear classifier on top of bag-of-words embeddings. This type of method is extremely fast to train and test, but the model size can be quite large.\n\nThis paper focuses on approximating the original approach with lossy compression techniques. Namely, the embeddings and classifier matrices A and B are compressed with Product Quantization, and an aggressive dictionary pruning is carried out. Experiments on various datasets (either with small or large number of classes) are conducted to tune the parameters and demonstrate the effectiveness of the approach. With a negligible loss in classification accuracy, an important reduction in term of model size (memory footprint) can be achieved, in the order of 100~1000 folds compared to the original size.\n\nThe paper is well written overall. The goal is clearly defined and well carried out, as well as the experiments. Different options for compressing the model data are evaluated and compared (e.g. PQ vs LSH), which is also interesting. Nevertheless the paper does not propose by itself any novel idea for text classification. It just focuses on adapting existing lossy compression techniques, which is not necessarily a problem. Specifically, it introduces:\n  - a straightforward variant of PQ for unnormalized vectors,\n  - dictionary pruning is cast as a set covering problem (which is NP-hard), but a greedy approach is shown to yield excellent results nonetheless,\n  - hashing tricks and bloom filter are simply borrowed from previous papers.\n\nThese techniques are quite generic and could as well be used in other works. \n\n\nHere are some minor problems with the paper:\n\n  - it is not made clear how the full model size is computed. What is exactly in the model? Which proportion of the full size do the A and B matrices, the dictionary, and the rest, account for? It is hard to follow where is the size bottleneck, which also seems to depend on the target application (i.e. small or large number of test classes). It would have been nice to provide a formula to calculate the total model size as a function of all parameters (k,b for PQ and K for dictionary, number of classes).\n  \n  - some parts lack clarity. For instance, the greedy approach to prune the dictionary is exposed in less than 4 lines (top of page 5), though it is far from being straightforward. Likewise, it is not clear why the binary search used for the hashing trick would introduce an overhead of a few hundreds of KB.\n  \n\nOverall this looks like a solid work, but with potentially limited impact research-wise.\n", "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "15 Dec 2016", "REVIEWER_CONFIDENCE": 3}, {"DATE": "08 Dec 2016", "TITLE": "comparison to non-deep baselines", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2"}, {"DATE": "03 Dec 2016", "TITLE": "Question", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1"}]}
{"text": "SURPRISE-BASED INTRINSIC MOTIVATION FOR DEEP REINFORCEMENT LEARNING\n1 INTRODUCTION\nA reinforcement learning agent uses experiences obtained from interacting with an unknown environment to learn behavior that maximizes a reward signal. The optimality of the learned behavior is strongly dependent on how the agent approaches the exploration/exploitation trade-off in that environment. If it explores poorly or too little, it may never find rewards from which to learn, and its behavior will always remain suboptimal; if it does find rewards but exploits them too intensely, it may wind up prematurely converging to suboptimal behaviors, and fail to discover more rewarding opportunities. Although substantial theoretical work has been done on optimal exploration strategies for environments with finite state and action spaces, we are here concerned with problems that have continuous state and/or action spaces, where algorithms with theoretical guarantees admit no obvious generalization or are prohibitively impractical to implement.\nSimple heuristic methods of exploring such as -greedy action selection and Gaussian control noise have been successful on a wide range of tasks, but are inadequate when rewards are especially sparse. For example, the Deep Q-Network approach of Mnih et al. [13] used -greedy exploration in training deep neural networks to play Atari games directly from raw pixels. On many games, the algorithm resulted in superhuman play; however, on games like Montezuma\u2019s Revenge, where rewards are extremely sparse, DQN (and its variants [25], [26], [15], [12]) with -greedy exploration failed to achieve scores even at the level of a novice human. Similarly, in benchmarking deep reinforcement learning for continuous control, Duan et al.[5] found that policy optimization algorithms that explored by acting according to the current stochastic policy, including REINFORCE and Trust Region Policy Optimization (TRPO), could succeed across a diverse slate of simulated robotics control tasks with well-defined, non-sparse reward signals (like rewards proportional to the forward velocity of the robot). Yet, when tested in environments with sparse rewards\u2014where the agent would only be able to attain rewards after first figuring out complex motion primitives without reinforcement\u2014every algorithm failed to attain scores better than random agents. The failure modes in all of these cases pertained to the nature of the exploration: the agents encountered reward signals so infrequently that they were never able to learn reward-seeking behavior.\nOne approach to encourage better exploration is via intrinsic motivation, where an agent has a task-independent, often information-theoretic intrinsic reward function which it seeks to maximize in addition to the reward from the environment. Examples of intrinsic motivation include empowerment, where the agent enjoys the level of control it has about its future; surprise, where the agent is excited to see outcomes that run contrary to its understanding of the world; and novelty, where the agent is excited to see new states (which is tightly connected to surprise, as shown in [2]). For in-depth reviews of the different types of intrinsic motivation, we direct the reader to [1] and [17].\nRecently, several applications of intrinsic motivation to the deep reinforcement learning setting (such as [2], [7], [22]) have found promising success. In this work, we build on that success by exploring scalable measures of surprise for intrinsic motivation in deep reinforcement learning. We formulate surprise as the KL-divergence of the true transition probability distribution from a transition model which is learned concurrently with the policy, and consider two approximations to this divergence which are easy to compute in practice. One of these approximations results in using the surprisal of a transition as an intrinsic reward; the other results in using a measure of learning progress which is closer to a Bayesian concept of surprise. Our contributions are as follows:\n1. we investigate surprisal and learning progress as intrinsic rewards across a wide range of environments in the deep reinforcement learning setting, and demonstrate empirically that the incentives (especially surprisal) result in efficient exploration,\n2. we evaluate the difficulty of the slate of sparse reward continuous control tasks introduced by Houthooft et al. [7] to benchmark exploration incentives, and introduce a new task to complement the slate,\n3. and we present an efficient method for learning the dynamics model (transition probabilities) concurrently with a policy.\nWe distinguish our work from prior work in a number of implementation details: unlike Bellemare et al. [2], we learn a transition model as opposed to a state-action occupancy density; unlike Stadie et al. [22], our formulation naturally encompasses environments with stochastic dynamics; unlike Houthooft et al. [7], we avoid the overhead of maintaining a distribution over possible dynamics models, and learn a single deep dynamics model.\nIn our empirical evaluations, we compare the performance of our proposed intrinsic rewards with other heuristic intrinsic reward schemes and to recent results from the literature. In particular, we compare to Variational Information Maximizing Exploration (VIME) [7], a method which approximately maximizes Bayesian surprise and currently achieves state-of-the-art performance on continuous control with sparse rewards. We show that our incentives can perform on the level of VIME at a lower computational cost.\n2 PRELIMINARIES\nWe begin by introducing notation which we will use throughout the paper. A Markov decision process (MDP) is a tuple, (S,A,R, P, \u00b5), where S is the set of states, A is the set of actions, R : S \u00d7 A \u00d7 S \u2192 R is the reward function, P : S \u00d7 A \u00d7 S \u2192 [0, 1] is the transition probability function (where P (s\u2032|s, a) is the probability of transitioning to state s\u2032 given that the previous state was s and the agent took action a in s), and \u00b5 : S \u2192 [0, 1] is the starting state distribution. A policy \u03c0 : S \u00d7 A \u2192 [0, 1] is a distribution over actions per state, with \u03c0(a|s) the probability of selecting a in state s. We aim to select a policy \u03c0 which maximizes a performance measure, L(\u03c0), which usually takes the form of expected finite-horizon total return (sum of rewards in a fixed time period), or expected infinite-horizon discounted total return (discounted sum of all rewards forever). In this paper, we use the finite-horizon total return formulation.\n3 SURPRISE INCENTIVES\nTo train an agent with surprise-based exploration, we alternate between making an update step to a dynamics model (an approximator of the MDP\u2019s transition probability function), and making a policy update step that maximizes a trade-off between policy performance and a surprise measure.\nThe dynamics model step makes progress on the optimization problem\nmin \u03c6 \u2212 1 |D| \u2211 (s,a,s\u2032)\u2208D logP\u03c6(s \u2032|s, a) + \u03b1f(\u03c6), (1)\nwhere D is is a dataset of transition tuples from the environment, P\u03c6 is the model we are learning, f is a regularization function, and \u03b1 > 0 is a regularization trade-off coefficient. The policy update step makes progress on an approximation to the optimization problem\nmax \u03c0 L(\u03c0) + \u03b7 E s,a\u223c\u03c0\n[DKL(P ||P\u03c6)[s, a]] , (2)\nwhere \u03b7 > 0 is an explore-exploit trade-off coefficient. The exploration incentive in (2), which we select to be the on-policy average KL-divergence of P\u03c6 from P , is intended to capture the agent\u2019s surprise about its experience. The dynamics model P\u03c6 should only be close to P on regions of the transition state space that the agent has already visited (because those transitions will appear in D and thus the model will be fit to them), and as a result, the KL divergence of P\u03c6 and P will be higher in unfamiliar places. Essentially, this exploits the generalization in the model to encourage the agent to go where it has not gone before. The surprise incentive in (2) gives the net effect of performing a reward shaping of the form\nr\u2032(s, a, s\u2032) = r(s, a, s\u2032) + \u03b7 (logP (s\u2032|s, a)\u2212 logP\u03c6(s\u2032|s, a)) , (3) where r(s, a, s\u2032) is the original reward and r\u2032(s, a, s\u2032) is the transformed reward, so ideally we could solve (2) by applying any reinforcement learning algorithm with these reshaped rewards. In practice, we cannot directly implement this reward reshaping because P is unknown. Instead, we consider two ways of finding an approximate solution to (2).\nIn one method, we approximate the KL-divergence by the cross-entropy, which is reasonable when H(P ) is finite (and small) and P\u03c6 is sufficiently far from P 1; that is, denoting the cross-entropy by H(P, P\u03c6)[s, a] . = Es\u2032\u223cP (\u00b7|s,a)[\u2212 logP\u03c6(s\u2032|s, a)], we assume\nDKL(P ||P\u03c6)[s, a] = H(P, P\u03c6)[s, a]\u2212H(P )[s, a] \u2248 H(P, P\u03c6)[s, a].\n(4)\nThis approximation results in a reward shaping of the form\nr\u2032(s, a, s\u2032) = r(s, a, s\u2032)\u2212 \u03b7 logP\u03c6(s\u2032|s, a); (5) here, the intrinsic reward is the surprisal of s\u2032 given the model P\u03c6 and the context (s, a).\nIn the other method, we maximize a lower bound on the objective in (2) by lower bounding the surprise term:\nDKL(P ||P\u03c6)[s, a] = DKL(P ||P\u03c6\u2032)[s, a] + E s\u2032\u223cP\n[ log P\u03c6\u2032(s \u2032|s, a)\nP\u03c6(s\u2032|s, a) ] \u2265 E s\u2032\u223cP [ log P\u03c6\u2032(s \u2032|s, a)\nP\u03c6(s\u2032|s, a)\n] .\n(6)\nThe bound (6) results in a reward shaping of the form\nr\u2032(s, a, s\u2032) = r(s, a, s\u2032) + \u03b7 (logP\u03c6\u2032(s \u2032|s, a)\u2212 logP\u03c6(s\u2032|s, a)) , (7)\nwhich requires a choice of \u03c6\u2032. From (6), we can see that the bound becomes tighter by minimizing DKL(P ||P\u03c6\u2032). As a result, we choose \u03c6\u2032 to be the parameters of the dynamics model after k updates based on (1), and \u03c6 to be the parameters from before the updates. Thus, at iteration t, the reshaped rewards are\nr\u2032(s, a, s\u2032) = r(s, a, s\u2032) + \u03b7 ( logP\u03c6t(s \u2032|s, a)\u2212 logP\u03c6t\u2212k(s\u2032|s, a) ) ; (8)\nhere, the intrinsic reward is the k-step learning progress at (s, a, s\u2032). It also bears a resemblance to Bayesian surprise; we expand on this similarity in the next section.\nIn our experiments, we investigate both the surprisal bonus (5) and the k-step learning progress bonus (8) (with varying values of k).\n1On the other hand, if H(P )[s, a] is non-finite everywhere\u2014for instance if the MDP has continuous states and deterministic transitions\u2014then as long as it has the same sign everywhere, Es,a\u223c\u03c0[H(P )[s, a]] is a constant with respect to \u03c0 and we can drop it from the optimization problem anyway.\n3.1 DISCUSSION\nIdeally, we would like the intrinsic rewards to vanish in the limit as P\u03c6 \u2192 P , because in this case, the agent should have sufficiently explored the state space, and should primarily learn from extrinsic rewards. For the proposed intrinsic reward in (5), this is not the case, and it may result in poor performance in that limit. The thinking goes that when P\u03c6 = P , the agent will be incentivized to seek out states with the noisiest transitions. However, we argue that this may not be an issue, because the intrinsic motivation seems mostly useful long before the dynamics model is fully learned. As long as the agent is able to find the extrinsic rewards before the intrinsic reward is just the entropy in P , the pathological noise-seeking behavior should not happen. On the other hand, the intrinsic reward in (8) should not suffer from this pathology, because in the limit, as the dynamics model converges, we should have P\u03c6t \u2248 P\u03c6t\u2212k . Then the intrinsic reward will vanish as desired. Next, we relate (8) to Bayesian surprise. The Bayesian surprise associated with a transition is the reduction in uncertainty over possibly dynamics models from observing it ([1],[8]):\nDKL (P (\u03c6|ht, at, st+1)||P (\u03c6|ht)) . Here, P (\u03c6|ht) is meant to represent a distribution over possible dynamics models parametrized by \u03c6 given the preceding history of observed states and actions ht (so ht includes st), and P (\u03c6|ht, at, st+1) is the posterior distribution over dynamics models after observing (at, st+1). By Bayes\u2019 rule, the dynamics prior and posterior are related to the model-based transition probabilities by\nP (\u03c6|ht, at, st+1) = P (\u03c6|ht)P (st+1|ht, at, \u03c6)\nE\u03c6\u223cP (\u00b7|ht) [P (st+1|ht, at, \u03c6)] ,\nso the Bayesian surprise can be expressed as E\n\u03c6\u223cPt+1 [logP (st+1|ht, at, \u03c6)]\u2212 log E \u03c6\u223cPt [P (st+1|ht, at, \u03c6)] , (9)\nwhere Pt+1 = P (\u00b7|ht, at, st+1) is the posterior and Pt = P (\u00b7|ht) is the prior. In this form, the resemblance between (9) and (8) is clarified. Although the update from \u03c6t\u2212k to \u03c6t is not Bayesian\u2014 and is performed in batch, instead of per transition sample\u2014we can imagine (8) might contain similar information to (9).\n3.2 IMPLEMENTATION DETAILS\nOur implementation usesL2 regularization in the dynamics model fitting, and we impose an additional constraint to keep model iterates close in the KL-divergence sense. Denoting the average divergence as\nD\u0304KL(P\u03c6\u2032 ||P\u03c6) = 1 |D| \u2211\n(s,a)\u2208D\nDKL(P\u03c6\u2032 ||P\u03c6)[s, a], (10)\nour dynamics model update is\n\u03c6i+1 = arg min \u03c6 \u2212 1 |D| \u2211 (s,a,s\u2032)\u2208D logP\u03c6(s \u2032|s, a) + \u03b1\u2016\u03c6\u201622 : D\u0304KL(P\u03c6||P\u03c6i) \u2264 \u03ba. (11)\nThe constraint value \u03ba is a hyper-parameter of the algorithm. We solve this optimization problem approximately using a single second-order step with a line search, as described by [20]; full details are given in supplementary material. D is a FIFO replay memory, and at each iteration, instead of using the entirety of D for the update step we sub-sample a batch d \u2282 D. Also, similarly to [7], we adjust the bonus coefficient \u03b7 at each iteration, to keep the average bonus magnitude upper-bounded (and usually fixed). Let \u03b70 denote the desired average bonus, and r+(s, a, s\u2032) denote the intrinsic reward; then, at each iteration, we set\n\u03b7 = \u03b70 max (\n1, 1|B| \u2223\u2223\u2223\u2211(s,a,s\u2032)\u2208B r+(s, a, s\u2032)\u2223\u2223\u2223) , where B is the batch of data used for the policy update step. This normalization improves the stability of the algorithm by keeping the scale of the bonuses fixed with respect to the scale of the extrinsic rewards. Also, in environments where the agent can die, we avoid the possibility of the intrinsic rewards becoming a living cost by translating all bonuses so that the mean is nonnegative. The basic outline of the algorithm is given as Algorithm 1. In all experiments, we use fully-factored Gaussian distributions for the dynamics models, where the means and variances are the outputs of neural networks.\nAlgorithm 1 Reinforcement Learning with Surprise Incentive Input: Initial policy \u03c00, dynamics model P\u03c60 repeat\ncollect rollouts on current policy \u03c0i add rollout (s, a, s\u2032) tuples to replay memory D compute reshaped rewards using (5) or (8) with dynamics model P\u03c6i normalize \u03b7 by the average intrinsic reward of the current batch of data update policy to \u03c0i+1 using any RL algorithm with the reshaped rewards update the dynamics model to P\u03c6i+1 according to (11)\nuntil training is completed\n4 EXPERIMENTS\nWe evaluate our proposed surprise incentives on a wide range of benchmarks that are challenging for naive exploration methods, including continuous control and discrete control tasks. Our continuous control tasks include the slate of sparse reward tasks introduced by Houthooft et al. [7]: sparse MountainCar, sparse CartPoleSwingup, and sparse HalfCheetah, as well as a new sparse reward task that we introduce here: sparse Swimmer. (We refer to these environments with the prefix \u2018sparse\u2019 to differentiate them from other versions which appear in the literature, where agents receive non-sparse reward signals.) Additionally, we evaluate performance on a highly-challenging hierarchical sparse reward task introduced by Duan et al [5], SwimmerGather. The discrete action tasks are several games from the Atari RAM domain of the OpenAI Gym [4]: Pong, BankHeist, Freeway, and Venture.\nEnvironments with deterministic and stochastic dynamics are represented in our benchmarks: the continuous control domains have deterministic dynamics, while the Gym Atari RAM games have stochastic dynamics. (In the Atari games, actions are repeated for a random number of frames.)\nWe use Trust Region Policy Optimization (TRPO) [20], a state-of-the-art policy gradient method, as our base reinforcement learning algorithm throughout our experiments, and we use the rllab implementations of TRPO and the continuous control tasks [5]. Full details for the experimental set-up are included in the appendix.\nOn all tasks, we compare against TRPO without intrinsic rewards, which we refer to as using naive exploration (in contrast to intrinsically motivated exploration). For the continuous control tasks, we also compare against intrinsic motivation using the L2 model prediction error,\nr+(s, a, s \u2032) = \u2016s\u2032 \u2212 \u00b5\u03c6(s, a)\u20162, (12)\nwhere \u00b5\u03c6 is the mean of the learned Gaussian distribution P\u03c6. The model prediction error was investigated as intrinsic motivation for deep reinforcement learning by Stadie et al [22], although they used a different method for learning the model \u00b5\u03c6. This comparison helps us verify whether or not our proposed form of surprise, as a KL-divergence from the true dynamics model, is useful. Additionally, we compare our performance against the performance reported by Houthooft et al. [7] for Variational Information Maximizing Exploration (VIME), a method where the intrinsic reward associated with a transition approximates its Bayesian surprise using variational methods. Currently, VIME has achieved state-of-the-art results on intrinsic motivation for continuous control.\nAs a final check for the continuous control tasks, we benchmark the tasks themselves, by measuring the performance of the surprisal bonus without any dynamics learning: r+(s, a, s\u2032) = \u2212 logP\u03c60(s\u2032|s, a), where \u03c60 are the original random parameters of P\u03c6. This allows us to verify whether our benchmark tasks actually require surprise to solve at all, or if random exploration strategies successfully solve them.\n4.1 CONTINUOUS CONTROL RESULTS\nMedian performance curves are shown in Figure 1 with interquartile ranges shown in shaded areas. Note that TRPO without intrinsic motivation failed on all tasks: the median score and upper quartile range for naive exploration were zero everywhere. Also note that TRPO with random exploration bonuses failed on most tasks, as shown separately in Figure 2. We found that surprise was not needed to solve MountainCar, but was necessary to perform well on the other tasks.\nThe surprisal bonus was especially robust across tasks, achieving good results in all domains and substantially exceeding the other baselines on the more challenging ones. The learning progress bonus for k = 1 was successful on CartpoleSwingup and HalfCheetah but it faltered in the others. Its weak performance in MountainCar was due to premature convergence of the dynamics model, which resulted in the agent receiving intrinsic rewards that were identically zero. (Given the simplicity of the environment, it is not surprising that the dynamics model converged so quickly.) In Swimmer, however, it seems that the learning progress bonuses did not inspire sufficient exploration. Because the Swimmer environment is effectively a stepping stone to the harder SwimmerGather, where the agent has to learn a motion primitive and collect target pellets, on SwimmerGather, we only evaluated the intrinsic rewards that had been successful on Swimmer.\nBoth surprisal and learning progress (with k = 1) exceeded the reported performance of VIME on HalfCheetah by learning to solve the task more quickly. On CartpoleSwingup, however, both were more susceptible to getting stuck in locally optimal policies, resulting in lower median scores than VIME. Surprisal performed comparably to VIME on SwimmerGather, the hardest task in the slate\u2014in the sense that after 1000 iterations, they both reached approximately the same median score\u2014although with greater variance than VIME.\nOur results suggest that surprisal is a viable alternative to VIME in terms of performance, and is highly favorable in terms of computational cost. In VIME, a backwards pass through the dynamics model must be computed for every transition tuple separately to compute the intrinsic rewards, whereas our surprisal bonus only requires forward passes through the dynamics model for intrinsic\nreward computation. (Limitations of current deep learning tool kits make it difficult to efficiently compute separate backwards passes, whereas almost all of them support highly parallel forward computations.) Furthermore, our dynamics model is substantially simpler than the Bayesian neural network dynamics model of VIME. To illustrate this point, in Figure 3 we show the results of a speed comparison making use of the open-source VIME code [6], with the settings described in the VIME paper. In our speed test, our bonus had a per-iteration speedup of a factor of 3 over VIME.2 We give a full analysis of the potential speedup in Appendix C.\n4.2 ATARI RAM DOMAIN RESULTS\nMedian performance curves are shown in Figure 4, with tasks arranged from (a) to (d) roughly in order of increasing difficulty.\nIn Pong, naive exploration naturally succeeds, so we are not surprised to see that intrinsic motivation does not improve performance. However, this serves as a sanity check to verify that our intrinsic rewards do not degrade performance. (As an aside, we note that the performance here falls short of the standard score of 20 for this domain because we truncate play at 5000 timesteps.)\nIn BankHeist, we find that intrinsic motivation accelerates the learning significantly. The agents with surprisal incentives reached high levels of performance (scores > 1000) 10% sooner than naive exploration, while agents with learning progress incentives reached high levels almost 20% sooner.\nIn Freeway, the median performance for TRPO without intrinsic motivation was adequate, but the lower quartile range was quite poor\u2014only 6 out of 10 runs ever found rewards. With the learning progress incentives, 8 out of 10 runs found rewards; with the surprisal incentive, all 10 did. Freeway is a game with very sparse rewards, where the agent effectively has to cross a long hallway before it can score a point, so naive exploration tends to exhibit random walk behavior and only rarely reaches the reward state. The intrinsic motivation helps the agent explore more purposefully.\n2We compute this by comparing the marginal time cost incurred just by the bonus in each case: that is, if Tvime, Tsurprisal, and Tnobonus denote the times to 15 iterations, we obtain the speedup as\nTvime \u2212 Tnobonus Tsurprisal \u2212 Tnobonus .\nIn Venture, we obtain our strongest results in the Atari domain. Venture is extremely difficult because the agent has to navigate a large map to find very sparse rewards, and the agent can be killed by enemies interspersed throughout. We found that our intrinsic rewards were able to substantially improve performance over naive exploration in this challenging environment. Here, the best performance was again obtained by the surprisal incentive, which usually inspired the agent to reach scores greater than 500.\n4.3 COMPARING INCENTIVES\nAmong our proposed incentives, we found that surprisal worked the best overall, achieving the most consistent performance across tasks. The learning progress-based incentives worked well on some domains, but generally not as well as surprisal. Interestingly, learning progress with k = 10 performed much worse on the continuous control tasks than with k = 1, but we observed virtually no difference in their performance on the Atari games; it is unclear why this should be the case.\nSurprisal strongly outperformed the L2 error based incentive on the harder continuous control tasks, learning to solve them more quickly and without forgetting. Because we used fully-factored Gaussians for all of our dyanmics models, the surprisal had the form\n\u2212 logP\u03c6(s\u2032|s, a) = n\u2211 i=1\n( (s\u2032i \u2212 \u00b5\u03c6,i(s, a))2\n2\u03c32\u03c6,i(s, a) + log \u03c3\u03c6,i(s, a)\n) + k\n2 log 2\u03c0,\nwhich essentially includes the L2-squared error norm as a sub-expression. The relative difference in performance suggests that the variance terms confer additional useful information about the novelty of a state-action pair.\n5 RELATED WORK\nSubstantial theoretical work has been done on optimal exploration in finite MDPs, resulting in algorithms such as E3 [10], R-max [3], and UCRL [9], which scale polynomially with MDP size. However, these works do not permit obvious generalizations to MDPs with continuous state and action spaces. C-PACE [18] provides a theoretical foundation for PAC-optimal exploration in MDPs with continuous state spaces, but it requires a metric on state spaces. Lopes et al. [11] investigated exploration driven by learning progress and proved theoretical guarantees for their approach in the finite MDP case, but they did not address the question of scaling their approach to continuous or high-dimensional MDPs. Also, although they formulated learning progress in the same way as (8), they formed intrinsic rewards differently. Conceptually and mathematically, our work is closest to prior work on curiosity and surprise [8, 19, 23, 24], although these works focus mainly on small finite MDPs.\nRecently, several intrinsic motivation strategies that deal specifically with deep reinforcement learning have been proposed. Stadie et al. [22] learn deterministic dynamics models by minimizing Euclidean loss\u2014whereas in our work, we learn stochastic dynamics with cross entropy loss\u2014and use L2 prediction errors for intrinsic motivation. Houthooft et al. [7] train Bayesian neural networks to approximate posterior distributions over dynamics models given observed data, by maximizing a variational lower bound; they then use second-order approximations of the Bayesian surprise as intrinsic motivation. Bellemare et al. [2] derived pseudo-counts from CTS density models over states and used those to form intrinsic rewards, notably resulting in dramatic performance improvement on Montezuma\u2019s Revenge, one of the hardest games in the Atari domain. Mohamed and Rezende [14] developed a scalable method of approximating empowerment, the mutual information between an agent\u2019s actions and the future state of the environment, using variational methods. Oh et al. [16] estimated state visit frequency using Gaussian kernels to compare against a replay memory, and used these estimates for directed exploration.\n6 CONCLUSIONS\nIn this work, we formulated surprise for intrinsic motivation as the KL-divergence of the true transition probabilities from learned model probabilities, and derived two approximations\u2014surprisal and k-step\nlearning progress\u2014that are scalable, computationally inexpensive, and suitable for application to high-dimensional and continuous control tasks. We showed that empirically, motivation by surprisal and 1-step learning progress resulted in efficient exploration on several hard deep reinforcement learning benchmarks. In particular, we found that surprisal was a robust and effective intrinsic motivator, outperforming other heuristics on a wide range of tasks, and competitive with the current state-of-the-art for intrinsic motivation in continuous control.\nACKNOWLEDGEMENTS\nWe thank Rein Houthooft for interesting discussions and for sharing data from the original VIME experiments. We also thank Rocky Duan, Carlos Florensa, Vicenc Rubies-Royo, Dexter Scobee, and Eric Mazumdar for insightful discussions and reviews of the preliminary manuscript.\nThis work is supported by TRUST (Team for Research in Ubiquitous Secure Technology) which receives support from NSF (award number CCF-0424422).\nA SINGLE STEP SECOND-ORDER OPTIMIZATION\nIn our experiments, we approximately solve several optimization problems by using a single secondorder step with a line search. This section will describe the exact methodology, which was originally given by Schulman et al. [20].\nWe consider the optimization problem\np\u2217 = max \u03b8 L(\u03b8) : D(\u03b8) \u2264 \u03b4, (13)\nwhere \u03b8 \u2208 Rn, and for some \u03b8old we have D(\u03b8old) = 0,\u2207\u03b8D(\u03b8old) = 0, and\u22072\u03b8D(\u03b8old) 0; also, \u2200\u03b8,D(\u03b8) \u2265 0. We suppose that \u03b4 is small, so the optimal point will be close to \u03b8old. We also suppose that the curvature of the constraint is much greater than the curvature of the objective. As a result, we feel justified in approximating the objective to linear order and the constraint to quadratic order:\nL(\u03b8) \u2248 L(\u03b8old) + gT (\u03b8 \u2212 \u03b8old) g . = \u2207\u03b8L(\u03b8old)\nD(\u03b8) \u2248 1 2 (\u03b8 \u2212 \u03b8old)TA(\u03b8 \u2212 \u03b8old) A . = \u22072\u03b8D(\u03b8old).\nWe now consider the approximate optimization problem,\np\u2217 \u2248 max \u03b8 gT (\u03b8 \u2212 \u03b8old) :\n1 2 (\u03b8 \u2212 \u03b8old)TA(\u03b8 \u2212 \u03b8old) \u2264 \u03b4.\nThis optimization problem is convex as long as A 0, which is an assumption that we make. (If this assumption seems to be empirically invalid, then we repair the issue by using the substitution A\u2192 A+ I , where I is the identity matrix, and > 0 is a small constant chosen so that we usually have A+ I 0.) This problem can be solved analytically by applying methods of duality, and its optimal point is\n\u03b8\u2217 = \u03b8old +\n\u221a 2\u03b4\ngTA\u22121g A\u22121g. (14)\nIt is possible that the parameter update step given by (14) may not exactly solve the original optimization problem (13)\u2014in fact, it may not even satisfy the constraint\u2014so we perform a line search between \u03b8old and \u03b8\u2217. Our update with the line search included is given by\n\u03b8 = \u03b8old + s k\n\u221a 2\u03b4\ngTA\u22121g A\u22121g, (15)\nwhere s \u2208 (0, 1) is a backtracking coefficient, and k is the smallest integer for which L(\u03b8) \u2265 L(\u03b8old) and D(\u03b8) \u2264 \u03b4. We select k by checking each of k = 1, 2, ...,K, where K is the maximum number of backtracks. If there is no value of k in that range which satisfies the conditions, no update is performed.\nBecause the optimization problems we solve with this method tend to involve thousands of parameters, inverting A is prohibitively computationally expensive. Thus in the implementation of this algorithm that we use, the search direction x = A\u22121g is found by using the conjugate gradient method to solve Ax = g; this avoids the need to invert A.\nWhen A and g are sample averages meant to stand in for expectations, we employ an additional trick to reduce the total number of computations necessary to solve Ax = g. The computation of A is more expensive than g, and so we use a smaller fraction of the population to estimate it quickly. Concretely, suppose that the original optimization problem\u2019s objective is Ez\u223cP [L(\u03b8, z)], and the constraint is Ez\u223cP [D(\u03b8, z)] \u2264 \u03b4, where z is some random variable and P is its distribution; furthermore, suppose that we have a dataset of samples D = {zi}i=1,...,N drawn on P , and we form an approximate optimization problem using these samples. Defining g(z) .= \u2207\u03b8L(\u03b8old, z) and A(z)\n. = \u22072\u03b8D(\u03b8old, z), we would need to solve(\n1 |D| \u2211 z\u2208D A(z)\n) x = 1\n|D| \u2211 z\u2208D g(z)\nto obtain the search direction x. However, because the computation of the average Hessian is expensive, we sub-sample a batch b \u2282 D to form it. As long as b is a large enough set, then the approximation\n1 |b| \u2211 z\u2208b A(z) \u2248 1 |D| \u2211 z\u2208D A(z) \u2248 E z\u223cP [A(z)]\nis good, and the search direction we obtain by solving( 1\n|b| \u2211 z\u2208b A(z)\n) x = 1\n|D| \u2211 z\u2208D g(z)\nis reasonable. The sub-sample ratio |b|/|D| is a hyperparameter of the algorithm.\nB EXPERIMENT DETAILS\nB.1 ENVIRONMENTS\nThe environments have the following state and action spaces: for the sparse MountainCar environment, S \u2286 R2, A \u2286 R1; for the sparse CartpoleSwingup task, S \u2286 R4, A \u2286 R1; for the sparse HalfCheetah\ntask, S \u2282 R20, A \u2286 R6; for the sparse Swimmer task, S \u2286 R13, A \u2286 R2; for the SwimmerGather task, S \u2286 R33, A \u2286 R2; for the Atari RAM domain, S \u2286 R128, A \u2286 {1, ..., 18}. For the sparse MountainCar task, the agent receives a reward of 1 only when it escapes the valley. For the sparse CartpoleSwingup task, the agent receives a reward of 1 only when cos(\u03b2) > 0.8, with \u03b2 the pole angle. For the sparse HalfCheetah task, the agent receives a reward of 1 when xbody \u2265 5. For the sparse Swimmer task, the agent receives a reward of 1 + |vbody| when |xbody| \u2265 2. Atari RAM states, by default, take on values from 0 to 256 in integer intervals. We use a simple preprocessing step to map them onto values in (\u22121/3, 1/3). Let x denote the raw RAM state, and s the preprocessed RAM state:\ns = 1\n3 ( x 128 \u2212 1 ) .\nB.2 POLICY AND VALUE FUNCTIONS\nFor all continuous control tasks we used fully-factored Gaussian policies, where the means of the action distributions were the outputs of neural networks, and the variances were separate trainable parameters. For the sparse MountainCar and sparse CartpoleSwingup tasks, the policy mean networks had a single hidden layer of 32 units. For sparse HalfCheetah, sparse Swimmer, and SwimmerGather, the policy mean networks were of size (64, 32). For the Atari RAM tasks, we used categorical distributions over actions, produced by neural networks of size (64, 32).\nThe value functions used for the sparse MountainCar and sparse CartpoleSwingup tasks were neural networks with a single hidden layer of 32 units. For sparse HalfCheetah, sparse Swimmer, and SwimmerGather, time-varying linear value functions were used, as described by Duan et al. [5]. For the Atari RAM tasks, the value functions were neural networks of size (64, 32). The neural network value functions were learned via single second-order step optimization; the linear baselines were obtained by least-squares fit at each iteration.\nAll neural networks were feed-forward, fully-connected networks with tanh activation units.\nB.3 TRPO HYPERPARAMETERS\nFor all tasks, the MDP discount factor \u03b3 was fixed to 0.995, and generalized advantage estimators (GAE) [21] were used, with the GAE \u03bb parameter fixed to 0.95.\nIn the table below, we show several other TRPO hyperparameters. Batch size refers to steps of experience collected at each iteration. The sub-sample factor is for the second-order optimization step, as detailed in Appendix A.\nB.4 EXPLORATION HYPERPARAMETERS\nFor all tasks, fully-factored Gaussian distributions were used as dynamics models, where the means and variances of the distributions were the outputs of neural networks.\nFor the sparse MountainCar and sparse CartpoleSwingup tasks, the means and variances were parametrized by single hidden layer neural networks with 32 units. For all other tasks, the means and variances were parametrized by neural networks with two hidden layers of size 64 units each. All networks used tanh activation functions.\nFor all continuous control tasks except SwimmerGather, we used replay memories of size 5, 000, 000, and a KL-divergence step size of \u03ba = 0.001. For SwimmerGather, the replay memory was the same size, but we set the KL-divergence size to \u03ba = 0.005. For the Atari RAM domain tasks, we used replay memories of size 1, 000, 000, and a KL-divergence step size of \u03ba = 0.01.\nFor all tasks except SwimmerGather and Venture, 5000 time steps of experience were sampled from the replay memory at each iteration of dynamics model learning to take a stochastic step on (11), and a sub-sample factor of 1 was used in the second-order step optimizer. For SwimmerGather and Venture, 10, 000 time steps of experience were sampled at each iteration, and a sub-sample factor of 0.5 was used in the optimizer.\nFor all continuous control tasks, the L2 penalty coefficient was set to \u03b1 = 1. For the Atari RAM tasks except for Venture, it was set to \u03b1 = 0.01. For Venture, it was set to \u03b1 = 0.1.\nFor all continuous control tasks except SwimmerGather, \u03b70 = 0.001. For SwimmerGather, \u03b70 = 0.0001. For the Atari RAM tasks, \u03b70 = 0.005.\nC ANALYSIS OF SPEEDUP COMPARED TO VIME\nIn this section, we provide an analysis of the time cost incurred by using VIME or our bonuses, and derive the potential magnitude of speedup attained by our bonuses versus VIME.\nAt each iteration, bonuses based on learned dynamics models incur two primary costs:\n\u2022 the time cost of fitting the dynamics model, \u2022 and the time cost of computing the rewards.\nWe denote the dynamics fitting costs for VIME and our methods as T fitvime and T fit ours. Although the Bayesian neural network dynamics model for VIME is more complex than our model, the fit times can work out to be similar depending on the choice of fitting algorithm. In our speed test, the fit times were nearly equivalent, but used different algorithms.\nFor the time cost of computing rewards, we first introduce the following quantities:\n\u2022 n: the number of CPU threads available, \u2022 tf : time for a forward pass through the model, \u2022 tb: time for a backward pass through the model, \u2022 N : batch size (number of samples per iteration), \u2022 k: the number of forward passes that can be performed simultaneously.\nFor our method, the time cost of computing rewards is\nT rewours = Ntf kn .\nFor VIME, things are more complex. Each reward requires the computation of a gradient through its model, which necessitates a forward and a backward pass. Because gradient calculations cannot be efficiently parallelized by any deep learning toolkits currently available3, each (s, a, s\u2032) tuple requires its own forward/backward pass. As a result, the time cost of computing rewards for VIME is:\nT rewvime = N(tf + tb)\nn .\nThe speedup of our method over VIME is therefore\nT fitvime + N(tf+tb) n\nT fitours + Ntf kn\n.\nIn the limit of large N , and with the approximation that tf \u2248 tb, the speedup is a factor of \u223c 2k. 3If this is not correct, please contact the authors so that we can issue a correction! But to the best of our knowledge, this is currently true, at time of publication.\n", "cats": {"READ": 1.0, "DONTREAD": 0.0}, "reviews": [{"IS_META_REVIEW": true, "comments": "This paper explores the topic of intrinsic motivation in the context of deep RL. It proposes a couple of variants derived from an auxiliary model-learning process (prediction error, surprise and learning progress), and shows that those can help exploration on a number of continuous control tasks (and the Atari game \u201cventure\u201d, maybe).\n\nNovelty: none of the proposed types of intrinsic motivation are novel, and it\u2019s arguable whether the application to deep RL is novel (see e.g. Kompella et al 2012).\n\nPotential: the idea of seeking out states where a transition model is uncertain is sensible, but also limited -- I would encourage the authors to also discuss the limitations. For example in a game like Go the transition model is trivially learned, so this approach would revert to random exploration. So other forms of learning progress or surprise derived from the agent\u2019s competence instead might be more promising in the long run? See also Srivastava et al 2012 for further thoughts.\n\nComputation time: I find the paper\u2019s claimed superiority over VIME to be overblown: the gain seems to stem almost exclusively from a faster initialization, but have very similar per-step cost? So given that VIME is also performing very competitively, what arguments can you advance for your own method(s)?"}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "The paper proposes an intuitive method for exploration, namely to build a model of the system dynamics and explore regions where this approximation differs from the observed data (i.e., how \"surprised\" the agent was by an observation). The idea is a nice one, and part of the benefit comes from the simplicity and wide applicability of the approach.\n \n The main drawback of this paper is simply that the resulting approach doesn't substantially outperform existing approaches, at least not to a degree where it seem like the paper should should be clearly accepted to ICLR. On the continuous control tasks, the advantage over VIME seems very unclear (at best the results are mixed, showing sometimes surprisal and sometime VIME do better), and on the Atari games no comparison is made against many of the methods tuned for this setting, such as Gorila (Nair, 2015) which achieves some of the best results we are aware of on the Venture game, which is definitely the strongest result in this current paper, but not as good as this previous work. We know the settings are different, but overall it seems like the approach is largely outperformed by existing approaches, and thus the advantage mainly comes from runtime. This is certainly an interesting take, but needs to be studied a lot more thoroughly before it would make a really compelling case. We would like to recomend this paper to the workshop track. The pros/cons are as follows:\n \n Pros:\n + Simple and intuitive method for exploration\n \n Cons:\n - Doesn't seem to substantially outperform existing methods\n - No comparison to many alternative approaches for some of the \"better\" results in the paper.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"TITLE": "Important extension of existing work on intrinsic motivation. Experimental results are less convincing.", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "The authors present a novel approach to surprise-based intrinsic motivation in deep reinforcement learning. The authors clearly explain the difference from other recent approaches to intrinsic motivation and back up their method with results from a broad class of discrete and continuous action domains. They present two tractable approximations to their framework - one which ignores the stochasticity of the true environmental dynamics, and one which approximates the rate of information gain (somewhat similar to Schmidhuber's formal theory of creativity, fun and intrinsic motivation). The results of this exploration bonus when added to TRPO are generally better than standard TRPO. However, I would have appreciated a more thorough comparison against other recent work on intrinsic motivation. For instance, Bellemare et al 2016 recently achieved significant performance gains on challenging Atari games like Montezuma's Revenge by combining DQN with an exploration bonus, however Montezuma's Revenge is not presented as an experiment here. Such comparisons would significantly improve the strength of the paper.", "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "18 Dec 2016", "REVIEWER_CONFIDENCE": 3}, {"TITLE": "Review", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "This paper provides a surprise-based intrinsic reward method for reinforcement learning, along with two practical algorithms for estimating those rewards. The ideas are similar to previous work in intrinsic motivation (including VIME and other work in intrinsic motivation). \nAs a positive, the methods are simple to implement, and provide benefits on a number of tasks.\nHowever, they are almost always outmatched by VIME, and not one of their proposed method is consistently the best of those proposed (perhaps the most consistent is the surprisal, which is unfortunately not asymptotically equal to the true reward). The authors claim massive speed up, but the numerical measurements show that VIME is slower to initialize but not significantly slower per iteration otherwise (perhaps a big O analysis would clarify the claims).\nOverall it's a decent, simple technique, perhaps slightly incremental on previous state of the art.", "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "17 Dec 2016", "REVIEWER_CONFIDENCE": 3}, {"TITLE": "No Title", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "This paper explores the topic of intrinsic motivation in the context of deep RL. It proposes a couple of variants derived from an auxiliary model-learning process (prediction error, surprise and learning progress), and shows that those can help exploration on a number of continuous control tasks (and the Atari game \u201cventure\u201d, maybe).\n\nNovelty: none of the proposed types of intrinsic motivation are novel, and it\u2019s arguable whether the application to deep RL is novel (see e.g. Kompella et al 2012).\n\nPotential: the idea of seeking out states where a transition model is uncertain is sensible, but also limited -- I would encourage the authors to also discuss the limitations. For example in a game like Go the transition model is trivially learned, so this approach would revert to random exploration. So other forms of learning progress or surprise derived from the agent\u2019s competence instead might be more promising in the long run? See also Srivastava et al 2012 for further thoughts.\n\nComputation time: I find the paper\u2019s claimed superiority over VIME to be overblown: the gain seems to stem almost exclusively from a faster initialization, but have very similar per-step cost? So given that VIME is also performing very competitively, what arguments can you advance for your own method(s)?", "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"DATE": "02 Dec 2016", "TITLE": "more random consistent baselines", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2"}, {"IS_META_REVIEW": true, "comments": "This paper explores the topic of intrinsic motivation in the context of deep RL. It proposes a couple of variants derived from an auxiliary model-learning process (prediction error, surprise and learning progress), and shows that those can help exploration on a number of continuous control tasks (and the Atari game \u201cventure\u201d, maybe).\n\nNovelty: none of the proposed types of intrinsic motivation are novel, and it\u2019s arguable whether the application to deep RL is novel (see e.g. Kompella et al 2012).\n\nPotential: the idea of seeking out states where a transition model is uncertain is sensible, but also limited -- I would encourage the authors to also discuss the limitations. For example in a game like Go the transition model is trivially learned, so this approach would revert to random exploration. So other forms of learning progress or surprise derived from the agent\u2019s competence instead might be more promising in the long run? See also Srivastava et al 2012 for further thoughts.\n\nComputation time: I find the paper\u2019s claimed superiority over VIME to be overblown: the gain seems to stem almost exclusively from a faster initialization, but have very similar per-step cost? So given that VIME is also performing very competitively, what arguments can you advance for your own method(s)?"}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "The paper proposes an intuitive method for exploration, namely to build a model of the system dynamics and explore regions where this approximation differs from the observed data (i.e., how \"surprised\" the agent was by an observation). The idea is a nice one, and part of the benefit comes from the simplicity and wide applicability of the approach.\n \n The main drawback of this paper is simply that the resulting approach doesn't substantially outperform existing approaches, at least not to a degree where it seem like the paper should should be clearly accepted to ICLR. On the continuous control tasks, the advantage over VIME seems very unclear (at best the results are mixed, showing sometimes surprisal and sometime VIME do better), and on the Atari games no comparison is made against many of the methods tuned for this setting, such as Gorila (Nair, 2015) which achieves some of the best results we are aware of on the Venture game, which is definitely the strongest result in this current paper, but not as good as this previous work. We know the settings are different, but overall it seems like the approach is largely outperformed by existing approaches, and thus the advantage mainly comes from runtime. This is certainly an interesting take, but needs to be studied a lot more thoroughly before it would make a really compelling case. We would like to recomend this paper to the workshop track. The pros/cons are as follows:\n \n Pros:\n + Simple and intuitive method for exploration\n \n Cons:\n - Doesn't seem to substantially outperform existing methods\n - No comparison to many alternative approaches for some of the \"better\" results in the paper.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"TITLE": "Important extension of existing work on intrinsic motivation. Experimental results are less convincing.", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "The authors present a novel approach to surprise-based intrinsic motivation in deep reinforcement learning. The authors clearly explain the difference from other recent approaches to intrinsic motivation and back up their method with results from a broad class of discrete and continuous action domains. They present two tractable approximations to their framework - one which ignores the stochasticity of the true environmental dynamics, and one which approximates the rate of information gain (somewhat similar to Schmidhuber's formal theory of creativity, fun and intrinsic motivation). The results of this exploration bonus when added to TRPO are generally better than standard TRPO. However, I would have appreciated a more thorough comparison against other recent work on intrinsic motivation. For instance, Bellemare et al 2016 recently achieved significant performance gains on challenging Atari games like Montezuma's Revenge by combining DQN with an exploration bonus, however Montezuma's Revenge is not presented as an experiment here. Such comparisons would significantly improve the strength of the paper.", "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "18 Dec 2016", "REVIEWER_CONFIDENCE": 3}, {"TITLE": "Review", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "This paper provides a surprise-based intrinsic reward method for reinforcement learning, along with two practical algorithms for estimating those rewards. The ideas are similar to previous work in intrinsic motivation (including VIME and other work in intrinsic motivation). \nAs a positive, the methods are simple to implement, and provide benefits on a number of tasks.\nHowever, they are almost always outmatched by VIME, and not one of their proposed method is consistently the best of those proposed (perhaps the most consistent is the surprisal, which is unfortunately not asymptotically equal to the true reward). The authors claim massive speed up, but the numerical measurements show that VIME is slower to initialize but not significantly slower per iteration otherwise (perhaps a big O analysis would clarify the claims).\nOverall it's a decent, simple technique, perhaps slightly incremental on previous state of the art.", "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "17 Dec 2016", "REVIEWER_CONFIDENCE": 3}, {"TITLE": "No Title", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "This paper explores the topic of intrinsic motivation in the context of deep RL. It proposes a couple of variants derived from an auxiliary model-learning process (prediction error, surprise and learning progress), and shows that those can help exploration on a number of continuous control tasks (and the Atari game \u201cventure\u201d, maybe).\n\nNovelty: none of the proposed types of intrinsic motivation are novel, and it\u2019s arguable whether the application to deep RL is novel (see e.g. Kompella et al 2012).\n\nPotential: the idea of seeking out states where a transition model is uncertain is sensible, but also limited -- I would encourage the authors to also discuss the limitations. For example in a game like Go the transition model is trivially learned, so this approach would revert to random exploration. So other forms of learning progress or surprise derived from the agent\u2019s competence instead might be more promising in the long run? See also Srivastava et al 2012 for further thoughts.\n\nComputation time: I find the paper\u2019s claimed superiority over VIME to be overblown: the gain seems to stem almost exclusively from a faster initialization, but have very similar per-step cost? So given that VIME is also performing very competitively, what arguments can you advance for your own method(s)?", "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"DATE": "02 Dec 2016", "TITLE": "more random consistent baselines", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2"}]}
{"text": "HIGHER ORDER RECURRENT NEURAL NETWORKS\n1 INTRODUCTION\nIn the recent resurgence of neural networks in deep learning, deep neural networks have achieved successes in various real-world applications, such as speech recognition, computer vision and natural language processing. Deep neural networks (DNNs) with a deep architecture of multiple nonlinear layers are an expressive model that can learn complex features and patterns in data. Each layer of DNNs learns a representation and transfers them to the next layer and the next layer may continue to extract more complicated features, and finally the last layer generates the desirable output. From early theoretical work, it is well known that neural networks may be used as the universal approximators to map from any fixed-size input to another fixed-size output. Recently, more and more empirical results have demonstrated that the deep structure in DNNs is not just powerful in theory but also can be reliably learned in practice from a large amount of training data.\nSequential modeling is a challenging problem in machine learning, which has been extensively studied in the past. Recently, many deep neural network based models have been successful in this area, as shown in various tasks such as language modeling Mikolov (2012), sequence generation Graves (2013); Sutskever et al. (2011), machine translation Sutskever et al. (2014) and speech recognition Graves et al. (2013). Among various neural network models, recurrent neural networks (RNNs) are appealing for modeling sequential data because they can capture long term dependency in sequential data using a simple mechanism of recurrent feedback. RNNs can learn to model sequential data over an extended period of time, then carry out rather complicated transformations on the sequential data. RNNs have been theoretically proved to be a turing complete machine Siegelmann & Sontag (1995). RNNs in principle can learn to map from one variable-length sequence to another. When unfolded in time, RNNs are equivalent to very deep neural networks that share model parameters and receive the input at each time step. The recursion in the hidden layer of RNNs can act as an excellent memory mechanism for the networks. In each time step, the learned recursion weights may decide what information to discard and what information to keep in order to relay onwards along time. While RNNs are theoretically powerful, the learning of RNNs needs to use the back-propagation through time (BPTT) method Werbos (1990) due to the internal recurrent cycles. Unfortunately, in practice, it turns out to be rather difficult to train RNNs to capture long-term dependency due to the fact that\nthe gradients in BPTT tend to either vanish or explode Bengio et al. (1994). Many heuristic methods have been proposed to solve these problems. For example, a simple method, called gradient clipping, is used to avoid gradient explosion Mikolov (2012). However, RNNs still suffer from the vanishing gradient problem since the gradients decay gradually as they are back-propagated through time. As a result, some new recurrent structures are proposed, such as long short-term memory (LSTM) Hochreiter & Schmidhuber (1997) and gated recurrent unit (GRU) Cho et al. (2014). These models use some learnable gates to implement rather complicated feedback structures, which ensure that some feedback paths can allow the gradients to flow back in time effectively. These models have given promising results in many practical applications, such as sequence modeling Graves (2013), language modeling Sundermeyer et al. (2012), hand-written character recognition Liwicki et al. (2012), machine translation Cho et al. (2014), speech recognition Graves et al. (2013).\nIn this paper, we explore an alternative method to learn recurrent neural networks (RNNs) to model long term dependency in sequential data. We propose to use more memory units to keep track of more preceding RNN states, which are all recurrently fed to the hidden layers as feedback through different weighted paths. Analogous to digital filters in signal processing, we call these new recurrent structures as higher order recurrent neural networks (HORNNs). At each time step, the proposed HORNNs directly combine multiple preceding hidden states from various history time steps, weighted by different matrices, to generate the feedback signal to each hidden layer. By aggregating more history information of the RNN states, HORNNs are provided with better short-term memory mechanism than the regular RNNs. Moreover, those direct connections to more previous RNN states allow the gradients to flow back smoothly in the BPTT learning stage. All of these ensure that HORNNs can be more effectively learned to capture long term dependency. Similar to RNNs and LSTMs, the proposed HORNNs are general enough for variety of sequential modeling tasks. In this work, we have evaluated HORNNs for the language modeling task on two popular data sets, namely the Penn Treebank (PTB) and English text8 sets. Experimental results have shown that HORNNs yield the state-of-the-art performance on both data sets, significantly outperforming the regular RNNs as well as the popular LSTMs.\n2 RELATED WORK\nHierarchical recurrent neural network proposed in Hihi & Bengio (1996) is one of the earliest papers that attempt to improve RNNs to capture long term dependency in a better way. It proposes to add linear time delayed connections to RNNs to improve the gradient descent learning algorithm to find a better solution, eventually solving the gradient vanishing problem. However, in this early work, the idea of multi-resolution recurrent architectures has only been preliminarily examined for some simple small-scale tasks. This work is somehow relevant to our work in this paper but the higher order RNNs proposed here differs in several aspects. Firstly, we propose to use weighted connections in the structure, instead of simple multi-resolution short-cut paths. This makes our models fall into the category of higher order models. Secondly, we have proposed to use various pooling functions in generating the feedback signals, which is critical in normalizing the dynamic ranges of gradients flowing from various paths. Our experiments have shown that the success of our models is largely attributed to this technique.\nThe most successful approach to deal with vanishing gradients so far is to use long short term memory (LSTM) model Hochreiter & Schmidhuber (1997). LSTM relies on a fairly sophisticated structure made of gates to control flow of information to the hidden neurons. The drawback of the LSTM is that it is complicated and slow to learn. The complexity of this model makes the learning very time consuming, and hard to scale for larger tasks. Another approach to address this issue is to add a hidden layer to RNNs Mikolov et al. (2014). This layer is responsible for capturing longer term dependencies in input data by making its weight matrix close to identity. Recently, clockwork RNNs Koutnik et al. (2014) are proposed to address this problem as well, which splits each hidden layer into several modules running at different clocks. Each module receives signals from input and computes its output at a predefined clock rate. Gated feedback recurrent neural networks Chung et al. (2015) attempt to implement a generalized version using the gated feedback connection between layers of stacked RNNs, allowing the model to adaptively adjust the connection between consecutive hidden layers.\nBesides, short-cut skipping connections were considered earlier in Wermter (1992), and more recently have been found useful in learning very deep feed-forward neural networks as well, such as Lee et al. (2014); He et al. (2015). These skipping connections between various layers of neural networks can improve the flow of information in both forward and backward passes. Among them, highway networks Srivastava et al. (2015) introduce rather sophisticated skipping connections between layers, controlled by some gated functions.\n3 HIGHER ORDER RECURRENT NEURAL NETWORKS\nA recurrent neural network (RNN) is a type of neural network suitable for modeling a sequence of arbitrary length. At each time step t, an RNN receives an input xt, the state of the RNN is updated recursively as follows (as shown in the left part of Figure 1):\nht = f(Winxt +Whht\u22121) (1)\nwhere f(\u00b7) is an nonlinear activation function, such as sigmoid or rectified linear (ReLU), and Win is the weight matrix in the input layer and Wh is the state to state recurrent weight matrix. Due to the recursion, this hidden layer may act as a short-term memory of all previous input data.\nGiven the state of the RNN, i.e., the current activation signals in the hidden layer ht, the RNN generates the output according to the following equation:\nyt = g(Woutht) (2)\nwhere g(\u00b7) denotes the softmax function and Wout is the weight matrix in the output layer. In principle, this model can be trained using the back-propagation through time (BPTT) algorithm Werbos (1990). This model has been used widely in sequence modeling tasks like language modeling Mikolov (2012).\n3.1 HIGHER ORDER RNNS (HORNNS)\nRNNs are very deep in time and the hidden layer at each time step represents the entire input history, which acts as a short-term memory mechanism. However, due to the gradient vanishing problem in back-propagation, it turns out to be very difficult to learn RNNs to model long-term dependency in sequential data.\nIn this paper, we extend the standard RNN structure to better model long-term dependency in sequential data. As shown in the right part of Figure 1, instead of using only the previous RNN state as the feedback signal, we propose to employ multiple memory units to generate the feedback signal at each time step by directly combining multiple preceding RNN states in the past, where these timedelayed RNN states go through separate feedback paths with different weight matrices. Analogous to the filter structures used in signal processing, we call this new recurrent structure as higher order RNNs, HORNNs in short. The order of HORNNs depends on the number of memory units used for feedback. For example, the model used in the right of Figure 1 is a 3rd-order HORNN. On the other hand, regular RNNs may be viewed as 1st-order HORNNs.\nIn HORNNs, the feedback signal is generated by combining multiple preceding RNN states. Therefore, the state of an N -th order HORNN is recursively updated as follows:\nht = f ( Winxt +\nN\u2211 n=1 Whnht\u2212n\n) (3)\nwhere {Whn | n = 1, \u00b7 \u00b7 \u00b7N} denotes the weight matrices used for various feedback paths. Similar to\nRNNs, HORNNs can also be unfolded in time to get rid of the recurrent cycles. As shown in Figure 2, we unfold a 3rd-order HORNN in time, which clearly shows that each HORNN state is explicitly decided by the current input xt and all previous 3 states in the past. This structure looks similar to the skipping short-cut paths in deep neural networks but each path in HORNNs maintains a learnable weight matrix. The new structure in HORNNs can significantly improve the model capacity to capture long-term dependency in sequential data. At each time step, by explicitly aggregating multiple preceding hidden activities, HORNNs may derive a good representation of the history information in sequences, leading to a significantly enhanced short-term memory mechanism.\nDuring the backprop learning procedure, these skipping paths directly connected to more previous hidden states of HORNNs may allow the gradients to flow more easily back in time, which eventually leads to a more effective learning of models to capture long term dependency in sequences. Therefore, this structure may help to largely alleviate the notorious problem of vanishing gradients in the RNN learning.\nObviously, HORNNs can be learned using the same BPTT algorithm as regular RNNs, except that the error signals at each time step need to be back-propagated to multiple feedback paths in the network. As shown in Figure 3, for a 3rd-order HORNN, at each time step t, the error signal from the hidden layer ht will have to be back-propagated into four different paths: i) the first one back to the input layer, xt; ii) three more feedback paths leading to three different histories in time scales, namely ht\u22121, ht\u22122 and ht\u22123.\nInterestingly enough, if we use a fully-unfolded implementation for HORNNs as in Figure 2, the overall computation complexity is comparable with regular RNNs. Given a whole sequence, we may first simultaneously compute all hidden activities (from xt to ht for all t). Secondly, we recursively update ht for all t using eq.(3). Finally, we use GPUs to compute all outputs together from the updated hidden states (from ht to yt for all t) based on eq.(2). The backward pass in learning can also be implemented in the same three-step procedure. Except the recursive updates in the second step (this issue remains the same in regular RNNs), all remaining computation steps can be formulated as large matrix multiplications. As a result, the computation of HORNNs can be implemented fairly efficiently using GPUs.\n3.2 POOLING FUNCTIONS FOR HORNNS\nAs discussed above, the shortcut paths in HORNNs may help the models to capture long-term dependency in sequential data. On the other hand, they may also complicate the learning in a different way. Due to different numbers of hidden layers along various paths, the signals flowing from different paths may vary dramatically in the dynamic range. For example, in the forward pass in Figure 2, three different feedback signals from different time scales, e.g. ht\u22121, ht\u22122 and ht\u22123, flow into\nthe hidden layer to compute the new hidden state ht. The dynamic range of these signals may vary dramatically from case to case. The situation may get even worse in the backward pass during the BPTT learning. For example, in a 3rd-order HORNN in Figure 2, the node ht\u22123 may directly receive an error signal from the node ht. In some cases, it may get so strong as to overshadow other error signals coming from closer neighbours of ht\u22121 and ht\u22122. This may impede the learning of HORNNs, yielding slow convergence or even poor performance.\nHere, we have proposed to use some pooling functions to calibrate the signals from different feedback paths before they are used to recursively generate a new hidden state, as shown in Figure 4. In the following, we will investigate three different choices for the pooling function in Figure 4, including max-based pooling, FOFE-based pooling and gated pooling.\n3.2.1 MAX-BASED POOLING\nMax-based pooling is a simple strategy that chooses the most responsive unit (exhibiting the largest activation value) among various paths to transfer to the hidden layer to generate the new hidden state. Many biological experiments have shown that biological neuron networks tend to use a similar strategy in learning and firing.\nIn this case, instead of using eq.(3), we use the following formula to update the hidden state of HORNNs: ht = f ( Winxt +max N n=1 (Whnht\u2212n) ) (4)\nwhere maximization is performed element-wisely to choose the maximum value in each dimension to feed to the hidden layer to generate the new hidden state. The aim here is to capture the most relevant feature and map it to a fixed predefined size.\nThe max pooling function is simple and biologically inspired. However, the max pooling strategy also has some serious disadvantages. For example, it has no forgetting mechanism and the signals may get stronger and stronger. Furthermore, it loses the order information of the preceding histories since it only choose the maximum values but it does not know where the maximum comes from.\n3.2.2 FOFE-BASED POOLING\nThe fixed-size ordinally-forgetting encoding (FOFE) method was proposed in Zhang et al. (2015) to encode any variable-length sequence of data into a fixed-size representation. In FOFE, a single forgetting factor \u03b1 (0 < \u03b1 < 1) is used to encode the position information in sequences based on the idea of exponential forgetting to derive invertible fixed-size representations. In this work, we borrow this simple idea of exponential forgetting to calibrate all preceding histories using a pre-selected forgetting factor as follows:\nht = f ( Winxt +\nN\u2211 n=1 \u03b1n \u00b7Whnht\u2212n\n) (5)\nwhere the forgetting factor \u03b1 is manually pre-selected between 0 < \u03b1 < 1. The above constant coefficients related to \u03b1 play an important role in calibrating signals from different paths in both\nforward and backward passes of HORNNs since they slightly underweight the older history over the recent one in an explicit way.\n3.2.3 GATED HORNNS\nIn this section, we follow the ideas of the learnable gates in LSTMs Hochreiter & Schmidhuber (1997) and GRUs Cho et al. (2014) as well as the recent soft-attention in Bahdanau et al. (2014). Instead of using constant coefficients derived from a forgetting factor, we may let the network automatically determine the combination weights based on the current state and input. In this case, we may use sigmoid gates to compute combination weights to regulate the information flowing from various feedback paths. The sigmoid gates take the current data and previous hidden state as input to decide how to weight all of the precede hidden states. The gate function weights how the current hidden state is generated based on all the previous time-steps of the hidden layer. This allows the network to potentially remember information for a longer period of time. In a gated HORNN, the hidden state is recursively computed as follows:\nht = f ( Winxt +\nN\u2211 n=1 rn ( Whnht\u2212n )) (6)\nwhere denotes element-wise multiplication of two equally-sized vectors, and the gate signal rn is calculated as\nrn = \u03c3 (W g 1nxt +W g 2nht\u2212n) (7)\nwhere \u03c3(\u00b7) is the sigmoid function, and W g1n and W g 2n denote two weight matrices introduced for each gate.\nNote that the computation complexity of gated HORNNs is comparable with LSTMs and GRUs, significantly exceeding the other HORNN structures because of the overhead from the gate functions in eq. (7).\n4 EXPERIMENTS\nIn this section, we evaluate the proposed higher order RNNs (HORNNs) on several language modeling tasks. A statistical language model (LM) is a probability distribution over sequences of words in natural languages. Recently, neural networks have been successfully applied to language modeling Bengio et al. (2003); Mikolov et al. (2011), yielding the state-of-the-art performance. In language modeling tasks, it is quite important to take advantage of the long-term dependency of natural languages. Therefore, it is widely reported that RNN based LMs can outperform feedforward neural networks in language modeling tasks. We have chosen two popular LM data sets, namely the Penn Treebank (PTB) and English text8 sets, to compare our proposed HORNNs with traditional n-gram LMs, RNN-based LMs and the state-of-the-art performance obtained by LSTMs Graves (2013); Mikolov et al. (2014), FOFE based feedforward NNs Zhang et al. (2015) and memory networks Sukhbaatar et al. (2015).\nIn our experiments, we use the mini-batch stochastic gradient decent (SGD) algorithm to train all neural networks. The number of back-propagation through time (BPTT) steps is set to 30 for all recurrent models. Each model update is conducted using a mini-batch of 20 subsequences, each of which is of 30 in length. All model parameters (weight matrices in all layers) are randomly initialized based on a Gaussian distribution with zero mean and standard deviation of 0.1. A hard clipping is set to 5.0 to avoid gradient explosion during the BPTT learning. The initial learning rate is set to 0.5 and we halve the learning rate at the end of each epoch if the cross entropy function on the validation set does not decrease. We have used the weight decay, momentum and column normalization Pachitariu & Sahani (2013) in our experiments to improve model generalization. In the FOFE-based pooling function for HORNNs, we set the forgetting factor, \u03b1, to 0.6. We have used 400 nodes in each hidden layer for the PTB data set and 500 nodes per hidden layer for the English text8 set. In our experiments, we do not use the dropout regularization Zaremba et al. (2014) in all experiments since it significantly slows down the training speed, not applicable to any larger corpora. 1\n1We will soon release the code for readers to reproduce all results reported in this paper.\n4.1 LANGUAGE MODELING ON PTB\nThe standard Penn Treebank (PTB) corpus consists of about 1M words. The vocabulary size is limited to 10k. The preprocessing method and the way to split data into training/validation/test sets are the same as Mikolov et al. (2011). PTB is a relatively small text corpus. We first investigate various model configurations for the HORNNs based on PTB and then compare the best performance with other results reported on this task.\n4.1.1 EFFECT OF ORDERS IN HORNNS\nIn the first experiment, we first investigate how the used orders in HORNNs may affect the performance of language models (as measured by perplexity). We have examined all different higher order model structures proposed in this paper, including HORNNs and various pooling functions in HORNNs. The orders of these examined models varies among 2, 3 and 4. We have listed the performance of different models on PTB in Table 1. As we may see, we are able to achieve a significant improvement in perplexity when using higher order RNNs for language models on PTB, roughly 10-20 reduction in PPL over regular RNNs. We can see that performance may improve slightly when the order is increased from 2 to 3 but no significant gain is observed when the order is further increased to 4. As a result, we choose the 3rd-order HORNN structure for the following experiments. Among all different HORNN structures, we can see that FOFE-based pooling and gated structures yield the best performance on PTB.\nIn language modeling, both input and output layers account for the major portion of model parameters. Therefore, we do not significantly increase model size when we go to higher order structures. For example, in Table 1, a regular RNN contains about 8.3 millions of weights while a 3rd-order HORNN (the same for max or FOFE pooling structures) has about 8.6 millions of weights. In comparison, an LSTM model has about 9.3 millions of weights and a 3rd-order gated HORNN has about 9.6 millions of weights.\nAs for the training speed, most HORNN models are only slightly slower than regular RNNs. For example, one epoch of training on PTB running in one NVIDIA\u2019s TITAN X GPU takes about 80 seconds for an RNN, about 120 seconds for a 3rd-order HORNN (the same for max or FOFE pooling structures). Similarly, training of gated HORNNs is also slightly slower than LSTMs. For example, one epoch on PTB takes about 200 seconds for an LSTM, and about 225 seconds for a 3rd-order gates HORNN.\n4.1.2 MODEL COMPARISON ON PENN TREEBANK\nAt last, we report the best performance of various HORNNs on the PTB test set in Table 2. We compare our 3rd-order HORNNs with all other models reported on this task, including RNN Mikolov et al. (2011), stack RNN Pascanu et al. (2014), deep RNN Pascanu et al. (2014), FOFE-FNN Zhang et al. (2015) and LSTM Graves (2013). 2 From the results in Table 2, we can see that our proposed higher order RNN architectures significantly outperform all other baseline models reported on this task. Both FOFE-based pooling and gated HORNNs have achieved the state-of-the-art performance,\n2All models in Table 2 do not use the dropout regularization, which is somehow equivalent to data augmentation. In Zaremba et al. (2014); Kim et al. (2015), the proposed LSTM-LMs (word level or character level) achieve lower perplexity but they both use the dropout regularization and much bigger models and it takes days to train the models, which is not applicable to other larger tasks.\nTable 2: Perplexities on the PTB test set for various examined models.\nModels Test KN 5-gram Mikolov et al. (2011) 141 RNN Mikolov et al. (2011) 123 CSLM5Aransa et al. (2015) 118.08 LSTM Graves (2013) 117 genCNN Wang et al. (2015) 116.4 Gated word&charMiyamoto & Cho (2016) 113.52 E2E Mem Net Sukhbaatar et al. (2015) 111 Stack RNN Pascanu et al. (2014) 110 Deep RNN Pascanu et al. (2014) 107 FOFE-FNN Zhang et al. (2015) 108 HORNN (3rd order) 108 Max HORNN (3rd order) 109 FOFE HORNN (3rd order) 101 Gated HORNN (3rd order) 100\nTable 3: Perplexities on the text8 test set for various models.\nModels Test RNN Mikolov et al. (2014) 184 LSTM Mikolov et al. (2014) 156 SCRNN Mikolov et al. (2014) 161 E2E Mem Net Sukhbaatar et al. (2015) 147 HORNN (3rd order) 172 Max HORNN (3rd order) 163 FOFE HORNN (3rd order) 154 Gated HORNN (3rd order) 144\ni.e., 100 in perplexity on this task. To the best of our knowledge, this is the best reported performance on PTB under the same training condition.\n4.2 LANGUAGE MODELING ON ENGLISH TEXT8\nIn this experiment, we will evaluate our proposed HORNNs on a much larger text corpus, namely the English text8 data set. The text8 data set contains a preprocessed version of the first 100 million characters downloaded from the Wikipedia website. We have used the same preprocessing method as Mikolov et al. (2014) to process the data set to generate the training and test sets. We have limited the vocabulary size to about 44k by replacing all words occurring less than 10 times in the training set with an <UNK> token. The text8 set is about 20 times larger than PTB in corpus size. The model training on text8 takes longer to finish. We have not tuned hyperparameters in this data set. We simply follow the best setting used in PTB to train all HORNNs for the text8 data set. Meanwhile, we also follow the same learning schedule used in Mikolov et al. (2014): We first initialize the learning rate to 0.5 and run 5 epochs using this learning rate; After that, the learning rate is halved at the end of every epoch.\nBecause the training is time-consuming, we have only evaluated 3rd-order HORNNs on the text8 data set. The perplexities of various HORNNs are summarized in Table 3. We have compared our HORNNs with all other baseline models reported on this task, including RNN Mikolov et al. (2014), LSTM Mikolov et al. (2014), SCRNN Mikolov et al. (2014) and end-to-end memory networks Sukhbaatar et al. (2015). Results have shown that all HORNN models work pretty well in this data set except the normal HORNN significantly underperforms the other three models. Among them, the gated HORNN model has achieved the best performance, i.e., 144 in perplexity on this task, which is slightly better than the recent result obtained by end-to-end memory networks (using a rather complicated structure). To the best of our knowledge, this is the best performance reported on this task.\n5 CONCLUSIONS\nIn this paper, we have proposed some new structures for recurrent neural networks, called as higher order RNNs (HORNNs). In these structures, we use more memory units to keep track of more preceding RNN states, which are all fed along various feedback paths to the hidden layer to generate the feedback signals. In this way, we may enhance the model to capture long term dependency in sequential data. Moreover, we have proposed to use several types of pooling functions to calibrate multiple feedback paths. Experiments have shown that the pooling technique plays a critical role in learning higher order RNNs effectively. In this work, we have examined HORNNs for the language modeling task using two popular data sets, namely the Penn Treebank (PTB) and text8 sets. Experimental results have shown that the proposed higher order RNNs yield the state-of-the-art per-\nformance on both data sets, significantly outperforming the regular RNNs as well as the popular LSTMs. As the future work, we are going to continue to explore HORNNs for other sequential modeling tasks, such as speech recognition, sequence-to-sequence modelling and so on.\n", "cats": {"READ": 0.0, "DONTREAD": 1.0}, "reviews": [{"IS_META_REVIEW": true, "comments": "This paper proposes an idea of looking n-steps backward when modelling sequences with RNNs. The proposed RNN does not only use the previous hidden state (t-1) but also looks further back ( (t - k) steps, where k=1,2,3,4 ). The paper also proposes a few different ways to aggregate multiple hidden states from the past.\n\n\nThe reviewer can see few issues with this paper.\n\nFirstly, the writing of this paper requires improvement. The introduction and abstract are wasting too much space just to explain unrelated facts or to describe already well-known things in the literature. Some of the statements written in the paper are misleading. For instance, it explains, \u201cAmong various neural network models, recurrent neural networks (RNNs) are appealing for modeling sequential data because they can capture long term dependency in sequential data using a simple mechanism of recurrent feedback\u201d and then it says RNNs cannot actually capture long-term dependencies that well. RNNs are appealing in the first place because they can handle variable length sequences and can model temporal relationships between each symbol in a sequence. The criticism against LSTMs is hard to accept when it says: LSTMs are slow and because of the slowness, they are hard to scale at larger tasks. But we all know that some companies are already using gigantic seq2seq models for their production (LSTMs are used as building blocks in their systems). This indicates that the LSTMs can be practically used in a very large-scale setting.\n\n\nSecondly, the idea proposed in the paper is incremental and not new to the field. There are other previous works that propose to use direct connections to the previous hidden states [1]. However, the previous works do not use aggregation of multiple number of previous hidden states. Most importantly, the paper fails to deliver a proper analysis on whether its main contribution is actually helpful to improve the problem posed in the paper. The new architecture is said that it handles the long-term dependencies better, however, there is no rigorous proof or intuitive design in the architecture that help us to understand why it should work better. By the design of the architecture, and speaking in very high-level, it seems like the model maybe helpful to mitigate the vanishing gradients issue by a linear factor. It is always a good practice to have at least one page to analyze the empirical findings in the paper.\n\n\nThirdly, the baseline models used in this paper are very weak. Their are plenty of other models that are trained and tested on word-level language modelling task using Penn Treebank corpus, but the paper only contains a few of outdated models. I cannot fully agree on the statement \u201cTo the best of our knowledge, this is the best performance on PTB under the same training condition\u201d, these days, RNN-based methods usually score below 80 in terms of the test perplexity, which are far lower than 100 achieved in this paper.\n\n\n[1] Zhang et al., \u201cArchitectural Complexity Measures of Recurrent Neural Networks\u201d, NIPS\u201916"}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "Paper presents the idea of using higher order recurrence in LSTMs. The ideas are well presented and easy to follow.\n However, the results are far from convincing, easily being below well established numbers in the domain. Since the mode is but a very simple extension of the baseline recurrent models using LSTMs that are state of the art on language modelling, it should have been easy to make a fair comparison by replacing the LSTMs of the state of the art models with higher order versions, but the authors did not do that. Their claimed numbers for SOTA are far from previously reported numbers in that setup, as pointed out by reviewers.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"TITLE": "Interesting idea, but not ready yet", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "The authors of the paper explore the idea of incorporating skip connections *over time* for RNNs. Even though the basic idea is not particularly innovative, a few proposals on how to merge that information into the current hidden state with different pooling functions are evaluated. The different models are compared on two popular text benchmarks.\n\nSome points.\n\n1) The experiments feature only NLP and only prediction tasks. It would have been nice to see the models in other domains, i.e. modelling a conditional distribution p(y|x), not only p(x). Further, sensory input data such as audio or video would have given further insight.\n\n2) As pointed out by other reviewers, it does not feel as if the comparisons to other models are fair. SOTA on NLP changes quickly and it is hard to place the experiments in the complete picture.\n\n3) It is claimed that this helps long-term prediction. I think the paper lacks a corresponding analysis, as pointed out in an earlier question of mine.\n\n4)  It is claimed that LSTM trains slow and is hard to scale. For one does this not match my personal experience. Then, the prevalence of LSTM systems in production systems (e.g. Google, Baidu, Microsoft, \u2026) clearly speaks against this.\n\n\nI like the basic idea of the paper, but the points above make me think it is not ready for publication.\n", "IS_META_REVIEW": false, "RECOMMENDATION": 4, "DATE": "20 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "can be improved", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "I think the backbone of the paper is interesting and could lead to something potentially quite useful. I like the idea of connecting signal processing with recurrent network and then using tools from one setting in the other. However, while the work has nuggets of very interesting observations, I feel they can be put together in a better way. \nI think the writeup and everything can be improved and I urge the authors to strive for this if the paper doesn't go through. I think some of the ideas of how to connect to the past are interesting, it would be nice to have more experiments or to try to understand better why this connections help and how.", "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "18 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "Incremental work", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "This paper proposes an idea of looking n-steps backward when modelling sequences with RNNs. The proposed RNN does not only use the previous hidden state (t-1) but also looks further back ( (t - k) steps, where k=1,2,3,4 ). The paper also proposes a few different ways to aggregate multiple hidden states from the past.\n\n\nThe reviewer can see few issues with this paper.\n\nFirstly, the writing of this paper requires improvement. The introduction and abstract are wasting too much space just to explain unrelated facts or to describe already well-known things in the literature. Some of the statements written in the paper are misleading. For instance, it explains, \u201cAmong various neural network models, recurrent neural networks (RNNs) are appealing for modeling sequential data because they can capture long term dependency in sequential data using a simple mechanism of recurrent feedback\u201d and then it says RNNs cannot actually capture long-term dependencies that well. RNNs are appealing in the first place because they can handle variable length sequences and can model temporal relationships between each symbol in a sequence. The criticism against LSTMs is hard to accept when it says: LSTMs are slow and because of the slowness, they are hard to scale at larger tasks. But we all know that some companies are already using gigantic seq2seq models for their production (LSTMs are used as building blocks in their systems). This indicates that the LSTMs can be practically used in a very large-scale setting.\n\n\nSecondly, the idea proposed in the paper is incremental and not new to the field. There are other previous works that propose to use direct connections to the previous hidden states [1]. However, the previous works do not use aggregation of multiple number of previous hidden states. Most importantly, the paper fails to deliver a proper analysis on whether its main contribution is actually helpful to improve the problem posed in the paper. The new architecture is said that it handles the long-term dependencies better, however, there is no rigorous proof or intuitive design in the architecture that help us to understand why it should work better. By the design of the architecture, and speaking in very high-level, it seems like the model maybe helpful to mitigate the vanishing gradients issue by a linear factor. It is always a good practice to have at least one page to analyze the empirical findings in the paper.\n\n\nThirdly, the baseline models used in this paper are very weak. Their are plenty of other models that are trained and tested on word-level language modelling task using Penn Treebank corpus, but the paper only contains a few of outdated models. I cannot fully agree on the statement \u201cTo the best of our knowledge, this is the best performance on PTB under the same training condition\u201d, these days, RNN-based methods usually score below 80 in terms of the test perplexity, which are far lower than 100 achieved in this paper.\n\n\n[1] Zhang et al., \u201cArchitectural Complexity Measures of Recurrent Neural Networks\u201d, NIPS\u201916\n", "IS_META_REVIEW": false, "RECOMMENDATION": 3, "DATE": "17 Dec 2016 (modified: 18 Dec 2016)", "REVIEWER_CONFIDENCE": 4}, {"DATE": "10 Dec 2016", "TITLE": "Long-term claim", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1"}, {"DATE": "08 Dec 2016", "TITLE": "Baseline experiment", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2"}, {"DATE": "06 Dec 2016", "TITLE": "Pre-review questions", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3"}, {"IS_META_REVIEW": true, "comments": "This paper proposes an idea of looking n-steps backward when modelling sequences with RNNs. The proposed RNN does not only use the previous hidden state (t-1) but also looks further back ( (t - k) steps, where k=1,2,3,4 ). The paper also proposes a few different ways to aggregate multiple hidden states from the past.\n\n\nThe reviewer can see few issues with this paper.\n\nFirstly, the writing of this paper requires improvement. The introduction and abstract are wasting too much space just to explain unrelated facts or to describe already well-known things in the literature. Some of the statements written in the paper are misleading. For instance, it explains, \u201cAmong various neural network models, recurrent neural networks (RNNs) are appealing for modeling sequential data because they can capture long term dependency in sequential data using a simple mechanism of recurrent feedback\u201d and then it says RNNs cannot actually capture long-term dependencies that well. RNNs are appealing in the first place because they can handle variable length sequences and can model temporal relationships between each symbol in a sequence. The criticism against LSTMs is hard to accept when it says: LSTMs are slow and because of the slowness, they are hard to scale at larger tasks. But we all know that some companies are already using gigantic seq2seq models for their production (LSTMs are used as building blocks in their systems). This indicates that the LSTMs can be practically used in a very large-scale setting.\n\n\nSecondly, the idea proposed in the paper is incremental and not new to the field. There are other previous works that propose to use direct connections to the previous hidden states [1]. However, the previous works do not use aggregation of multiple number of previous hidden states. Most importantly, the paper fails to deliver a proper analysis on whether its main contribution is actually helpful to improve the problem posed in the paper. The new architecture is said that it handles the long-term dependencies better, however, there is no rigorous proof or intuitive design in the architecture that help us to understand why it should work better. By the design of the architecture, and speaking in very high-level, it seems like the model maybe helpful to mitigate the vanishing gradients issue by a linear factor. It is always a good practice to have at least one page to analyze the empirical findings in the paper.\n\n\nThirdly, the baseline models used in this paper are very weak. Their are plenty of other models that are trained and tested on word-level language modelling task using Penn Treebank corpus, but the paper only contains a few of outdated models. I cannot fully agree on the statement \u201cTo the best of our knowledge, this is the best performance on PTB under the same training condition\u201d, these days, RNN-based methods usually score below 80 in terms of the test perplexity, which are far lower than 100 achieved in this paper.\n\n\n[1] Zhang et al., \u201cArchitectural Complexity Measures of Recurrent Neural Networks\u201d, NIPS\u201916"}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "Paper presents the idea of using higher order recurrence in LSTMs. The ideas are well presented and easy to follow.\n However, the results are far from convincing, easily being below well established numbers in the domain. Since the mode is but a very simple extension of the baseline recurrent models using LSTMs that are state of the art on language modelling, it should have been easy to make a fair comparison by replacing the LSTMs of the state of the art models with higher order versions, but the authors did not do that. Their claimed numbers for SOTA are far from previously reported numbers in that setup, as pointed out by reviewers.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"TITLE": "Interesting idea, but not ready yet", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "The authors of the paper explore the idea of incorporating skip connections *over time* for RNNs. Even though the basic idea is not particularly innovative, a few proposals on how to merge that information into the current hidden state with different pooling functions are evaluated. The different models are compared on two popular text benchmarks.\n\nSome points.\n\n1) The experiments feature only NLP and only prediction tasks. It would have been nice to see the models in other domains, i.e. modelling a conditional distribution p(y|x), not only p(x). Further, sensory input data such as audio or video would have given further insight.\n\n2) As pointed out by other reviewers, it does not feel as if the comparisons to other models are fair. SOTA on NLP changes quickly and it is hard to place the experiments in the complete picture.\n\n3) It is claimed that this helps long-term prediction. I think the paper lacks a corresponding analysis, as pointed out in an earlier question of mine.\n\n4)  It is claimed that LSTM trains slow and is hard to scale. For one does this not match my personal experience. Then, the prevalence of LSTM systems in production systems (e.g. Google, Baidu, Microsoft, \u2026) clearly speaks against this.\n\n\nI like the basic idea of the paper, but the points above make me think it is not ready for publication.\n", "IS_META_REVIEW": false, "RECOMMENDATION": 4, "DATE": "20 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "can be improved", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "I think the backbone of the paper is interesting and could lead to something potentially quite useful. I like the idea of connecting signal processing with recurrent network and then using tools from one setting in the other. However, while the work has nuggets of very interesting observations, I feel they can be put together in a better way. \nI think the writeup and everything can be improved and I urge the authors to strive for this if the paper doesn't go through. I think some of the ideas of how to connect to the past are interesting, it would be nice to have more experiments or to try to understand better why this connections help and how.", "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "18 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "Incremental work", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "This paper proposes an idea of looking n-steps backward when modelling sequences with RNNs. The proposed RNN does not only use the previous hidden state (t-1) but also looks further back ( (t - k) steps, where k=1,2,3,4 ). The paper also proposes a few different ways to aggregate multiple hidden states from the past.\n\n\nThe reviewer can see few issues with this paper.\n\nFirstly, the writing of this paper requires improvement. The introduction and abstract are wasting too much space just to explain unrelated facts or to describe already well-known things in the literature. Some of the statements written in the paper are misleading. For instance, it explains, \u201cAmong various neural network models, recurrent neural networks (RNNs) are appealing for modeling sequential data because they can capture long term dependency in sequential data using a simple mechanism of recurrent feedback\u201d and then it says RNNs cannot actually capture long-term dependencies that well. RNNs are appealing in the first place because they can handle variable length sequences and can model temporal relationships between each symbol in a sequence. The criticism against LSTMs is hard to accept when it says: LSTMs are slow and because of the slowness, they are hard to scale at larger tasks. But we all know that some companies are already using gigantic seq2seq models for their production (LSTMs are used as building blocks in their systems). This indicates that the LSTMs can be practically used in a very large-scale setting.\n\n\nSecondly, the idea proposed in the paper is incremental and not new to the field. There are other previous works that propose to use direct connections to the previous hidden states [1]. However, the previous works do not use aggregation of multiple number of previous hidden states. Most importantly, the paper fails to deliver a proper analysis on whether its main contribution is actually helpful to improve the problem posed in the paper. The new architecture is said that it handles the long-term dependencies better, however, there is no rigorous proof or intuitive design in the architecture that help us to understand why it should work better. By the design of the architecture, and speaking in very high-level, it seems like the model maybe helpful to mitigate the vanishing gradients issue by a linear factor. It is always a good practice to have at least one page to analyze the empirical findings in the paper.\n\n\nThirdly, the baseline models used in this paper are very weak. Their are plenty of other models that are trained and tested on word-level language modelling task using Penn Treebank corpus, but the paper only contains a few of outdated models. I cannot fully agree on the statement \u201cTo the best of our knowledge, this is the best performance on PTB under the same training condition\u201d, these days, RNN-based methods usually score below 80 in terms of the test perplexity, which are far lower than 100 achieved in this paper.\n\n\n[1] Zhang et al., \u201cArchitectural Complexity Measures of Recurrent Neural Networks\u201d, NIPS\u201916\n", "IS_META_REVIEW": false, "RECOMMENDATION": 3, "DATE": "17 Dec 2016 (modified: 18 Dec 2016)", "REVIEWER_CONFIDENCE": 4}, {"DATE": "10 Dec 2016", "TITLE": "Long-term claim", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1"}, {"DATE": "08 Dec 2016", "TITLE": "Baseline experiment", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2"}, {"DATE": "06 Dec 2016", "TITLE": "Pre-review questions", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3"}]}
{"text": "DEEP REINFORCEMENT LEARNING\n1 INTRODUCTION\nOur work is inspired by empirical findings and theories in psychology indicating that infant learning and thinking is similar to that of adult scientists (Gopnik, 2012). One important view in developmental science is that babies are endowed with a small number of separable systems of core knowledge for reasoning about objects, actions, number, space, and possibly social interactions (Spelke & Kinzler, 2007). The object core system covering aspects such as cohesion, continuity, and contact, enables babies and other animals to solve object related tasks such as reasoning about oclusion and predicting how objects behave.\nCore knowledge research has motivated the development of methods that endow agents with physics priors and perception modules so as to infer intrinsic physical properties rapidly from data (Battaglia et al., 2013; Wu et al., 2015; 2016; Stewart & Ermon, 2016). For instance, using physics engines and mental simulation, it becomes possible to infer quantities such as mass from visual input (Hamrick et al., 2016; Wu et al., 2015).\nIn early stages of life, infants spend a lot of time interacting with objects in a seemingly random manner (Smith & Gasser, 2005). They interact with objects in multiple ways, including throwing, pushing, pulling, breaking, and biting. It is quite possible that this process of actively engaging with objects and watching the consequences of their actions helps infants understand different physical properties of the object which cannot be observed directly using their sensory systems. It seems infants run a series of \u201cphysical\u201d experiments to enhance their knowledge about the world (Gopnik, 2012). The act of performing an experiment is useful both for quickly adapting an agent\u2019s policy to a new environment and for understanding object properties in a holistic manner. Despite impressive advances in artificial intelligence that have led to superhuman performance in Go, Atari and natural language processing, it is still unclear if these systems behind these advances can rival the scientific intuition of even a small child.\nWhile we draw inspiration from child development, it must be emphasized that our purpose is not to provide an account of learning and thinking in humans, but rather to explore how similar types of understanding might be learned by artificial agents in a grounded way. To this end we show that we can build agents that can learn to experiment so as to learn representations that are informative about physical properties of objects, using deep reinforcement learning. The act of conducting an experiment involves the agent having a belief about the world, which it then updates by observing the consequences of actions it performs.\nWe investigate the ability of agents to learn to perform experiments to infer object properties through two environments\u2014Which is Heavier and Towers. In the Which is Heavier environment, the agent is able to apply forces to blocks and it must infer which of the blocks is the heaviest. In the Towers environment the agent\u2019s task is to infer how many rigid bodies a tower is composed of by knocking it down. Unlike Wu et al. (2015), we assume that the agent has no prior knowledge about physical properties of objects, or the laws of physics, and hence must interact with the objects in order to learn to answer questions about these properties.\nOur results indicate that in the case Which is Heavier environment our agents learn experimentation strategies that are similar to those we would expect from an algorithm designed with knowledge of the underlying structure of the environment. In the Towers environment we show that our agents learn a closed loop policy that can adapt to a varying time scale. In both environments we show that when using the learned interaction policies agents are more accurate and often take less time to produce correct answers than when following randomized interaction policies.\n2 WHAT IS THIS PAPER ABOUT?\nThis is an unusual paper in that it does not present a new model or propose a new algorithm. There is a reinforcement learning task at the core of each of our experiments, but the algorithm and models we use to solve it are not new, and many other existing approaches should be expected to perform equally well if they were to be substituted in the same setting.\nThis paper is a step towards agents that understand objects and intuitive reasoning in physical worlds. Our best AI agents currently fail on simple control tasks and simple games, such as Montezuma\u2019s Revenge, because when they look at a screen that has a ladder, a key and a skull they don\u2019t immediately know that keys open doors, that skulls are probably hazardous and best avoided, that ladders allow us to defy gravity, etc. The understanding of physics, relations and objects enables children to solve seemingly simple problems that our best existing AI agents do not come close to begin to solve.\nEndowing our agents with knowledge of objects would help enormously with planning, reasoning and exploration, and yet, doing so is far from trivial. What is an object? It turns out this question does not have a straightforward answer, and this paper is based around the idea that staring at a thing is not enough to understand what it is.\nChildren understand their world by engaging with it. Poking something to find that it is soft, tasting it to discover it is delicious, or hitting it to see if it falls down. Much of the knowledge people have of the world is the result of interaction. Vision or open loop perception alone is not enough.\nThis paper introduces tasks where we can evaluate the ability of agents to learn about these \u201chidden\u201d properties of objects. This requires environments where the tasks depend on these properties (otherwise the agents have no incentive to learn about them) and also that we have a way to probe for this understanding in agents that complete the tasks.\nPrevious approaches to this problem have relied on either explicit knowledge of the underlying structure of the environment (e.g. hard-wired physical laws) or on exploiting correlations between material appearance and physical properties (see Section 7 for much more detail). One of the contributions of this paper is to show that our agents can still learn about properties of objects, even when the connection between material appearance and physical properties is broken. This setting allows us to show that our agents are not merely learning that blocks are heavy; they are learning how to check if blocks are heavy.\nNone of the previous approaches give a complete account of how agents could come to understand the physical properties of the world around them. Specifying a model manually is difficult to scale,\ngeneralize and to ground in perception. Making predictions from only visual properties will fail to distinguish between objects that look similar, and it will certainly be unable to distinguish between a sack full of rocks and a sack full of tennis balls.\n3 ANSWERING QUESTIONS THROUGH INTERACTION\nWe pose the problem of experimentation as that of answering questions about non-visual properties of objects present in the environment. We design environments that ask questions about these properties by providing rewards when the agent is able to infer them correctly, and we train agents to answer these questions using reinforcement learning.\nWe design environments that follow a three phase structure:\nInteraction Initially there is an exploration phase, where the agent is free to interact with the environment and gather information.\nLabeling The interaction phase ends when the agent produces a labeling action through which it communicates its answer to the implicit question posed by the environment.\nReward When the agent produces as labeling action, the environment responds with a reward, positive for a correct answer and negative for incorrect, and the episode terminates. The episode terminates automatically with a negative reward if the agent does not produce a labeling action before a maximum time limit is reached.\nCrucially, the transition between interaction and labeling does not happen at a fixed time, but is initiated by the agent. This is achieved by providing the agent with the ability to produce either an interaction action or a labeling action at every time step. This allows the agent to decide when enough information has been gathered, and forces it to balance the trade-off between answering now given its current knowledge, or delaying its answer to gather more information.\nThe optimal trade-off between information gathering and risk of answering incorrectly depends on two factors. The first factor is the difficulty of the question and the second is the cost of information. The difficulty is environment specific and is addressed later when we describe the environments. The cost of information can be generically controlled by varying the discount factor during learning. A small discount factor places less emphasis on future rewards and encourages the agent to answer as quickly as possible. On the other hand, a large discount factor encourages the agent to spend more time gathering information in order to increase the likelihood of choosing the correct answer.\nOur use of \u201cquestions\u201d and \u201canswers\u201d differs from how these terms are used elsewhere in the literature. Sutton et al. (2011) talk about a value function as a question, and the agent provides an answer in the form of an approximation of the value. The answer incorporates the agent\u2019s knowledge, and the match between the actual value and the agent\u2019s approximation grounds what it means for this knowledge to be accurate.\nIn our usage the environment (or episode) itself is the question, and answers come in the form of labeling actions. In each episode there is a correct answer whose semantics is grounded in the sign of the reward function, and the accuracy of an agents knowledge is assessed by the frequency with which it is able to choose the correct answer.\nUsing reward (rather than value) to ground our semantics means that we have a straightforward way to ask questions that do not depend on the agent\u2019s behavior. For example, we can easily ask the question \u201cWhich block is heaviest?\u201d without making the question contingent on a particular information acquisition strategy.\n4 AGENT ARCHITECTURE AND TRAINING\nWe use the same basic agent architecture and training procedure for all of our experiments, making only minimal modifications in order to adapt the agents to different observation spaces and actuators. For all experiments we train recurrent agents using an LSTM with 100 hidden units. When working from features we feed the observations into the LSTM directly. When training from pixels we first scale the observations to 84x84 pixels and feed them through a three convolution layers, each\nfollowed by a ReLU non-linearity. The three layers have 32, 64, 64 square filters with sizes 8, 4, 3, which are applied at strides of 4, 2, 1 respectively. We train the agents using Asynchronous Advantage Actor Critic (Mnih et al., 2016), but ensure that the unroll length is always greater than the timeout length so the agent network is unrolled over the entirety of each episode.\n5 WHICH IS HEAVIER\nThe Which is Heavier environment is designed to ask a question about the relative masses of different objects in a scene. We assign masses to objects in a way that is uncorrelated with their appearance in order to ensure that the task is not solvable without interaction.\n5.1 ENVIRONMENT\nThe environment is diagrammed in the left panel of Figure 1. It consists of four blocks, which are constrained to only move vertically. The blocks are always the same size, but vary in mass between episodes. The agent\u2019s strength (i.e. magnitude of force it can apply) remains constant between episodes.\nThe question to answer in this environment is which of the four blocks is the heaviest. Since the mass of each block is randomly assigned in each episode, the agent must poke the blocks and observe how they respond in order to make this determination. Assigning masses randomly ensures it is not possible to solve this task from vision (or features) alone, since the appearance and identity of each block imparts no information about its mass in the current episode. The only way to obtain information about the masses of the blocks is to interact with them and watch how they respond.\nThe Which is Heavier environment is designed to encode a latent bandit problem through a \u201cphysical\u201d lens. Each block corresponds to an arm of the bandit, and the reward obtained by pulling each arm is proportional to the mass of the block. Identifying the heaviest block can then be seen as a best arm identification problem (Audibert & Bubeck, 2010). Best arm identification is a well studied problem in experimental design, and understanding of how an optimal solution to the latent bandit should behave is used to guide our analysis of the agents we train on this task.\nIt is important to emphasize that we cannot simply apply standard bandit algorithms here, because we impose a much higher level of prior ignorance on our algorithms than that setting allows. Bandit algorithms assume that rewards are observed directly, whereas our agents observe mass through its role in dynamics (and in the case of learning from pixels, through the lens of vision as well). To maintain a bandit setting one could imagine parameterizing this transformation from reward to observation, and perhaps even learning the mapping as well; however, doing so requires explicitly acknowledging the mapping in the design of the learning algorithm, which we avoid doing. Moreover, acknowledging this mapping in any way requires the a-priori recognition of the existence of the latent bandit structure. From the perspective of our learning algorithm the mere existence of such a structure also lies beyond the veil of ignorance.\nControlling the distribution of masses allows us to control the difficulty of this task. In particular, by controlling the size of the mass gap between the two heaviest blocks we can make the task more\nor less difficult. We generate masses in the range [0, 1] and scale them to an appropriate range for the agent\u2019s strength.\nWe use the following scheme for controlling the difficulty of the Which is Heavier environment. First we select one of the blocks uniformly at random to be the \u201cheavy\u201d block and designate the remaining three as \u201clight\u201d blocks. We sample the mass of the heavy block from Beta(\u03b2, 1) and the mass of the light blocks from Beta(1, \u03b2). The single parameter \u03b2 effectively controls the distribution of mass gaps (and thus controls the difficulty), with large values of \u03b2 leading to easier problems. Figure 1 shows the distribution of mass gaps for three values of \u03b2 that we use in our experiments.\nWe distinguish between problem level and instance level difficulty for this domain. Instance level difficulty refers to the size of the mass gap in a single episode. If the mass gap is small it is harder to determine which block is heaviest, and we say that one episode is more difficult than another by comparing their mass gaps. Problem level difficulty refers to the shape of the generating distribution of mass gaps (e.g. as shown in the right panel of Figure 1). A distribution that puts more mass on configurations that have a small mass gap will tend to generate more episodes that are difficult at the instance level, and we say that one distribution is more difficult than another if it is more likely to generate instances with small mass gaps. We control the problem level difficulty through \u03b2, but we incorporate both problem and instance level difficulty in our analysis.\nWe set the episode length limit to 100 steps in this environment, which is sufficient time to be much longer than a typical episode by a successfully trained agent.\n5.2 ACTUATORS\nThe obvious choice for actuation in physical domains is some kind of arm or hand based manipulator. However, controlling an arm or hand is quite challenging on its own, requiring a fair amount of dexterity on the part of the agent. The manipulation problem, while very interesting in its own right, is orthogonal to our goals in this work. Therefore we avoid the problem of learning dexterous manipulation by providing the agent with a much simpler form of actuation.\nWe call the actuation strategy for this environment direct actuation, which allows the agent to affect forces on the different blocks directly. At every time step the agent can output one out of eight possible actions. The first four actions result in an application of a vertical force of fixed magnitude to center of mass of each of the four blocks respectively. The remaining actions are labeling actions and correspond to agent\u2019s selection of which is the heaviest block.\n5.3 EXPERIMENTS\nOur first experiment is a sanity check to show that we can train agents successfully on the Which is Heavier environment using both features and pixels. This experiment is designed simply to show that our task is solvable, and to illustrate that by changing the problem difficulty we can make the task very hard.\nWe present two additional experiments showing how varying difficulty leads to differentiated behavior both at the problem level and at the instance level. In both cases knowledge of the latent bandit problem allows us to make predictions about how an experimenting agent should behave, and our experiments are designed to show that qualitatively correct behavior is obtained by our agents in spite of their a-priori ignorance of the underlying bandit problem.\nWe show that as we increase the problem difficulty the learned policies transition from guessing immediately when a heavy block is found to strongly preferring to poke all blocks before making a decision. This corresponds to the observation that if it is unlikely for more than one arm to give high reward then any high reward arm is likely to be best.\nWe also observe that our agents can adapt their behavior to the difficulty of individual problem instances. We show that a single agent will tend to spend longer gathering information when the particular problem instance is more difficult. This corresponds to the observation that when the two best arms have similar reward then more information is required to accurately distinguish them.\nFinally, we conduct an experiment comparing our learned information gathering policies to a randomized baseline method. This experiment shows that agents more reliably produce the correct label by following their learned interaction policies than by observing the environment being driven by random actions.\nSuccess in learning For this experiment we trained several agents at three different difficulties corresponding to \u03b2 \u2208 {3, 5, 10}. For each problem difficulty we trained agents on both feature observations, which includes the z coordinate of each of the four blocks; and also using raw pixels, providing 84 \u00d7 84 pixel RGB rendering of the scene to the agent. Representative learning curves for each condition are shown in Figure 2. The curves are smoothed over time and show a running estimate of the probability of success, rather than showing the reward directly.\nThe agents do not reach perfect performance on this task, with more difficult problems plateauing at progressively lower performance. This can be explained by looking at the distributions of instance level difficulties generated by different settings of \u03b2, which is shown in the right panel of Figure 1. For higher difficulties (lower values of \u03b2) there is a substantial probability of generating problem instances where the mass gap is near 0, which makes distinguishing between the two heaviest blocks very difficult.\nPopulation strategy differentiation For this experiment we trained agents at three different difficulties corresponding to \u03b2 \u2208 {3, 5, 10} all using a discount factor of \u03b3 = 0.95 which corresponds a relatively high cost of gathering information. We trained three agents for each difficulty and show results aggregated across the different replicas.\nAfter training, each agent was run for 10,000 steps under the same conditions they were exposed to during training. We record the number and length of episodes executed during the testing period as well as the outcome of each episode. Episodes are terminated by timeout after 100 steps, but the vast majority of episodes are terminated in < 30 steps by the agent producing a label. Since episodes vary in length not all agents complete the same number of episodes during testing.\nThe left plot in Figure 3 shows histograms of the episode lengths broken down by task difficulty. The dashed vertical line indicates an episode length of four interaction steps, which is the minimum number of actions required for the agents to interact with every block. At a task difficulty of \u03b2 = 10 the agents appear to learn simply to search for a single heavy block (which can be found with an average of two interactions). However, at a task difficulty of \u03b2 = 3 we see a strong bias away from terminating the episode before taking at least four exploratory actions.\nIndividual strategy differentiation For this experiment we trained agents using the same three task difficulties as in the previous experiment, but with an increased discount factor of \u03b3 = 0.99.\nThis decreases the cost of exploration and encourages the agents to gather more information before producing a label, leading to longer episodes.\nAfter training, each agent was run for 100,000 steps under the same conditions they were exposed to during training. We record the length of each episode, as well as the mass gap between the two heaviest blocks in each episode. In the same way that we use the distribution of mass gaps as a measure of task difficulty, we can use the mass gap in a single episode as a measure of the difficulty of that specific problem instance. We again exclude from analysis the very small proportion of episodes that terminate by timeout.\nThe right plots in Figure 3 show the relationship between the mass gap and episode length across the testing runs of two different agents. From these plots we can see how a single agent has learned to adapt its behavior based on the difficulty of a single problem instance. Although the variance is high, there is a clear correlation between the mass gap and the length of the episodes. This behavior reflects what we would expect from a solution to the latent bandit problem; more information is required to identify the best arm when the second best arm is nearly as good.\nRandomized interaction For this experiment we trained several agents using both feature and pixel observations at the same three task difficulties with a discount of \u03b3 = 0.95. In total we trained six sets of agents for this experiment.\nAfter training, each agent was run for 10,000 steps under the same conditions used during training. We record the outcome of each episode, as well as the number of steps taken by each agent before it chooses a label. For each agent we repeat the experiment using both the agent\u2019s learned interaction policy as well as a randomized interaction policy.\nThe randomized interaction policy is obtained as follows: At each step the agent chooses a candidate action using its learned policy. If the candidate action is a labeling action then it is passed to the environment unchanged (and the episode terminates). However, if the candidate action is an interaction action then we replace the agent action with a new interaction action chosen uniformly at random from the available action set. When following the randomized interaction policy the agent has no control over the information gathering process, but still controls when each episode ends, and what label is chosen.\nFigure 4 compares the learned interaction policies to the randomized interaction baselines. The results show that the effect on episode length is small, with no consistent bias towards longer or shorter episodes across difficulties and observation types. However, the learned interaction policies produce more accurate labels across all permutations.\n6 TOWERS\nThe Towers environment is designed to ask agents to count the number of cohesive rigid bodies in a scene. The environment is designed so that in its initial configuration it is not possible to determine the number of rigid bodies from vision or features alone.\n6.1 ENVIRONMENT\nThe environment is diagrammed in the left panel of Figure 5. It consists of a tower of five blocks which can move freely in three dimensions. The initial block tower is always in the same configuration but in each episode we bolt together different subsets of the blocks to form larger rigid bodies as shown in the figure.\nThe question to answer in this environment is how many rigid bodies are formed from the primitive blocks. Since which blocks are bound together is randomly assigned in each episode, and binding forces are invisible, the agent must poke the tower and observe how it falls down in order to determine how many rigid bodies it is composed of. We parameterize the environment in such a way that the distribution over the number of separate blocks in the tower is uniform. This ensures that there is no single action strategy that achieves high reward.\n6.2 ACTUATORS\nIn the Towers environment, we used two actuators: direct actuation, which is similar to the Which is Heavier environment; and the fist actuator, described below. In case of the direct actuation, the agent can output one out of 25 actions. At every time step, the agent can apply a force of fixed magnitude in either of +x, -x, +y or -y direction to one out of the five blocks. If two blocks are glued together, both blocks move under the effect of force. We use towers of five blocks, which results in 20 different possible actions. The remaining actions are labeling actions that are used by the agent to indicate the number of distinct blocks in the tower.\nThe fist is a large spherical object that the agent can actuate by setting velocities in a 2D horizontal plane. Unlike direct actuation, the agent cannot apply any direct forces to the objects that constitute the tower, but only manipulate them by pushing or hitting them with the fist. At every time step agent can output one of nine actions. The first four actions corresponds to setting the velocity of the fist to a constant amount in (+x, -x, +y, -y) directions respectively. The remaining actions are labeling actions, that are used by the agent to indicate the number of distinct blocks in the tower.\nIn order to investigate if the agent learns a strategy of stopping after a fixed number of time steps or whether it integrates sensory information in a non-trivial manner we used a notion of \u201ccontrol time step\u201d. The idea of control time step is similar to that of action repeats and if the physics simulation time step is 0.025s and control time step is 0.1s, it means that the same action is repeated 4 times. For the direct actuators we use an episode timeout of 26 steps and for both actuator types.\n6.3 EXPERIMENTS\nOur first experiment is again intended to show that we can train agents in this environment. We show simply that the task is solvable by our agents using both types of actuation.\nThe second experiment shows that the agents learn to wait for an observation where they can identify the number of rigid bodies before producing an answer. This is designed to show that the agents find a closed loop strategy for counting the number of rigid bodies. An alternative hypothesis would be that agents learn to wait for (approximately) the same number of steps each time and then take their best guess.\nOur third experiment compares the learned policy to a randomized interaction policy and shows that agents are able to determine the correct number of blocks in the tower more quickly and more reliably when using their learned policy to gather information.\nSuccess in learning For this experiment we trained several agents on the Towers environment using different pairings of actuators and perception. The features observations include the 3d position of each primitive block, and when training using raw pixels we provide an 84\u00d7 84 pixel RGB rendering of the scene as the agent observation. Figure 6 shows learning curves for each combination of actuator and observation type.\nIn all cases we obtain agents that solve the task nearly perfectly, although when training from pixels we find that the range of hyperparameters which train successfully is narrower than when training from features. Interestingly, the fist actuators lead to the fastest learning, in spite of the fact that the agent must manipulate the blocks indirectly through the fist. One possible explanation is that the fist can affect multiple blocks in one action step, whereas in the direct actuation only one block can be affected per time step.\nWaiting for information For this experiment we trained an agent with pixel observations and the fist actuator on the towers task with an control time step of 0.1 seconds and examine its behavior at test time with a smaller delay between actions. Reducing the control time step means that from the agent perspective time has been slowed down. Moving the fist a fixed amount of distance takes longer, as does waiting for the block tower to collapse once it has been hit.\nAfter training the agent was run for 10000 steps for a range of different control time steps. We record the outcome of each episode, as well as the number of steps taken by the agent before it chooses a label. None of the test episodes terminate by timeout, so we include all of them in the analysis.\nThe plot in Figure 5 shows the probability of answering correctly, as well as the median length of each episode measured in seconds. In terms of absolute performance we see a small drop compared to the training setting, where the agent is essentially perfect, but the agent performance remains good even for substantially smaller control timesteps than were used during training.\nWe also observe that the episodes with different time steps take approximate the same amount of real time across the majority of the tested range. This corresponds to a large change in episode length as measured by number of agent actions, since with an control time step of 0.01 the agent must execute 10x as many actions to cover the same amount of real time as compared to the control time step used during training. From this we can infer that the agent has learned to wait for an informative observation before producing a label, as opposed to a simpler degenerate strategy of waiting a fixed amount of steps before answering.\nRandomized interaction For this experiment we trained several agents for each combination of actuator and observation type, and examine their behavior when observing an environment driven by a random interaction policy. The randomized interaction policy is identical to the randomized baseline used in the Which is Heavier environment.\nAfter training, each agent was run for 10,000 steps. We record the outcome of each episode, as well as the number of steps taken by the agent before it chooses a label. For each agent we repeat the experiment using both the agent\u2019s learned interaction policy as well as the randomized interaction policy.\nFigure 7 compares the learned interaction policies to the randomized interaction baselines. The results show that the agents tend to produce labels more quickly when following their learned interaction policies, and also that the labels they produce in this way are much more accurate.\n7 RELATED WORK\nDeep learning techniques in conjunction with vast labeled datasets have yielded powerful models for image classification (Krizhevsky et al., 2012; He et al., 2016) and speech recognition (Hinton et al., 2012). In recent years, as we have approached human level performance on these tasks, there has been a strong interest in the computer vision field in moving beyond semantic classification, to tasks that require a deeper and more nuanced understanding of the world.\nInspired by developmental studies (Smith & Gasser, 2005), some recent works have focused on learning representations by predicting physical embodiment quantities such as ego-motion (Agrawal et al., 2015; Jayaraman & Grauman, 2015), instead of symbolic labels. Extending the realm of things-to-be-predicted to include quantities beyond class labels, such as viewer centric parameters (Doersch et al., 2015) or the poses of humans within a scene (Delaitre et al., 2012; Fouhey et al., 2014), has been shown to improve the quality of feature learning and scene understanding. Researchers have looked at cross modal learning, for example synthesizing sounds from visual images (Owens et al., 2015), using summary statistics of audio to learn features for object recognition (Owens et al., 2016) or image colorization (Zhang et al., 2016).\nInverting the prediction tower, another line of work has focused on learning about the visual world by synthesizing, rather than analyzing, images. Major cornerstones of recent work in this area include the Variational Autoencoders of Kingma & Welling (2014), the Generative Adversarial Networks\nof (Goodfellow et al., 2014), and more recently autoregressive models have been very successful (van den Oord et al., 2016).\nBuilding on models of single image synthesis there have been many works on predicting the evolution of video frames over time (Ranzato et al., 2014; Srivastava et al., 2015; van den Oord et al., 2016). Xue et al. (2016) have approached this problem by designing a variational autoencoder architecture that uses the latent stochastic units of the VAE to make choices about the direction of motion of objects, and generates future frames conditioned on these choices.\nA different form of uncertainty in video prediction can arise from the effect of actions taken by an agent. In environments with deterministic dynamics (where the possibility of \u201cknown unknowns\u201d can, in principle, be eliminated), very accurate action-conditional predictions of future frames can be made (Oh et al., 2015). Introducing actions into the prediction process amounts to learning a latent forward dynamics model, which can be exploited to plan actions to achieve novel goals (Watter et al., 2015; Assael et al., 2015; Fragkiadaki et al., 2016). In these works, frame synthesis plays the role of a regularizer, preventing collapse of the feature space where the dynamics model lives.\nAgrawal et al. (2016) break the dependency between frame synthesis and dynamics learning by replacing frame synthesis with an inverse dynamics model. The forward model plays the same role as in the earlier works, but here feature space collapse is prevented by ensuring that the model can decode actions from pairs of time-adjacent images. Several works, including Agrawal et al. (2016) and Assael et al. (2015) mentioned above but also Pinto et al. (2016); Pinto & Gupta (2016); Levine et al. (2016), have gone further in coupling feature learning and dynamics. The learned dynamics models can be used for control not only after learning but also during the learning process in order to collect data in a more targeted way, which has been shown to improve the speed and quality of learning in robot manipulation tasks.\nA key challenge of learning from dynamics is collecting the appropriate data. An ingenious solution to this is to import real world data into a physics engine and simulate the application of forces in order to generate ground truth data. This is the approach taken by Mottaghi et al. (2016), who generate an \u201cinteractable\u201d data set of scenes, which they use to generate a static data set of image and force pairs, along with the ground truth trajectory of a target object in response to the application of the indicated force.\nWhen the purpose is learning an intuitive understanding of dynamics it is possible to do interesting work with entirely synthetic data (Fragkiadaki et al., 2016; Lerer et al., 2016). Lerer et al. (2016) show that convolutional networks can learn to make judgments about the stability of synthetic block towers based on a single image of the tower. They also show that their model trained on synthetic data is able to generalize to make accurate judgments about photographs of similar block towers built in the real world.\nMaking intuitive judgments about block towers has been extensively studied in the psychophysics literature. There is substantial evidence connecting the behavior of human judgments to inference over an explicit latent physics model (Hegarty, 2004; Hamrick et al., 2011; Battaglia et al., 2013). Humans can infer mass by watching movies of complex rigid body dynamics (Hamrick et al., 2016).\nA major component of the above line of work is analysis by synthesis, in which understanding of a physical process is obtained by learning to invert it. Observations are assumed to be generated from an explicitly parameterized generative model of the true physical process, and provide constraints to an inference process run over the parameters of this model. The analysis by synthesis approach has been extremely influential due to its power to explain human judgments and generalization patterns in a variety of situations (Lake et al., 2015).\nGalileo (Wu et al., 2015) is a particularly relevant instance of tying together analysis by synthesis and deep learning for understanding dynamics. This system first infers the physical parameters (mass and friction coefficient) of a variety of blocks by watching videos of them sliding down slopes and colliding with other blocks. This stage of the system uses an off-the-shelf object tracker to ground inference over the parameters of a physical simulator, and the inference is achieved by matching simulated and observed block trajectories. The inferred physical parameters are used to train a deep network to predict the physical parameters from the initial frame of video. At test time the system is evaluated by using the deep network to infer physical parameters of new blocks, which can be fed into the physics engine and used to answer questions about behaviors not observed at training time.\nPhysics 101 (Wu et al., 2016) is an extension of Galileo that more fully embraces deep learning. Instead of using a first pass of analysis by synthesis to infer physical parameters based on observations, a deep network is trained to regress the output of an object tracker directly, and the relevant physical laws are encoded directly into the architecture of the model. The authors show that they can use latent intrinsic physical properties inferred in this way to make novel predictions. The approach of encoding physical models as architecture constraints has also been proposed by Stewart & Ermon (2016).\nMany of the works discussed thus far, including Galileo and Physics 101, are restricted to passive sensing. Pinto et al. (2016); Pinto & Gupta (2016); Agrawal et al. (2016); Levine et al. (2016) are exceptions to this because they learn their models using a sequential greedy data collection bootstrapping strategy. Active sensing, it appears, is an important aspect of visual object learning in toddlers as argued by Bambach et al. (2016), providing motivation for the approach presented here.\nIn computer vision, it is well known that recognition performance can be improved by moving so as to acquire new views of an object or scene. Jayaraman & Grauman (2016), for example, apply deep reinforcement learning to construct an agent that chooses how to acquire new views of an object so as to classify it into a semantic category, and their related work section surveys many other efforts in active vision.\nWhile Jayaraman & Grauman (2016) and others share deep reinforcement learning and active sensing in common with our work, their goal is to learn a policy that can be applied to images to make decisions based on vision. In contrast, the goal in this paper is to study how agents learn to experiment continually so as to learn representations to answer questions about intrinsic properties of objects. In particular, our focus is on tasks that can only be solved by interaction and not by vision alone.\n8 CONCLUSION AND FUTURE DIRECTIONS\nDespite recent advances in artificial intelligence, machines still lack a common sense understanding of our physical world. There has been impressive progress in recognizing objects, segmenting object boundaries and even describing visual scenes with natural language. However, these tasks are not enough for machines to infer physical properties of objects such as mass, friction or deformability.\nWe introduce a deep reinforcement learning agent that actively interacts with physical objects to infer their hidden properties. Our approach is inspired by findings from the developmental psychology literature indicating that infants spend a lot of their early time experimenting with objects through random exploration (Smith & Gasser, 2005; Gopnik, 2012; Spelke & Kinzler, 2007). By letting our agents conduct physical experiments in an interactive simulated environment, they learn to manip-\nulate objects and observe the consequences to infer hidden object properties. We demonstrate the efficacy of our approach on two important physical understanding tasks\u2014inferring mass and counting the number of objects under strong visual ambiguities. Our empirical findings suggest that our agents learn different strategies for these tasks that balance the cost of gathering information against the cost of making mistakes in different situations.\nScientists and children are able not only to probe the environment to discover things about it, but they can also leverage their findings to answer new questions. In this paper we have shown that agents can be trained to gather knowledge to answer questions about hidden properties, but we have not addressed the larger issue of theory building, or transfer of this information. Given agents that can make judgments about mass and numerosity, how can they be enticed to leverage this knowledge to solve new tasks?\nAnother important aspect of understanding through interaction is that that the shape of the interactions influences behavior. We touched on this in the Towers environment where we looked at two different actuation styles, but there is much more to be done here. Thinking along these lines leads naturally to exploring tool use. We showed that agents can make judgments about object mass by hitting them, but could we train an agent to make similar judgments using a scale?\nFinally, we have made no attempt in this work to optimize data efficiency, but learning physical properties from fewer samples is an important direction to pursue.\nACKNOWLEDGMENTS\nWe would like to thank Matt Hoffman for several enlightening discussions about bandits. We would also like to thank the ICLR reviewers, whose helpful feedback allowed us to greatly improve the paper.\n", "cats": {"READ": 1.0, "DONTREAD": 0.0}, "reviews": [{"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "The following statement best summarizes the contribution: \"This paper shows that model free RL methods can learn how to gather information about physical properties of objects, even when this information is not available to a passive observer, and use this information to make decisions.\" So this is not a paper about new theory or algorithms, but rather about solving the problem of acquiring knowledge about the physics of the world around us, which is important for many problems and helps explain human performance in many tasks. There are still some concerns about the depth-of-analysis of the paper, but on balance, it is seen as an unconventional but interesting paper. As per AnonReviewer6, the final version could still aim to better address \"What should other researchers focus on if they are trying to build agents that can understand physics intuitively (building off this work)?\"\n -- Area chair", "OTHER_KEYS": "ICLR 2017 pcs"}, {"DATE": "16 Jan 2017", "TITLE": "Why This Paper is Important", "IS_META_REVIEW": false, "comments": "The machine learning field has profited from hundreds of yearly publications that propose a new algorithm, which outperforms baseline algorithms and has a theoretical proof for some idealized form of the algorithm. This paper, however, is different. It does not propose a new algorithm. It incidentally uses an RL algorithm, but many other RL algorithms could have been used just as well. This paper is not about new models or algorithms.\n\nSo what is this paper about? It is an initial research step toward understanding objects and intuitive reasoning in physical worlds. Why is this important? Our best AI agents currently fail on simple control tasks and simple games, such as Montezuma\u2019s Revenge, because when they look at a screen that has a ladder, a key and a skull they don\u2019t immediately know that keys open doors, that skulls are probably hazardous and best avoided, that ladders allow us to defy gravity, etc. The understanding of physics, relations and objects enables children to solve seemingly simple problems that our best existing AI agents do not come close to begin to solve.   \n\nEndowing our agents with knowledge of objects would help enormously with planning, reasoning and exploration. Yet, this is far from trivial. First, what is an object? It turns out this is not an easy question to answer and has baffled psychologists and philosophers. To simplify matters, in this paper we restrict our attention to rigid bodies. Staring at a thing in one\u2019s hands is not enough to understand what it is. Vision or open loop perception in general is not enough. Trying to pull the thing apart to see whether it is a single entity might help. (Children do indeed love to tear things like paper apart to understand them.) Touching the thing to see what happens also helps --- perhaps it lights up and starts beeping. Further interaction might reveal that it enables us to remotely talk to someone else, to tweet, etc. Much of the knowledge gained is the result of interaction, that is perception by action. \n\nThis paper is also about designing tasks to understand how agents acquire intuitive reasoning and experimentation strategies in physical worlds. An abundant body of evidence in psychology --- see eg the works of Gerd Gigerenzer ", "OTHER_KEYS": "Misha Denil"}, {"DATE": "16 Jan 2017", "TITLE": "Updated Environments", "IS_META_REVIEW": false, "comments": "We would like to highlight to all reviewers that the experiments have been updated substantially since the previous version of the paper.  In particular, we now include the missing results for Fist Pixels as requested by Reviewer 3, as well as additional experiments comparing our models to random baseline policies in both environments.\n\nIn the process of making these updates we have have made several minor changes to the environments themselves. These changes have made the tower environment harder to solve, but do not alter any conclusions drawn from the experiments.", "OTHER_KEYS": "Misha Denil"}, {"TITLE": "Interesting ideas while lack of the details", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer7", "comments": "This paper addresses the question of how to utilize physical interactions to answer questions about physical outcomes. This question falls into a popular stream in ML community -- understanding physics. The paper moved a step further and worked on experimental setups where there is no prior about the physical properties/rules and it uses a deep reinforcement learning (DRL) technique to address the problem. My overall opinion about this paper is: an interesting attempt and idea, yet without a clear contribution.\n\nThe experimental setups are quite interesting. The goal is to figure out which blocks are heavier or which blocks are glued together -- only by pushing and pulling objects around without any prior. The paper also shows reasonable performances on each task with detailed scenarios.\n\nWhile these experiments and results are interesting, the contribution is unclear. My main question is: does this result bring us any new insight? While the scenarios are interesting and focused on physical experiments, this is not any more different (potentially easier) than learning from playing games (e.g. Atari). In other words, are the tasks really different from other typical popular DRL tasks? To this end, I would have been more excited if authors showed some more new insights or experiments on learned representations and etc. Currently, the paper only discusses the factual outcome. For example, it describes the experimental setup and how much performances an agent could achieve. The authors could probably dissect the learned representations further, or discuss how the experimental results are linked to the human behavior or physical properties/laws.\n\nI am very in-between for my overall rating. I think the paper could have a deeper analysis. I however recommend the acceptance because of its merit of the idea.\n\n\n\nThe followings are some detailed questions (not directly impacting my overall rating):\n(1) Page 2 \"we assume that the agent has no prior knowledge about the physical properties of objects, or the laws of physics, and hence must interact with the objects in order to learn to answer questions about these properties.\": why does one \"must\" interact with objects in order to learn about the properties? Can't we also learn through observation?\n\n(2) Figure 1right is missing a Y-axis label.\n\n(3) Page 3: A relating to bandit is interesting, but the formal approach is all based on DRL.\n\n(4) Page 5 \"which makes distinguishing between the two heaviest blocks very difficult\": I am a bit confused why having a small mass gap makes the task harder (unless it's really close to 0). Shouldn't a machine be possible to distinguish even a pixel difference of speed? If not, isn't this just because of the network architecture?\n\n(5) Page 5 \"Since the agents exhibit similar performance using pixels and features we conduct the remaining experiments in this section using feature observations, since these agents are substantially faster to train.\": How about at least showing a correlation of performances at the instance level (rather than average performances)? Even so, I think this is a bit of big conclusion.\n\n(6) Throughout the papers, I felt that many conclusions (e.g. difficulty and etc) are based on a particularly chosen training distribution. For example, how does an agent really know when the instance is any more difficult? Doesn't this really depend on the empirically learned distribution of training samples (i.e. P(m_3 | m_1, m_2), where m_i indicates masses of object 1, 2, and 3)? In other words, does what's hard/easy matter much unless this is more thoroughly tested over various types of distributions?\n\n(7) Any baseline approach?", "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "31 Dec 2016", "REVIEWER_CONFIDENCE": 3}, {"TITLE": "Conceptually interesting and valuable, but lacks commitment to precise definitions", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer5", "comments": "This paper investigates the question of gathering information (answering question)\nthrough direct interaction with the environment. In that sense, it is closely\nrelated to \"active learning\" in supervised learning, or to the fundamental\nproblem of exploration-exploitation in RL. The authors consider a specific \ninstance of this problem in a physics domain and learn\ninformation-seeking policies using recent deep RL methods.\n\nThe paper is mostly empirical and explores the effect of changing the\ncost of information (via the discount factor) on the structure of the learned\npolicies. It also shows that general-purpose deep policy gradient methods are\nsufficient powerful to learn such tasks. The proposed environment is, to my knowledge,\nnovel as well the task formulation in section 2. (And it would be very valuable to the\nthe community if the environment would be open-sourced)\n\nThe expression \"latent structure/dynamics\" is used throughout the text and the connection\nwith bandits is mentioned in section 4. It therefore seems that authors aspire\nfor more generality with their approach but the paper doesn't quite fully ground\nthe proposed approach formally in any existing framework nor does it provide a\nnew one completely.\n\nFor example: how does your approach formalize the concept of \"questions\" and \"answers\" ?\nWhat makes a question \"difficult\" ? How do you quantify \"difficulty\" ?\nHow do you define the \"cost of information\"? What are its units (bits, scalar reward), its semantics ?\nDo you you have an MDP or a POMDP ? What kind of MDP do you consider ?\nHow do you define your discounted MDP ? What is the state and action spaces ?\nSome important problem structure under the \"interaction/labeling/reward\"\nparagraph of section 2 would be worth expressing directly in your definition\nof the MDP: labeling actions can only occur during the \"labeling phase\" and that the transition\nand reward functions have a specific structure (positive/negative, lead to absorbing state).\nThe notion of \"phase\" could perhaps be implemented by considering an augmented state space : $\\tilde s = (s, phase)$\n", "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 3}, {"TITLE": "Review", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer6", "comments": "This paper purports to investigate the ability of RL agents to perform \u2018physics experiments\u2019 in an environment, to infer physical properties about the objects in that environment. The problem is very well motivated; indeed, inferring the physical properties of objects is a crucial skill for intelligent agents, and there has been relatively little work in this direction, particularly in deep RL. The paper is also well-written.\n\nAs there are no architectural or theoretical contributions of the paper (and none are claimed), the main novelty comes in the task application \u2013 using a recurrent A3C model for two tasks that simulate an agent interacting with an environment to infer physical properties of objects. More specifically, two tasks are considered \u2013 moving blocks to determine their mass, and poking towers such that they fall to determine the number of rigid bodies they are composed of. These of course represent a very limited cross-section of the prerequisite abilities for an agent to understand physics. This in itself is not a bad thing, but since there is no comparison of different (simpler) RL agents on the tasks, it is difficult to determine if the tasks selected are challenging. As mentioned in the pre-review question, the \u2018Which is Heavier\u2019 task seems quite easy due to the actuator set-up, and the fact that the model simply must learn to take the difference between successive block positions (which are directly encoded as features in most experiments).  Thus, it is not particularly surprising that the RL agent can solve the proposed tasks. \n\nThe main claim beyond solving two proposed tasks related to physics simulation is that \u201cthe agents learn different strategies for these tasks that balance the cost of gathering information against the cost of making mistakes\u201d. The \u2018cost of gathering information\u2019 is implemented by multiplying the reward with a value of gamma < 1. This is somewhat interesting behaviour, but is hardly surprising given the problem setup.\n\nOne item the authors highlight is that their approach of learning about physical object properties through interaction is different from many previous approaches, which use visual cues. However, the authors also note that this in itself is not novel, and has been explored in other work (e.g. Agrawal et al. (2016)). I think it\u2019s crucial for the authors to discuss these approaches in more detail (potentially along with removing some other, less relevant information from the related work section), and specifically highlight why the proposed tasks in this paper are interesting compared to, for example, learning to move objects towards certain end positions by poking them.\n\nTo discern the level of contribution of the paper, one must ask the following questions: \n\n1)\thow much do these two tasks contribute (above previous work) to the goal of having agents learn the properties of objects by interaction; and\n2)\thow much do the results of the RL agent on these tasks contribute to our understanding of agents that interact with their environment to learn physical properties of objects? \n\nIt is difficult to know exactly, but due to the concerns outlined above, I am not convinced that the answers to (1) or (2) are \u201cto a significant extent\u201d. In particular, for (1), since the proposed agent is able to essentially solve both tasks, it is not clear that the tasks can be used to benchmark more advanced agents (e.g. it can\u2019t be used as a set of bAbI-like tasks). \n\nAnother possible concern, as pointed out by Reviewer 3, is that the description of the model is extremely concise. It would be nice to have, for example, a diagram illustrating the inputs and outputs to the model at each time step, to ease replication.\n\nOverall, it is important to make progress towards agents that can learn to discover physical properties of their environment, and the paper contributes in this direction. However, the technical contributions of this paper are rather limited \u2013 thus, it is not clear to what extent the paper pushes forward research in this direction beyond previous work that is mentioned. It would be nice, for example, to have some discussion about the future of agents that learn physics from interaction (speculation on more difficult versions of the tasks in this paper), and how the proposed approach fits into that picture.  \n\n---------------\nEDIT: score updated, see comments below\n", "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "16 Dec 2016 (modified: 18 Jan 2017)", "REVIEWER_CONFIDENCE": 3}, {"IMPACT": 4, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "TITLE": "mixed opinion", "comments": "This paper presents interesting experimental findings that state-of-the-art deep reinforcement learning methods enable agent learning of latent (physical) properties in its environment. The paper formulates the problem of an agent labeling environmental properties after interacting with the environment based on its actions, and applies the deep reinforcement learning model to evaluate whether such learning is possible. The approach jointly learns the convolutional layers for pixel-based perception and its later layers for learning actions based on reinforcement signals.\n\nWe have a mixed opinion about this paper. The paper is written clearly and presents interesting experimental findings. It introduces and formulates a problem potentially important for many robotics applications. Simultaneously, the paper suffers from lacking algorithmic contributions and missing (some of) crucial experiments to confirm its true benefits.\n\nPros:\n\n+ This paper introduces a new problem of learning latent properties in the agent's environment.\n\n+ The paper presents a framework to appropriately combine existing tools to address the formulated problem.\n\n+ The paper tries reinforcement learning with image inputs and fist-like actuator actions. This will lead to its direct application to robots.\n\nCons:\n\n- Lacking algorithmic contribution: this paper applies existing tools/methods to solve the problem rather than developing something new or extending them. The approach essentially is training LSTMs with convolutional layers using the previous Asynchronous Advantage Actor Critic.\n\n- In the Towers experiment, the results of probably the most important setting, \"Fist Pixels\", are missing. This setting receiving pixel inputs and using the Fist actuator in a continuous space is the setting closest to real-world robots, and thus is very important to confirm whether the proposed approach will be directly applicable to real-world robots. However, Figure 5 is missing the results with this setting. Is there any reason behind this?\n\n- The paper lacks its comparison to any baseline methods. Without explicit baselines, it is difficult to see what the agent is really learning and what aspect of the proposed approach is benefitting the task. For instance, in the Towers task, how would an agent randomly pushing/hitting the tower (using 'Fist') a number of times and then passively observing its consequence to produce a label perform compared to this approach? That is, how would an approach with a fixed action policy (but with everything else) perform compared to the full deep reinforcement learning version?", "IS_ANNOTATED": true, "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "16 Dec 2016 (modified: 18 Jan 2017)", "REVIEWER_CONFIDENCE": 4}, {"DATE": "03 Dec 2016", "TITLE": "Related work, bandit, discount factor", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer5"}, {"IMPACT": 4, "TITLE": "clarification of technical contributions", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "", "IS_ANNOTATED": true, "IS_META_REVIEW": false, "DATE": "02 Dec 2016"}, {"DATE": "02 Dec 2016", "TITLE": "which is heavier experiment", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer6"}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "The following statement best summarizes the contribution: \"This paper shows that model free RL methods can learn how to gather information about physical properties of objects, even when this information is not available to a passive observer, and use this information to make decisions.\" So this is not a paper about new theory or algorithms, but rather about solving the problem of acquiring knowledge about the physics of the world around us, which is important for many problems and helps explain human performance in many tasks. There are still some concerns about the depth-of-analysis of the paper, but on balance, it is seen as an unconventional but interesting paper. As per AnonReviewer6, the final version could still aim to better address \"What should other researchers focus on if they are trying to build agents that can understand physics intuitively (building off this work)?\"\n -- Area chair", "OTHER_KEYS": "ICLR 2017 pcs"}, {"DATE": "16 Jan 2017", "TITLE": "Why This Paper is Important", "IS_META_REVIEW": false, "comments": "The machine learning field has profited from hundreds of yearly publications that propose a new algorithm, which outperforms baseline algorithms and has a theoretical proof for some idealized form of the algorithm. This paper, however, is different. It does not propose a new algorithm. It incidentally uses an RL algorithm, but many other RL algorithms could have been used just as well. This paper is not about new models or algorithms.\n\nSo what is this paper about? It is an initial research step toward understanding objects and intuitive reasoning in physical worlds. Why is this important? Our best AI agents currently fail on simple control tasks and simple games, such as Montezuma\u2019s Revenge, because when they look at a screen that has a ladder, a key and a skull they don\u2019t immediately know that keys open doors, that skulls are probably hazardous and best avoided, that ladders allow us to defy gravity, etc. The understanding of physics, relations and objects enables children to solve seemingly simple problems that our best existing AI agents do not come close to begin to solve.   \n\nEndowing our agents with knowledge of objects would help enormously with planning, reasoning and exploration. Yet, this is far from trivial. First, what is an object? It turns out this is not an easy question to answer and has baffled psychologists and philosophers. To simplify matters, in this paper we restrict our attention to rigid bodies. Staring at a thing in one\u2019s hands is not enough to understand what it is. Vision or open loop perception in general is not enough. Trying to pull the thing apart to see whether it is a single entity might help. (Children do indeed love to tear things like paper apart to understand them.) Touching the thing to see what happens also helps --- perhaps it lights up and starts beeping. Further interaction might reveal that it enables us to remotely talk to someone else, to tweet, etc. Much of the knowledge gained is the result of interaction, that is perception by action. \n\nThis paper is also about designing tasks to understand how agents acquire intuitive reasoning and experimentation strategies in physical worlds. An abundant body of evidence in psychology --- see eg the works of Gerd Gigerenzer ", "OTHER_KEYS": "Misha Denil"}, {"DATE": "16 Jan 2017", "TITLE": "Updated Environments", "IS_META_REVIEW": false, "comments": "We would like to highlight to all reviewers that the experiments have been updated substantially since the previous version of the paper.  In particular, we now include the missing results for Fist Pixels as requested by Reviewer 3, as well as additional experiments comparing our models to random baseline policies in both environments.\n\nIn the process of making these updates we have have made several minor changes to the environments themselves. These changes have made the tower environment harder to solve, but do not alter any conclusions drawn from the experiments.", "OTHER_KEYS": "Misha Denil"}, {"TITLE": "Interesting ideas while lack of the details", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer7", "comments": "This paper addresses the question of how to utilize physical interactions to answer questions about physical outcomes. This question falls into a popular stream in ML community -- understanding physics. The paper moved a step further and worked on experimental setups where there is no prior about the physical properties/rules and it uses a deep reinforcement learning (DRL) technique to address the problem. My overall opinion about this paper is: an interesting attempt and idea, yet without a clear contribution.\n\nThe experimental setups are quite interesting. The goal is to figure out which blocks are heavier or which blocks are glued together -- only by pushing and pulling objects around without any prior. The paper also shows reasonable performances on each task with detailed scenarios.\n\nWhile these experiments and results are interesting, the contribution is unclear. My main question is: does this result bring us any new insight? While the scenarios are interesting and focused on physical experiments, this is not any more different (potentially easier) than learning from playing games (e.g. Atari). In other words, are the tasks really different from other typical popular DRL tasks? To this end, I would have been more excited if authors showed some more new insights or experiments on learned representations and etc. Currently, the paper only discusses the factual outcome. For example, it describes the experimental setup and how much performances an agent could achieve. The authors could probably dissect the learned representations further, or discuss how the experimental results are linked to the human behavior or physical properties/laws.\n\nI am very in-between for my overall rating. I think the paper could have a deeper analysis. I however recommend the acceptance because of its merit of the idea.\n\n\n\nThe followings are some detailed questions (not directly impacting my overall rating):\n(1) Page 2 \"we assume that the agent has no prior knowledge about the physical properties of objects, or the laws of physics, and hence must interact with the objects in order to learn to answer questions about these properties.\": why does one \"must\" interact with objects in order to learn about the properties? Can't we also learn through observation?\n\n(2) Figure 1right is missing a Y-axis label.\n\n(3) Page 3: A relating to bandit is interesting, but the formal approach is all based on DRL.\n\n(4) Page 5 \"which makes distinguishing between the two heaviest blocks very difficult\": I am a bit confused why having a small mass gap makes the task harder (unless it's really close to 0). Shouldn't a machine be possible to distinguish even a pixel difference of speed? If not, isn't this just because of the network architecture?\n\n(5) Page 5 \"Since the agents exhibit similar performance using pixels and features we conduct the remaining experiments in this section using feature observations, since these agents are substantially faster to train.\": How about at least showing a correlation of performances at the instance level (rather than average performances)? Even so, I think this is a bit of big conclusion.\n\n(6) Throughout the papers, I felt that many conclusions (e.g. difficulty and etc) are based on a particularly chosen training distribution. For example, how does an agent really know when the instance is any more difficult? Doesn't this really depend on the empirically learned distribution of training samples (i.e. P(m_3 | m_1, m_2), where m_i indicates masses of object 1, 2, and 3)? In other words, does what's hard/easy matter much unless this is more thoroughly tested over various types of distributions?\n\n(7) Any baseline approach?", "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "31 Dec 2016", "REVIEWER_CONFIDENCE": 3}, {"TITLE": "Conceptually interesting and valuable, but lacks commitment to precise definitions", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer5", "comments": "This paper investigates the question of gathering information (answering question)\nthrough direct interaction with the environment. In that sense, it is closely\nrelated to \"active learning\" in supervised learning, or to the fundamental\nproblem of exploration-exploitation in RL. The authors consider a specific \ninstance of this problem in a physics domain and learn\ninformation-seeking policies using recent deep RL methods.\n\nThe paper is mostly empirical and explores the effect of changing the\ncost of information (via the discount factor) on the structure of the learned\npolicies. It also shows that general-purpose deep policy gradient methods are\nsufficient powerful to learn such tasks. The proposed environment is, to my knowledge,\nnovel as well the task formulation in section 2. (And it would be very valuable to the\nthe community if the environment would be open-sourced)\n\nThe expression \"latent structure/dynamics\" is used throughout the text and the connection\nwith bandits is mentioned in section 4. It therefore seems that authors aspire\nfor more generality with their approach but the paper doesn't quite fully ground\nthe proposed approach formally in any existing framework nor does it provide a\nnew one completely.\n\nFor example: how does your approach formalize the concept of \"questions\" and \"answers\" ?\nWhat makes a question \"difficult\" ? How do you quantify \"difficulty\" ?\nHow do you define the \"cost of information\"? What are its units (bits, scalar reward), its semantics ?\nDo you you have an MDP or a POMDP ? What kind of MDP do you consider ?\nHow do you define your discounted MDP ? What is the state and action spaces ?\nSome important problem structure under the \"interaction/labeling/reward\"\nparagraph of section 2 would be worth expressing directly in your definition\nof the MDP: labeling actions can only occur during the \"labeling phase\" and that the transition\nand reward functions have a specific structure (positive/negative, lead to absorbing state).\nThe notion of \"phase\" could perhaps be implemented by considering an augmented state space : $\\tilde s = (s, phase)$\n", "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 3}, {"TITLE": "Review", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer6", "comments": "This paper purports to investigate the ability of RL agents to perform \u2018physics experiments\u2019 in an environment, to infer physical properties about the objects in that environment. The problem is very well motivated; indeed, inferring the physical properties of objects is a crucial skill for intelligent agents, and there has been relatively little work in this direction, particularly in deep RL. The paper is also well-written.\n\nAs there are no architectural or theoretical contributions of the paper (and none are claimed), the main novelty comes in the task application \u2013 using a recurrent A3C model for two tasks that simulate an agent interacting with an environment to infer physical properties of objects. More specifically, two tasks are considered \u2013 moving blocks to determine their mass, and poking towers such that they fall to determine the number of rigid bodies they are composed of. These of course represent a very limited cross-section of the prerequisite abilities for an agent to understand physics. This in itself is not a bad thing, but since there is no comparison of different (simpler) RL agents on the tasks, it is difficult to determine if the tasks selected are challenging. As mentioned in the pre-review question, the \u2018Which is Heavier\u2019 task seems quite easy due to the actuator set-up, and the fact that the model simply must learn to take the difference between successive block positions (which are directly encoded as features in most experiments).  Thus, it is not particularly surprising that the RL agent can solve the proposed tasks. \n\nThe main claim beyond solving two proposed tasks related to physics simulation is that \u201cthe agents learn different strategies for these tasks that balance the cost of gathering information against the cost of making mistakes\u201d. The \u2018cost of gathering information\u2019 is implemented by multiplying the reward with a value of gamma < 1. This is somewhat interesting behaviour, but is hardly surprising given the problem setup.\n\nOne item the authors highlight is that their approach of learning about physical object properties through interaction is different from many previous approaches, which use visual cues. However, the authors also note that this in itself is not novel, and has been explored in other work (e.g. Agrawal et al. (2016)). I think it\u2019s crucial for the authors to discuss these approaches in more detail (potentially along with removing some other, less relevant information from the related work section), and specifically highlight why the proposed tasks in this paper are interesting compared to, for example, learning to move objects towards certain end positions by poking them.\n\nTo discern the level of contribution of the paper, one must ask the following questions: \n\n1)\thow much do these two tasks contribute (above previous work) to the goal of having agents learn the properties of objects by interaction; and\n2)\thow much do the results of the RL agent on these tasks contribute to our understanding of agents that interact with their environment to learn physical properties of objects? \n\nIt is difficult to know exactly, but due to the concerns outlined above, I am not convinced that the answers to (1) or (2) are \u201cto a significant extent\u201d. In particular, for (1), since the proposed agent is able to essentially solve both tasks, it is not clear that the tasks can be used to benchmark more advanced agents (e.g. it can\u2019t be used as a set of bAbI-like tasks). \n\nAnother possible concern, as pointed out by Reviewer 3, is that the description of the model is extremely concise. It would be nice to have, for example, a diagram illustrating the inputs and outputs to the model at each time step, to ease replication.\n\nOverall, it is important to make progress towards agents that can learn to discover physical properties of their environment, and the paper contributes in this direction. However, the technical contributions of this paper are rather limited \u2013 thus, it is not clear to what extent the paper pushes forward research in this direction beyond previous work that is mentioned. It would be nice, for example, to have some discussion about the future of agents that learn physics from interaction (speculation on more difficult versions of the tasks in this paper), and how the proposed approach fits into that picture.  \n\n---------------\nEDIT: score updated, see comments below\n", "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "16 Dec 2016 (modified: 18 Jan 2017)", "REVIEWER_CONFIDENCE": 3}, {"IMPACT": 4, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "TITLE": "mixed opinion", "comments": "This paper presents interesting experimental findings that state-of-the-art deep reinforcement learning methods enable agent learning of latent (physical) properties in its environment. The paper formulates the problem of an agent labeling environmental properties after interacting with the environment based on its actions, and applies the deep reinforcement learning model to evaluate whether such learning is possible. The approach jointly learns the convolutional layers for pixel-based perception and its later layers for learning actions based on reinforcement signals.\n\nWe have a mixed opinion about this paper. The paper is written clearly and presents interesting experimental findings. It introduces and formulates a problem potentially important for many robotics applications. Simultaneously, the paper suffers from lacking algorithmic contributions and missing (some of) crucial experiments to confirm its true benefits.\n\nPros:\n\n+ This paper introduces a new problem of learning latent properties in the agent's environment.\n\n+ The paper presents a framework to appropriately combine existing tools to address the formulated problem.\n\n+ The paper tries reinforcement learning with image inputs and fist-like actuator actions. This will lead to its direct application to robots.\n\nCons:\n\n- Lacking algorithmic contribution: this paper applies existing tools/methods to solve the problem rather than developing something new or extending them. The approach essentially is training LSTMs with convolutional layers using the previous Asynchronous Advantage Actor Critic.\n\n- In the Towers experiment, the results of probably the most important setting, \"Fist Pixels\", are missing. This setting receiving pixel inputs and using the Fist actuator in a continuous space is the setting closest to real-world robots, and thus is very important to confirm whether the proposed approach will be directly applicable to real-world robots. However, Figure 5 is missing the results with this setting. Is there any reason behind this?\n\n- The paper lacks its comparison to any baseline methods. Without explicit baselines, it is difficult to see what the agent is really learning and what aspect of the proposed approach is benefitting the task. For instance, in the Towers task, how would an agent randomly pushing/hitting the tower (using 'Fist') a number of times and then passively observing its consequence to produce a label perform compared to this approach? That is, how would an approach with a fixed action policy (but with everything else) perform compared to the full deep reinforcement learning version?", "IS_ANNOTATED": true, "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "16 Dec 2016 (modified: 18 Jan 2017)", "REVIEWER_CONFIDENCE": 4}, {"DATE": "03 Dec 2016", "TITLE": "Related work, bandit, discount factor", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer5"}, {"IMPACT": 4, "TITLE": "clarification of technical contributions", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "", "IS_ANNOTATED": true, "IS_META_REVIEW": false, "DATE": "02 Dec 2016"}, {"DATE": "02 Dec 2016", "TITLE": "which is heavier experiment", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer6"}]}
{"text": "1 INTRODUCTION\nWord embeddings computed using diverse methods are basic building blocks for Natural Language Processing (NLP) and Information Retrieval (IR). They capture the similarities between words (e.g., (Bengio et al., 2003; Collobert & Weston, 2008; Mikolov et al., 2013a; Pennington et al., 2014)). Recent work has tried to compute embeddings that capture the semantics of word sequences (phrases, sentences, and paragraphs), with methods ranging from simple additional composition of the word vectors to sophisticated architectures such as convolutional neural networks and recurrent neural networks (e.g., (Iyyer et al., 2015; Le & Mikolov, 2014; Kiros et al., 2015; Socher et al., 2011; Blunsom et al., 2014; Tai et al., 2015; Wang et al., 2016)). Recently, (Wieting et al., 2016) learned general-purpose, paraphrastic sentence embeddings by starting with standard word embeddings and modifying them based on supervision from the Paraphrase pairs dataset (PPDB), and constructing sentence embeddings by training a simple word averaging model. This simple method leads to better performance on textual similarity tasks than a wide variety of methods and serves as a good initialization for textual classification tasks. However, supervision from the paraphrase dataset seems crucial, since they report that simple average of the initial word embeddings does not work very well.\nHere we give a new sentence embedding method that is embarrassingly simple: just compute the weighted average of the word vectors in the sentence and then remove the projections of the average vectors on their first principal component (\u201ccommon component removal\u201d). Here the weight of a word w is a/(a+ p(w)) with a being a parameter and p(w) the (estimated) word frequency; we call\nthis smooth inverse frequency (SIF). This method achieves significantly better performance than the unweighted average on a variety of textual similarity tasks, and on most of these tasks even beats some sophisticated supervised methods tested in (Wieting et al., 2016), including some RNN and LSTM models. The method is well-suited for domain adaptation settings, i.e., word vectors trained on various kinds of corpora are used for computing the sentence embeddings in different testbeds. It is also fairly robust to the weighting scheme: using the word frequencies estimated from different corpora does not harm the performance; a wide range of the parameters a can achieve close-to-best results, and an even wider range can achieve significant improvement over unweighted average.\nOf course, this SIF reweighting is highly reminiscent of TF-IDF reweighting from information retrieval (Sparck Jones, 1972; Robertson, 2004) if one treats a \u201csentence\u201d as a \u201cdocument\u201d and make the reasonable assumption that the sentence doesn\u2019t typically contain repeated words. Such reweightings (or related ideas like removing frequent words from the vocabulary) are a good rule of thumb but has not had theoretical justification in a word embedding setting.\nThe current paper provides a theoretical justification for the reweighting using a generative model for sentences, which is a simple modification for the Random Walk on Discourses model for generating text in (Arora et al., 2016). In that paper, it was noted that the model theoretically implies a sentence embedding, namely, simple average of embeddings of all the words in it.\nWe modify this theoretical model, motivated by the empirical observation that most word embedding methods, since they seek to capture word cooccurence probabilities using vector inner product, end up giving large vectors to frequent words, as well as giving unnecessarily large inner products to word pairs, simply to fit the empirical observation that words sometimes occur out of context in documents. These anomalies cause the average of word vectors to have huge components along semantically meaningless directions. Our modification to the generative model of (Arora et al., 2016) allows \u201csmoothing\u201d terms, and then a max likelihood calculation leads to our SIF reweighting.\nInterestingly, this theoretically derived SIF does better (by a few percent points) than traditional TFIDF in our setting. The method also improves the sentence embeddings of Wieting et al., as seen in Table 1. Finally, we discovered that \u2014contrary to widespread belief\u2014Word2Vec(CBOW) also does not use simple average of word vectors in the model, as misleadingly suggested by the usual expression Pr[w|w1, w2, . . . , w5] \u221d exp(vw \u00b7 ( 15 \u2211 i vwi)). A dig into the implementation shows it implicitly uses a weighted average of word vectors \u2014again, different from TF-IDF\u2014 and this weighting turns out to be quite similar in effect to ours. (See Section 3.1.)\n2 RELATED WORK\nWord embeddings. Word embedding methods represent words as continuous vectors in a low dimensional space which capture lexical and semantic properties of words. They can be obtained from the internal representations from neural network models of text (Bengio et al., 2003; Collobert & Weston, 2008; Mikolov et al., 2013a) or by low rank approximation of co-occurrence statistics (Deerwester et al., 1990; Pennington et al., 2014). The two approaches are known to be closely related (Levy & Goldberg, 2014; Hashimoto et al., 2016; Arora et al., 2016).\nOur work is most directly related work to (Arora et al., 2016), which proposed a random walk model for generating words in the documents. Our sentence vector can be seen as approximate inference of the latent variables in their generative model.\nPhrase/Sentence/Paragraph embeddings. Previous works have computed phrase or sentence embeddings by composing word embeddings using operations on vectors and matrices e.g., (Mitchell & Lapata, 2008; 2010; Blacoe & Lapata, 2012). They found that coordinate-wise multiplication of the vectors performed very well among the binary operations studied. Unweighted averaging is also found to do well in representing short phrases (Mikolov et al., 2013a). Another approach is recursive neural networks (RNNs) defined on the parse tree, trained with supervision (Socher et al., 2011) or without (Socher et al., 2014). Simple RNNs can be viewed as a special case where the parse tree is replaced by a simple linear chain. For example, the skip-gram model (Mikolov et al., 2013b) is extended to incorporate a latent vector for the sequence, or to treat the sequences rather than the word as basic units. In (Le & Mikolov, 2014) each paragraph was assumed to have a latent paragraph vector, which influences the distribution of the words in the paragraph. Skip-thought\nof (Kiros et al., 2015) tries to reconstruct the surrounding sentences from surrounded one and treats the hidden parameters as their vector representations. RNNs using long short-term memory (LSTM) capture long-distance dependency and have also been used for modeling sentences (Tai et al., 2015). Other neural network structures include convolution neural networks, such as (Blunsom et al., 2014) that uses a dynamic pooling to handle input sentences of varying length and do well in sentiment prediction and classification tasks.\nThe directed inspiration for our work is (Wieting et al., 2016) which learned paraphrastic sentence embeddings by using simple word averaging and also updating standard word embeddings based on supervision from paraphrase pairs; the supervision being used for both initialization and training.\n3 A SIMPLE METHOD FOR SENTENCE EMBEDDING\nWe briefly recall the latent variable generative model for text in (Arora et al., 2016). The model treats corpus generation as a dynamic process, where the t-th word is produced at step t. The process is driven by the random walk of a discourse vector ct \u2208 <d. Each word w in the vocabulary has a vector in <d as well; these are latent variables of the model. The discourse vector represents \u201cwhat is being talked about.\u201d The inner product between the discourse vector ct and the (time-invariant) word vector vw for word w captures the correlations between the discourse and the word. The probability of observing a word w at time t is given by a log-linear word production model from Mnih and Hinton:\nPr[w emitted at time t | ct] \u221d exp (\u3008ct, vw\u3009) . (1)\nThe discourse vector ct does a slow random walk (meaning that ct+1 is obtained from ct by adding a small random displacement vector), so that nearby words are generated under similar discourses. It was shown in (Arora et al., 2016) that under some reasonable assumptions this model generates behavior \u2013in terms of word-word cooccurence probabilities\u2014that fits empirical works like word2vec and Glove. The random walk model can be relaxed to allow occasional big jumps in ct, since a simple calculation shows that they have negligible effect on cooccurrence probabilities of words. The word vectors computed using this model are reported to be similar to those from Glove and word2vec(CBOW).\nOur improved Random Walk model. Clearly, it is tempting to define the sentence embedding as follows: given a sentence s, do a MAP estimate of the discourse vectors that govern this sentence. We note that we assume the discourse vector ct doesn\u2019t change much while the words in the sentence were emitted, and thus we can replace for simplicity all the ct\u2019s in the sentence s by a single discourse vector cs. In the paper (Arora et al., 2016), it was shown that the MAP estimate of cs is \u2014up to multiplication by scalar\u2014the average of the embeddings of the words in the sentence.\nIn this paper, towards more realistic modeling, we change the model (1) as follows. This model has two types of \u201csmoothing term\u201d, which are meant to account for the fact that some words occur out of context, and that some frequent words (presumably \u201cthe\u201d, \u201cand \u201d etc.) appear often regardless of the discourse. We first introduce an additive term \u03b1p(w) in the log-linear model, where p(w) is the unigram probability (in the entire corpus) of word and \u03b1 is a scalar. This allows words to occur even if their vectors have very low inner products with cs. Secondly, we introduce a common discourse vector c0 \u2208 <d which serves as a correction term for the most frequent discourse that is often related to syntax. (Other possible correction is left to future work.) It boosts the co-occurrence probability of words that have a high component along c0.\nConcretely, given the discourse vector cs, the probability of a word w is emitted in the sentence s is modeled by,\nPr[w emitted in sentence s | cs] = \u03b1p(w) + (1\u2212 \u03b1) exp (\u3008c\u0303s, vw\u3009)\nZc\u0303s , (2)\nwhere c\u0303s = \u03b2c0 + (1\u2212 \u03b2)cs, c0 \u22a5 cs where \u03b1 and \u03b2 are scalar hyperparameters, and Zc\u0303s = \u2211 w\u2208V exp (\u3008c\u0303s, vw\u3009) is the normalizing constant (the partition function). We see that the model allows a word w unrelated to the discourse cs to be omitted for two reasons: a) by chance from the term \u03b1p(w); b) if w is correlated with the common discourse vector c0.\nAlgorithm 1 Sentence Embedding Input: Word embeddings {vw : w \u2208 V}, a set of sentences S, parameter a and estimated probabil-\nities {p(w) : w \u2208 V} of the words. Output: Sentence embeddings {vs : s \u2208 S}\n1: for all sentence s in S do 2: vs \u2190 1|s| \u2211 w\u2208s a a+p(w)vw 3: end for 4: Compute the first principal component u of {vs : s \u2208 S} 5: for all sentence s in S do 6: vs \u2190 vs \u2212 uu>vs 7: end for\nComputing the sentence embedding. The word embeddings yielded by our model are actually the same 1 The sentence embedding will be defined as the max likelihood estimate for the vector cs that generated it. ( In this case MLE is the same as MAP since the prior is uniform.) We borrow the key modeling assumption of (Arora et al., 2016), namely that the word vw\u2019s are roughly uniformly dispersed, which implies that the partition function Zc is roughly the same in all directions. So assume that Zc\u0303s is roughly the same, say Z for all c\u0303s. By the model (2) the likelihood for the sentence is\np[s | cs] = \u220f w\u2208s p(w | cs) = \u220f w\u2208s [ \u03b1p(w) + (1\u2212 \u03b1)exp (\u3008vw, c\u0303s\u3009) Z ] .\nLet\nfw(c\u0303s) = log\n[ \u03b1p(w) + (1\u2212 \u03b1)exp (\u3008vw, c\u0303s\u3009)\nZ ] denote the log likelihood of sentence s. Then, by simple calculus we have,\n\u2207fw(c\u0303s) = 1 \u03b1p(w) + (1\u2212 \u03b1) exp (\u3008vw, c\u0303s\u3009) /Z 1\u2212 \u03b1 Z exp (\u3008vw, c\u0303s\u3009) vw.\nThen by Taylor expansion, we have,\nfw(c\u0303s) \u2248 fw(0) +\u2207fw(0)>c\u0303s\n= constant + (1\u2212 \u03b1)/(\u03b1Z)\np(w) + (1\u2212 \u03b1)/(\u03b1Z) \u3008vw, c\u0303s\u3009 .\nTherefore, the maximum likelihood estimator for c\u0303s on the unit sphere (ignoring normalization) is approximately,2\narg max \u2211 w\u2208s fw(c\u0303s) \u221d \u2211 w\u2208s\na\np(w) + a vw, where a = 1\u2212 \u03b1 \u03b1Z . (3)\nThat is, the MLE is approximately a weighted average of the vectors of the words in the sentence. Note that for more frequent words w, the weight a/(p(w) + a) is smaller, so this naturally leads to a down weighting of the frequent words.\nTo estimate cs, we estimate the direction c0 by computing the first principal component of c\u0303s\u2019s for a set of sentences. In other words, the final sentence embedding is obtained by subtracting the projection of c\u0303s\u2019s to their first principal component. This is summarized in Algorithm 1.\n3.1 CONNECTION TO SUBSAMPLING PROBABILITIES IN WORD2VEC\nWord2vec (Mikolov et al., 2013b) uses a sub-sampling technique which downsamples word w with probability proportional to 1/ \u221a p(w) where p(w) is the marginal probability of the word w. This\n1We empirically discovered the significant common component c0 in word vectors built by existing methods, which inspired us to propose our theoretical model of this paper.\n2Note that maxc:\u2016c\u2016=1 C + \u3008c, g\u3009 = g/\u2016g\u2016 for any constant C.\nheuristic not only speeds up the training but also learns more regular word representations. Here we explain that this corresponds to an implicit reweighting of the word vectors in the model and therefore the statistical benefit should be of no surprise.\nRecall the vanilla CBOW model of word2vec:\nPr[wt | wt\u22121, . . . , wt\u22125] \u221d exp (\u3008v\u0304t, vw\u3009) , where v\u0304t = 1\n5 5\u2211 i=1 vwt\u2212i . (4)\nIt can be shown that the loss (MLE) for the single word vector vw (from this occurrence) can be abstractly written in the form,\ng(vw) = \u03b3(\u3008v\u0304t, vw\u3009) + negative sampling terms ,\nwhere \u03b3(x) = log(1/(1 + e\u2212x)) is the logistic function. Therefore, the gradient of g(vw) is\n\u2207g(vw) = \u03b3\u2032(\u3008v\u0304t, vw\u3009)v\u0304t = \u03b1(vwt\u22125 + vwt\u22124 + vwt\u22123 + vwt\u22122 + vwt\u22121) , (5)\nwhere \u03b1 is a scalar. That is, without the sub-sampling trick, the update direction is the average of the word vectors in the window.\nThe sub-sampling trick in (Mikolov et al., 2013b) randomly selects the summands in equation (5) to \u201cestimate\u201d the gradient. Specifically, the sampled update direction is\n\u2207\u0303g(vw) = \u03b1(J5vwt\u22125 + J4vwt\u22124 + J3vwt\u22123 + J2vwt\u22122 + J1vwt\u22121) (6)\nwhere Jk\u2019s are Bernoulli random variables with Pr [Jk = 1] = q(wt\u2212k) , min { 1, \u221a 10\u22125\np(wt\u2212k)\n} .\nHowever, we note that \u2207\u0303g(vw) is (very) biased estimator! We have that the expectation of \u2207\u0303g(vw) is a weighted sum of the word vectors,\nE [ \u2207\u0303g(vw) ] = \u03b1(q(wt\u22125)vwt\u22125 + q(wt\u22124)vwt\u22124 + q(wt\u22123)vwt\u22123 + q(wt\u22122)vwt\u22122 + q(wt\u22121)vwt\u22121) .\nIn fact, the expectation E[\u2207\u0303g(vw)] corresponds to the gradient of a modified word2vec model with the average v\u0304t (in equation (4)) being replaced by the weighted average \u22115 k=1 q(wt\u2212k)vwt\u2212k . Such a weighted model can also share the same form of what we derive from our random walk model as in equation (3). Moreover, the weighting q(wi) closely tracks our weighting scheme a/(a + p(w)) when using parameter a = 10\u22124; see Figure 1 for an illustration. Therefore, the expected gradient here is approximately the estimated discourse vector in our model! Thus, word2vec with sub-sampling gradient heuristic corresponds to a stochastic gradient update method for using our weighting scheme.\n4 EXPERIMENTS\n4.1 TEXTUAL SIMILARITY TASKS\nDatasets. We test our methods on the 22 textual similarity datasets including all the datasets from SemEval semantic textual similarity (STS) tasks (2012-2015) (Agirre et al., 2012; 2013; 2014; Agirrea et al., 2015), and the SemEval 2015 Twitter task (Xu et al., 2015) and the SemEval 2014 Semantic Relatedness task (Marelli et al., 2014). The objective of these tasks is to predict the similarity between two given sentences. The evaluation criterion is the Pearson\u2019s coefficient between the predicted scores and the ground-truth scores.\nExperimental settings. We will compare our method with the following:\n1. Unsupervised: ST, avg-GloVe, tfidf-GloVe. ST denotes the skip-thought vectors (Kiros et al., 2015), avg-GloVe denotes the unweighted average of the GloVe vectors (Pennington et al., 2014),3 and tfidf-GloVe denotes the weighted average of GloVe vectors using TF-IDF weights.\n2. Semi-supervised: avg-PSL. This method uses the unweighted average of the PARAGRAMSL999 (PSL) word vectors from (Wieting et al., 2015). The word vectors are trained using labeled data, but the sentence embedding are computed by unweighted average without training.\n3. Supervised: PP, PP-proj., DAN, RNN, iRNN, LSTM (o.g.), LSTM (no). All these methods are initialized with PSL word vectors and then trained on the PPDB dataset. PP and PPproj. are proposed in (Wieting et al., 2016). The first is an average of the word vectors, and the second additionally adds a linear projection. The word vectors are updated during the training. DAN denotes the deep averaging network of (Iyyer et al., 2015). RNN denotes the classical recurrent neural network, and iRNN denotes a variant with the activation being the identity, and the weight matrices initialized to identity. The LSTM is the version from (Gers et al., 2002), either with output gates (denoted as LSTM(o.g.)) or without (denoted as LSTM (no)).\nOur method can be applied to any types of word embeddings. So we denote the sentence embeddings obtained by applying our method to word embeddings method \u201cXXX\u201d as \u201cXXX+WR\u201d.4 To get a completely unsupervised method, we apply it to the GloVe vectors, denoted as GloVe+WR. The weighting parameter a is fixed to 10\u22123, and the word frequencies p(w) are estimated from the\n3We used the vectors that are publicly available at http://nlp.stanford.edu/projects/glove/. They are 300- dimensional vectors that were trained on the 840 billion token Common Crawl corpus.\n4\u201cW\u201d stands for the smooth inverse frequency weighting scheme, and \u201cR\u201d stands for removing the common components.\ncommoncrawl dataset.5 This is denoted by GloVe+WR in Table 1. We also apply our method on the PSL vectors, denoted as PSL+WR, which is a semi-supervised method.\nResults. The results are reported in Table 1. Each year there are 4 to 6 STS tasks. For clarity, we only report the average result for the STS tasks each year; the detailed results are in the appendix.\nThe unsupervised method GloVe+WR improves upon avg-GloVe significantly by 10% to 30%, and beats the baselines by large margins. It achieves better performance than LSTM and RNN and is comparable to DAN, even though the later three use supervision. This demonstrates the power of this simple method: it can be even stronger than highly-tuned supervisedly trained sophisticated models. Using TF-IDF weighting scheme also improves over the unweighted average, but not as much as our method.\nThe semi-supervised method PSL+WR achieves the best results for four out of the six tasks and is comparable to the best in the rest of two tasks. Overall, it outperforms the avg-PSL baseline and all the supervised models initialized with the same PSL vectors. This demonstrates the advantage of our method over the training for those models.\nWe also note that the top singular vectors c0 of the datasets seem to roughly correspond to the syntactic information or common words. For example, closest words (by cosine similarity) to c0 in the SICK dataset are \u201cjust\u201d, \u201cwhen\u201d, \u201ceven\u201d, \u201cone\u201d, \u201cup\u201d, \u201clittle\u201d, \u201cway\u201d, \u201cthere\u201d, \u201cwhile\u201d, and \u201cbut.\u201d\nFinally, in the appendix, we showed that our two ideas all contribute to the improvement: for GloVe vectors, using smooth inverse frequency weighting alone improves over unweighted average by about 5%, using common component removal alone improves by 10%, and using both improves by 13%.\n4.1.1 EFFECT OF WEIGHTING PARAMETER ON PERFORMANCE\nWe study the sensitivity of our method to the weighting parameter a, the method for computing word vectors, and the estimated word probabilities p(w). First, we test the performance of three\n5It is possible to tune the parameter a to get better results. The effect of a and the corpus for estimating word frequencies are studied in Section 4.1.1.\ntypes of word vectors (PSL, GloVe, and SN) on the STS 2012 tasks. SN vectors are trained on the enwiki dataset (Wikimedia, 2012) using the method in (Arora et al., 2016), while PSL and GloVe vectors are those used in Table 1. We enumerate a \u2208 {10\u2212i, 3 \u00d7 10\u2212i : 1 \u2264 i \u2264 5} and use the p(w) estimated on the enwiki dataset. Figure 2a shows that for all three kinds of word vectors, a wide range of a leads to significantly improved performance over the unweighted average. Best performance occurs from a = 10\u22123 to a = 10\u22124.\nNext, we fix a = 10\u22123 and use four very different datasets to estimate p(w): enwiki (wikipedia, 3 billion tokens), poliblogs (Yano et al., 2009) (political blogs, 5 million), commoncrawl (Buck et al., 2014) (Internet crawl, 800 billion), text8 (Mahoney, 2008) (wiki subset, 1 million). Figure 2b shows performance is almost the same for all four settings.\nThe fact that our method can be applied on different types of word vectors trained on different corpora also suggests it should be useful across different domains. This is especially important for unsupervised methods, since the unlabeled data available may be collected in a different domain from the target application.\n4.2 SUPERVISED TASKS\nThe sentence embeddings obtained by our method can be used as features for downstream supervised tasks. We consider three tasks: the SICK similarity task, the SICK entailment task, and the Stanford Sentiment Treebank (SST) binary classification task (Socher et al., 2013). To highlight the representation power of the sentence embeddings learned unsupervisedly, we fix the embeddings and only learn the classifier. Setup of supervised tasks mostly follow (Wieting et al., 2016) to allow fair comparison, i.e., the classifier a linear projection followed by the classifier in (Kiros et al., 2015). The linear projection maps the sentence embeddings into 2400 dimension (the same as the skip-thought vectors), and is learned during the training. We compare our method to PP, DAN, RNN, and LSTM, which are the methods used in Section 4.1. We also compare to the skip-thought vectors (with improved training in (Lei Ba et al., 2016)).\nResults. Our method gets better or comparable performance compared to the competitors. It gets the best results for two of the tasks. This demonstrates the power of our simple method. We emphasize that our embeddings are unsupervisedly learned, while DAN, RNN, LSTM are trained with supervision. Furthermore, skip-thought vectors are much higher dimensional than ours (though projected into higher dimension, the original 300 dimensional embeddings contain all the information).\nThe advantage is not as significant as in the textual similarity tasks. This is possibly because similarity tasks rely directly upon cosine similarity, which favors our method\u2019s approach of removing the common components (which can be viewed as a form of denoising), while in supervised tasks, with the cost of some label information, the classifier can pick out the useful components and ignore the common ones.\nFinally, we speculate that our method doesn\u2019t outperform RNN\u2019s and LSTM\u2019s for sentiment tasks because (a) the word vectors \u2014and more generally the distributional hypothesis of meaning \u2014has known limitations for capturing sentiment due to the \u201cantonym problem\u201d, (b) also in our weighted average scheme, words like \u201cnot\u201d that may be important for sentiment analysis are downweighted a lot. To address (a), there is existing work on learning better word embeddings for sentiment analysis (e.g., (Maas et al., 2011)). To address (b), it is possible to design weighting scheme (or learn weights) for this specific task.\n4.3 THE EFFECT OF THE ORDER OF WORDS IN SENTENCES\nA interesting feature of our method is that it ignores the word order. This is in contrast to that RNN\u2019s and LSTM\u2019s can potentially take advantage of the word order. The fact that our method achieves better or comparable performance on these benchmarks raise the following question: is word order not important in these benchmarks? We conducted an experiment suggesting that word order does play some role.\nWe trained and tested RNN/LSTM on the supervised tasks where the words in each sentence are randomly shuffled, and the results are reported in Table 3.6 It can be observed that the performance drops noticeably. Thus our method \u2014which ignores word order\u2014must be much better at exploiting the semantics than RNN\u2019s and LSTM\u2019s. An interesting future direction is to explore if some ensemble idea can combine the advantages of both approaches.\n5 CONCLUSIONS\nThis work provided a simple approach to sentence embedding, based on the discourse vectors in the random walk model for generating text (Arora et al., 2016). It is simple and unsupervised, but achieves significantly better performance than baselines on various textual similarity tasks, and can even beat sophisticated supervised methods such as some RNN and LSTM models. The sentence embeddings obtained can be used as features in downstream supervised tasks, which also leads to better or comparable results compared to the sophisticated methods.\n6 ACKNOWLEDGEMENTS\nWe thank the reviewers for insightful comments. We also thank the authors of (Wieting et al., 2016; Bowman et al., 2015) for sharing their code or the preprocessed datasets.\nThis work was supported in part by NSF grants CCF-1527371, DMS-1317308, Simons Investigator Award, Simons Collaboration Grant, and ONRN00014- 16-1-2329. Tengyu Ma was supported in addition by Simons Award in Theoretical Computer Science and IBM PhD Fellowship.\nA DETAILS OF EXPERIMENTAL SETTING\nA.1 UNSUPERVISED TASK: TEXTUAL SIMILARITY\nThe competitors. We give a brief overview of the competitors. RNN is the classical recurrent neural network:\nht = f(WxW xt w +Whht\u22121 + b)\nwhere f is the activation, Wx,Wh and b are parameters, and xt is the t-th token in the sentence. The sentence embedding of RNN is just the hidden vector of the last token. iRNN is a special RNN with the activation being the identity, the weight matrices initialized to identity, and b initialized to zero. LSTM (Hochreiter & Schmidhuber, 1997) is a recurrent neural network architecture designed to capture long-distance dependencies. Here, the version from (Gers et al., 2002) is used.\nThe supervised methods are initialized with PARAGRAM-SL999 (PSL) vectors, and trained using the approach of (Wieting et al., 2016) on the XL section of the PPDB data (Pavlick et al., 2015) which contains about 3 million unique phrase pairs. After training, the final models can be used to generate sentence embeddings on the test data. For hyperparameter tuning they used 100k examples sampled from PPDB XXL and trained for 5 epochs. Then after finding the hyperparameters that maximize Spearmans coefficients on the Pavlick et al. PPDB task, they are trained on the entire XL section of PPDB for 10 epochs. See (Wieting et al., 2016) and related papers for more details about these methods.\nThe tfidf-GloVe method is a weighted average of the GloVe vectors, where the weights are defined by the TF-IDF scheme. More precisely, the embedding of a sentence s is\nvs = 1 |s| \u2211 w\u2208s IDFwvw (7)\nwhere IDFw is the inverse document frequency of w, and |s| denotes the number of words in the sentence. Here, the TF part of the TF-IDF scheme is taken into account by the sum over w \u2208 s. Furthermore, when computing IDFw, each sentence is viewed as a \u201cdocument\u201d:\nIDFw := log 1 +N\n1 +Nw\nwhere N is the total number of sentences and Nw is the number of sentences containing w, and 1 is added to avoid division by 0. In the experiments, we use all the textual similarity datasets to compute IDFw.\nDetailed experimental results. In the main body we present the average results for STS tasks by year. Each year there are actually 4 to 6 STS tasks, as shown in Table 4. Note that tasks with the same name in different years are actually different tasks. Here we provide the results for each tasks in Table 5. PSL+WR achieves the best results on 12 out of 22 tasks, PP and PP-proj. achieves on 3, tfidf-GloVe achieves on 2, and DAN, iRNN, and GloVe+WR achieves on 1. In general, our method improves the performance significantly compared to the unweighted average, though on rare cases such as MSRpar it can decrease the performance.\nEffects of smooth inverse frequency and common component removal. There are two key ideas in our methods: smooth inverse frequency weighting (W) and common component removal (R). It is instructive to see their effects separately. Let GloVe+W denote the embeddings using only smooth inverse frequency, and GloVe+R denote that using only common component removal. Similarly define PSL+W and PSL+R. The results for these methods are shown in Table 6. When using GloVe vectors, W alone improves the performance of the unweighted average baseline by about 5%, R alone improves by 10%, W and R together improves by 13%. When using PSL vectors, W gets 10%, R gets 10%, W and R together gets 13%. In summary, both techniques are important for obtaining significant advantage over the unweighted average.\nA.2 SUPERVISED TASKS\nSetup of supervised tasks mostly follow (Wieting et al., 2016) to allow fair comparison: the sentence embeddings are fixed and fed into some classifier which are trained. For the SICK similarity task, given a pair of sentences with embeddings vL and vR, first do a linear projection:\nhL = WpvL, hR = WpvR\nwhere Wp is of size 300 \u00d7 dp and is learned during training. dp = 2400 matches the dimension of the skip-thought vectors. Then hL and hR are used in the objective function from (Tai et al., 2015). Almost the same approach is used for the entailment task.For the sentiment task, the classifier has a fully-connected layer with a sigmoid activation followed by a softmax layer.\nRecall that our method has two steps: smooth inverse frequency weighting and common component removal. For the experiments here, we do not perform the common component removal, since it can be absorbed into the projection step. For the weighted average, the hyperparameter a is enumerated in {10\u2212i, 3 \u00d7 10\u2212i : 2 \u2264 i \u2264 3}. The other hyperparameters are enumerated as in (Wieting et al., 2016), and the same validation approach is used to select the final values.\nA.3 ADDITIONAL SUPERVISED TASKS\nHere we report the experimental results on two more datasets, comparing to known results on them.\nSNLI. The first experiment is for the 3-class classification task on the SNLI dataset (Bowman et al., 2015). To compare to the results in (Bowman et al., 2015), we used their experimental setup. In particular, we applied our method to 300 dimensional GloVe vectors and used an additional tanh neural network layer to map these 300d embeddings into 300 dimensional space, then used the code provided by the authors of (Bowman et al., 2015), trained the classifier on our 100 dimensional sentence embedding for 120 passes over the data, using their default hyperparameters. The results are shown in Table 7. Our method indeed gets slightly better performance.\nOur test accuracy is worse than those using more sophisticated models (e.g., using attention mechanism), which are typically 83% - 88%; see the website of the SNLI project7 for a summary. An interesting direction is to study whether our idea can be combined with these sophisticated models to get improved performance.\nIMDB. The second experiment is the sentiment analysis task on the IMDB dataset, studied in (Wang & Manning, 2012). Since the intended application is semi-supervised or transfer learning, we also compared performance with fewer labeled examples.\n7http://nlp.stanford.edu/projects/snli/\nOur method gets worse performance on the full dataset, but its decrease in performance is better with less labeled examples, showing the benefit of using word embeddings. Note that our sentence embeddings are unsupervised, while that in the NB-SVM method takes advantage of the labels. Another comment is that sentiment analysis appears to be the best case for Bag-Of-Word methods, whereas it may be the worst case for word embedding methods (See Table 2) due to the well-known antonymy problem \u2014distributional hypothesis fails for distinguishing \u201cgood\u201d from \u201cbad.\u201d\n", "cats": {"READ": 1.0, "DONTREAD": 0.0}, "reviews": [{"IS_META_REVIEW": true, "comments": "This paper presents a new theoretically-principled method of representing sentences as vectors. The experiments show that vectors produced by this method perform well on similarity and entailment benchmarks, surpassing some RNN-based methods too.\n\nOverall, this is an interesting empirical result, especially since the model is not order-sensitive (as far as I can tell). I would like to see some more discussion on why such a simple model does better than LSTMs at capturing similarity and entailment. Could this be an artifact of these benchmarks?"}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "A new method for sentence embedding that is simple and performs well. Important contribution that will attract attention and help move the field forward.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"SUBSTANCE": 4, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "TITLE": "Accept", "comments": "This is a good paper with an interesting probabilistic motivation for weighted bag of words models.\nThe (hopefully soon) added comparison to Wang and Manning will make it stronger. \nThough it is sad that for sufficiently large datasets, NB-SVM still works better.\n\nIn the second to last paragraph of the introduction you describe a problem of large cooccurrence counts which was already fixed by the Glove embeddings with their weighting function f.\n\nMinor comments:\n\n\"The capturing the similarities\" -- typo in line 2 of intro.\n\"Recently, (Wieting et al.,2016) learned\" -- use citet instead of parenthesized citation\n ", "SOUNDNESS_CORRECTNESS": 3, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "22 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "TITLE": "Interesting model and analysis", "comments": "This paper proposes a simple way to reweight the word embedding in the simple composition function for sentence representation. This paper also shows the connection between this new weighting scheme and some previous work.\n\nHere are some comments on technical details:\n\n- The word \"discourse\" is confusing. I am not sure whether the words \"discourse\" in \"discourse vector c_s\" and the one in \"most frequent discourse\" have the same meaning.\n- Is there any justification about $c_0$ related to syntac?\n- Not sure what thie line means: \"In fact the new model was discovered by our detecting the common component c0 in existing embeddings.\" in section \"Computing the sentence embedding\"\n- Is there any explanation about the results on sentiment in Table 2?", "SOUNDNESS_CORRECTNESS": 4, "ORIGINALITY": 4, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "19 Dec 2016", "CLARITY": 5, "REVIEWER_CONFIDENCE": 4}, {"TITLE": "Accept", "RECOMMENDATION_UNOFFICIAL": 3, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "This paper presents a new theoretically-principled method of representing sentences as vectors. The experiments show that vectors produced by this method perform well on similarity and entailment benchmarks, surpassing some RNN-based methods too.\n\nOverall, this is an interesting empirical result, especially since the model is not order-sensitive (as far as I can tell). I would like to see some more discussion on why such a simple model does better than LSTMs at capturing similarity and entailment. Could this be an artifact of these benchmarks?", "IS_ANNOTATED": true, "IS_META_REVIEW": false, "RECOMMENDATION": 8, "DATE": "14 Dec 2016", "CLARITY": 4, "REVIEWER_CONFIDENCE": 3}, {"TITLE": "Performance on the SNLI dataset", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "RECOMMENDATION_UNOFFICIAL": 3, "comments": "", "IS_ANNOTATED": true, "IS_META_REVIEW": false, "DATE": "02 Dec 2016", "CLARITY": 4}, {"OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "TITLE": "Experimental setup", "comments": "", "SOUNDNESS_CORRECTNESS": 4, "ORIGINALITY": 4, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "DATE": "02 Dec 2016", "CLARITY": 5}, {"SUBSTANCE": 4, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "TITLE": "Comparison", "comments": "", "SOUNDNESS_CORRECTNESS": 3, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "DATE": "02 Dec 2016"}, {"DATE": "17 Nov 2016", "TITLE": "a simpler baseline?", "IS_META_REVIEW": false, "comments": "We (J. Mu and P. Viswanath) thoroughly enjoyed the authors' previous work on linear algebraic structure of word senses (cf. ", "OTHER_KEYS": "Jiaqi Mu"}, {"DATE": "07 Nov 2016", "TITLE": "ICLR Paper Format", "IS_META_REVIEW": false, "comments": "Dear Authors,\n\nPlease resubmit your paper in the ICLR 2017 format with the correct font for your submission to be considered. Thank you!", "OTHER_KEYS": "Tara N Sainath"}, {"IS_META_REVIEW": true, "comments": "This paper presents a new theoretically-principled method of representing sentences as vectors. The experiments show that vectors produced by this method perform well on similarity and entailment benchmarks, surpassing some RNN-based methods too.\n\nOverall, this is an interesting empirical result, especially since the model is not order-sensitive (as far as I can tell). I would like to see some more discussion on why such a simple model does better than LSTMs at capturing similarity and entailment. Could this be an artifact of these benchmarks?"}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "A new method for sentence embedding that is simple and performs well. Important contribution that will attract attention and help move the field forward.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"SUBSTANCE": 4, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "TITLE": "Accept", "comments": "This is a good paper with an interesting probabilistic motivation for weighted bag of words models.\nThe (hopefully soon) added comparison to Wang and Manning will make it stronger. \nThough it is sad that for sufficiently large datasets, NB-SVM still works better.\n\nIn the second to last paragraph of the introduction you describe a problem of large cooccurrence counts which was already fixed by the Glove embeddings with their weighting function f.\n\nMinor comments:\n\n\"The capturing the similarities\" -- typo in line 2 of intro.\n\"Recently, (Wieting et al.,2016) learned\" -- use citet instead of parenthesized citation\n ", "SOUNDNESS_CORRECTNESS": 3, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "22 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "TITLE": "Interesting model and analysis", "comments": "This paper proposes a simple way to reweight the word embedding in the simple composition function for sentence representation. This paper also shows the connection between this new weighting scheme and some previous work.\n\nHere are some comments on technical details:\n\n- The word \"discourse\" is confusing. I am not sure whether the words \"discourse\" in \"discourse vector c_s\" and the one in \"most frequent discourse\" have the same meaning.\n- Is there any justification about $c_0$ related to syntac?\n- Not sure what thie line means: \"In fact the new model was discovered by our detecting the common component c0 in existing embeddings.\" in section \"Computing the sentence embedding\"\n- Is there any explanation about the results on sentiment in Table 2?", "SOUNDNESS_CORRECTNESS": 4, "ORIGINALITY": 4, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "19 Dec 2016", "CLARITY": 5, "REVIEWER_CONFIDENCE": 4}, {"TITLE": "Accept", "RECOMMENDATION_UNOFFICIAL": 3, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "This paper presents a new theoretically-principled method of representing sentences as vectors. The experiments show that vectors produced by this method perform well on similarity and entailment benchmarks, surpassing some RNN-based methods too.\n\nOverall, this is an interesting empirical result, especially since the model is not order-sensitive (as far as I can tell). I would like to see some more discussion on why such a simple model does better than LSTMs at capturing similarity and entailment. Could this be an artifact of these benchmarks?", "IS_ANNOTATED": true, "IS_META_REVIEW": false, "RECOMMENDATION": 8, "DATE": "14 Dec 2016", "CLARITY": 4, "REVIEWER_CONFIDENCE": 3}, {"TITLE": "Performance on the SNLI dataset", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "RECOMMENDATION_UNOFFICIAL": 3, "comments": "", "IS_ANNOTATED": true, "IS_META_REVIEW": false, "DATE": "02 Dec 2016", "CLARITY": 4}, {"OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "TITLE": "Experimental setup", "comments": "", "SOUNDNESS_CORRECTNESS": 4, "ORIGINALITY": 4, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "DATE": "02 Dec 2016", "CLARITY": 5}, {"SUBSTANCE": 4, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "TITLE": "Comparison", "comments": "", "SOUNDNESS_CORRECTNESS": 3, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "DATE": "02 Dec 2016"}, {"DATE": "17 Nov 2016", "TITLE": "a simpler baseline?", "IS_META_REVIEW": false, "comments": "We (J. Mu and P. Viswanath) thoroughly enjoyed the authors' previous work on linear algebraic structure of word senses (cf. ", "OTHER_KEYS": "Jiaqi Mu"}, {"DATE": "07 Nov 2016", "TITLE": "ICLR Paper Format", "IS_META_REVIEW": false, "comments": "Dear Authors,\n\nPlease resubmit your paper in the ICLR 2017 format with the correct font for your submission to be considered. Thank you!", "OTHER_KEYS": "Tara N Sainath"}]}
{"text": "LEARNING LOCOMOTION SKILLS USING DEEPRL: DOES\nThe use of deep reinforcement learning allows for high-dimensional state descriptors, but little is known about how the choice of action representation impacts the learning difficulty and the resulting performance. We compare the impact of four different action parameterizations (torques, muscle-activations, target joint angles, and target joint-angle velocities) in terms of learning time, policy robustness, motion quality, and policy query rates. Our results are evaluated on a gaitcycle imitation task for multiple planar articulated figures and multiple gaits. We demonstrate that the local feedback provided by higher-level action parameterizations can significantly impact the learning, robustness, and quality of the resulting policies.\n1 INTRODUCTION\nThe introduction of deep learning models to reinforcement learning (RL) has enabled policies to operate directly on high-dimensional, low-level state features. As a result, deep reinforcement learning (DeepRL) has demonstrated impressive capabilities, such as developing control policies that can map from input image pixels to output joint torques (Lillicrap et al., 2015). However, the quality and robustness often falls short of what has been achieved with hand-crafted action abstractions, e.g., Coros et al. (2011); Geijtenbeek et al. (2013). While much is known about the learning of state representations, the choice of action parameterization is a design decision whose impact is not yet well understood.\nJoint torques can be thought of as the most basic and generic representation for driving the movement of articulated figures, given that muscles and other actuation models eventually result in joint torques. However this ignores the intrinsic embodied nature of biological systems, particularly the synergy between control and biomechanics. Passive-dynamics, such as elasticity and damping from muscles and tendons, play an integral role in shaping motions: they provide mechanisms for energy storage, and mechanical impedance which generates instantaneous feedback without requiring any explicit computation. Loeb coins the term preflexes (Loeb, 1995) to describe these effects, and their impact on motion control has been described as providing intelligence by mechanics (Blickhan et al., 2007). This can also be thought of as a kind of partitioning of the computations between the control and physical system.\nIn this paper we explore the impact of four different actuation models on learning to control dynamic articulated figure locomotion: (1) torques (Tor); (2) activations for musculotendon units (MTU); (3) target joint angles for proportional-derivative controllers (PD); and (4) target joint velocities (Vel). Because Deep RL methods are capable of learning control policies for all these models, it now becomes possible to directly assess how the choice of actuation model affects the learning difficulty. We also assess the learned policies with respect to robustness, motion quality, and policy query rates. We show that action spaces which incorporate local feedback can significantly improve learning speed and performance, while still preserving the generality afforded by torque-level control. Such parameterizations also allow for more complex body structures and subjective improvements in motion quality.\nOur specific contributions are: (1) We introduce a DeepRL framework for motion imitation tasks; (2) We evaluate the impact of four different actuation models on learned control policies according to four criteria; and (3) We propose an optimization approach that combines policy learning and actuator optimization, allowing neural networks to effective control complex muscle models.\n2 BACKGROUND\nOur task will be structured as a standard reinforcement problem where an agent interacts with its environment according to a policy in order to maximize a reward signal. The policy \u03c0(s, a) = p(a|s) represents the conditional probability density function of selecting action a \u2208 A in state s \u2208 S. At each control step t, the agent observes a state st and samples an action at from \u03c0. The environment in turn responds with a scalar reward rt, and a new state s\u2032t = st+1 sampled from its dynamics p(s\u2032|s, a). For a parameterized policy \u03c0\u03b8(s, a), the goal of the agent is learn the parameters \u03b8 which maximizes the expected cumulative reward\nJ(\u03c0\u03b8) = E [ T\u2211 t=0 \u03b3trt \u2223\u2223\u2223\u2223\u2223\u03c0\u03b8 ]\nwith \u03b3 \u2208 [0, 1] as the discount factor, and T as the horizon. The gradient of the expected reward O\u03b8J(\u03c0\u03b8) can be determined according to the policy gradient theorem (Sutton et al., 2001), which provides a direction of improvement to adjust the policy parameters \u03b8.\nO\u03b8J(\u03c0\u03b8) = \u222b S d\u03b8(s) \u222b A O\u03b8log(\u03c0\u03b8(s, a))A(s, a)da ds\nwhere d\u03b8(s) = \u222b S \u2211T t=0 \u03b3\ntp0(s0)p(s0 \u2192 s|t, \u03c0\u03b8)ds0 is the discounted state distribution, where p0(s) represents the initial state distribution, and p(s0 \u2192 s|t, \u03c0\u03b8) models the likelihood of reaching state s by starting at s0 and following the policy \u03c0\u03b8(s, a) for t steps (Silver et al., 2014). A(s, a) represents a generalized advantage function. The choice of advantage function gives rise to a family of policy gradient algorithms, but in this work, we will focus on the one-step temporal difference advantage function (Schulman et al., 2015)\nA(st, at) = rt + \u03b3V (s \u2032 t)\u2212 V (st) where V (s) = E [\u2211T\nt=0 \u03b3 trt \u2223\u2223\u2223s0 = s, \u03c0\u03b8] is the state-value function, and can be defined recursively via the Bellman equation\nV (st) = E rt,s\u2032t\n[rt + \u03b3V (s \u2032 t)|st, \u03c0\u03b8]\nA parameterized value function V\u03c6(s), with parameters \u03c6, can be learned iteratively in a manner similar to Q-Learning by minimizing the Bellman loss,\nL(\u03c6) = E st,rt,s\u2032t\n[ 1\n2 (yt \u2212 V\u03c6(st))2\n] , yt = rt + \u03b3V\u03c6(s \u2032 t)\n\u03c0\u03b8 and V\u03c6 can be trained in tandem using an actor-critic framework (Konda & Tsitsiklis, 2000).\nIn this work, each policy will be represented as a gaussian distribution with a parameterized mean \u00b5\u03b8(s) and fixed covariance matrix \u03a3 = diag{\u03c32i }, where \u03c3i is manually specified for each action parameter. Actions can be sampled from the distribution by applying gaussian noise to the mean action at = \u00b5\u03b8(st) +N (0,\u03a3) The corresponding policy gradient will assume the form\nO\u03b8J(\u03c0\u03b8) = \u222b S d\u03b8(s) \u222b A O\u03b8\u00b5\u03b8(s)\u03a3 \u22121 (a\u2212 \u00b5\u03b8(s))A(s, a)da ds\nwhich can be interpreted as shifting the mean of the action distribution towards actions that lead to higher than expected rewards, while moving away from actions that lead to lower than expected rewards.\n3 TASK REPRESENTATION\n3.1 REFERENCE MOTION\nIn our task, the goal of a policy is to imitate a given reference motion {q\u2217t } which consists of a sequence of kinematic poses q\u2217t in reduced coordinates. The reference velocity q\u0307\u2217t at a given time t is approximated by finite-difference q\u0307\u2217t \u2248 q\u2217t+4t\u2212q \u2217 t\n4t . Reference motions are generated via either using a recorded simulation result from a preexisting controller (\u201cSim\u201d), or via manually-authored keyframes. Since hand-crafted reference motions may not be physically realizable, the goal is to closely reproduce a motion while satisfying physical constraints.\n3.2 STATES\nTo define the state of the agent, a feature transformation \u03a6(q, q\u0307) is used to extract a set of features from the reduced-coordinate pose q and velocity q\u0307. The features consist of the height of the root (pelvis) from the ground, the position of each link with respect to the root, and the center of mass velocity of each link. When training a policy to imitate a cyclic reference motion {q\u2217t }, knowledge of the motion phase can help simplify learning. Therefore, we augment the state features with a set of target features \u03a6(q\u2217t , q\u0307t \u2217), resulting in a combined state represented by st = (\u03a6(qt, q\u0307t),\u03a6(q\u2217t , q\u0307t \u2217)). Similar results can also be achieved by providing a single motion phase variable as a state feature, as we show in Figure 15 (supplemental material).\n3.3 ACTIONS\nWe train separate policies for each of the four actuation models, as described below. Each actuation model also has related actuation parameters, such as feedback gains for PD-controllers and musculotendon properties for MTUs. These parameters can be manually specified, as we do for the PD and Vel models, or they can be optimized for the task at hand, as for the MTU models. Table 1 provides a list of actuator parameters for each actuation model.\nTarget Joint Angles (PD): Each action represents a set of target angles q\u0302, where q\u0302i specifies the target angles for joint i. q\u0302 is applied to PD-controllers which compute torques according to \u03c4 i = kip(q\u0302 i \u2212 qi) + kid(\u02c6\u0307qi \u2212 q\u0307i), where \u02c6\u0307qi = 0, and kip and kid are manually-specified gains.\nTarget Joint Velocities (Vel): Each action specifies a set of target velocities \u02c6\u0307q which are used to compute torques according to \u03c4 i = kid(\u02c6\u0307q\ni \u2212 q\u0307i), where the gains kid are specified to be the same as those used for target angles.\nTorques (Tor): Each action directly specifies torques for every joint, and constant torques are applied for the duration of a control step. Due to torque limits, actions are bounded by manually specified limits for each joint. Unlike the other actuation models, the torque model does not require additional actuator parameters, and can thus be regarded as requiring the least amount of domain knowledge. Torque limits are excluded from the actuator parameter set as they are common for all parameterizations.\nMuscle Activations (MTU): Each action specifies activations for a set of musculotendon units (MTU). Detailed modeling and implementation information are available in Wang et al. (2012). Each MTU is modeled as a contractile element (CE) attached to a serial elastic element (SE) and parallel elastic element (PE). The force exerted by the MTU can be calculated according to FMTU = FSE = FCE + FPE . Both FSE and FPE are modeled as passive springs, while FCE is actively controlled according to FCE = aMTUF0fl(lCE)fv(vCE), with aMTU being the muscle activation, F0 the maximum isometric force, lCE and vCE being the length and velocity of the contractile element. The functions fl(lCE) and fv(vCE) represent the force-length and force-velocity relationships, modeling the variations in the maximum force that can be exerted by a muscle as a function of its length and contraction velocity. Analytic forms are available in Geyer et al. (2003). Activations are bounded between [0, 1]. The length of each contractile element lCE are included as state features. To simplify control and reduce the number of internal state parameters per MTU, the policies directly control muscle activations instead of indirectly through excitations (Wang et al., 2012).\n3.4 REWARD\nThe reward function consists of a weighted sum of terms that encourage the policy to track a reference motion.\nr = wposerpose + wvelrvel + wendrend + wrootrroot + wcomrcom\nwpose = 0.5, wvel = 0.05, wend = 0.15, wroot = 0.1, wcom = 0.2\nDetails of each term are available in the supplemental material. rpose penalizes deviation of the character pose from the reference pose, and rvel penalizes deviation of the joint velocities. rend and rroot accounts for the position error of the end-effectors and root. rcom penalizes deviations in the center of mass velocity from that of the reference motion.\n3.5 INITIAL STATE DISTRIBUTION\nWe design the initial state distribution, p0(s), to sample states uniformly along the reference trajectory. At the start of each episode, q\u2217 and q\u0307\u2217 are sampled from the reference trajectory, and used to initialize the pose and velocity of the agent. This helps guide the agent to explore states near the target trajectory.\n4 ACTOR-CRITIC LEARNING ALGORITHM\nInstead of directly using the temporal difference advantage function, we adapt a positive temporal difference (PTD) update as proposed by Van Hasselt (2012).\nA(s, a) = I [\u03b4 > 0] =\n{ 1, \u03b4 > 0\n0, otherwise\n\u03b4 = r + \u03b3V (s\u2032)\u2212 V (s) Unlike more conventional policy gradient methods, PTD is less sensitive to the scale of the advantage function and avoids instabilities that can result from negative TD updates. For a Gaussian policy, a negative TD update moves the mean of the distribution away from an observed action, effectively shifting the mean towards an unknown action that may be no better than the current mean action (Van Hasselt, 2012). In expectation, these updates converges to the true policy gradient, but for stochastic estimates of the policy gradient, these updates can cause the agent to adopt undesirable behaviours which affect subsequent experiences collected by the agent. Furthermore, we incorporate experience replay, which has been demonstrated to improve stability when training neural network policies with Q-learning in discrete action spaces. Experience replay often requires off-policy methods, such as importance weighting, to account for differences between the policy being trained and the behavior policy used to generate experiences (Wawrzyn\u0301Ski & Tanwani, 2013). However, we have not found importance weighting to be beneficial for PTD.\nStochastic policies are used during training for exploration, while deterministic policy are deployed for evaluation at runtime. The choice between a stochastic and deterministic policy can be specified by the addition of a binary indicator variable \u03bb \u2208 [0, 1]\nat = \u00b5\u03b8(st) + \u03bbN (0,\u03a3)\nwhere \u03bb = 1 corresponds to a stochastic policy with exploration noise, and \u03bb = 0 corresponds to a deterministic policy that always selects the mean of the distribution. Noise from a stochastic policy will result in a state distribution that differs from that of the deterministic policy at runtime. To imitate this discrepancy, we incorporate -greedy exploration in addition to the original Gaussian exploration. During training, \u03bb is determined by a Bernoulli random variable \u03bb \u223c Ber( ), where \u03bb = 1 with probability \u2208 [0, 1]. The exploration rate is annealed linearly from 1 to 0.2 over 500k iterations, which slowly adjusts the state distribution encountered during training to better resemble the distribution at runtime. Since the policy gradient is defined for stochastic policies, only tuples recorded with exploration noise (i.e. \u03bb = 1) can be used to update the actor, while the critic can be updated using all tuples.\nTraining proceeds episodically, where the initial state of each episode is sampled from p0(s), and the episode duration is drawn from an exponential distribution with a mean of 2s. To discourage falling, an episode will also terminate if any part of the character\u2019s trunk makes contact with the ground for an extended period of time, leaving the agent with zero reward for all subsequent steps. Algorithm 1 in the supplemental material summarizes the complete learning process.\nMTU Actuator Optimization: Actuation models such as MTUs are defined by further parameters whose values impact performance (Geijtenbeek et al., 2013). Geyer et al. (2003) uses existing anatomical estimates for humans to determine MTU parameters, but such data is not be available for more arbitrary creatures. Alternatively, Geijtenbeek et al. (2013) uses covariance matrix adaptation (CMA), a derivative-free evolutionary search strategy, to simultaneously optimize MTU and policy parameters. This approach is limited to policies with reasonably low dimensional parameter spaces, and is thus ill-suited for neural network models with hundreds of thousands of parameters. To avoid manual-tuning of actuator parameters, we propose a heuristic approach that alternates between policy learning and actuator optimization, as detailed in the supplemental material.\n5 RESULTS\nThe motions are best seen in the supplemental video https://youtu.be/L3vDo3nLI98. We evaluate the action parameterizations by training policies for a simulated 2D biped, dog, and raptor as shown in Figure 1. Depending on the agent and the actuation model, our systems have 58\u2013214 state dimensions, 6\u201344 action dimensions, and 0\u2013282 actuator parameters, as summarized in Table 3 (supplemental materials). The MTU models have at least double the number of action parameters because they come in antagonistic pairs. As well, additional MTUs are used for the legs to more accurately reflect bipedal biomechanics. This includes MTUs that span multiple joints.\nEach policy is represented by a three layer neural network, as illustrated in Figure 8 (supplemental material) with 512 and 256 fully-connected units, followed by a linear output layer where the number of output units vary according to the number of action parameters for each character and actuation model. ReLU activation functions are used for both hidden layers. Each network has approximately 200k parameters. The value function is represented by a similar network, except having a single linear output unit. The policies are queried at 60Hz for a control step of about 0.0167s. Each network is randomly initialized and trained for about 1 million iterations, requiring 32 million tuples, the equivalent of approximately 6 days of simulated time. Each policy requires about 10 hours for the biped, and 20 hours for the raptor and dog on an 8-core Intel Xeon E5-2687W.\nOnly the actuator parameters for MTUs are optimized with Algorithm 2, since the parameters for the other actuation models are few and reasonably intuitive to determine. The initial actuator parameters \u03c80 are manually specified, while the initial policy parameters \u03b80 are randomly initialized. Each pass optimizes \u03c8 using CMA for 250 generations with 16 samples per generation, and \u03b8 is trained for 250k iterations. Parameters are initialized with values from the previous pass. The expected value of each CMA sample of \u03c8 is estimated using the average cumulative reward over 16 rollouts with a duration of 10s each. Separate MTU parameters are optimized for each character and motion. Each set of parameters is optimized for 6 passes following Algorithm 2, requiring approximately 50 hours. Figure 5 illustrates the performance improvement per pass. Figure 6 compares the performance of MTUs before and after optimization. For most examples, the optimized actuator parameters significantly improve learning speed and final performance. For the sake of comparison, after a set of actuator parameters has been optimized, a new policy is retrained with the new actuator parameters and its performance compared to the other actuation models.\nPolicy Performance and Learning Speed: Figure 2 shows learning curves for the policies and the performance of the final policies are summarized in Table 4. Performance is evaluated using the normalized cumulative reward (NCR), calculated from the average cumulative reward over 32 episodes with lengths of 10s, and normalized by the maximum and minimum cumulative reward possible for each episode. No discounting is applied when calculating the NCR. The initial state of each episode is sampled from the reference motion according to p(s0). To compare learning speeds, we use the normalized area under each learning curve (AUC) as a proxy for the learning speed of a particular actuation model, where 0 represents the worst possible performance and no progress during training, and 1 represents the best possible performance without requiring training.\nPD performs well across all examples, achieving comparable-to-the-best performance for all motions. PD also learns faster than the other parameterizations for 5 of the 7 motions. The final performance of Tor is among the poorest for all the motions. Differences in performance appear more pronounced as characters become more complex. For the simple 7-link biped, most parameterizations achieve similar performance. However, for the more complex dog and raptor, the performance of Tor policies deteriorate with respect to other policies such as PD and Vel. MTU policies often exhibited the slowest learning speed, which may be a consequence of the higher dimensional action spaces, i.e., requiring antagonistic muscle pairs, and complex muscle dynamics. Nonetheless, once optimized, the MTU policies produce more natural motions and responsive behaviors as compared to other parameterizations. We note that the naturalness of motions is not well captured by the reward, since it primarily gauges similarity to the reference motion, which may not be representative of natural responses when perturbed from the nominal trajectory. A sensitivity analysis of the policies\u2019 performance to variations in network architecture and hyperparameters are available in the supplemental material.\nPolicy Robustness: To evaluate robustness, we recorded the NCR achieved by each policy when subjected to external perturbations. The perturbations assume the form of random forces applied\nto the trunk of the characters. Figure 3 illustrates the performance of the policies when subjected to perturbations of different magnitudes. The magnitude of the forces are constant, but direction varies randomly. Each force is applied for 0.1 to 0.4s, with 1 to 4s between each perturbation. Performance is estimated using the average over 128 episodes of length 20s each. For the biped walk, the Tor policy is significantly less robust than those for the other types of actions, while the MTU policy is the least robust for the raptor run. Overall, the PD policies are among the most robust for all the motions. In addition to external forces, we also evaluate robustness over randomly generated terrain consisting of bumps with varying heights and slopes with varying steepness. We evaluate the performance on irregular terrain (Figure 12, supplemental material). There are few discernible patterns for this test. The Vel and MTU policies are significantly worse than the Tor and PD policies for the dog bound on the bumpy terrain. The unnatural jittery behavior of the dog Tor policy proves to be surprisingly robust for this scenario. We suspect that the behavior prevents the trunk from contacting the ground for extended periods for time, and thereby escaping our system\u2019s fall detection.\nQuery Rate: Figure 4 compares the performance of different parameterizations for different policy query rates. Separate policies are trained with queries of 15Hz, 30Hz, 60Hz, and 120Hz. Actuation models that incorporate low-level feedback such as PD and Vel, appear to cope more effectively to lower query rates, while the Tor degrades more rapidly at lower query rates. It is not yet obvious to us why MTU policies appear to perform better at lower query rates and worse at higher rates. Lastly, Figure 14 shows the policy outputs as a function of time for the four actuation models, for a particular joint, as well as showing the resulting joint torque. Interestingly, the MTU action is visibly smoother than the other actions and results in joint torques profiles that are smoother than those seen for PD and Vel.\n6 RELATED WORK\nDeepRL has driven impressive recent advances in learning motion control, i.e., solving for continuous-action control problems using reinforcement learning. All four of the actions types that we explore have seen previous use in the machine learning literature. Wawrzyn\u0301Ski & Tanwani (2013) use an actor-critic approach with experience replay to learn skills for an octopus arm (actuated by a simple muscle model) and a planar half cheetah (actuated by joint-based PD-controllers).\nRecent work on deterministic policy gradients (Lillicrap et al., 2015) and on RL benchmarks, e.g., OpenAI Gym, generally use joint torques as the action space, as do the test suites in recent work (Schulman et al., 2015) on using generalized advantage estimation. Other recent work uses: the PR2 effort control interface as a proxy for torque control (Levine et al., 2015); joint velocities (Gu et al., 2016); velocities under an implicit control policy (Mordatch et al., 2015); or provide abstract actions (Hausknecht & Stone, 2015). Our learning procedures are based on prior work using actorcritic approaches with positive temporal difference updates (Van Hasselt, 2012).\nWork in biomechanics has long recognized the embodied nature of the control problem and the view that musculotendon systems provide \u201cpreflexes\u201d (Loeb, 1995) that effectively provide a form intelligence by mechanics (Blickhan et al., 2007), as well as allowing for energy storage. The control strategies for physics-based character simulations in computer animation also use all the forms of actuation that we evaluate in this paper. Representative examples include quadratic programs that solve for joint torques (de Lasa et al., 2010), joint velocities for skilled bicycle stunts (Tan et al., 2014), muscle models for locomotion (Wang et al., 2012; Geijtenbeek et al., 2013), mixed use of feed-forward torques and joint target angles (Coros et al., 2011), and joint target angles computed by learned linear (time-indexed) feedback strategies (Liu et al., 2016). Lastly, control methods in robotics use a mix of actuation types, including direct-drive torques (or their virtualized equivalents), series elastic actuators, PD control, and velocity control. These methods often rely heavily on modelbased solutions and thus we do not describe these in further detail here.\n7 CONCLUSIONS\nOur experiments suggest that action parameterizations that include basic local feedback, such as PD target angles, MTU activations, or target velocities, can improve policy performance and learning speed across different motions and character morphologies. Such models more accurately reflect the embodied nature of control in biomechanical systems, and the role of mechanical components in shaping the overall dynamics of motions and their control. The difference between low-level and high-level action parameterizations grow with the complexity of the characters, with high-level parameterizations scaling more gracefully to complex characters. As a caveat, there may well be tasks, such as impedance control, where lower-level action parameterizations such as Tor may prove advantageous. We believe that no single action parameterization will be the best for all problems. However, since objectives for motion control problems are often naturally expressed in terms of kinematic properties, higher-level actions such as target joint angles and velocities may be effective for a wide variety of motion control problems. We hope that our work will help open discussions around the choice of action parameterizations.\nOur results have only been demonstrated on planar articulated figure simulations; the extension to 3D currently remains as future work. Furthermore, our current torque limits are still large as compared to what might be physically realizable. Tuning actuator parameters for complex actuation models such as MTUs remains challenging. Though our actuator optimization technique is able to improve performance as compared to manual tuning, the resulting parameters may still not be optimal for the desired task. Therefore, our comparisons of MTUs to other action parameterizations may not be reflective of the full potential of MTUs with more optimal actuator parameters. Furthermore, our actuator optimization currently tunes parameters for a specific motion, rather than a larger suite of motions, as might be expected in nature.\nSince the reward terms are mainly calculated according to joint positions and velocities, it may seem that it is inherently biased in favour of PD and Vel. However, the real challenges for the control policies lie elsewhere, such as learning to compensate for gravity and ground-reaction forces, and learning foot-placement strategies that are needed to maintain balance for the locomotion gaits. The reference pose terms provide little information on how to achieve these hidden aspects of motion control that will ultimately determine the success of the locomotion policy. While we have yet to provide a concrete answer for the generalization of our results to different reward functions, we believe that the choice of action parameterization is a design decision that deserves greater attention regardless of the choice of reward function.\nFinally, it is reasonable to expect that evolutionary processes would result in the effective co-design of actuation mechanics and control capabilities. Developing optimization and learning algorithms to allow for this kind of co-design is a fascinating possibility for future work.\n", "cats": {"READ": 1.0, "DONTREAD": 0.0}, "reviews": [{"IS_META_REVIEW": true, "comments": "This paper addresses a question that is often overlooked in reinforcement learning or locomotion experiment.\n\nMy biggest point of critique is that it's difficult to draw conclusions or reason beyond the results of the experiments. \nThe authors only consider a single neural network architecture and a single reward function. For example, is the torque controller limited by the policy network?  \nMy suggestion is to vary the number of neurons or show that the same results hold for a different state representation (e.g. trained on pixel data). In the paper's current form, the term \"DeepRL\" seems arbitrary.\n\nOn the positive side, the paper is well-structured and easy to read. The experiments are sound, clear and easy to interpret. \nIt's definitely an interesting line of work and beyond the extension to 3D, I would argue that considering more realistic physical constraints (e.g. actuator constraints, communication delays etc. on real robots) could greatly improve the impact of this work."}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "After reading the paper and the reviews, I believe that the paper presents a solid contribution and a detailed empirical exploration of the choice of action space representations for continuous control trajectory tracking tasks, but has limited relevance to the ICLR audience in its present form.\n \n The conclusion that PD controllers with learned gains provide improved learning speed and sometimes better final results than the \"default\" joint torque representation is intriguing and has some implications for future work on trajectory tracking for simulated articulated rigid body characters, but it's unclear from the current results what conclusion there is to take away for general algorithm or model design. Since the only evaluation is on trajectory tracking, it's also not even clear (as pointed out by other reviewers) to what degree these results will generalize to other tasks. In fact, a contrarian view might be that PD tracking is specifically a good fit for trajectory tracking with a known reference trajectory, but might be a poor fit for accomplishing a particular task, where more open-loop behaviors might be more optimal. The passive compliance of MTUs is also shown to often not be beneficial, but it's again not clear whether this might in fact be an artifact of the trajectory tracking task.\n \n But perhaps more problematically, the primary conclusions of the paper are of limited relevance when it comes to design of algorithms or models (or at least, the authors don't discuss this), but are more relevant for practitioners interested in applying deep RL for learning controllers for articulated rigid body systems such as robots or virtual characters. As such, it's not clear what fraction of the ICLR audience will find the paper relevant to their interests.\n \n I would strongly encourage the authors to continue their research: there is a lot to like about the present paper, including an elegant reinforcement learning methodology, rigorous empirical results for the specific case of trajectory following, and some very nice videos and examples.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"DATE": "03 Jan 2017", "TITLE": "New Revision", "IS_META_REVIEW": false, "comments": "We would like to thank the reviewers again for their feedback and suggestions. We have revised the paper to better address the points raised in the reviews. To help track the changes, we have highlighted them in blue. On page 15 of the supplemental material, you will find a new set experiments comparing performance across different initializations of the network, different network architectures, and sensitivity to the amount of exploration noise applied during training. We hope these new additions will help to address some of the previously raised concerns.\n\nWe look forward to receiving additional feedback from the reviewers.\nThank you,", "OTHER_KEYS": "Xue Bin Peng"}, {"DATE": "21 Dec 2016", "TITLE": "Reply to reviews", "IS_META_REVIEW": false, "comments": "Thank you for the feedback and comments in the reviews + questions.\nPlease find below a summary of our further reflections, which we will be incorporating into the paper.\n\nAdditional experiments\n\nWe are currently performing several additional experiments, which we will add to the next version of the paper:\n- Analysis of variance of performance for multiple runs of training (Anon Rev 2)\n- Sensitivity to exploration noise, i.e., covariance matrix, (Anon Rev 2)\n- Impact of network architecture (Anon Rev 1)\n\nRe:   Relevance of paper to ICLR audience\n\n1) While much is known about learning input (state) representations, much less is known about the extent to which output (action) representations matter for continuous control policies, and whether useful \u201coutput representations\u201d can be learned. This paper doesn\u2019t provide all the answers, but it shows that output representations do matter (for locomotion): they significantly impact the learning rate, robustness, overall performance, and policy query rate.\n\n2) Many methods for continuous action spaces use physics-based simulations of locomotion as one of their key examples. However, the use of torques as the defacto unit of action is rather arbitrary; it ignores the notion that the passive properties of  actuation mechanisms, e.g., muscles, can make a significant contribution to motion control, i.e., \u201cintelligence by mechanics\u201d (Blickhan et al., 2007).This can be thought of as a kind of partitioning of the computations between the control and the physical system. Our paper provides a first look at the impact of such \u201cpartitioning\u201d.\n\n3) A question that we would like to draw more attention to is: should musculo-tendon systems (as seen in nature) and other actuation details be considered as part of the control policy or part of the environment?  I.e., can they be safely ignored because the control policy would \u201clearn the useful aspects\u201d anyhow? \n\nRe: generalization of results\n\nHere are our thoughts on this;  we will update the paper to reflect these.\n\n1) Admittedly, we do not provide a concrete answer for the generalization to arbitrary objective functions, and we will clearly state this in the paper. However physics-based locomotion is commonly used as an example in continuous-action RL work. While we do optimize for only one reward function (imitation), we do explore results for different characters, different motions, different query rates, and overall policy robustness. We believe that these are all critical dimensions for testing capabilities and generalization for locomotion control. The imitation objective can also be used as a motion prior and so it is likely to be useful more generally.  It further allows us to produce more natural motions (subjectively speaking) than previously seen for similar work. We hope that the capability of learning more natural locomotion policies, even if this arrives in part via the objective function rather than a theoretical advancement, will be of broad interest to the ICLR community.\n\n2) We believe that no single action parameterization will be the best for all problems. However, since objectives for motion control problems are often naturally expressed in terms of kinematic properties, higher-level actions such as target joint angles and velocities may be effective for a wide variety of motion control problems. We hope that our work will help open discussions around the choice of action parameterizations.\n\nRe: specificity of results to the use of a reference pose cost\n\nWhile the reward terms are mainly calculated according to joint positions and velocities, the real challenge for the control policy lies with: (a) learning to compensate for various state-dependent forces, such as gravity and ground-reaction forces, and (b) learning strategies such as foot-placement that are needed to maintain balance for all the locomotion gaits. The reference pose terms provides no information on how to achieve these \u201chidden\u201d aspects of motion control that will ultimately determine the success of the locomotion policy. There is also only a weak correlation between the action space and the actual reference poses for any of the action spaces; the action space trajectories illustrated in Figure 11 are all quite different from the actual right hip angle trajectory (not shown, but it varies smoothly over time).\n\nRe: considering more realistic physical constraints (e.g. actuator constraints, communication delays etc. on real robots) could greatly improve the impact of this work. (Anon Rev 1)\n\nWe enforce torque limits and joint limits, which are shared across all actuation models. We do not currently include activation time delays for any of the models.\n", "OTHER_KEYS": "Xue Bin Peng"}, {"TITLE": "Nice paper comparing control parameterisations in simulated locomotion domains.", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "The paper is straightforward, easy to read, and has clear results. \n\nSince all these parameterisations end up outputting torques, it seems like there shouldn't be much difference between them. There is a known function that convert from one representation to another (or at least to torques). Is it not possible that the only reason proportional control is a little better is that the tracking cost is a function of positions?\n\nWould we get the same result if there was no reference-pose cost, only a locomotion cost?\n\nWould we get the same result if the task was to spin a top? My guess is no. \n\nThis work is interesting, but not likely to generalise to other scenarios, and in that sense is rather limited.\n\nThe video is nice.\n", "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "20 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "Review", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "Paper studies deep reinforcement learning paradigm for controlling high dimensional characters. Experiments compare the effect different control parameterizations (torques, muscle-activations, PD control with target joint positions and target joint velocities) have on the performance of reinforcement learning and optimized control policies. Evaluated are different planer gate cycle trajectories. It is illustrated that more abstract parameterizations are in fact better and result in more robust and higher quality policies. \n\n> Significance & Originality:\n\nThe explored parameterizations are relatively standard in humanoid control. The real novelty is systematic evaluation of the various parameterizations. I think this type of study is important and insightful. However, the findings are very specific to the problem and specific tested architecture. Its not clear that findings will transferable to other networks on other control problems/domains. As such for the ICLR community, this may have limited breadth and perhaps would have broader appeal in robotics / graphics community. \n\n> Clarity:\n\nThe paper is well written and is pretty easy to understand for someone who has some background with constrained multi-body simulation and control. \n\n> Experiments:\n\nExperimental validation is lacking somewhat in my opinion. Given that this is a fundamentally experimental paper, I would have liked to see more analysis of sensitivity to various parameters and analysis of variance of performance when policy is optimized multiple times.", "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "17 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "Interesting topic, sound experiments, hard to draw conclusions.", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "This paper addresses a question that is often overlooked in reinforcement learning or locomotion experiment.\n\nMy biggest point of critique is that it's difficult to draw conclusions or reason beyond the results of the experiments. \nThe authors only consider a single neural network architecture and a single reward function. For example, is the torque controller limited by the policy network?  \nMy suggestion is to vary the number of neurons or show that the same results hold for a different state representation (e.g. trained on pixel data). In the paper's current form, the term \"DeepRL\" seems arbitrary.\n\nOn the positive side, the paper is well-structured and easy to read. The experiments are sound, clear and easy to interpret. \nIt's definitely an interesting line of work and beyond the extension to 3D, I would argue that considering more realistic physical constraints (e.g. actuator constraints, communication delays etc. on real robots) could greatly improve the impact of this work.\n", "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 3}, {"DATE": "03 Dec 2016", "TITLE": "Sensitivity to the manually specified parameters", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2"}, {"DATE": "02 Dec 2016", "TITLE": "Influence of objective and reward functions?", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1"}, {"IS_META_REVIEW": true, "comments": "This paper addresses a question that is often overlooked in reinforcement learning or locomotion experiment.\n\nMy biggest point of critique is that it's difficult to draw conclusions or reason beyond the results of the experiments. \nThe authors only consider a single neural network architecture and a single reward function. For example, is the torque controller limited by the policy network?  \nMy suggestion is to vary the number of neurons or show that the same results hold for a different state representation (e.g. trained on pixel data). In the paper's current form, the term \"DeepRL\" seems arbitrary.\n\nOn the positive side, the paper is well-structured and easy to read. The experiments are sound, clear and easy to interpret. \nIt's definitely an interesting line of work and beyond the extension to 3D, I would argue that considering more realistic physical constraints (e.g. actuator constraints, communication delays etc. on real robots) could greatly improve the impact of this work."}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "After reading the paper and the reviews, I believe that the paper presents a solid contribution and a detailed empirical exploration of the choice of action space representations for continuous control trajectory tracking tasks, but has limited relevance to the ICLR audience in its present form.\n \n The conclusion that PD controllers with learned gains provide improved learning speed and sometimes better final results than the \"default\" joint torque representation is intriguing and has some implications for future work on trajectory tracking for simulated articulated rigid body characters, but it's unclear from the current results what conclusion there is to take away for general algorithm or model design. Since the only evaluation is on trajectory tracking, it's also not even clear (as pointed out by other reviewers) to what degree these results will generalize to other tasks. In fact, a contrarian view might be that PD tracking is specifically a good fit for trajectory tracking with a known reference trajectory, but might be a poor fit for accomplishing a particular task, where more open-loop behaviors might be more optimal. The passive compliance of MTUs is also shown to often not be beneficial, but it's again not clear whether this might in fact be an artifact of the trajectory tracking task.\n \n But perhaps more problematically, the primary conclusions of the paper are of limited relevance when it comes to design of algorithms or models (or at least, the authors don't discuss this), but are more relevant for practitioners interested in applying deep RL for learning controllers for articulated rigid body systems such as robots or virtual characters. As such, it's not clear what fraction of the ICLR audience will find the paper relevant to their interests.\n \n I would strongly encourage the authors to continue their research: there is a lot to like about the present paper, including an elegant reinforcement learning methodology, rigorous empirical results for the specific case of trajectory following, and some very nice videos and examples.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"DATE": "03 Jan 2017", "TITLE": "New Revision", "IS_META_REVIEW": false, "comments": "We would like to thank the reviewers again for their feedback and suggestions. We have revised the paper to better address the points raised in the reviews. To help track the changes, we have highlighted them in blue. On page 15 of the supplemental material, you will find a new set experiments comparing performance across different initializations of the network, different network architectures, and sensitivity to the amount of exploration noise applied during training. We hope these new additions will help to address some of the previously raised concerns.\n\nWe look forward to receiving additional feedback from the reviewers.\nThank you,", "OTHER_KEYS": "Xue Bin Peng"}, {"DATE": "21 Dec 2016", "TITLE": "Reply to reviews", "IS_META_REVIEW": false, "comments": "Thank you for the feedback and comments in the reviews + questions.\nPlease find below a summary of our further reflections, which we will be incorporating into the paper.\n\nAdditional experiments\n\nWe are currently performing several additional experiments, which we will add to the next version of the paper:\n- Analysis of variance of performance for multiple runs of training (Anon Rev 2)\n- Sensitivity to exploration noise, i.e., covariance matrix, (Anon Rev 2)\n- Impact of network architecture (Anon Rev 1)\n\nRe:   Relevance of paper to ICLR audience\n\n1) While much is known about learning input (state) representations, much less is known about the extent to which output (action) representations matter for continuous control policies, and whether useful \u201coutput representations\u201d can be learned. This paper doesn\u2019t provide all the answers, but it shows that output representations do matter (for locomotion): they significantly impact the learning rate, robustness, overall performance, and policy query rate.\n\n2) Many methods for continuous action spaces use physics-based simulations of locomotion as one of their key examples. However, the use of torques as the defacto unit of action is rather arbitrary; it ignores the notion that the passive properties of  actuation mechanisms, e.g., muscles, can make a significant contribution to motion control, i.e., \u201cintelligence by mechanics\u201d (Blickhan et al., 2007).This can be thought of as a kind of partitioning of the computations between the control and the physical system. Our paper provides a first look at the impact of such \u201cpartitioning\u201d.\n\n3) A question that we would like to draw more attention to is: should musculo-tendon systems (as seen in nature) and other actuation details be considered as part of the control policy or part of the environment?  I.e., can they be safely ignored because the control policy would \u201clearn the useful aspects\u201d anyhow? \n\nRe: generalization of results\n\nHere are our thoughts on this;  we will update the paper to reflect these.\n\n1) Admittedly, we do not provide a concrete answer for the generalization to arbitrary objective functions, and we will clearly state this in the paper. However physics-based locomotion is commonly used as an example in continuous-action RL work. While we do optimize for only one reward function (imitation), we do explore results for different characters, different motions, different query rates, and overall policy robustness. We believe that these are all critical dimensions for testing capabilities and generalization for locomotion control. The imitation objective can also be used as a motion prior and so it is likely to be useful more generally.  It further allows us to produce more natural motions (subjectively speaking) than previously seen for similar work. We hope that the capability of learning more natural locomotion policies, even if this arrives in part via the objective function rather than a theoretical advancement, will be of broad interest to the ICLR community.\n\n2) We believe that no single action parameterization will be the best for all problems. However, since objectives for motion control problems are often naturally expressed in terms of kinematic properties, higher-level actions such as target joint angles and velocities may be effective for a wide variety of motion control problems. We hope that our work will help open discussions around the choice of action parameterizations.\n\nRe: specificity of results to the use of a reference pose cost\n\nWhile the reward terms are mainly calculated according to joint positions and velocities, the real challenge for the control policy lies with: (a) learning to compensate for various state-dependent forces, such as gravity and ground-reaction forces, and (b) learning strategies such as foot-placement that are needed to maintain balance for all the locomotion gaits. The reference pose terms provides no information on how to achieve these \u201chidden\u201d aspects of motion control that will ultimately determine the success of the locomotion policy. There is also only a weak correlation between the action space and the actual reference poses for any of the action spaces; the action space trajectories illustrated in Figure 11 are all quite different from the actual right hip angle trajectory (not shown, but it varies smoothly over time).\n\nRe: considering more realistic physical constraints (e.g. actuator constraints, communication delays etc. on real robots) could greatly improve the impact of this work. (Anon Rev 1)\n\nWe enforce torque limits and joint limits, which are shared across all actuation models. We do not currently include activation time delays for any of the models.\n", "OTHER_KEYS": "Xue Bin Peng"}, {"TITLE": "Nice paper comparing control parameterisations in simulated locomotion domains.", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "The paper is straightforward, easy to read, and has clear results. \n\nSince all these parameterisations end up outputting torques, it seems like there shouldn't be much difference between them. There is a known function that convert from one representation to another (or at least to torques). Is it not possible that the only reason proportional control is a little better is that the tracking cost is a function of positions?\n\nWould we get the same result if there was no reference-pose cost, only a locomotion cost?\n\nWould we get the same result if the task was to spin a top? My guess is no. \n\nThis work is interesting, but not likely to generalise to other scenarios, and in that sense is rather limited.\n\nThe video is nice.\n", "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "20 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "Review", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "Paper studies deep reinforcement learning paradigm for controlling high dimensional characters. Experiments compare the effect different control parameterizations (torques, muscle-activations, PD control with target joint positions and target joint velocities) have on the performance of reinforcement learning and optimized control policies. Evaluated are different planer gate cycle trajectories. It is illustrated that more abstract parameterizations are in fact better and result in more robust and higher quality policies. \n\n> Significance & Originality:\n\nThe explored parameterizations are relatively standard in humanoid control. The real novelty is systematic evaluation of the various parameterizations. I think this type of study is important and insightful. However, the findings are very specific to the problem and specific tested architecture. Its not clear that findings will transferable to other networks on other control problems/domains. As such for the ICLR community, this may have limited breadth and perhaps would have broader appeal in robotics / graphics community. \n\n> Clarity:\n\nThe paper is well written and is pretty easy to understand for someone who has some background with constrained multi-body simulation and control. \n\n> Experiments:\n\nExperimental validation is lacking somewhat in my opinion. Given that this is a fundamentally experimental paper, I would have liked to see more analysis of sensitivity to various parameters and analysis of variance of performance when policy is optimized multiple times.", "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "17 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "Interesting topic, sound experiments, hard to draw conclusions.", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "This paper addresses a question that is often overlooked in reinforcement learning or locomotion experiment.\n\nMy biggest point of critique is that it's difficult to draw conclusions or reason beyond the results of the experiments. \nThe authors only consider a single neural network architecture and a single reward function. For example, is the torque controller limited by the policy network?  \nMy suggestion is to vary the number of neurons or show that the same results hold for a different state representation (e.g. trained on pixel data). In the paper's current form, the term \"DeepRL\" seems arbitrary.\n\nOn the positive side, the paper is well-structured and easy to read. The experiments are sound, clear and easy to interpret. \nIt's definitely an interesting line of work and beyond the extension to 3D, I would argue that considering more realistic physical constraints (e.g. actuator constraints, communication delays etc. on real robots) could greatly improve the impact of this work.\n", "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 3}, {"DATE": "03 Dec 2016", "TITLE": "Sensitivity to the manually specified parameters", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2"}, {"DATE": "02 Dec 2016", "TITLE": "Influence of objective and reward functions?", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1"}]}
{"text": "LEARNING INVARIANT REPRESENTATIONS OF PLANAR CURVES\n1 INTRODUCTION\nThe discussion on invariance is a strong component of the solutions to many classical problems in numerical differential geometry. A typical example is that of planar shape analysis where one desires to have a local function of the contour which is invariant to rotations, translations and reflections like the Euclidean curvature. This representation can be used to obtain correspondence between the shapes and also to compare and classify them. However, the numerical construction of such functions from discrete sampled data is non-trivial and requires robust numerical techniques for their stable and efficient computation.\nConvolutional neural networks have been very successful in recent years in solving problems in image processing, recognition and classification. Efficient architectures have been studied and developed to extract semantic features from images invariant to a certain class or category of transformations. Coupled with efficient optimization routines and more importantly, a large amount of data, a convolutional neural network can be trained to construct invariant representations and semantically significant features of images as well as other types of data such as speech and language. It is widely acknowledged that such networks have superior representational power compared to more principled methods with more handcrafted features such as wavelets, Fourier methods, kernels etc. which are not optimal for more semantic data processing tasks.\nIn this paper we connect two seemingly different fields: convolutional neural network based metric learning methods and numerical differential geometry. The results we present are the outcome of investigating the question: \u201dCan metric learning methods be used to construct invariant geometric quantities?\u201d By training with a Siamese configuration involving only positive and negative examples of Euclidean transformations, we show that the network is able to train for an invariant geometric function of the curve which can be contrasted with a theoretical quantity: Euclidean curvature. An example of each can be seen Figure 1. We compare the learned invariant functions with axiomatic counterparts and provide a discussion on their relationship. Analogous to principled constructions like curvature-scale space methods and integral invariants, we develop a multi-scale representation using a data-dependent learning based approach. We show that network models are able to construct geometric invariants that are numerically more stable and robust than these more principled approaches. We contrast the computational work-flow of a typical numerical geometry pipeline with that of the convolutional neural network model and develop a relationship among them highlighting important geometric ideas.\nIn Section 2 we begin by giving a brief summary of the theory and history of invariant curve representations. In Section 3 we explain our main contribution of casting the problem into the form which\nenables training a convolutional neural network for generating invariant signatures to the Euclidean and Similarity group transformations. Section 4 provides a discussion on developing a multi-scale representation followed by the experiments and discussion in Section 5.\n2 BACKGROUND\nAn invariant representation of a curve is the set of signature functions assigned to every point of the curve which does not change despite the action of a certain type of transformation. A powerful theorem from E. Cartan (Cartan (1983)) and Sophus Lie (Ackerman (1976)) characterizes the space of these invariant signatures. It begins with the concept of arc-length which is a generalized notion of the length along a curve. Given a type of transformation, one can construct an intrinsic arclength that is independent of the parameterization of the curve, and compute the curvature with respect to this arc-length. The fundamental invariants of the curve, known as differential invariants (Bruckstein & Netravali (1995), Calabi et al. (1998)) are the set of functions comprising of the curvature and its successive derivatives with respect to the invariant arc-length. These differential invariants are unique in a sense that two curves are related by the group transformation if and only if their differential invariant signatures are identical. Moreover, every invariant of the curve is a\nfunction of these fundamental differential invariants. Consider C(p) = [ x(p) y(p) ] : a planar curve with\ncoordinates x and y parameterized by some parameter p. The Euclidean arc-length, is given by\ns(p) = \u222b p 0 |Cp| dp = \u222b p 0 \u221a x2p + y 2 p dp, (1)\nwhere xp = dxdp , and yp = dy dp and the principal invariant signature, that is the Euclidean curvature is given by\n\u03ba(p) = det(Cp, Cpp) |Cp|3 = xpypp \u2212 ypxpp\n(x2p + y 2 p)\n3 2\n. (2)\nThus, we have the Euclidean differential invariant signatures given by the set {\u03ba, \u03bas, \u03bass ...} for every point on the curve. Cartan\u2019s theorem provides an axiomatic construction of invariant signatures and the uniqueness property of the theorem guarantees their theoretical validity. Their importance is highlighted from the fact that any invariant is a function of the fundamental differential invariants.\nThe difficulty with differential invariants is their stable numerical computation. Equations 1 and 2, involve non-linear functions of derivatives of the curve and this poses serious numerical issues for their practical implementation where noise and poor sampling techniques are involved. Apart from methods like Pajdla & Van Gool (1995) and Weiss (1993), numerical considerations motivated the development of multi-scale representations. These methods used alternative constructions of invariant signatures which were robust to noise. More importantly, they allowed a hierarchical representation, in which the strongest and the most global components of variation in the contour of the curve are encoded in signatures of higher scale, and as we go lower, the more localized and rapid changes get injected into the representation. Two principal methods in this category are scale-space methods and integral invariants. In scale-space methods (Mokhtarian & Mackworth (1992); Sapiro & Tannenbaum (1995); Bruckstein et al. (1996)), the curve is subjected to an invariant evolution process where it can be evolved to different levels of abstraction. See Figure 5. The curvature function\nat each evolved time t is then recorded as an invariant. For example, {\u03ba(s, t), \u03bas(s, t), \u03bass(s, t)...} would be the Euclidean-invariant representations at scale t.\nIntegral invariants (Manay et al. (2004); Fidler et al. (2008); Pottmann et al. (2009); Hong & Soatto (2015)) are invariant signatures which compute integral measures along the curve. For example, for each point on the contour, the integral area invariant computes the area of the region obtained from the intersection of a ball of radius r placed at that point and the interior of the contour. The integral nature of the computation gives the signature robustness to noise and by adjusting different radii of the ball r one can associate a scale-space of responses for this invariant. Fidler et al. (2008) and Pottmann et al. (2009) provide a detailed treatise on different types of integral invariants and their properties.\nIt is easy to observe that differential and integral invariants can be thought of as being obtained from non-linear operations of convolution filters. The construction of differential invariants employ filters for which the action is equivalent to numerical differentiation (high pass filtering) whereas integral invariants use filters which act like numerical integrators (low pass filtering) for stabilizing the invariant. This provides a motivation to adopt a learning based approach and we demonstrate that the process of estimating these filters and functions can be outsourced to a learning framework. We use the Siamese configuration for implementing this idea. Such configurations have been used in signature verification (Bromley et al. (1993)), face-verification and recognition(Sun et al. (2014); Taigman et al. (2014); Hu et al. (2014)), metric learning (Chopra et al. (2005)), image descriptors (Carlevaris-Bianco & Eustice (2014)), dimensionality reduction (Hadsell et al. (2006)) and also for generating 3D shape descriptors for correspondence and retrieval (Masci et al. (2015); Xie et al. (2015)). In these papers, the goal was to learn the descriptor and hence the similarity metric from data using notions of only positive and negative examples. We use the same framework for estimation of geometric invariants. However, in contrast to these methods, we contribute an analysis of the output descriptor and provide a geometric context to the learning process. The contrastive loss function driving the training ensures that the network chooses filters which push and pull different features of the curve into the invariant by balancing a mix of robustness and fidelity.\n3 TRAINING FOR INVARIANCE\nA planar curve can be represented either explicitly by sampling points on the curve or using an implicit representation such as level sets (Kimmel (2012)). We work with an explicit representation of simple curves (open or closed) with random variable sampling of the points along the curve. Thus, every curve is a N \u00d7 2 array denoting the X and Y coordinates of the N points. We build a convolutional neural network which inputs a curve and outputs a representation or signature for every point on the curve. We can interpret this architecture as an algorithmic scheme of representing a function over the curve. However feeding in a single curve is insufficient and instead we run this convolutional architecture in a Siamese configuration (Figure 2) that accepts a curve and a\ntransformed version (positive) of the curve or an unrelated curve (negative). By using two identical copies of the same network sharing weights to process these two curves we are able to extract geometric invariance by using a loss function to require that the two arms of the Siamese configuration must produce values that are minimally different for curves which are related by Euclidean transformations representing positive examples and maximally different for carefully constructed negative examples. To fully enable training of our network we build a large dataset comprising of positive and negative examples of the relevant transformations from a database of curves. We choose to minimize the contrastive loss between the two outputs of the Siamese network as this directs the network architecture to model a function over the curve which is invariant to the transformation.\n3.1 LOSS FUNCTION\nWe employ the contrastive loss function (Chopra et al. (2005); LeCun et al. (2006)) for training our network. The Siamese configuration comprises of two identical networks of Figure 3 computing signatures for two separate inputs of data. Associated to each input pair is a label which indicates whether or not that pair is a positive (\u03bb = 1) or a negative (\u03bb = 0) example (Figure 2). Let C1i and C2i be the curves imputed to first and second arm of the configuration for the ith example of the data with label \u03bbi. Let S\u0398(C) denote the output of the network for a given set of weights \u0398 for input curve C. The contrastive loss function is given by:\nC(\u0398) = 1 N { i=N\u2211 i=1 \u03bbi || S\u0398(C1i)\u2212S\u0398(C2i) || + (1\u2212\u03bbi) max( 0, \u00b5 \u2212 || S\u0398(C1i)\u2212S\u0398(C2i) || ) } , (3) where \u00b5 is a cross validated hyper-parameter known as margin which defines the metric threshold beyond which negative examples are penalized.\n3.2 ARCHITECTURE\nThe network inputs a N \u00d7 2 array representing the coordinates of N points along the curve. The sequential nature of the curves and the mostly 1D-convolution operations can also be looked at from the point of view of temporal signals using recurrent neural network architectures. Here however we choose instead to use a multistage CNN pipeline. The network, given by one arm of the Siamese configuration, comprises of three stages that use layer units which are typically considered the basic building blocks of modern CNN architectures. Each stage contains two sequential batches of convolutions appended with rectified linear units (ReLU) and ending with a max unit. The convolutional unit comprises of convolutions with 15 filters of width 5 as depicted in Figure 3. The max unit computes the maximum of 15 responses per point to yield an intermediate output after each stage. The final stage is followed by a linear layer which linearly combines the responses to yield the final output. Since, every iteration of convolution results in a reduction of the sequence length, sufficient padding is provided on both ends of the curve. This ensures that the value of the signature at a point is the result of the response of the computation resulting from the filter centered around that point.\n3.3 BUILDING REPRESENTATIVE DATASETS AND IMPLEMENTATION\nIn order to train for invariance, we need to build a dataset with two major attributes: First, it needs to contain a large number of examples of the transformation and second, the curves involved in the training need to have sufficient richness in terms of different patterns of sharp edges, corners, smoothness, noise and sampling factors to ensure sufficient generalizability of the model. To sufficiently span the space of Euclidean transformations, we generate random two dimensional rotations by uniformly sampling angles from [\u2212\u03c0, \u03c0]. The curves are normalized by removing the mean and dividing by the standard deviation thereby achieving invariance to translations and uniform scaling. The contours are extracted from the shapes of the MPEG7 Database (Latecki et al. (2000)) as shown in first part of Figure 4. It comprises a total of 1400 shapes containing 70 different categories of objects. 700 of the total were used for training and 350 each for testing and validation. The positive examples are constructed by taking a curve and randomly transforming it by a rotation, translation and reflection and pairing them together. The negative examples are obtained by pairing curves which are deemed dissimilar as explained in Section 4. The contours are extracted and each contour is sub-sampled to 500 points. We build the training dataset of 10, 000 examples with approximately 50% each for the positive and negative examples. The network and training is performed using the Torch library Collobert et al. (2002). We trained using Adagrad Duchi et al. (2011) at a learning rate of 5 \u00d7 10\u22124 and a batch size of 10. We set the contrastive loss hyperparameter margin \u00b5 = 1 and Figure 4 shows the error plot for training and the convergence of the loss to a minimum. The rest of this work describes how we can observe and extend the efficacy of the trained network on new data.\n4 MULTI-SCALE REPRESENTATIONS\nInvariant representations at varying levels of abstraction have a theoretical interest as well as practical importance to them. Enumeration at different scales enables a hierarchical method of analysis which is useful when there is noise and hence stability is desired in the invariant. As mentioned in Section 2, the invariants constructed from scale-space methods and integral invariants, naturally allow for such a decomposition by construction.\nA valuable insight for multi-scale representations is provided in the theorems of Gage, Hamilton and Grayson (Gage & Hamilton (1986); Grayson (1987)). It says that if we evolve any smooth nonintersecting planar curve with mean curvature flow, which is invariant to Euclidean transformations, it will ultimately converge into a circle before vanishing into a point. The curvature corresponding to this evolution follows a profile as shown in Figure 5, going from a possibly noisy descriptive feature to a constant function. In our framework, we observe an analogous behavior in a data-dependent setting. The positive part of the loss function (\u03bb = 1) forces the network to push the outputs of the positive examples closer, whereas the negative part (\u03bb = 0) forces the weights of network to push the outputs of the negative examples apart, beyond the distance barrier of \u00b5. If the training data does not contain any negative example, it is easy to see that the weights of the network will converge to a point which will yield a constant output that trivially minimizes the loss function in Equation 3.\nCurvature: \u03ba\nFigure 5: Curve evolution and the corresponding curvature profile.\n1 2 3 4 5\n1 2 3 4 5\nFigure 6: Experiments with multi-scale representations. Each signature is the output of a network trained on a dataset with training examples formed as per the rows of Table 1. Index1 indicates low and 5 indicates a higher level of abstraction.\nThis is analogous to that point in curvature flow which yields a circle and therefore has a constant curvature.\nDesigning the negative examples of the training data provides the means to obtain a multi-scale representation. Since we are training for a local descriptor of a curve, that is, a function whose value at a point depends only on its local neighborhood, a negative example must pair curves such that corresponding points on each curve must have different local neighborhoods. One such possibility is to construct negative examples which pair curves with their smoothed or evolved versions as in Table 1. Minimizing the loss function in equation 3 would lead to an action which pushes apart the signatures of the curve and its evolved or smoothed counterpart, thereby injecting the signature with fidelity and descriptiveness. We construct separate data-sets where the negative examples are drawn as shown in the rows of Table1 and train a network model for each of them using the loss function 3. In our experiments we perform smoothing by using a local polynomial regression with weighted linear least squares for obtaining the evolved contour. Figure 6 shows the outputs of these different networks which demonstrate a scale-space like behavior.\n5 EXPERIMENTS AND DISCUSSION\nAbility to handle low signal to noise ratios and efficiency of computation are typical qualities desired in a geometric invariant. To test the numerical stability and robustness of the invariant signatures\nwe designed two experiments. In the first experiment, we add increasing levels of zero-mean Gaussian noise to the curve and compare the three types of signatures: differential (Euclidean curvature), integral (integral area invariant) and the output of our network (henceforth termed as network invariant) as shown in Figure 7. Apart from adding noise, we also rotate the curve to obtain a better assessment of the Euclidean invariance property. In Figure 8, we test descriptiveness of the signature under noisy conditions in a shape retrieval task for a set of 30 shapes with 6 different categories. For every curve, we generate 5 signatures at different scales for the integral and the network invariant and use them as a representation for that shape. We use the Hausdorff distance as a distance measure (Bronstein et al. (2008)) between the two sets of signatures to rank the shapes for retrieval. Figure 7 and 8 demonstrate the robustness of the network especially at high noise levels.\nIn the second experiment, we decimate a high resolution contour at successive resolutions by randomly sub-sampling and redistributing a set of its points (marked blue in Figure 9) and observe the signatures at certain fixed points (marked red in Figure 9) on the curve. Figure 9 shows that the network is able to handle these changes in sampling and compares well with the integral invariant. Figures 7 and Figure 9 represent behavior of geometric signatures for two different tests: large noise for a moderate strength of signal and low signal for a moderate level of noise.\n6 CONCLUSION\nWe have demonstrated a method to learn geometric invariants of planar curves. Using just positive and negative examples of Euclidean transformations, we showed that a convolutional neural network\n70% 50% 30%\n20% 10% 5%\n0 10 20 30 40 50 60\nis able to effectively discover and encode transform-invariant properties of curves while remaining numerically robust in the face of noise. By using a geometric context to the training process we were able to develop novel multi-scale representations from a learning based approach without explicitly\nenforcing such behavior. As compared to a more axiomatic framework of modeling with differential geometry and engineering with numerical analysis, we demonstrated a way of replacing this pipeline with a deep learning framework which combines both these aspects. The non-specific nature of this framework can be seen as providing the groundwork for future deep learning data based problems in differential geometry.\nACKNOWLEDGMENTS\nThis project has received funding from the European Research Council (ERC) under the European Unions Horizon 2020 research and innovation program (grant agreement No 664800)\n", "cats": {"READ": 1.0, "DONTREAD": 0.0}, "reviews": [{"IS_META_REVIEW": true, "comments": "Pros : \n- New representation with nice properties that are derived and compared with a mathematical baseline and background\n- A simple algorithm to obtain the representation\n\nCons :\n- The paper sounds like an applied maths paper, but further analysis on the nature of the representation could be done, for instance, by understanding the nature of each layer, or at least, the first."}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "This work proposes learning of local representations of planar curves using convolutional neural networks.\n Invariance to rigid transformations and discriminability are enforced with a metric learning framework using a siamese architecture. Preliminary experiments on toy datasets compare favorably with predefined geometric invariants (both differential and integral). \n \n The reviewers found value on the problem set-up and the proposed model, and were generally satisfied with the author's response. They also expressed concern that the experimental section is currently a bit weak and does not include any real data. Also, the paper does not offer theoretical insights that inform us about the design of the representation or about the provable invariance guarantees. All things considered, the AC recommends acceptance in the form of a poster, but strongly encourages the authors to strengthen the work both in the experimental and the theoretical aspects.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"DATE": "27 Dec 2016", "TITLE": "Changes in the updated draft", "IS_META_REVIEW": false, "comments": "Based on the constructive suggestions of the reviewers, we have updated the paper with two minor changes:\n\n(1.) We have added a figure in the appendix section, showing the learned filters from the first layer of the network.\n\n(2.) We have added an additional line in Section 4:  \"Since we are training for a local descriptor of a curve, that is, a function whose value at a point depends only on its local neighborhood, a negative example must pair curves such that corresponding points on each curve must have different local neighborhoods\",  in order to highlight the locality property of our framework. ", "OTHER_KEYS": "Gautam Pai"}, {"TITLE": "filling a much needed gap?", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "I'm torn on this one. Seeing the MPEG-7 dataset and references to curvature scale space brought to mind the old saying that \"if it's not worth doing, it's not worth doing well.\" There is no question that the MPEG-7 dataset/benchmark got saturated long ago, and it's quite surprising to see it in a submission to a modern ML conference. I brought up the question of \"why use this representation\" with the authors and they said their \"main purpose was to connect the theory of differential geometry of curves with the computational engine of a convolutional neural network.\" Fair enough. I agree these are seemingly different fields, and the authors deserve some credit for connecting them. If we give them the benefit of the doubt that this was worth doing, then the approach they pursue using a Siamese configuration makes sense, and their adaptation of deep convnet frameworks to 1D signals is reasonable. To the extent that the old invariant based methods made use of smoothed/filtered representations coupled with nonlinearities, it's sensible to revisit this problem using convnets. I wouldn't mind seeing this paper accepted, since it's different from the mainstream, but I worry about there being too narrow an audience at ICLR that still cares about this type of shape representation.", "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 5}, {"TITLE": "Limited theoretical novelty and evaluation", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "Authors show that a contrastive loss for a Siamese architecture can be used for learning representations for planar curves. With the proposed framework, authors are able to learn a representation which is comparable to traditional differential or integral invariants, as evaluated on few toy examples.\n\nThe paper is generally well written and shows an interesting application of the Siamese architecture. However, the experimental evaluation and the results show that these are rather preliminary results as not many of the choices are validated. My biggest concern is in the choice of the negative samples, as the network basically learns only to distinguish between shapes at different scales, instead of recognizing different shapes. It is well known fact that in order to achieve a good performance with the contrastive loss, one has to be careful about the hard negative sampling, as using too easy negatives may lead to inferior results. Thus, this may be the underlying reason for such choice of the negatives? Unfortunately, this is not discussed in the paper.\n\nFurthermore the paper misses a more thorough quantitative evaluation and concentrates more on showing particular examples, instead of measuring more robust statistics over multiple curves (invariance to noise and sampling artifacts).\n\nIn general, the paper shows interesting first steps in this direction, however it is not clear whether the experimental section is strong and thorough enough for the ICLR conference. Also the novelty of the proposed idea is limited as Siamese networks are used for many years and this work only shows that they can be applied to a different task.", "IS_META_REVIEW": false, "RECOMMENDATION": 5, "DATE": "16 Dec 2016 (modified: 17 Jan 2017)", "REVIEWER_CONFIDENCE": 2}, {"TITLE": "An interesting representation", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "Pros : \n- New representation with nice properties that are derived and compared with a mathematical baseline and background\n- A simple algorithm to obtain the representation\n\nCons :\n- The paper sounds like an applied maths paper, but further analysis on the nature of the representation could be done, for instance, by understanding the nature of each layer, or at least, the first.\n", "IS_META_REVIEW": false, "RECOMMENDATION": 8, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 3}, {"DATE": "09 Dec 2016", "TITLE": "parametric vs. rasterized representation", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3"}, {"DATE": "02 Dec 2016", "TITLE": "Pre-Review Qeustions", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2"}, {"DATE": "01 Dec 2016", "TITLE": "Technical questions on the architecture", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1"}, {"IS_META_REVIEW": true, "comments": "Pros : \n- New representation with nice properties that are derived and compared with a mathematical baseline and background\n- A simple algorithm to obtain the representation\n\nCons :\n- The paper sounds like an applied maths paper, but further analysis on the nature of the representation could be done, for instance, by understanding the nature of each layer, or at least, the first."}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "This work proposes learning of local representations of planar curves using convolutional neural networks.\n Invariance to rigid transformations and discriminability are enforced with a metric learning framework using a siamese architecture. Preliminary experiments on toy datasets compare favorably with predefined geometric invariants (both differential and integral). \n \n The reviewers found value on the problem set-up and the proposed model, and were generally satisfied with the author's response. They also expressed concern that the experimental section is currently a bit weak and does not include any real data. Also, the paper does not offer theoretical insights that inform us about the design of the representation or about the provable invariance guarantees. All things considered, the AC recommends acceptance in the form of a poster, but strongly encourages the authors to strengthen the work both in the experimental and the theoretical aspects.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"DATE": "27 Dec 2016", "TITLE": "Changes in the updated draft", "IS_META_REVIEW": false, "comments": "Based on the constructive suggestions of the reviewers, we have updated the paper with two minor changes:\n\n(1.) We have added a figure in the appendix section, showing the learned filters from the first layer of the network.\n\n(2.) We have added an additional line in Section 4:  \"Since we are training for a local descriptor of a curve, that is, a function whose value at a point depends only on its local neighborhood, a negative example must pair curves such that corresponding points on each curve must have different local neighborhoods\",  in order to highlight the locality property of our framework. ", "OTHER_KEYS": "Gautam Pai"}, {"TITLE": "filling a much needed gap?", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "I'm torn on this one. Seeing the MPEG-7 dataset and references to curvature scale space brought to mind the old saying that \"if it's not worth doing, it's not worth doing well.\" There is no question that the MPEG-7 dataset/benchmark got saturated long ago, and it's quite surprising to see it in a submission to a modern ML conference. I brought up the question of \"why use this representation\" with the authors and they said their \"main purpose was to connect the theory of differential geometry of curves with the computational engine of a convolutional neural network.\" Fair enough. I agree these are seemingly different fields, and the authors deserve some credit for connecting them. If we give them the benefit of the doubt that this was worth doing, then the approach they pursue using a Siamese configuration makes sense, and their adaptation of deep convnet frameworks to 1D signals is reasonable. To the extent that the old invariant based methods made use of smoothed/filtered representations coupled with nonlinearities, it's sensible to revisit this problem using convnets. I wouldn't mind seeing this paper accepted, since it's different from the mainstream, but I worry about there being too narrow an audience at ICLR that still cares about this type of shape representation.", "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 5}, {"TITLE": "Limited theoretical novelty and evaluation", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "Authors show that a contrastive loss for a Siamese architecture can be used for learning representations for planar curves. With the proposed framework, authors are able to learn a representation which is comparable to traditional differential or integral invariants, as evaluated on few toy examples.\n\nThe paper is generally well written and shows an interesting application of the Siamese architecture. However, the experimental evaluation and the results show that these are rather preliminary results as not many of the choices are validated. My biggest concern is in the choice of the negative samples, as the network basically learns only to distinguish between shapes at different scales, instead of recognizing different shapes. It is well known fact that in order to achieve a good performance with the contrastive loss, one has to be careful about the hard negative sampling, as using too easy negatives may lead to inferior results. Thus, this may be the underlying reason for such choice of the negatives? Unfortunately, this is not discussed in the paper.\n\nFurthermore the paper misses a more thorough quantitative evaluation and concentrates more on showing particular examples, instead of measuring more robust statistics over multiple curves (invariance to noise and sampling artifacts).\n\nIn general, the paper shows interesting first steps in this direction, however it is not clear whether the experimental section is strong and thorough enough for the ICLR conference. Also the novelty of the proposed idea is limited as Siamese networks are used for many years and this work only shows that they can be applied to a different task.", "IS_META_REVIEW": false, "RECOMMENDATION": 5, "DATE": "16 Dec 2016 (modified: 17 Jan 2017)", "REVIEWER_CONFIDENCE": 2}, {"TITLE": "An interesting representation", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "Pros : \n- New representation with nice properties that are derived and compared with a mathematical baseline and background\n- A simple algorithm to obtain the representation\n\nCons :\n- The paper sounds like an applied maths paper, but further analysis on the nature of the representation could be done, for instance, by understanding the nature of each layer, or at least, the first.\n", "IS_META_REVIEW": false, "RECOMMENDATION": 8, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 3}, {"DATE": "09 Dec 2016", "TITLE": "parametric vs. rasterized representation", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3"}, {"DATE": "02 Dec 2016", "TITLE": "Pre-Review Qeustions", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2"}, {"DATE": "01 Dec 2016", "TITLE": "Technical questions on the architecture", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1"}]}
{"text": "1 INTRODUCTION\nA central factor in the application of machine learning to a given task is the inductive bias, i.e. the choice of hypotheses space from which learned functions are taken. The restriction posed by the inductive bias is necessary for practical learning, and reflects prior knowledge regarding the task at hand. Perhaps the most successful exemplar of inductive bias to date manifests itself in the use of convolutional networks (LeCun and Bengio (1995)) for computer vision tasks. These hypotheses spaces are delivering unprecedented visual recognition results (e.g. Krizhevsky et al. (2012); Szegedy et al. (2015); Simonyan and Zisserman (2014); He et al. (2015)), largely responsible for the resurgence of deep learning (LeCun et al. (2015)). Unfortunately, our formal understanding of the inductive bias behind convolutional networks is limited \u2013 the assumptions encoded into these models, which seem to form an excellent prior knowledge for imagery data, are for the most part a mystery.\nExisting works studying the inductive bias of deep networks (not necessarily convolutional) do so in the context of depth efficiency, essentially arguing that for a given amount of resources, more layers result in higher expressiveness. More precisely, depth efficiency refers to a situation where a function realized by a deep network of polynomial size, requires super-polynomial size in order to be realized (or approximated) by a shallower network. In recent years, a large body of research was devoted to proving existence of depth efficiency under different types of architectures (see for example Delalleau and Bengio (2011); Pascanu et al. (2013); Montufar et al. (2014); Telgarsky (2015); Eldan and Shamir (2015); Poggio et al. (2015); Mhaskar et al. (2016)). Nonetheless, despite the wide attention it is receiving, depth efficiency does not convey the complete story behind the inductive bias of deep networks. While it does suggest that depth brings forth functions that are otherwise unattainable, it does not explain why these functions are useful. Loosely speaking, the\nhypotheses space of a polynomially sized deep network covers a small fraction of the space of all functions. We would like to understand why this small fraction is so successful in practice.\nA specific family of convolutional networks gaining increased attention is that of convolutional arithmetic circuits. These models follow the standard paradigm of locality, weight sharing and pooling, yet differ from the most conventional convolutional networks in that their point-wise activations are linear, with non-linearity originating from product pooling. Recently, Cohen et al. (2016b) analyzed the depth efficiency of convolutional arithmetic circuits, showing that besides a negligible (zero measure) set, all functions realizable by a deep network require exponential size in order to be realized (or approximated) by a shallow one. This result, termed complete depth efficiency, stands in contrast to previous depth efficiency results, which merely showed existence of functions efficiently realizable by deep networks but not by shallow ones. Besides their analytic advantage, convolutional arithmetic circuits are also showing promising empirical performance. In particular, they are equivalent to SimNets \u2013 a deep learning architecture that excels in computationally constrained settings (Cohen and Shashua (2014); Cohen et al. (2016a)), and in addition, have recently been utilized for classification with missing data (Sharir et al. (2016)). Motivated by these theoretical and practical merits, we focus our analysis in this paper on convolutional arithmetic circuits, viewing them as representative of the class of convolutional networks. We empirically validate our conclusions with both convolutional arithmetic circuits and convolutional rectifier networks \u2013 convolutional networks with rectified linear (ReLU, Nair and Hinton (2010)) activation and max or average pooling. Adaptation of the formal analysis to networks of the latter type, similarly to the adaptation of the analysis in Cohen et al. (2016b) carried out by Cohen and Shashua (2016), is left for future work.\nOur analysis approaches the study of inductive bias from the direction of function inputs. Specifically, we study the ability of convolutional arithmetic circuits to model correlation between regions of their input. To analyze the correlations of a function, we consider different partitions of input regions into disjoint sets, and ask how far the function is from being separable w.r.t. these partitions. Distance from separability is measured through the notion of separation rank (Beylkin and Mohlenkamp (2002)), which can be viewed as a surrogate of the L2 distance from the closest separable function. For a given function and partition of its input, high separation rank implies that the function induces strong correlation between sides of the partition, and vice versa.\nWe show that a deep network supports exponentially high separation ranks for certain input partitions, while being limited to polynomial or linear (in network size) separation ranks for others. The network\u2019s pooling geometry effectively determines which input partitions are favored in terms of separation rank, i.e. which partitions enjoy the possibility of exponentially high separation rank with polynomial network size, and which require network to be exponentially large. The standard choice of square contiguous pooling windows favors interleaved (entangled) partitions over coarse ones that divide the input into large distinct areas. Other choices lead to different preferences, for example pooling windows that join together nodes with their spatial reflections lead to favoring partitions that split the input symmetrically. We conclude that in terms of modeled correlations, pooling geometry controls the inductive bias, and the particular design commonly employed in practice orients it towards the statistics of natural images (nearby pixels more correlated than ones that are far apart). Moreover, when processing data that departs from the usual domain of natural imagery, prior knowledge regarding its statistics can be used to derive respective pooling schemes, and accordingly tailor the inductive bias.\nWith regards to depth efficiency, we show that separation ranks under favored input partitions are exponentially high for all but a negligible set of the functions realizable by a deep network. Shallow networks on the other hand, treat all partitions equally, and support only linear (in network size) separation ranks. Therefore, almost all functions that may be realized by a deep network require a replicating shallow network to have exponential size. By this we return to the complete depth efficiency result of Cohen et al. (2016b), but with an added important insight into the benefit of functions brought forth by depth \u2013 they are able to efficiently model strong correlation under favored partitions of the input.\nThe remainder of the paper is organized as follows. Sec. 2 provides a brief presentation of necessary background material from the field of tensor analysis. Sec. 3 describes the convolutional arithmetic circuits we analyze, and their relation to tensor decompositions. In sec. 4 we convey the concept of separation rank, on which we base our analyses in sec. 5 and 6. The conclusions from our analyses are empirically validated in sec. 7. Finally, sec. 8 concludes.\n2 PRELIMINARIES\nThe analyses carried out in this paper rely on concepts and results from the field of tensor analysis. In this section we establish the minimal background required in order to follow our arguments 1 , referring the interested reader to Hackbusch (2012) for a broad and comprehensive introduction to the field.\nThe core concept in tensor analysis is a tensor, which for our purposes may simply be thought of as a multi-dimensional array. The order of a tensor is defined to be the number of indexing entries in the array, which are referred to as modes. The dimension of a tensor in a particular mode is defined as the number of values that may be taken by the index in that mode. For example, a 4-by-3 matrix is a tensor of order 2, i.e. it has two modes, with dimension 4 in mode 1 and dimension 3 in mode 2. If A is a tensor of order N and dimension Mi in each mode i \u2208 [N ] := {1, . . . , N}, the space of all configurations it can take is denoted, quite naturally, by RM1\u00d7\u00b7\u00b7\u00b7\u00d7MN .\nA fundamental operator in tensor analysis is the tensor product, which we denote by \u2297. It is an operator that intakes two tensors A \u2208 RM1\u00d7\u00b7\u00b7\u00b7\u00d7MP and B \u2208 RMP+1\u00d7\u00b7\u00b7\u00b7\u00d7MP+Q (orders P and Q respectively), and returns a tensor A \u2297 B \u2208 RM1\u00d7\u00b7\u00b7\u00b7\u00d7MP+Q (order P + Q) defined by: (A \u2297 B)d1...dP+Q = Ad1...dP \u00b7 BdP+1...dP+Q . Notice that in the case P = Q = 1, the tensor product reduces to the standard outer product between vectors, i.e. if u \u2208 RM1 and v \u2208 RM2 , then u\u2297 v is no other than the rank-1 matrix uv> \u2208 RM1\u00d7M2 . We now introduce the important concept of matricization, which is essentially the rearrangement of a tensor as a matrix. SupposeA is a tensor of order N and dimension Mi in each mode i \u2208 [N ], and let (I, J) be a partition of [N ], i.e. I and J are disjoint subsets of [N ] whose union gives [N ]. We may write I = {i1, . . . , i|I|} where i1 < \u00b7 \u00b7 \u00b7 < i|I|, and similarly J = {j1, . . . , j|J|} where j1 < \u00b7 \u00b7 \u00b7 < j|J|. The matricization of A w.r.t. the partition (I, J), denoted JAKI,J , is the\n\u220f|I| t=1Mit -by-\u220f|J|\nt=1Mjt matrix holding the entries ofA such thatAd1...dN is placed in row index 1+ \u2211|I| t=1(dit \u2212\n1) \u220f|I| t\u2032=t+1Mit\u2032 and column index 1 + \u2211|J| t=1(djt \u2212 1) \u220f|J| t\u2032=t+1Mjt\u2032 . If I = \u2205 or J = \u2205, then by\ndefinition JAKI,J is a row or column (respectively) vector of dimension \u220fN t=1Mt holding Ad1...dN\nin entry 1 + \u2211N t=1(dt \u2212 1) \u220fN t\u2032=t+1Mt\u2032 .\nA well known matrix operator is the Kronecker product, which we denote by . For two matrices A \u2208 RM1\u00d7M2 and B \u2208 RN1\u00d7N2 , A B is the matrix in RM1N1\u00d7M2N2 holding AijBkl in row index (i \u2212 1)N1 + k and column index (j \u2212 1)N2 + l. Let A and B be tensors of orders P and Q respectively, and let (I, J) be a partition of [P +Q]. The basic relation that binds together the tensor product, the matricization operator, and the Kronecker product, is:\nJA\u2297 BKI,J = JAKI\u2229[P ],J\u2229[P ] JBK(I\u2212P )\u2229[Q],(J\u2212P )\u2229[Q] (1) where I \u2212 P and J \u2212 P are simply the sets obtained by subtracting P from each of the elements in I and J respectively. In words, eq. 1 implies that the matricization of the tensor product between A and B w.r.t. the partition (I, J) of [P + Q], is equal to the Kronecker product between two matricizations: that of A w.r.t. the partition of [P ] induced by the lower values of (I, J), and that of B w.r.t. the partition of [Q] induced by the higher values of (I, J).\n3 CONVOLUTIONAL ARITHMETIC CIRCUITS\nThe convolutional arithmetic circuit architecture on which we focus in this paper is the one considered in Cohen et al. (2016b), portrayed in fig. 1(a). Instances processed by a network are represented as N -tuples of s-dimensional vectors. They are generally thought of as images, with the s-dimensional vectors corresponding to local patches. For example, instances could be 32-by-32 RGB images, with local patches being 5 \u00d7 5 regions crossing the three color bands. In this case, assuming a patch is taken around every pixel in an image (boundaries padded), we have N = 1024 and s = 75. Throughout the paper, we denote a general instance by X = (x1, . . . ,xN ), with x1 . . .xN \u2208 Rs standing for its patches.\n1 The definitions we give are actually concrete special cases of more abstract algebraic definitions as given in Hackbusch (2012). We limit the discussion to these special cases since they suffice for our needs and are easier to grasp.\nThe first layer in a network is referred to as representation. It consists of applying M representation functions f\u03b81 . . .f\u03b8M : Rs \u2192 R to all patches, thereby creating M feature maps. In the case where representation functions are chosen as f\u03b8d(x) = \u03c3(w > d x + bd), with parameters \u03b8d = (wd, bd) \u2208 Rs \u00d7 R and some point-wise activation \u03c3(\u00b7), the representation layer reduces to a standard convolutional layer. More elaborate settings are also possible, for example modeling the representation as a cascade of convolutional layers with pooling in-between. Following the representation, a network includes L hidden layers indexed by l = 0. . .L\u2212 1. Each hidden layer l begins with a 1 \u00d7 1 conv operator, which is simply a three-dimensional convolution with rl channels and filters of spatial dimensions 1-by-1. 2 This is followed by spatial pooling, that decimates feature maps by taking products of non-overlapping two-dimensional windows that cover the spatial extent. The last of the L hidden layers (l = L\u22121) reduces feature maps to singletons (its pooling operator is global), creating a vector of dimension rL\u22121. This vector is mapped into Y network outputs through a final dense linear layer.\nAltogether, the architectural parameters of a network are the type of representation functions (f\u03b8d ), the pooling window shapes and sizes (which in turn determine the number of hidden layers L), and the number of channels in each layer (M for representation, r0. . .rL\u22121 for hidden layers, Y for output). Given these architectural parameters, the learnable parameters of a network are the representation weights (\u03b8d for channel d), the conv weights (al,\u03b3 for channel \u03b3 of hidden layer l), and the output weights (aL,y for output node y).\nFor a particular setting of weights, every node (neuron) in a given network realizes a function from (Rs)N to R. The receptive field of a node refers to the indexes of input patches on which its function may depend. For example, the receptive field of node j in channel \u03b3 of conv oper-\n2 Cohen et al. (2016b) consider two settings for the 1 \u00d7 1 conv operator. The first, referred to as weight sharing, is the one described above, and corresponds to standard convolution. The second is more general, allowing filters that slide across the previous layer to have different weights at different spatial locations. It is shown in Cohen et al. (2016b) that without weight sharing, a convolutional arithmetic circuit with one hidden layer (or more) is universal, i.e. can realize any function if its size (width) is unbounded. This property is imperative for the study of depth efficiency, as that requires shallow networks to ultimately be able to replicate any function realized by a deep network. In this paper we limit the presentation to networks with weight sharing, which are not universal. We do so because they are more conventional, and since our entire analysis is oblivious to whether or not weights are shared (applies as is to both settings). The only exception is where we reproduce the depth efficiency result of Cohen et al. (2016b). There, we momentarily consider networks without weight sharing.\nator at hidden layer 0 is {j}, and that of an output node is [N ], corresponding to the entire input. Denote by h(l,\u03b3,j) the function realized by node j of channel \u03b3 in conv operator at hidden layer l, and let I(l,\u03b3,j) \u2282 [N ] be its receptive field. By the structure of the network it is evident that I(l,\u03b3,j) does not depend on \u03b3, so we may write I(l,j) instead. Moreover, assuming pooling windows are uniform across channels (as customary with convolutional networks), and taking into account the fact that they do not overlap, we conclude that I(l,j1) and I(l,j2) are necessarily disjoint if j1 6=j2. A simple induction over l = 0. . .L \u2212 1 then shows that h(l,\u03b3,j) may be expressed as h(l,\u03b3,j)(xi1 , . . . ,xiT ) = \u2211M d1...dT=1 A(l,\u03b3,j)d1...dT \u220fT t=1 f\u03b8dt (xit), where {i1, . . . , iT } stands for the receptive field I(l,j), and A(l,\u03b3,j) is a tensor of order T = |I(l,j)| and dimension M in each mode, with entries given by polynomials in the network\u2019s conv weights {al,\u03b3}l,\u03b3 . Taking the induction one step further (from last hidden layer to network output), we obtain the following expression for functions realized by network outputs:\nhy (x1, . . . ,xN ) = \u2211M\nd1...dN=1 Ayd1...dN \u220fN i=1 f\u03b8di (xi) (2)\ny \u2208 [Y ] here is an output node index, and hy is the function realized by that node. Ay is a tensor of order N and dimension M in each mode, with entries given by polynomials in the network\u2019s conv weights {al,\u03b3}l,\u03b3 and output weights aL,y . Hereafter, terms such as function realized by a network or coefficient tensor realized by a network, are to be understood as referring to hy orAy respectively. Next, we present explicit expressions for Ay under two canonical networks \u2013 deep and shallow.\nDeep network. Consider a network as in fig. 1(a), with pooling windows set to cover four entries each, resulting in L = log4N hidden layers. The linear weights of such a network are {a0,\u03b3 \u2208 RM}\u03b3\u2208[r0] for conv operator in hidden layer 0, {al,\u03b3 \u2208 Rrl\u22121}\u03b3\u2208[rl] for conv operator in hidden layer l = 1. . .L \u2212 1, and {aL,y \u2208 RrL\u22121}y\u2208[Y ] for dense output operator. They determine the coefficient tensor Ay (eq. 2) through the following recursive decomposition:\n\u03c61,\u03b3\ufe38\ufe37\ufe37\ufe38 order 4\n= \u2211r0\n\u03b1=1 a1,\u03b3\u03b1 \u00b7 \u22974a0,\u03b1 , \u03b3 \u2208 [r1]\n\u00b7 \u00b7 \u00b7 \u03c6l,\u03b3\ufe38\ufe37\ufe37\ufe38\norder 4l\n= \u2211rl\u22121\n\u03b1=1 al,\u03b3\u03b1 \u00b7 \u22974\u03c6l\u22121,\u03b1 , l \u2208 {2. . .L\u2212 1}, \u03b3 \u2208 [rl]\n\u00b7 \u00b7 \u00b7 Ay\ufe38\ufe37\ufe37\ufe38\norder 4L N\n= \u2211rL\u22121\n\u03b1=1 aL,y\u03b1 \u00b7 \u22974\u03c6L\u22121,\u03b1 (3)\nal,\u03b3\u03b1 and a L,y \u03b1 here are scalars representing entry \u03b1 in the vectors a l,\u03b3 and aL,y respectively, and the symbol\u2297with a superscript stands for a repeated tensor product, e.g.\u22974a0,\u03b1 := a0,\u03b1\u2297a0,\u03b1\u2297a0,\u03b1\u2297 a0,\u03b1. To verify that under pooling windows of size four Ay is indeed given by eq. 3, simply plug the rows of the decomposition into eq. 2, starting from bottom and continuing upwards. For context, eq. 3 describes what is known as a hierarchical tensor decomposition (see chapter 11 in Hackbusch (2012)), with underlying tree over modes being a full quad-tree (corresponding to the fact that the network\u2019s pooling windows cover four entries each).\nShallow network. The second network we pay special attention to is shallow, comprising a single hidden layer with global pooling \u2013 see illustration in fig. 1(b). The linear weights of such a network are {a0,\u03b3 \u2208 RM}\u03b3\u2208[r0] for hidden conv operator and {a1,y \u2208 Rr0}y\u2208[Y ] for dense output operator. They determine the coefficient tensor Ay (eq. 2) as follows:\nAy = \u2211r0\n\u03b3=1 a1,y\u03b3 \u00b7 \u2297Na0,\u03b3 (4)\nwhere a1,y\u03b3 stands for entry \u03b3 of a 1,y , and again, the symbol \u2297 with a superscript represents a repeated tensor product. The tensor decomposition in eq. 4 is an instance of the classic CP decomposition, also known as rank-1 decomposition (see Kolda and Bader (2009) for a historic survey).\nTo conclude this section, we relate the background material above, as well as our contribution described in the upcoming sections, to the work of Cohen et al. (2016b). The latter shows that with\narbitrary coefficient tensorsAy , functions hy as in eq. 2 form a universal hypotheses space. It is then shown that convolutional arithmetic circuits as in fig. 1(a) realize such functions by applying tensor decompositions to Ay , with the type of decomposition determined by the structure of a network (number of layers, number of channels in each layer etc.). The deep network (fig. 1(a) with size-4 pooling windows and L = log4N hidden layers) and the shallow network (fig. 1(b)) presented hereinabove are two special cases, whose corresponding tensor decompositions are given in eq. 3 and 4 respectively. The central result in Cohen et al. (2016b) relates to inductive bias through the notion of depth efficiency \u2013 it is shown that in the parameter space of a deep network, all weight settings but a set of (Lebesgue) measure zero give rise to functions that can only be realized (or approximated) by a shallow network if the latter has exponential size. This result does not relate to the characteristics of instances X = (x1, . . . ,xN ), it only treats the ability of shallow networks to replicate functions realized by deep networks.\nIn this paper we draw a line connecting the inductive bias to the nature of X , by studying the relation between a network\u2019s architecture and its ability to model correlation among patches xi. Specifically, in sec. 4 we consider partitions (I, J) of [N ] (I \u00b7\u222aJ = [N ], where \u00b7\u222a stands for disjoint union), and present the notion of separation rank as a measure of the correlation modeled between the patches indexed by I and those indexed by J . In sec. 5.1 the separation rank of a network\u2019s function hy w.r.t. a partition (I, J) is proven to be equal to the rank of JAyKI,J \u2013 the matricization of the coefficient tensor Ay w.r.t. (I, J). Sec. 5.2 derives lower and upper bounds on this rank for a deep network, showing that it supports exponential separation ranks with polynomial size for certain partitions, whereas for others it is required to be exponentially large. Subsequently, sec. 5.3 establishes an upper bound on rankJAyKI,J for shallow networks, implying that these must be exponentially large in order to model exponential separation rank under any partition, and thus cannot efficiently replicate a deep network\u2019s correlations. Our analysis concludes in sec. 6, where we discuss the pooling geometry of a deep network as a means for controlling the inductive bias by determining a correspondence between partitions (I, J) and spatial partitions of the input. Finally, we demonstrate experimentally in sec. 7 how different pooling geometries lead to superior performance in different tasks. Our experiments include not only convolutional arithmetic circuits, but also convolutional rectifier networks, i.e. convolutional networks with ReLU activation and max or average pooling.\n4 SEPARATION RANK\nIn this section we define the concept of separation rank for functions realized by convolutional arithmetic circuits (sec. 3), i.e. real functions that take as input X = (x1, . . . ,xN ) \u2208 (Rs)N . The separation rank serves as a measure of the correlations such functions induce between different sets of input patches, i.e. different subsets of the variable set {x1, . . . ,xN}. Let (I, J) be a partition of input indexes, i.e. I and J are disjoint subsets of [N ] whose union gives [N ]. We may write I = {i1, . . . , i|I|} where i1 < \u00b7 \u00b7 \u00b7 < i|I|, and similarly J = {j1, . . . , j|J|} where j1 < \u00b7 \u00b7 \u00b7 < j|J|. For a function h : (Rs)N \u2192 R, the separation rank w.r.t. the partition (I, J) is defined as follows: 3\nsep(h; I, J) := min { R \u2208 N \u222a {0} : \u2203g1. . .gR : (Rs)|I| \u2192 R, g\u20321. . .g\u2032R : (Rs)|J| \u2192 R s.t. (5)\nh(x1, . . . ,xN ) = \u2211R\n\u03bd=1 g\u03bd(xi1 , . . . ,xi|I|)g\n\u2032 \u03bd(xj1 , . . . ,xj|J|) } In words, it is the minimal number of summands that together give h, where each summand is separable w.r.t. (I, J), i.e. is equal to a product of two functions \u2013 one that intakes only patches indexed by I , and another that intakes only patches indexed by J . One may wonder if it is at all possible to express h through such summands, i.e. if the separation rank of h is finite. From the theory of tensor products between L2 spaces (see Hackbusch (2012) for a comprehensive coverage), we know that any h\u2208L2((Rs)N ), i.e. any h that is measurable and square-integrable, may be approximated arbitrarily well by summations of the form \u2211R \u03bd=1 g\u03bd(xi1 , . . . ,xi|I|)g \u2032 \u03bd(xj1 , . . . ,xj|J|). Exact realization however is only guaranteed at the limit R \u2192 \u221e, thus in general the separation rank of h\n3 If I = \u2205 or J = \u2205 then by definition sep(h; I, J) = 1 (unless h \u2261 0, in which case sep(h; I, J) = 0).\nneed not be finite. Nonetheless, as we show in sec. 5, for the class of functions we are interested in, namely functions realizable by convolutional arithmetic circuits, separation ranks are always finite.\nThe concept of separation rank was introduced in Beylkin and Mohlenkamp (2002) for numerical treatment of high-dimensional functions, and has since been employed for various applications, e.g. quantum chemistry (Harrison et al. (2003)), particle engineering (Hackbusch (2006)) and machine learning (Beylkin et al. (2009)). If the separation rank of a function w.r.t. a partition of its input is equal to 1, the function is separable, meaning it does not model any interaction between the sets of variables. Specifically, if sep(h; I, J) = 1 then there exist g : (Rs)|I| \u2192 R and g\u2032 : (Rs)|J| \u2192 R such that h(x1, . . . ,xN ) = g(xi1 , . . . ,xi|I|)g\u2032(xj1 , . . . ,xj|J|), and the function h cannot take into account consistency between the values of {xi1 , . . . ,xi|I|} and those of {xj1 , . . . ,xj|J|}. In a statistical setting, if h is a probability density function, this would mean that {xi1 , . . . ,xi|I|} and {xj1 , . . . ,xj|J|} are statistically independent. The higher sep(h; I, J) is, the farther h is from this situation, i.e. the more it models dependency between {xi1 , . . . ,xi|I|} and {xj1 , . . . ,xj|J|}, or equivalently, the stronger the correlation it induces between the patches indexed by I and those indexed by J .\nThe interpretation of separation rank as a measure of deviation from separability is formalized in app. B, where it is shown that sep(h; I, J) is closely related to the L2 distance of h from the set of separable functions w.r.t. (I, J). Specifically, we define D(h; I, J) as the latter distance divided by the L2 norm of h 4 , and show that sep(h; I, J) provides an upper bound on D(h; I, J). While it is not possible to lay out a general lower bound onD(h; I, J) in terms of sep(h; I, J), we show that the specific lower bounds on sep(h; I, J) underlying our analyses can be translated into lower bounds on D(h; I, J). This implies that our results, facilitated by upper and lower bounds on separation ranks of convolutional arithmetic circuits, may equivalently be framed in terms of L2 distances from separable functions.\n5 CORRELATION ANALYSIS\nIn this section we analyze convolutional arithmetic circuits (sec. 3) in terms of the correlations they can model between sides of different input partitions, i.e. in terms of the separation ranks (sec. 4) they support under different partitions (I, J) of [N ]. We begin in sec. 5.1, establishing a correspondence between separation ranks and coefficient tensor matricization ranks. This correspondence is then used in sec. 5.2 and 5.3 to analyze the deep and shallow networks (respectively) presented in sec. 3. We note that we focus on these particular networks merely for simplicity of presentation \u2013 the analysis can easily be adapted to account for alternative networks with different depths and pooling schemes.\n5.1 FROM SEPARATION RANK TO MATRICIZATION RANK\nLet hy be a function realized by a convolutional arithmetic circuit, with corresponding coefficient tensor Ay (eq. 2). Denote by (I, J) an arbitrary partition of [N ], i.e. I \u00b7\u222aJ = [N ]. We are interested in studying sep(hy; I, J) \u2013 the separation rank of hy w.r.t. (I, J) (eq. 5). As claim 1 below states, assuming representation functions {f\u03b8d}d\u2208[M ] are linearly independent (if they are not, we drop dependent functions and modify Ay accordingly 5 ), this separation rank is equal to the rank of JAyKI,J \u2013 the matricization of the coefficient tensor Ay w.r.t. the partition (I, J). Our problem thus translates to studying ranks of matricized coefficient tensors.\nClaim 1. Let hy be a function realized by a convolutional arithmetic circuit (fig. 1(a)), with corresponding coefficient tensor Ay (eq. 2). Assume that the network\u2019s representation functions f\u03b8d are linearly independent, and that they, as well as the functions g\u03bd , g\u2032\u03bd in the definition of separation\n4 The normalization (division by norm) is of critical importance \u2013 without it rescaling h would accordingly rescale D(h; I, J), rendering the latter uninformative in terms of deviation from separability. 5 Suppose for example that f\u03b8M is dependent, i.e. there exist \u03b11 . . . \u03b1M\u22121 \u2208 R such that f\u03b8M (x) =\u2211M\u22121 d=1 \u03b1d\u00b7f\u03b8d(x). We may then plug this into eq. 2, and obtain an expression for hy that has f\u03b81 . . .f\u03b8M\u22121 as representation functions, and a coefficient tensor with dimension M \u2212 1 in each mode. Continuing in this fashion, one arrives at an expression for hy whose representation functions are linearly independent.\nrank (eq. 5), are measurable and square-integrable. 6 Then, for any partition (I, J) of [N ], it holds that sep(hy; I, J) = rankJAyKI,J .\nProof. See app. A.1.\nAs the linear weights of a network vary, so do the coefficient tensors (Ay) it gives rise to. Accordingly, for a particular partition (I, J), a network does not correspond to a single value of rankJAyKI,J , but rather supports a range of values. We analyze this range by quantifying its maximum, which reflects the strongest correlation that the network can model between the input patches indexed by I and those indexed by J . One may wonder if the maximal value of rankJAyKI,J is the appropriate statistic to measure, as a-priori, it may be that rankJAyKI,J is maximal for very few of the network\u2019s weight settings, and much lower for all the rest. Apparently, as claim 2 below states, this is not the case, and in fact rankJAyKI,J is maximal under almost all of the network\u2019s weight settings. Claim 2. Consider a convolutional arithmetic circuit (fig. 1(a)) with corresponding coefficient tensor Ay (eq. 2). Ay depends on the network\u2019s linear weights \u2013 {al,\u03b3}l,\u03b3 and aL,y , thus for a given partition (I, J) of [N ], rankJAyKI,J is a function of these weights. This function obtains its maximum almost everywhere (w.r.t. Lebesgue measure).\nProof. See app. A.2.\n5.2 DEEP NETWORK\nIn this subsection we study correlations modeled by the deep network presented in sec. 3 (fig. 1(a) with size-4 pooling windows and L = log4N hidden layers). In accordance with sec. 5.1, we do so by characterizing the maximal ranks of coefficient tensor matricizations under different partitions.\nRecall from eq. 3 the hierarchical decomposition expressing a coefficient tensor Ay realized by the deep network. We are interested in matricizations of this tensor under different partitions of [N ]. Let (I, J) be an arbitrary partition, i.e. I \u00b7\u222aJ = [N ]. Matricizing the last level of eq. 3 w.r.t. (I, J), while applying the relation in eq. 1, gives:\nJAyKI,J = \u2211rL\u22121\n\u03b1=1 aL,y\u03b1 \u00b7\nq \u03c6L\u22121,\u03b1 \u2297 \u03c6L\u22121,\u03b1 \u2297 \u03c6L\u22121,\u03b1 \u2297 \u03c6L\u22121,\u03b1 y I,J\n= \u2211rL\u22121\n\u03b1=1 aL,y\u03b1 \u00b7\nq \u03c6L\u22121,\u03b1 \u2297 \u03c6L\u22121,\u03b1 y I\u2229[2\u00b74L\u22121],J\u2229[2\u00b74L\u22121]\nq \u03c6L\u22121,\u03b1 \u2297 \u03c6L\u22121,\u03b1 y (I\u22122\u00b74L\u22121)\u2229[2\u00b74L\u22121],(J\u22122\u00b74L\u22121)\u2229[2\u00b74L\u22121]\nApplying eq. 1 again, this time to matricizations of the tensor \u03c6L\u22121,\u03b1 \u2297 \u03c6L\u22121,\u03b1, we obtain: JAyKI,J = \u2211rL\u22121\n\u03b1=1 aL,y\u03b1 \u00b7\nq \u03c6L\u22121,\u03b1 y I\u2229[4L\u22121],J\u2229[4L\u22121]\nq \u03c6L\u22121,\u03b1 y (I\u22124L\u22121)\u2229[4L\u22121],(J\u22124L\u22121)\u2229[4L\u22121] q \u03c6L\u22121,\u03b1\ny (I\u22122\u00b74L\u22121)\u2229[4L\u22121],(J\u22122\u00b74L\u22121)\u2229[4L\u22121]\nq \u03c6L\u22121,\u03b1 y (I\u22123\u00b74L\u22121)\u2229[4L\u22121],(J\u22123\u00b74L\u22121)\u2229[4L\u22121]\nFor every k \u2208 [4] define IL\u22121,k := (I\u2212(k\u22121)\u00b74L\u22121)\u2229[4L\u22121] and JL\u22121,k := (J\u2212(k\u22121)\u00b74L\u22121)\u2229 [4L\u22121]. In words, (IL\u22121,k, JL\u22121,k) represents the partition induced by (I, J) on the k\u2019th quadrant of [N ], i.e. on the k\u2019th size-4L\u22121 group of input patches. We now have the following matricized version of the last level in eq. 3:\nJAyKI,J = \u2211rL\u22121\n\u03b1=1 aL,y\u03b1 \u00b7 4 t=1 J\u03c6L\u22121,\u03b1KIL\u22121,t,JL\u22121,t 6 Square-integrability of representation functions f\u03b8d may seem as a limitation at first glance, as for example neurons f\u03b8d(x) = \u03c3(w > d x + bd), with parameters \u03b8d = (wd, bd) \u2208 Rs \u00d7 R and sigmoid or ReLU activation \u03c3(\u00b7), do not meet this condition. However, since in practice our inputs are bounded (e.g. they represent image pixels by holding intensity values), we may view functions as having compact support, which, as long as they are continuous (holds in all cases of interest), ensures square-integrability.\nwhere the symbol with a running index stands for an iterative Kronecker product. To derive analogous matricized versions for the upper levels of eq. 3, we define for l \u2208 {0. . .L \u2212 1}, k \u2208 [N/4l]: Il,k := (I \u2212 (k \u2212 1) \u00b7 4l) \u2229 [4l] Jl,k := (J \u2212 (k \u2212 1) \u00b7 4l) \u2229 [4l] (6) That is to say, (Il,k, Jl,k) represents the partition induced by (I, J) on the set of indexes {(k \u2212 1) \u00b7 4l + 1, . . . , k \u00b7 4l}, i.e. on the k\u2019th size-4l group of input patches. With this notation in hand, traversing upwards through the levels of eq. 3, with repeated application of the relation in eq. 1, one arrives at the following matrix decomposition for JAyKI,J :\nJ\u03c61,\u03b3KI1,k,J1,k\ufe38 \ufe37\ufe37 \ufe38 M |I1,k|-by-M |J1,k|\n= \u2211r0\n\u03b1=1 a1,\u03b3\u03b1 \u00b7 4 t=1 Ja0,\u03b1KI0,4(k\u22121)+t,J0,4(k\u22121)+t , \u03b3 \u2208 [r1]\n\u00b7 \u00b7 \u00b7\nJ\u03c6l,\u03b3KIl,k,Jl,k\ufe38 \ufe37\ufe37 \ufe38 M |Il,k|-by-M |Jl,k|\n= \u2211rl\u22121\n\u03b1=1 al,\u03b3\u03b1 \u00b7 4 t=1 J\u03c6l\u22121,\u03b1KIl\u22121,4(k\u22121)+t,Jl\u22121,4(k\u22121)+t , l \u2208 {2. . .L\u2212 1}, \u03b3 \u2208 [rl]\n\u00b7 \u00b7 \u00b7\nJAyKI,J\ufe38 \ufe37\ufe37 \ufe38 M |I|-by-M |J|\n= \u2211rL\u22121\n\u03b1=1 aL,y\u03b1 \u00b7 4 t=1 J\u03c6L\u22121,\u03b1KIL\u22121,t,JL\u22121,t (7)\nEq. 7 expresses JAyKI,J \u2013 the matricization w.r.t. the partition (I, J) of a coefficient tensorAy realized by the deep network, in terms of the network\u2019s conv weights {al,\u03b3}l,\u03b3 and output weights aL,y . As discussed above, our interest lies in the maximal rank that this matricization can take. Theorem 1 below provides lower and upper bounds on this maximal rank, by making use of eq. 7, and of the rank-multiplicative property of the Kronecker product (rank(A B) = rank(A)\u00b7rank(B)). Theorem 1. Let (I, J) be a partition of [N ], and JAyKI,J be the matricization w.r.t. (I, J) of a coefficient tensor Ay (eq. 2) realized by the deep network (fig. 1(a) with size-4 pooling windows). For every l \u2208 {0. . .L\u2212 1} and k \u2208 [N/4l], define Il,k and Jl,k as in eq. 6. Then, the maximal rank that JAyKI,J can take (when network weights vary) is:\n\u2022 No smaller than min{r0,M}S , where S := |{k \u2208 [N/4] : I1,k 6= \u2205 \u2227 J1,k 6= \u2205}|.\n\u2022 No greater than min{Mmin{|I|,|J|}, rL\u22121 \u220f4 t=1 c\nL\u22121,t}, where c0,k := 1 for k \u2208 [N ], and cl,k := min{Mmin{|Il,k|,|Jl,k|}, rl\u22121 \u220f4 t=1 c l\u22121,4(k\u22121)+t} for l \u2208 [L\u2212 1], k \u2208 [N/4l].\nProof. See app. A.3.\nThe lower bound in theorem 1 is exponential in S, the latter defined to be the number of size-4 patch groups that are split by the partition (I, J), i.e. whose indexes are divided between I and J . Partitions that split many of the size-4 patch groups will thus lead to a large lower bound. For example, consider the partition (Iodd, Jeven) defined as follows:\nIodd = {1, 3, . . . , N \u2212 1} Jeven = {2, 4, . . . , N} (8) This partition splits all size-4 patch groups (S = N/4), leading to a lower bound that is exponential in the number of patches (N ).\nThe upper bound in theorem 1 is expressed via constants cl,k, defined recursively over levels l = 0. . .L \u2212 1, with k ranging over 1. . .N/4l for each level l. What prevents cl,k from growing double-exponentially fast (w.r.t. l) is the minimization with Mmin{|Il,k|,|Jl,k|}. Specifically, if min{|Il,k| , |Jl,k|} is small, i.e. if the partition induced by (I, J) on the k\u2019th size-4l group of patches is unbalanced (most of the patches belong to one side of the partition, and only a few belong to the other), cl,k will be of reasonable size. The higher this takes place in the hierarchy (i.e. the larger l is), the lower our eventual upper bound will be. In other words, if partitions induced by (I, J) on size-4l patch groups are unbalanced for large values of l, the upper bound in theorem 1 will be small. For example, consider the partition (I low, Jhigh) defined by:\nI low = {1, . . . , N/2} Jhigh = {N/2 + 1, . . . , N} (9)\nUnder (I low, Jhigh), all partitions induced on size-4L\u22121 patch groups (quadrants of [N ]) are completely one-sided (min{|IL\u22121,k|, |JL\u22121,k|} = 0 for all k \u2208 [4]), resulting in the upper bound being no greater than rL\u22121 \u2013 linear in network size.\nTo summarize this discussion, theorem 1 states that with the deep network, the maximal rank of a coefficient tensor matricization w.r.t. (I, J), highly depends on the nature of the partition (I, J) \u2013 it will be exponentially high for partitions such as (Iodd, Jeven), that split many size-4 patch groups, while being only polynomial (or linear) for partitions like (I low, Jhigh), under which size-4l patch groups are unevenly divided for large values of l. Since the rank of a coefficient tensor matricization w.r.t. (I, J) corresponds to the strength of correlation modeled between input patches indexed by I and those indexed by J (sec. 5.1), we conclude that the ability of a polynomially sized deep network to model correlation between sets of input patches highly depends on the nature of these sets.\n5.3 SHALLOW NETWORK\nWe now turn to study correlations modeled by the shallow network presented in sec. 3 (fig. 1(b)). In line with sec. 5.1, this is achieved by characterizing the maximal ranks of coefficient tensor matricizations under different partitions.\nRecall from eq. 4 the CP decomposition expressing a coefficient tensor Ay realized by the shallow network. For an arbitrary partition (I, J) of [N ], i.e. I \u00b7\u222aJ = [N ], matricizing this decomposition with repeated application of the relation in eq. 1, gives the following expression for JAyKI,J \u2013 the matricization w.r.t. (I, J) of a coefficient tensor realized by the shallow network:\nJAyKI,J = \u2211r0\n\u03b3=1 a1,y\u03b3 \u00b7\n( |I|a0,\u03b3 )( |J|a0,\u03b3 )> (10)\n|I|a0,\u03b3 and |J|a0,\u03b3 here are column vectors of dimensions M |I| and M |J| respectively, standing for the Kronecker products of a0,\u03b3 \u2208 RM with itself |I| and |J | times (respectively). Eq. 10 immediately leads to two observations regarding the ranks that may be taken by JAyKI,J . First, they depend on the partition (I, J) only through its division size, i.e. through |I| and |J |. Second, they are no greater than min{Mmin{|I|,|J|}, r0}, meaning that the maximal rank is linear (or less) in network size. In light of sec. 5.1 and 5.2, these findings imply that in contrast to the deep network, which with polynomial size supports exponential separation ranks under favored partitions, the shallow network treats all partitions (of a given division size) equally, and can only give rise to an exponential separation rank if its size is exponential.\nSuppose now that we would like to use the shallow network to replicate a function realized by a polynomially sized deep network. So long as the deep network\u2019s function admits an exponential separation rank under at least one of the favored partitions (e.g. (Iodd, Jeven) \u2013 eq. 8), the shallow network would have to be exponentially large in order to replicate it, i.e. depth efficiency takes place. 7 Since all but a negligible set of the functions realizable by the deep network give rise to maximal separation ranks (sec 5.1), we obtain the complete depth efficiency result of Cohen et al. (2016b). However, unlike Cohen et al. (2016b), which did not provide any explanation for the usefulness of functions brought forth by depth, we obtain an insight into their utility \u2013 they are able to efficiently model strong correlation under favored partitions of the input.\n6 INDUCTIVE BIAS THROUGH POOLING GEOMETRY\nThe deep network presented in sec. 3, whose correlations we analyzed in sec. 5.2, was defined as having size-4 pooling windows, i.e. pooling windows covering four entries each. We have yet\n7 Convolutional arithmetic circuits as we have defined them (sec. 3) are not universal. In particular, it may very well be that a function realized by a polynomially sized deep network cannot be replicated by the shallow network, no matter how large (wide) we allow it to be. In such scenarios depth efficiency does not provide insight into the complexity of functions brought forth by depth. To obtain a shallow network that is universal, thus an appropriate gauge for depth efficiency, we may remove the constraint of weight sharing, i.e. allow the filters in the hidden conv operator to hold different weights at different spatial locations (see Cohen et al. (2016b) for proof that this indeed leads to universality). All results we have established for the original shallow network remain valid when weight sharing is removed. In particular, the separation ranks of the network are still linear in its size. This implies that as suggested, depth efficiency indeed holds.\nto specify the shapes of these windows, or equivalently, the spatial (two-dimensional) locations of nodes grouped together in the process of pooling. In compliance with standard convolutional network design, we now assume that the network\u2019s (size-4) pooling windows are contiguous square blocks, i.e. have shape 2 \u00d7 2. Under this configuration, the network\u2019s functional description (eq. 2 with Ay given by eq. 3) induces a spatial ordering of input patches 8 , which may be described by the following recursive process:\n\u2022 Set the index of the top-left patch to 1.\n\u2022 For l = 1, . . ., L = log4N : Replicate the already-assigned top-left 2l\u22121-by-2l\u22121 block of indexes, and place copies on its right, bottom-right and bottom. Then, add a 4l\u22121 offset to all indexes in the right copy, a 2 \u00b7 4l\u22121 offset to all indexes in the bottom-right copy, and a 3 \u00b7 4l\u22121 offset to all indexes in the bottom copy.\nWith this spatial ordering (illustrated in fig. 1(c)), partitions (I, J) of [N ] convey a spatial pattern. For example, the partition (Iodd, Jeven) (eq. 8) corresponds to the pattern illustrated on the left of fig. 1(c), whereas (I low, Jhigh) (eq. 9) corresponds to the pattern illustrated on the right. Our analysis (sec. 5.2) shows that the deep network is able to model strong correlation under (Iodd, Jeven), while being inefficient for modeling correlation under (I low, Jhigh). More generally, partitions for which S, defined in theorem 1, is high, convey patterns that split many 2 \u00d7 2 patch blocks, i.e. are highly entangled. These partitions enjoy the possibility of strong correlation. On the other hand, partitions for which min{|Il,k| , |Jl,k|} is small for large values of l (see eq. 6 for definition of Il,k and Jl,k) convey patterns that divide large 2l \u00d7 2l patch blocks unevenly, i.e. separate the input to distinct contiguous regions. These partitions, as we have seen, suffer from limited low correlations.\nWe conclude that with 2\u00d7 2 pooling, the deep network is able to model strong correlation between input regions that are highly entangled, at the expense of being inefficient for modeling correlation between input regions that are far apart. Had we selected a different pooling regime, the preference of input partition patterns in terms of modeled correlation would change. For example, if pooling windows were set to group nodes with their spatial reflections (horizontal, vertical and horizontalvertical), coarse patterns that divide the input symmetrically, such as the one illustrated on the right of fig. 1(c), would enjoy the possibility of strong correlation, whereas many entangled patterns would now suffer from limited low correlation. The choice of pooling shapes thus serves as a means for controlling the inductive bias in terms of correlations modeled between input regions. Square contiguous windows, as commonly employed in practice, lead to a preference that complies with our intuition regarding the statistics of natural images (nearby pixels more correlated than distant ones). Other pooling schemes lead to different preferences, and this allows tailoring a network to data that departs from the usual domain of natural imagery. We demonstrate this experimentally in the next section, where it is shown how different pooling geometries lead to superior performance in different tasks.\n7 EXPERIMENTS\nThe main conclusion from our analyses (sec. 5 and 6) is that the pooling geometry of a deep convolutional network controls its inductive bias by determining which correlations between input regions can be modeled efficiently. We have also seen that shallow networks cannot model correlations efficiently, regardless of the considered input regions. In this section we validate these assertions empirically, not only with convolutional arithmetic circuits (subject of our analyses), but also with convolutional rectifier networks \u2013 convolutional networks with ReLU activation and max or average pooling. For conciseness, we defer to app. C some details regarding our implementation. The latter is fully available online at https://github.com/HUJI-Deep/inductive-pooling.\n8 The network\u2019s functional description assumes a one-dimensional full quad-tree grouping of input patch indexes. That is to say, it assumes that in the first pooling operation (hidden layer 0), the nodes corresponding to patches x1,x2,x3,x4 are pooled into one group, those corresponding to x5,x6,x7,x8 are pooled into another, and so forth. Similar assumptions hold for the deeper layers. For example, in the second pooling operation (hidden layer 1), the node with receptive field {1, 2, 3, 4}, i.e. the one corresponding to the quadruple of patches {x1,x2,x3,x4}, is assumed to be pooled together with the nodes whose receptive fields are {5, 6, 7, 8}, {9, 10, 11, 12} and {13, 14, 15, 16}.\nOur experiments are based on a synthetic classification benchmark inspired by medical imaging tasks. Instances to be classified are 32-by-32 binary images, each displaying a random distorted oval shape (blob) with missing pixels in its interior (holes). For each image, two continuous scores in range [0, 1] are computed. The first, referred to as closedness, reflects how morphologically closed a blob is, and is defined to be the ratio between the number of pixels in the blob, and the number of pixels in its closure (see app. D for exact definition of the latter). The second score, named symmetry, reflects the degree to which a blob is left-right symmetric about its center. It is measured by cropping the bounding box around a blob, applying a left-right flip to the latter, and computing the ratio between the number of pixels in the intersection of the blob and its reflection, and the number of pixels in the blob. To generate labeled sets for classification (train and test), we render multiple images, sort them according to their closedness and symmetry, and for each of the two scores, assign the label \u201chigh\u201d to the top 40% and the label \u201clow\u201d to the bottom 40% (the mid 20% are considered ill-defined). This creates two binary (two-class) classification tasks \u2013 one for closedness and one for symmetry (see fig. 2 for a sample of images participating in both tasks). Given that closedness is a property of a local nature, we expect its classification task to require a predictor to be able to model strong correlations between neighboring pixels. Symmetry on the other hand is a property that relates pixels to their reflections, thus we expect its classification task to demand that a predictor be able to model correlations across distances.\nWe evaluated the deep convolutional arithmetic circuit considered throughout the paper (fig. 1(a) with size-4 pooling windows) under two different pooling geometries. The first, referred to as square, comprises standard 2 \u00d7 2 pooling windows. The second, dubbed mirror, pools together nodes with their horizontal, vertical and horizontal-vertical reflections. In both cases, input patches (xi) were set as individual pixels, resulting in N = 1024 patches and L = log4N = 5 hidden layers. M = 2 representation functions (f\u03b8d ) were fixed, the first realizing the identity on binary inputs (f\u03b81(b) = b for b \u2208 {0, 1}), and the second realizing negation (f\u03b82(b) = 1\u2212 b for b \u2208 {0, 1}). Classification was realized through Y = 2 network outputs, with prediction following the stronger activation. The number of channels across all hidden layers was uniform, and varied between 8 and 128. Fig. 3 shows the results of applying the deep network with both square and mirror pooling, to both closedness and symmetry tasks, where each of the latter has 20000 images for training and 4000 images for testing. As can be seen in the figure, square pooling significantly outperforms mirror pooling in closedness classification, whereas the opposite occurs in symmetry classification. This complies with our discussion in sec. 6, according to which square pooling supports modeling correlations between entangled (neighboring) regions of the input, whereas mirror pooling puts focus on correlations between input regions that are symmetric w.r.t. one another. We thus obtain a demonstration of how prior knowledge regarding a task at hand may be used to tailor the inductive bias of a deep convolutional network by designing an appropriate pooling geometry.\nIn addition to the deep network, we also evaluated the shallow convolutional arithmetic circuit analyzed in the paper (fig. 1(b)). The architectural choices for this network were the same as those\nDeep convolutional arithmetic circuit\ndescribed above for the deep network besides the number of hidden channels, which in this case applied to the network\u2019s single hidden layer, and varied between 64 and 4096. The highest train and test accuracies delivered by this network (with 4096 hidden channels) were roughly 62% on closedness task, and 77% on symmetry task. The fact that these accuracies are inferior to those of the deep network, even when the latter\u2019s pooling geometry is not optimal for the task at hand, complies with our analysis in sec. 5. Namely, it complies with the observation that separation ranks (correlations) are sometimes exponential and sometimes polynomial with the deep network, whereas with the shallow one they are never more than linear in network size.\nFinally, to assess the validity of our findings for convolutional networks in general, not just convolutional arithmetic circuits, we repeated the above experiments with convolutional rectifier networks. Namely, we placed ReLU activations after every conv operator, switched the pooling operation from product to average, and re-evaluated the deep (square and mirror pooling geometries) and shallow networks. We then reiterated this process once more, with pooling operation set to max instead of average. The results obtained by the deep networks are presented in fig. 4. The shallow network with average pooling reached train/test accuracies of roughly 58% on closedness task, and 55% on symmetry task. With max pooling, performance of the shallow network did not exceed chance. Altogether, convolutional rectifier networks exhibit the same phenomena observed with convolutional arithmetic circuits, indicating that the conclusions from our analyses likely apply to such networks as well. Formal adaptation of the analyses to convolutional rectifier networks, similarly to the adaptation of Cohen et al. (2016b) carried out in Cohen and Shashua (2016), is left for future work.\n8 DISCUSSION\nThrough the notion of separation rank, we studied the relation between the architecture of a convolutional network, and its ability to model correlations among input regions. For a given input partition, the separation rank quantifies how far a function is from separability, which in a probabilistic setting, corresponds to statistical independence between sides of the partition.\nOur analysis shows that a polynomially sized deep convolutional arithmetic circuit supports exponentially high separation ranks for certain input partitions, while being limited to polynomial or linear (in network size) separation ranks for others. The network\u2019s pooling window shapes effectively determine which input partitions are favored in terms of separation rank, i.e. which partitions enjoy the possibility of exponentially high separation ranks with polynomial network size, and which require network to be exponentially large. Pooling geometry thus serves as a means for controlling the inductive bias. The particular pooling scheme commonly employed in practice \u2013 square contiguous windows, favors interleaved partitions over ones that divide the input to distinct areas, thus orients the inductive bias towards the statistics of natural images (nearby pixels more correlated than distant\n0 20 40 60 80 100 120 140\nbreadth (# of channels in each hidden layer)\n70\n75\n80\n85\n90\n95\n100\na cc\nu ra\ncy [\n% ]\nclosedness task\n0 20 40 60 80 100 120 140\nbreadth (# of channels in each hidden layer)\n70\n75\n80\n85\n90\n95\n100\na cc\nu ra\ncy [\n% ]\nsymmetry task\nsquare pool - train square pool - test mirror pool - train mirror pool - test\nDeep convolutional rectifier network (average pooling)\nDeep convolutional rectifier network (max pooling)\nones). Other pooling schemes lead to different preferences, and this allows tailoring the network to data that departs from the usual domain of natural imagery.\nAs opposed to deep convolutional arithmetic circuits, shallow ones support only linear (in network size) separation ranks. Therefore, in order to replicate a function realized by a deep network (exponential separation rank), a shallow network must be exponentially large. By this we derive the depth efficiency result of Cohen et al. (2016b), but in addition, provide an insight into the benefit of functions brought forth by depth \u2013 they are able to efficiently model strong correlation under favored partitions of the input.\nWe validated our conclusions empirically, with convolutional arithmetic circuits as well as convolutional rectifier networks \u2013 convolutional networks with ReLU activation and max or average pooling. Our experiments demonstrate how different pooling geometries lead to superior performance in different tasks. Specifically, we evaluate deep networks in the measurement of shape continuity, a task of a local nature, and show that standard square pooling windows outperform ones that join together nodes with their spatial reflections. In contrast, when measuring shape symmetry, modeling correlations across distances is of vital importance, and the latter pooling geometry is superior to the conventional one. Shallow networks are inefficient at modeling correlations of any kind, and indeed lead to poor performance on both tasks.\nFinally, our analyses and results bring forth the possibility of expanding the coverage of correlations efficiently modeled by a deep convolutional network. Specifically, by blending together multiple pooling geometries in the hidden layers of a network, it is possible to facilitate simultaneous support for a wide variety of correlations suiting data of different types. Investigation of this direction, from both theoretical and empirical perspectives, is viewed as a promising avenue for future research.\nACKNOWLEDGMENTS\nThis work is supported by Intel grant ICRI-CI #9-2012-6133, by ISF Center grant 1790/12 and by the European Research Council (TheoryDL project). Nadav Cohen is supported by a Google Doctoral Fellowship in Machine Learning.\nA DEFERRED PROOFS\nA.1 PROOF OF CLAIM 1\nWe prove the equality in two steps, first showing that sep(hy; I, J)\u2264rankJAyKI,J , and then establishing the converse. The first step is elementary, and does not make use of the representation functions\u2019 (f\u03b8d ) linear independence, or of measurability/square-integrability. The second step does rely on these assumptions, and employs slightly more advanced mathematical machinery. Throughout the proof, we assume without loss of generality that the partition (I, J) of [N ] is such that I takes on lower values, while J takes on higher ones. That is to say, we assume that I = {1, . . . , |I|} and J = {|I|+ 1, . . . , N}. 9\nTo prove that sep(hy; I, J)\u2264rankJAyKI,J , denote by R the rank of JAyKI,J . The latter is an M |I|-by-M |J| matrix, thus there exist vectors u1. . .uR \u2208 RM |I| and v1. . .vR \u2208 RM |J| such that JAyKI,J = \u2211R \u03bd=1 u\u03bdv > \u03bd . For every \u03bd \u2208 [R], let B\u03bd be the tensor of order |I| and dimension M in each mode whose arrangement as a column vector gives u\u03bd , i.e. whose matricization w.r.t. the partition ([|I|], \u2205) is equal to u\u03bd . Similarly, let C\u03bd , \u03bd \u2208 [R], be the tensor of order |J | = N \u2212 |I| and dimension M in each mode whose matricization w.r.t. the partition (\u2205, [|J |]) (arrangement as a row vector) is equal to v>\u03bd . It holds that:\nJAyKI,J = \u2211R\n\u03bd=1 u\u03bdv\n> \u03bd\n= \u2211R\n\u03bd=1 JB\u03bdK[|I|],\u2205 JC\u03bdK\u2205,[|J|] = \u2211R\n\u03bd=1 JB\u03bdKI\u2229[|I|],J\u2229[|I|] JC\u03bdK(I\u2212|I|)\u2229[|J|],(J\u2212|I|)\u2229[|J|] = \u2211R\n\u03bd=1 JB\u03bd \u2297 C\u03bdKI,J = r\u2211R\n\u03bd=1 B\u03bd \u2297 C\u03bd z I,J\nwhere the third equality relies on the assumption I = {1, . . . , |I|}, J = {|I|+ 1, . . . , N}, the fourth equality makes use of the relation in eq. 1, and the last equality is based on the linearity of the matricization operator. Since matricizations are merely rearrangements of tensors, the fact that JAyKI,J = J \u2211R \u03bd=1 B\n\u03bd\u2297C\u03bdKI,J implies Ay = \u2211R \u03bd=1 B \u03bd \u2297 C\u03bd , or equivalently, Ayd1...dN = \u2211R \u03bd=1 B \u03bd d1...d|I|\n\u00b7 C\u03bdd|I|+1...dN for every d1. . .dN \u2208 [M ]. Plugging this into eq. 2 gives:\nhy (x1, . . . ,xN ) = \u2211M\nd1...dN=1 Ayd1...dN \u220fN i=1 f\u03b8di (xi)\n= \u2211M\nd1...dN=1\n\u2211R \u03bd=1 B\u03bdd1...d|I| \u00b7 C \u03bd d|I|+1...dN \u220fN i=1 f\u03b8di (xi)\n= \u2211R\n\u03bd=1 (\u2211M d1...d|I|=1 B\u03bdd1...d|I| \u220f|I| i=1 f\u03b8di (xi) ) \u00b7 (\u2211M\nd|I|+1...dN=1 C\u03bdd|I|+1...dN \u220fN i=|I|+1 f\u03b8di (xi) ) (11)\nFor every \u03bd \u2208 [R], define the functions g\u03bd : (Rs)|I| \u2192 R and g\u2032\u03bd : (Rs)|J| \u2192 R as follows:\ng\u03bd(x1, . . . ,x|I|) := \u2211M\nd1...d|I|=1 B\u03bdd1...d|I| \u220f|I| i=1 f\u03b8di (xi)\ng\u2032\u03bd(x1, . . . ,x|J|) := \u2211M\nd1...d|J|=1 C\u03bdd1...d|J| \u220f|J| i=1 f\u03b8di (xi)\n9 To see that this does not limit generality, denote I = {i1, . . . , i|I|} and J = {j1, . . . , j|J|}, and define an auxiliary function h\u2032y by permuting the entries of hy such that those indexed by I are on the left and those indexed by J on the right, i.e. h\u2032y(xi1 , . . . ,xi|I| ,xj1 , . . . ,xj|J|) = hy(x1, . . . ,xN ). Obviously sep(hy; I, J) = sep(h\u2032y; I \u2032, J \u2032), where the partition (I \u2032, J \u2032) is defined by I \u2032 = {1, . . . , |I|} and J \u2032 = {|I| + 1, . . . , N}. Analogously to the definition of h\u2032y , let A\u2032y be the tensor obtained by permuting the modes of Ay such that those indexed by I are on the left and those indexed by J on the right, i.e. A\u2032ydi1 ...di|I|dj1 ...dj|J| = A y d1...dN\n. It is not difficult to see that matricizing A\u2032y w.r.t. (I \u2032, J \u2032) is equivalent to matricizing Ay w.r.t. (I, J), i.e. JA\u2032yKI\u2032,J\u2032 = JAyKI,J , and in particular rankJA\u2032yKI\u2032,J\u2032 = rankJAyKI,J . Moreover, since by definition Ay is a coefficient tensor corresponding to hy (eq. 2), A\u2032y will be a coefficient tensor that corresponds to h\u2032y . Now, our proof will show that sep(h\u2032y; I \u2032, J \u2032) = rankJA\u2032yKI\u2032,J\u2032 , which, in light of the equalities above, implies sep(hy; I, J) = rankJAyKI,J , as required.\nSubstituting these into eq. 11 leads to:\nhy (x1, . . . ,xN ) = \u2211R\n\u03bd=1 g\u03bd(x1, . . . ,x|I|)g\n\u2032 \u03bd(x|I|+1, . . . ,xN )\nwhich by definition of the separation rank (eq. 5), implies sep(hy; I, J)\u2264R. By this we have shown that sep(hy; I, J)\u2264rankJAyKI,J , as required.\nFor proving the converse inequality, i.e. sep(hy; I, J)\u2265rankJAyKI,J , we rely on basic concepts and results from functional analysis, or more specifically, from the topic of L2 spaces. While a full introduction to this topic is beyond our scope (the interested reader is referred to Rudin (1991)), we briefly lay out here the minimal background required in order to follow our proof. For any n \u2208 N, L2(Rn) is formally defined as the Hilbert space of Lebesgue measurable square-integrable real functions over Rn 10 , equipped with standard (pointwise) addition and scalar multiplication, as well as the inner product defined by integration over point-wise multiplication. For our purposes, L2(Rn) may simply be thought of as the (infinite-dimensional) vector space of functions g : Rn \u2192 R satisfying \u222b g2 < \u221e, with inner product defined by \u3008g1, g2\u3009 := \u222b g1\u00b7g2. Our proof will make use of the following basic facts related to L2 spaces:\nFact 1. If V is a finite-dimensional subspace of L2(Rn), then any g\u2208L2(Rn) may be expressed as g = p+ \u03b4, with p\u2208V and \u03b4\u2208V \u22a5 (i.e. \u03b4 is orthogonal to all elements in V ). Moreover, such a representation is unique, so in the case where g\u2208V , we necessarily have p = g and \u03b4 \u2261 0.\nFact 2. If g\u2208L2(Rn), g\u2032\u2208L2(Rn\u2032), then the function (x1,x2)7\u2192g(x1)\u00b7g\u2032(x2) belongs to L2(Rn \u00d7 Rn\u2032).\nFact 3. Let V and V \u2032 be finite-dimensional subspaces of L2(Rn) and L2(Rn\u2032) respectively, and define U\u2282L2(Rn \u00d7 Rn\u2032) to be the subspace spanned by {(x1,x2) 7\u2192p(x1)\u00b7p\u2032(x2) : p\u2208V, p\u2032\u2208V \u2032}. Given g\u2208L2(Rn), g\u2032\u2208L2(Rn\u2032), consider the function (x1,x2) 7\u2192g(x1)\u00b7g\u2032(x2) in L2(Rn \u00d7 Rn\u2032). This function belongs to U\u22a5 if g\u2208V \u22a5 or g\u2032\u2208V \u2032\u22a5.\nFact 4. If g1. . .gm\u2208L2(Rn) are linearly independent, then for any k \u2208 N, the set of functions {(x1, . . . ,xk) 7\u2192 \u220fk i=1 gdi(xi)}d1...dk\u2208[m] is linearly independent in L 2((Rn)k).\nTo facilitate application of the theory of L2 spaces, we now make use of the assumption that the network\u2019s representation functions f\u03b8d , as well as the functions g\u03bd , g \u2032 \u03bd in the definition of separation rank (eq. 5), are measurable and square-integrable. Taking into account the expression given in eq. 2 for hy , as well as fact 2 above, one readily sees that f\u03b81 . . .f\u03b8M\u2208L\n2(Rs) implies hy\u2208L2((Rs)N ). The separation rank sep(hy; I, J) will be the minimal non-negative integer R such that there exist g1. . .gR\u2208L2((Rs)|I|) and g\u20321. . .g\u2032R\u2208L2((Rs)|J|) for which:\nhy(x1, . . . ,xN ) = \u2211R\n\u03bd=1 g\u03bd(x1, . . . ,x|I|)g\n\u2032 \u03bd(x|I|+1, . . . ,xN ) (12)\nWe would like to show that sep(hy; I, J)\u2265rankJAyKI,J . Our strategy for achieving this will be to start from eq. 12, and derive an expression for JAyKI,J comprising a sum of R rank-1 matrices. As an initial step along this path, define the following finite-dimensional subspaces:\nV := span { (x1, . . . ,x|I|) 7\u2192 \u220f|I| i=1 f\u03b8di (xi) } d1...d|I|\u2208[M ] \u2282 L2 ( (Rs)|I| )\n(13)\nV \u2032 := span { (x1, . . . ,x|J|) 7\u2192 \u220f|J| i=1 f\u03b8di (xi) } d1...d|J|\u2208[M ] \u2282 L2 ( (Rs)|J| )\n(14)\nU := span { (x1, . . . ,xN ) 7\u2192 \u220fN\ni=1 f\u03b8di (xi) } d1...dN\u2208[M ] \u2282 L2 ( (Rs)N )\n(15)\nNotice that hy\u2208U (eq. 2), and that U is the span of products from V and V \u2032, i.e.:\nU = span { (x1, . . . ,xN ) 7\u2192p(x1, . . . ,x|I|)\u00b7p\u2032(x|I|+1, . . . ,xN ) : p\u2208V, p\u2032\u2208V \u2032 }\n(16)\nReturning to eq. 12, we apply fact 1 to obtain orthogonal decompositions of g1. . .gR w.r.t. V , and of g\u20321. . .g\u2032R w.r.t. V \u2032. This gives p1. . .pR\u2208V , \u03b41. . .\u03b4R\u2208V \u22a5, p\u20321. . .p\u2032R\u2208V \u2032 and \u03b4\u20321. . .\u03b4\u2032R\u2208V \u2032\u22a5, such that g\u03bd = p\u03bd + \u03b4\u03bd and\n10 More precisely, elements of the space are equivalence classes of functions, where two functions are considered equivalent if the set in Rn on which they differ has measure zero.\ng\u2032\u03bd = p \u2032 \u03bd + \u03b4 \u2032 \u03bd for every \u03bd \u2208 [R]. Plug this into eq. 12:\nhy(x1, . . . ,xN ) = \u2211R\n\u03bd=1 g\u03bd(x1, . . . ,x|I|)\u00b7g\u2032\u03bd(x|I|+1, . . . ,xN )\n= \u2211R\n\u03bd=1\n( p\u03bd(x1, . . . ,x|I|) + \u03b4\u03bd(x1, . . . ,x|I|) ) \u00b7 ( p\u2032\u03bd(x|I|+1, . . . ,xN ) + \u03b4 \u2032 \u03bd(x|I|+1, . . . ,xN )\n) = \u2211R \u03bd=1 p\u03bd(x1, . . . ,x|I|)\u00b7p\u2032\u03bd(x|I|+1, . . . ,xN )\n+ \u2211R\n\u03bd=1 p\u03bd(x1, . . . ,x|I|)\u00b7\u03b4\u2032\u03bd(x|I|+1, . . . ,xN )\n+ \u2211R\n\u03bd=1 \u03b4\u03bd(x1, . . . ,x|I|)\u00b7p\u2032\u03bd(x|I|+1, . . . ,xN )\n+ \u2211R\n\u03bd=1 \u03b4\u03bd(x1, . . . ,x|I|)\u00b7\u03b4\u2032\u03bd(x|I|+1, . . . ,xN )\nGiven that U is the span of products from V and V \u2032 (eq. 16), and that p\u03bd\u2208V, \u03b4\u03bd\u2208V \u22a5, p\u2032\u03bd\u2208V \u2032, \u03b4\u2032\u03bd\u2208V \u2032\u22a5, one readily sees that the first term in the latter expression belongs to U , while, according to fact 3, the second, third and fourth terms are orthogonal to U . We thus obtained an orthogonal decomposition of hy w.r.t. U . Since hy is contained in U , the orthogonal component must vanish (fact 1), and we amount at:\nhy(x1, . . . ,xN ) = \u2211R\n\u03bd=1 p\u03bd(x1, . . . ,x|I|)\u00b7p\u2032\u03bd(x|I|+1, . . . ,xN ) (17)\nFor every \u03bd \u2208 [R], let B\u03bd and C\u03bd be coefficient tensors of p\u03bd and p\u2032\u03bd w.r.t. the functions that span V and V \u2032 (eq. 13 and 14), respectively. Put formally, B\u03bd and C\u03bd are tensors of orders |I| and |J | (respectively), with dimension M in each mode, meeting:\np\u03bd(x1, . . . ,x|I|) = \u2211M\nd1...d|I|=1 B\u03bdd1...d|I| \u220f|I| i=1 f\u03b8di (xi)\np\u2032\u03bd(x1, . . . ,x|J|) = \u2211M\nd1...d|J|=1 C\u03bdd1...d|J| \u220f|J| i=1 f\u03b8di (xi)\nSubstitute into eq. 17:\nhy (x1, . . . ,xN ) = \u2211R\n\u03bd=1 (\u2211M d1...d|I|=1 B\u03bdd1...d|I| \u220f|I| i=1 f\u03b8di (xi) ) \u00b7 (\u2211M\nd|I|+1...dN=1 C\u03bdd|I|+1...dN \u220fN i=|I|+1 f\u03b8di (xi) ) = \u2211R \u03bd=1 \u2211M d1...dN=1 B\u03bdd1...d|I| \u00b7 C \u03bd d|I|+1...dN \u220fN i=1 f\u03b8di (xi)\n= \u2211M\nd1...dN=1\n(\u2211R \u03bd=1 B\u03bdd1...d|I| \u00b7 C \u03bd d|I|+1...dN )\u220fN i=1 f\u03b8di (xi)\nCompare this expression for hy to that given in eq. 2:\u2211M d1...dN=1 (\u2211R \u03bd=1 B\u03bdd1...d|I| \u00b7 C \u03bd d|I|+1...dN )\u220fN i=1 f\u03b8di (xi) = \u2211M d1...dN=1 Ayd1...dN \u220fN i=1 f\u03b8di (xi) (18) At this point we utilize the given linear independence of f\u03b81 . . .f\u03b8M\u2208L\n2(Rs), from which it follows (fact 4) that the functions spanning U (eq. 15) are linearly independent in L2((Rs)N ). Both sides of eq. 18 are linear combinations of these functions, thus their coefficients must coincide:\nAyd1...dN = \u2211R \u03bd=1 B\u03bdd1...d|I| \u00b7 C \u03bd d|I|+1...dN ,\u2200d1. . .dN \u2208 [M ] \u21d0\u21d2 A y = \u2211R \u03bd=1 B\u03bd \u2297 C\u03bd\nMatricizing the tensor equation on the right w.r.t. (I, J) gives: JAyKI,J = r\u2211R\n\u03bd=1 B\u03bd \u2297 C\u03bd z I,J\n= \u2211R\n\u03bd=1 JB\u03bd \u2297 C\u03bdKI,J = \u2211R\n\u03bd=1 JB\u03bdKI\u2229[|I|],J\u2229[|I|] JC\u03bdK(I\u2212|I|)\u2229[|J|],(J\u2212|I|)\u2229[|J|] = \u2211R\n\u03bd=1 JB\u03bdK[|I|],\u2205 JC\u03bdK\u2205,[|J|]\nwhere the second equality is based on the linearity of the matricization operator, the third equality relies on the relation in eq. 1, and the last equality makes use of the assumption I = {1, . . . , |I|}, J = {|I| + 1, . . . , N}.\nFor every \u03bd \u2208 [R], JB\u03bdK[|I|],\u2205 is a column vector of dimension M |I| and JC\u03bdK\u2205,[|J|] is a row vector of dimension M |J|. Denoting these by u\u03bd and v>\u03bd respectively, we may write:\nJAyKI,J = \u2211R\n\u03bd=1 u\u03bdv\n> \u03bd\nThis shows that rankJAyKI,J\u2264R. Since R is a general non-negative integer that admits eq. 12, we may take it to be minimal, i.e. to be equal to sep(hy; I, J) \u2013 the separation rank of hy w.r.t. (I, J). By this we obtain rankJAyKI,J\u2264sep(hy; I, J), which is what we set out to prove.\nA.2 PROOF OF CLAIM 2\nThe claim is framed in measure theoretical terms, and in accordance, so will its proof be. While a complete introduction to measure theory is beyond our scope (the interested reader is referred to Jones (2001)), we briefly convey here the intuition behind the concepts we will be using, as well as facts we rely upon. The Lebesgue measure is defined over sets in a Euclidean space, and may be interpreted as quantifying their \u201cvolume\u201d. For example, the Lebesgue measure of a unit hypercube is one, of the entire space is infinity, and of a finite set of points is zero. In this context, when a phenomenon is said to occur almost everywhere, it means that the set of points in which it does not occur has Lebesgue measure zero, i.e. is negligible. An important result we will make use of (proven in Caron and Traynor (2005) for example) is the following. Given a polynomial defined over n real variables, the set of points in Rn on which it vanishes is either the entire space (when the polynomial in question is the zero polynomial), or it must have Lebesgue measure zero. In other words, if a polynomial is not identically zero, it must be different from zero almost everywhere.\nHeading on to the proof, we recall from sec. 3 that the entries of the coefficient tensor Ay (eq. 2) are given by polynomials in the network\u2019s conv weights {al,\u03b3}l,\u03b3 and output weights aL,y . Since JAyKI,J \u2013 the matricization of Ay w.r.t. the partition (I, J), is merely a rearrangement of the tensor as a matrix, this matrix too has entries given by polynomials in the network\u2019s linear weights. Now, denote by r the maximal rank taken by JAyKI,J as network weights vary, and consider a specific setting of weights for which this rank is attained. We may assume without loss of generality that under this setting, the top-left r-by-r block of JAyKI,J is non-singular. The corresponding minor, i.e. the determinant of the sub-matrix (JAyKI,J)1:r,1:r , is thus a polynomial defined over {al,\u03b3}l,\u03b3 and aL,y which is not identically zero. In light of the above, this polynomial is different from zero almost everywhere, implying that rank(JAyKI,J)1:r,1:r = r almost everywhere. Since rankJAyKI,J\u2265rank(JAyKI,J)1:r,1:r , and since by definition r is the maximal rank that JAyKI,J can take, we have that rankJAyKI,J is maximal almost everywhere.\nA.3 PROOF OF THEOREM 1\nThe matrix decomposition in eq. 7 expresses JAKI,J in terms of the network\u2019s linear weights \u2013 {a0,\u03b3 \u2208 RM}\u03b3\u2208[r0] for conv operator in hidden layer 0, {a\nl,\u03b3 \u2208 Rrl\u22121}\u03b3\u2208[rl] for conv operator in hidden layer l = 1. . .L\u22121, and aL,y \u2208 RrL\u22121 for node y of dense output operator. We prove lower and upper bounds on the maximal rank that JAKI,J can take as these weights vary. Our proof relies on the rank-multiplicative property of the Kronecker product (rank(A B) = rank(A)\u00b7rank(B) for any real matrices A and B \u2013 see Bellman (1970) for proof), but is otherwise elementary.\nBeginning with the lower bound, consider the following weight setting (e\u03b3 here stands for a vector holding 1 in entry \u03b3 and 0 at all other entries, 0 stands for a vector holding 0 at all entries, and 1 stands for a vector holding 1 at all entries, with the dimension of a vector to be understood by context):\na0,\u03b3 = { e\u03b3 , \u03b3\u2264min{r0,M} 0 , otherwise (19)\na1,\u03b3 = { 1 , \u03b3 = 1 0 , otherwise\nal,\u03b3 = { e1 , \u03b3 = 1 0 , otherwise for l = 2. . .L\u2212 1\naL,y = e1\nLet n \u2208 [N/4]. Recalling the definition of Il,k and Jl,k from eq. 6, consider the sets I1,n and J1,n, as well as I0,4(n\u22121)+t and J0,4(n\u22121)+t for t \u2208 [4]. (I1,n, J1,n) is a partition of [4], i.e. I1,n \u00b7\u222aJ1,n = [4], and for every t \u2208 [4] we have I0,4(n\u22121)+t = {1} and J0,4(n\u22121)+t = \u2205 if t belongs to I1,n, and otherwise I0,4(n\u22121)+t = \u2205\nand J0,4(n\u22121)+t = {1} if t belongs to J1,n. This implies that for an arbitrary vector v, the matricization JvKI0,4(n\u22121)+t,J0,4(n\u22121)+t is equal to v if t\u2208I1,n, and to v> if t\u2208J1,n. Accordingly, for any \u03b3 \u2208 [r0]:\n4 t=1 Ja0,\u03b3KI0,4(n\u22121)+t,J0,4(n\u22121)+t =  (a0,\u03b3 a0,\u03b3 a0,\u03b3 a0,\u03b3) , |I1,n| = 4 |J1,n| = 0 (a0,\u03b3 a0,\u03b3 a0,\u03b3)(a0,\u03b3)> , |I1,n| = 3 |J1,n| = 1 (a0,\u03b3 a0,\u03b3)(a0,\u03b3 a0,\u03b3)> , |I1,n| = 2 |J1,n| = 2 (a0,\u03b3)(a0,\u03b3 a0,\u03b3 a0,\u03b3)> , |I1,n| = 1 |J1,n| = 3 (a0,\u03b3 a0,\u03b3 a0,\u03b3 a0,\u03b3)> , |I1,n| = 0 |J1,n| = 4\nAssume that \u03b3 \u2264 min{r0,M}. By our setting a0,\u03b3 = e\u03b3 , so the above matrix holds 1 in a single entry and 0 in all the rest. Moreover, if the matrix is not a row or column vector, i.e. if both I1,n and J1,n are non-empty, the column index and row index of the entry holding 1 are both unique w.r.t. \u03b3, i.e. they do not repeat as \u03b3 ranges over 1 . . .min{r0,M}. We thus have:\nrank (\u2211min{r0,M} \u03b3=1 4 t=1 Ja0,\u03b3KI0,4(n\u22121)+t,J0,4(n\u22121)+t ) = { min{r0,M} , I1,n 6= \u2205 \u2227 J1,n 6= \u2205 1 , I1,n = \u2205 \u2228 J1,n = \u2205\nSince we set a1,1 = 1 and a0,\u03b3 = 0 for \u03b3 > min{r0,M}, we may write:\nrank (\u2211r0 \u03b3=1 a1,1\u03b3 \u00b7 4 t=1 Ja0,\u03b3KI0,4(n\u22121)+t,J0,4(n\u22121)+t ) = { min{r0,M} , I1,n 6= \u2205 \u2227 J1,n 6= \u2205 1 , I1,n = \u2205 \u2228 J1,n = \u2205\nThe latter matrix is by definition equal to J\u03c61,1KI1,n,J1,n (see top row of eq. 7), and so for every n \u2208 [N/4]:\nrank q \u03c61,1 y I1,n,J1,n = { min{r0,M} , I1,n 6= \u2205 \u2227 J1,n 6= \u2205 1 , I1,n = \u2205 \u2228 J1,n = \u2205 (20)\nNow, the fact that we set aL,y = e1 and al,1 = e1 for l = 2. . .L\u2212 1, implies that the second to last levels of the decomposition in eq. 7 collapse to:\nJAyKI,J = N/4\nt=1 J\u03c61,1KI1,t,J1,t Applying the rank-multiplicative property of the Kronecker product, and plugging in eq. 20, we obtain:\nrankJAyKI,J = \u220fN/4\nt=1 rankJ\u03c61,1KI1,t,J1,t = min{r0,M}S\nwhere S := |{t \u2208 [N/4] : I1,t 6= \u2205 \u2227 J1,t 6= \u2205}|. This equality holds for the specific weight setting we defined in eq. 19. Maximizing over all weight settings gives the sought after lower bound:\nmax {al,\u03b3}l,\u03b3 ,aL,y\nrankJAyKI,J \u2265 min{r0,M}S\nMoving on to the upper bound, we show by induction over l = 1. . .L\u22121 that for any k \u2208 [N/4l] and \u03b3 \u2208 [rl], the rank of J\u03c6l,\u03b3KIl,k,Jl,k is no greater than cl,k, regardless of the chosen weight setting. For the base case l = 1 we have:\nJ\u03c61,\u03b3KI1,k,J1,k = \u2211r0\n\u03b1=1 a1,\u03b3\u03b1 \u00b7 4 t=1 Ja0,\u03b1KI0,4(k\u22121)+t,J0,4(k\u22121)+t The M |I1,k|-by-M |J1,k| matrix J\u03c61,\u03b3KI1,k,J1,k is given here as a sum of r0 rank-1 terms, thus obviously its rank is no greater than min{Mmin{|I1,k|,|J1,k|}, r0}. Since by definition c0,t = 1 for all t \u2208 [N ], we may write:\nrankJ\u03c61,\u03b3KI1,k,J1,k \u2264 min { Mmin{|I1,k|,|J1,k|}, r0 \u220f4 t=1 c0,4(k\u22121)+t }\nc1,k is defined by the right hand side of this inequality, so our inductive hypotheses holds for l = 1. For l > 1: J\u03c6l,\u03b3KIl,k,Jl,k = \u2211rl\u22121\n\u03b1=1 al,\u03b3\u03b1 \u00b7 4 t=1 J\u03c6l\u22121,\u03b1KIl\u22121,4(k\u22121)+t,Jl\u22121,4(k\u22121)+t Taking ranks:\nrankJ\u03c6l,\u03b3KIl,k,Jl,k = rank (\u2211rl\u22121\n\u03b1=1 al,\u03b3\u03b1 \u00b7 4 t=1\nJ\u03c6l\u22121,\u03b1KIl\u22121,4(k\u22121)+t,Jl\u22121,4(k\u22121)+t )\n\u2264 \u2211rl\u22121\n\u03b1=1 rank ( 4 t=1 J\u03c6l\u22121,\u03b1KIl\u22121,4(k\u22121)+t,Jl\u22121,4(k\u22121)+t )\n= \u2211rl\u22121\n\u03b1=1 \u220f4 t=1 rankJ\u03c6l\u22121,\u03b1KIl\u22121,4(k\u22121)+t,Jl\u22121,4(k\u22121)+t\n\u2264 \u2211rl\u22121\n\u03b1=1 \u220f4 t=1 cl\u22121,4(k\u22121)+t\n= rl\u22121 \u220f4\nt=1 cl\u22121,4(k\u22121)+t\nwhere we used rank sub-additivity in the second line, the rank-multiplicative property of the Kronecker product in the third line, and our inductive hypotheses for l \u2212 1 in the fourth line. Since the number rows and columns in J\u03c6l,\u03b3KIl,k,Jl,k is M |Il,k| and M |Jl,k| respectively, we may incorporate these terms into the inequality, obtaining:\nrankJ\u03c6l,\u03b3KIl,k,Jl,k \u2264 min { Mmin{|Il,k|,|Jl,k|}, rl\u22121 \u220f4 t=1 cl\u22121,4(k\u22121)+t }\nThe right hand side here is equal to cl,k by definition, so our inductive hypotheses indeed holds for all l = 1. . .L \u2212 1. To establish the sought after upper bound on the rank of JAyKI,J , we recall that the latter is given by:\nJAyKI,J = \u2211rL\u22121\n\u03b1=1 aL,y\u03b1 \u00b7 4 t=1 J\u03c6L\u22121,\u03b1KIL\u22121,t,JL\u22121,t Carry out a series of steps similar to before, while making use of our inductive hypotheses for l = L\u2212 1:\nrankJAyKI,J = rank (\u2211rL\u22121\n\u03b1=1 aL,y\u03b1 \u00b7 4 t=1\nJ\u03c6L\u22121,\u03b1KIL\u22121,t,JL\u22121,t )\n\u2264 \u2211rL\u22121\n\u03b1=1 rank ( 4 t=1 J\u03c6L\u22121,\u03b1KIL\u22121,t,JL\u22121,t )\n= \u2211rL\u22121\n\u03b1=1 \u220f4 t=1 rankJ\u03c6L\u22121,\u03b1KIL\u22121,t,JL\u22121,t\n\u2264 \u2211rL\u22121\n\u03b1=1 \u220f4 t=1 cL\u22121,t\n= rL\u22121 \u220f4\nt=1 cL\u22121,t\nSince JAyKI,J has M |I| rows and M |J| columns, we may include these terms in the inequality, thus reaching the upper bound we set out to prove.\nB SEPARATION RANK AND THE L2 DISTANCE FROM SEPARABLE FUNCTIONS\nOur analysis of correlations modeled by convolutional networks is based on the concept of separation rank, conveyed in sec. 4. When the separation rank of a function w.r.t. a partition of its input is equal to 1, the function is separable, meaning it does not model any interaction between sides of the partition. We argued that the higher the separation rank, the farther the function is from this situation, i.e. the stronger the correlation it induces between sides of the partition. In the current appendix we formalize this argument, by relating separation rank to the L2 distance from the set of separable functions. We begin by defining and characterizing a normalized (scale invariant) version of this distance (app. B.1). It is then shown (app. B.2) that separation rank provides an upper bound on the normalized distance. Finally, a lower bound that applies to deep convolutional arithmetic circuits is derived (app. B.3), based on the lower bound for their separation ranks established in sec. 5.2. Together, these steps imply that our entire analysis, facilitated by upper and lower bounds on separation ranks of convolutional arithmetic circuits, can be interpreted as based on upper and lower bounds on (normalized) L2 distances from separable functions.\nIn the text hereafter, we assume familiarity of the reader with the contents of sec. 2, 3, 4, 5 and the proofs given in app. A. We also rely on basic knowledge in the topic of L2 spaces (see discussion in app. A.1 for minimal background required in order to follow our arguments), as well as several results concerning singular values of matrices. In line with sec. 5, an assumption throughout this appendix is that all functions in question are measurable and square-integrable (i.e. belong to L2 over the respective Euclidean space), and in app. B.3, we also make use of the fact that representation functions (f\u03b8d ) of a convolutional arithmetic circuit can be regarded as linearly independent (see sec. 5.1). Finally, for convenience, we now fix (I, J) \u2013 an arbitrary partition of [N ]. Specifically, I and J are disjoint subsets of [N ] whose union gives [N ], denoted by I = {i1, . . . , i|I|} with i1 < \u00b7 \u00b7 \u00b7 < i|I|, and J = {j1, . . . , j|J|} with j1 < \u00b7 \u00b7 \u00b7 < j|J|.\nB.1 NORMALIZED L2 DISTANCE FROM SEPARABLE FUNCTIONS\nFor a function h\u2208L2((Rs)N ) (which is not identically zero), the normalized L2 distance from the set of separable functions w.r.t. (I, J), is defined as follows:\nD(h; I, J) := 1\n\u2016h\u2016 \u00b7 infg\u2208L2((Rs)|I|) g\u2032\u2208L2((Rs)|J|) \u2225\u2225\u2225h(x1, . . . ,xN )\u2212 g(xi1 , . . . ,xi|I|)g\u2032(xj1 , . . . ,xj|J|)\u2225\u2225\u2225 (21) where \u2016\u00b7\u2016 refers to the norm of L2 space, e.g. \u2016h\u2016 := ( \u222b (Rs)N h\n2)1/2. In words, D(h; I, J) is defined as the minimal L2 distance between h and a function that is separable w.r.t. (I, J), divided by the norm of h. The\nnormalization (division by \u2016h\u2016) admits scale invariance to D(h; I, J), and is of critical importance \u2013 without it, rescaling h would accordingly rescale the distance measure, rendering the latter uninformative in terms of deviation from separability.\nIt is worthwhile noting the resemblance between D(h; I, J) and the concept of mutual information (see Cover and Thomas (2012) for a comprehensive introduction). Both measures quantify the interaction that a normalized function 11 induces between input variables, by measuring distance from separable functions. The difference between the measures is threefold. First, mutual information considers probability density functions (non-negative and in L1), while D(h; I, J) applies to functions in L2. Second, the notion of distance in mutual information is quantified through the Kullback-Leibler divergence, whereas in D(h; I, J) it is simply the L2 metric. Third, while mutual information evaluates the distance from a specific separable function \u2013 product of marginal distributions, D(h; I, J) evaluates the minimal distance across all separable functions.\nWe now turn to establish a spectral characterization of D(h; I, J), which will be used in app. B.2 and B.3 for deriving upper and lower bounds (respectively). Assume we have the following expression for h:\nh(x1, . . . ,xN ) = \u2211m\n\u00b5=1 \u2211m\u2032 \u00b5\u2032=1 A\u00b5,\u00b5\u2032 \u00b7 \u03c6\u00b5(xi1 , . . . ,xi|I|)\u03c6 \u2032 \u00b5\u2032(xj1 , . . . ,xj|J|) (22)\nwherem andm\u2032 are positive integers, A is anm-by-m\u2032 real matrix, and {\u03c6\u00b5}m\u00b5=1, {\u03c6\u2032\u00b5\u2032}m \u2032\n\u00b5\u2032=1 are orthonormal sets of functions in L2((Rs)|I|), L2((Rs)|J|) respectively. We refer to such expression as an orthonormal separable decomposition of h, with A being its coefficient matrix. We will show that for any orthonormal separable decomposition, D(h; I, J) is given by the following formula:\nD(h; I, J) = \u221a 1\u2212 \u03c3 2 1(A)\n\u03c321(A) + \u00b7 \u00b7 \u00b7+ \u03c32min{m,m\u2032}(A) (23)\nwhere \u03c31(A) \u2265 \u00b7 \u00b7 \u00b7 \u2265 \u03c3min{m,m\u2032}(A) \u2265 0 are the singular values of the coefficient matrix A. This implies that if the largest singular value of A accounts for a significant portion of the spectral energy, the normalized L2 distance of h from separable functions is small. On the other hand, if all but a fraction of the spectral energy is attributed to trailing singular values, h is far from being separable (D(h; I, J) is close to 1).\nAs a first step in deriving eq. 23, we show that \u2016h\u20162 = \u03c321(A) + \u00b7 \u00b7 \u00b7+ \u03c32min{m,m\u2032}(A):\n\u2016h\u20162 = (1)\n\u222b h2(x1, . . . ,xN )dx1\u00b7 \u00b7 \u00b7dxN\n= (2) \u222b (\u2211m \u00b5=1 \u2211m\u2032 \u00b5\u2032=1 A\u00b5,\u00b5\u2032 \u00b7 \u03c6\u00b5(xi1 , . . . ,xi|I|)\u03c6 \u2032 \u00b5\u2032(xj1 , . . . ,xj|J|) )2 dx1\u00b7 \u00b7 \u00b7dxN\n= (3) \u222b \u2211m \u00b5,\u00b5\u0304=1 \u2211m\u2032 \u00b5\u2032,\u00b5\u0304\u2032=1 A\u00b5,\u00b5\u2032A\u00b5\u0304,\u00b5\u0304\u2032 \u00b7 \u03c6\u00b5(xi1 , . . . ,xi|I|)\u03c6 \u2032 \u00b5\u2032(xj1 , . . . ,xj|J|)\n\u00b7\u03c6\u00b5\u0304(xi1 , . . . ,xi|I|)\u03c6 \u2032 \u00b5\u0304\u2032(xj1 , . . . ,xj|J|)dx1\u00b7 \u00b7 \u00b7dxN\n= (4) \u2211m \u00b5,\u00b5\u0304=1 \u2211m\u2032 \u00b5\u2032,\u00b5\u0304\u2032=1 A\u00b5,\u00b5\u2032A\u00b5\u0304,\u00b5\u0304\u2032 \u222b \u03c6\u00b5(xi1 , . . . ,xi|I|)\u03c6 \u2032 \u00b5\u2032(xj1 , . . . ,xj|J|)\n\u00b7\u03c6\u00b5\u0304(xi1 , . . . ,xi|I|)\u03c6 \u2032 \u00b5\u0304\u2032(xj1 , . . . ,xj|J|)dx1\u00b7 \u00b7 \u00b7dxN\n= (5) \u2211m \u00b5,\u00b5\u0304=1 \u2211m\u2032 \u00b5\u2032,\u00b5\u0304\u2032=1 A\u00b5,\u00b5\u2032A\u00b5\u0304,\u00b5\u0304\u2032 \u222b \u03c6\u00b5(xi1 , . . . ,xi|I|)\u03c6\u00b5\u0304(xi1 , . . . ,xi|I|)dxi1 \u00b7 \u00b7 \u00b7dxi|I|\n\u00b7 \u222b \u03c6\u2032\u00b5\u2032(xj1 , . . . ,xj|J|)\u03c6 \u2032 \u00b5\u0304\u2032(xj1 , . . . ,xj|J|)dxj1 \u00b7 \u00b7 \u00b7dxj|J|\n= (6) \u2211m \u00b5,\u00b5\u0304=1 \u2211m\u2032 \u00b5\u2032,\u00b5\u0304\u2032=1 A\u00b5,\u00b5\u2032A\u00b5\u0304,\u00b5\u0304\u2032 \u00b7 { 1 , \u00b5 = \u00b5\u0304 0 , otherwise } \u00b7 { 1 , \u00b5\u2032 = \u00b5\u0304\u2032 0 , otherwise } = (7) \u2211m \u00b5=1 \u2211m\u2032 \u00b5\u2032=1 A2\u00b5,\u00b5\u2032\n= (8)\n\u03c321(A) + \u00b7 \u00b7 \u00b7+ \u03c32min{m,m\u2032}(A) (24)\nEquality (1) here originates from the definition of L2 norm. (2) is obtained by plugging in the expression in eq. 22. (3) is merely an arithmetic manipulation. (4) follows from the linearity of integration. (5) makes use\n11 An equivalent definition ofD(h; I, J) is the minimalL2 distance between h/ \u2016h\u2016 and a function separable w.r.t. (I, J). Accordingly, we may view D(h; I, J) as operating on normalized functions.\nof Fubini\u2019s theorem (see Jones (2001)). (6) results from the orthonormality of {\u03c6\u00b5}m\u00b5=1 and {\u03c6\u2032\u00b5\u2032}m \u2032\n\u00b5\u2032=1. (7) is a trivial computation. Finally, (8) is an outcome of the fact that the squared Frobenius norm of a matrix, i.e. the sum of squares over its entries, is equal to the sum of squares over its singular values (see Golub and Van Loan (2013) for proof).\nLet g\u2208L2((Rs)|I|). By fact 1 in app. A.1, there exist scalars \u03b11 . . . \u03b1m \u2208 R, and a function \u03b4\u2208L2((Rs)|I|) orthogonal to span{\u03c61 . . . \u03c6m}, such that g = \u2211m \u00b5=1 \u03b1\u00b5 \u00b7\u03c6\u00b5+\u03b4. Similarly, for any g\n\u2032\u2208L2((Rs)|J|) there exist \u03b1\u20321 . . . \u03b1 \u2032 m\u2032 \u2208 R and \u03b4\u2032\u2208span{\u03c6\u20321 . . . \u03c6\u2032m\u2032}\u22a5 such that g\u2032 = \u2211m\u2032 \u00b5\u2032=1 \u03b1 \u2032 \u00b5\u2032 \u00b7\u03c6\u2032\u00b5\u2032 + \u03b4\u2032. Fact 2 in app. A.1 indicates that the function given by (x1, . . . ,xN ) 7\u2192g(xi1 , . . . ,xi|I|)g \u2032(xj1 , . . . ,xj|J|) belongs toL\n2((Rs)N ). We may express it as follows:\ng(xi1 , . . . ,xi|I|)g \u2032(xj1 , . . . ,xj|J|) = \u2211m \u00b5=1 \u2211m\u2032 \u00b5\u2032=1 \u03b1\u00b5\u03b1 \u2032 \u00b5\u2032 \u00b7 \u03c6\u00b5(xi1 , . . . ,xi|I|)\u03c6 \u2032 \u00b5\u2032(xj1 , . . . ,xj|J|)\n+ (\u2211m\n\u00b5=1 \u03b1\u00b5 \u00b7 \u03c6\u00b5(xi1 , . . . ,xi|I|)\n) \u00b7 \u03b4\u2032(xj1 , . . . ,xj|J|)\n+\u03b4(xi1 , . . . ,xi|I|) \u00b7 (\u2211m\u2032 \u00b5\u2032=1 \u03b1\u2032\u00b5\u2032 \u00b7 \u03c6\u2032\u00b5\u2032(xj1 , . . . ,xj|J|) ) +\u03b4(xi1 , . . . ,xi|I|)\u03b4 \u2032(xj1 , . . . ,xj|J|)\nAccording to fact 3 in app. A.1, the second, third and fourth terms on the right hand side of the above are orthogonal to span{(x1, . . . ,xN ) 7\u2192\u03c6\u00b5(xi1 , . . . ,xi|I|)\u03c6 \u2032 \u00b5\u2032(xj1 , . . . ,xj|J|)}\u00b5\u2208[m],\u00b5\u2032\u2208[m\u2032]. Denote their summation by E(x1, . . . ,xN ), and subtract the overall function from h (given by eq. 22): h(x1, . . . ,xN )\u2212 g(xi1 , . . . ,xi|I|)g \u2032(xj1 , . . . ,xj|J|)\n= \u2211m\n\u00b5=1 \u2211m\u2032 \u00b5\u2032=1 A\u00b5,\u00b5\u2032 \u00b7 \u03c6\u00b5(xi1 , . . . ,xi|I|)\u03c6 \u2032 \u00b5\u2032(xj1 , . . . ,xj|J|)\n\u2212 \u2211m\n\u00b5=1 \u2211m\u2032 \u00b5\u2032=1 \u03b1\u00b5\u03b1 \u2032 \u00b5\u2032 \u00b7 \u03c6\u00b5(xi1 , . . . ,xi|I|)\u03c6 \u2032 \u00b5\u2032(xj1 , . . . ,xj|J|)\u2212 E(x1, . . . ,xN )\n= \u2211m\n\u00b5=1 \u2211m\u2032 \u00b5\u2032=1 (A\u00b5,\u00b5\u2032 \u2212 \u03b1\u00b5\u03b1\u2032\u00b5\u2032) \u00b7 \u03c6\u00b5(xi1 , . . . ,xi|I|)\u03c6 \u2032 \u00b5\u2032(xj1 , . . . ,xj|J|)\u2212 E(x1, . . . ,xN )\nSince the two terms in the latter expression are orthogonal to one another, we have:\u2225\u2225\u2225h(x1, . . . ,xN )\u2212 g(xi1 , . . . ,xi|I|)g\u2032(xj1 , . . . ,xj|J|)\u2225\u2225\u22252 = \u2225\u2225\u2225\u2225\u2211m\u00b5=1\u2211m\u2032\u00b5\u2032=1(A\u00b5,\u00b5\u2032 \u2212 \u03b1\u00b5\u03b1\u2032\u00b5\u2032) \u00b7 \u03c6\u00b5(xi1 , . . . ,xi|I|)\u03c6\u2032\u00b5\u2032(xj1 , . . . ,xj|J|) \u2225\u2225\u2225\u22252 + \u2016E(x1, . . . ,xN )\u20162\nApplying a sequence of steps as in eq. 24 to the first term in the second line of the above, we obtain:\u2225\u2225\u2225h(x1, . . . ,xN )\u2212 g(xi1 , . . . ,xi|I|)g\u2032(xj1 , . . . ,xj|J|)\u2225\u2225\u22252 = m\u2211 \u00b5=1 m\u2032\u2211 \u00b5\u2032=1 (A\u00b5,\u00b5\u2032\u2212\u03b1\u00b5\u03b1\u2032\u00b5\u2032)2+\u2016E(x1, . . . ,xN )\u20162\nE(x1, . . . ,xN ) = 0 if \u03b4 and \u03b4\u2032 are the zero functions, implying that:\u2225\u2225\u2225h(x1, . . . ,xN )\u2212 g(xi1 , . . . ,xi|I|)g\u2032(xj1 , . . . ,xj|J|)\u2225\u2225\u22252 \u2265\u2211m\u00b5=1\u2211m\u2032\u00b5\u2032=1(A\u00b5,\u00b5\u2032 \u2212 \u03b1\u00b5\u03b1\u2032\u00b5\u2032)2 with equality holding if g = \u2211m \u00b5=1 \u03b1\u00b5 \u00b7\u03c6\u00b5 and g \u2032 = \u2211m\u2032 \u00b5\u2032=1 \u03b1 \u2032 \u00b5\u2032 \u00b7\u03c6\u2032\u00b5\u2032 . Now, \u2211m \u00b5=1 \u2211m\u2032 \u00b5\u2032=1(A\u00b5,\u00b5\u2032 \u2212\u03b1\u00b5\u03b1 \u2032 \u00b5\u2032) 2 is the squared Frobenius distance between the matrix A and the rank-1 matrix \u03b1\u03b1\u2032>, where \u03b1 and \u03b1\u2032 are column vectors holding \u03b11 . . . \u03b1m and \u03b1\u20321 . . . \u03b1\u2032m\u2032 respectively. This squared distance is greater than or equal to the sum of squares over the second to last singular values of A, and moreover, the inequality holds with equality for proper choices of \u03b1 and \u03b1\u2032 (Eckart and Young (1936)). From this we conclude that:\u2225\u2225\u2225h(x1, . . . ,xN )\u2212 g(xi1 , . . . ,xi|I|)g\u2032(xj1 , . . . ,xj|J|)\u2225\u2225\u22252 \u2265 \u03c322(A) + \u00b7 \u00b7 \u00b7+ \u03c32min{m,m\u2032}(A) with equality holding if g and g\u2032 are set to \u2211m \u00b5=1 \u03b1\u00b5 \u00b7\u03c6\u00b5 and \u2211m\u2032 \u00b5\u2032=1 \u03b1 \u2032 \u00b5\u2032 \u00b7\u03c6\u2032\u00b5\u2032 (respectively) for proper choices of \u03b11 . . . \u03b1m and \u03b1\u20321 . . . \u03b1\u2032m\u2032 . We thus have the infimum over all possible g, g \u2032:\ninf g\u2208L2((Rs)|I|) g\u2032\u2208L2((Rs)|J|) \u2225\u2225\u2225h(x1, . . . ,xN )\u2212 g(xi1 , . . . ,xi|I|)g\u2032(xj1 , . . . ,xj|J|)\u2225\u2225\u22252 = \u03c322(A) + \u00b7 \u00b7 \u00b7+ \u03c32min{m,m\u2032}(A) (25)\nRecall that we would like to derive the formula in eq. 23 forD(h; I, J), assuming h is given by the orthonormal separable decomposition in eq. 22. Taking square root of the equalities established in eq. 24 and 25, and plugging them into the definition of D(h; I, J) (eq. 21), we obtain the sought after result.\nB.2 UPPER BOUND THROUGH SEPARATION RANK\nWe now relate D(h; I, J) \u2013 the normalized L2 distance of h\u2208L2((Rs)N ) from the set of separable functions w.r.t. (I, J) (eq. 21), to sep(h; I, J) \u2013 the separation rank of h w.r.t. (I, J) (eq. 5). Specifically, we make use of the formula in eq. 23 to derive an upper bound on D(h; I, J) in terms of sep(h; I, J).\nAssuming h has finite separation rank (otherwise the bound we derive is trivial), we may express it as:\nh(x1, . . . ,xN ) = \u2211R\n\u03bd=1 g\u03bd(xi1 , . . . ,xi|I|)g\n\u2032 \u03bd(xj1 , . . . ,xj|J|) (26)\nwhereR is some positive integer (necessarily greater than or equal to sep(h; I, J)), and g1. . .gR\u2208L2((Rs)|I|), g\u20321. . .g \u2032 R\u2208L2((Rs)|J|). Let {\u03c61, . . . , \u03c6m}\u2282L2((Rs)|I|) and {\u03c6\u20321, . . . , \u03c6\u2032m\u2032}\u2282L2((Rs)|J|) be two sets of orthonormal functions spanning span{g1. . .gR} and span{g\u20321. . .g\u2032R} respectively. By definition, for every \u03bd\u2208R there exist \u03b1\u03bd,1 . . . \u03b1\u03bd,m \u2208 R and \u03b1\u2032\u03bd,1 . . . \u03b1\u2032\u03bd,m\u2032 \u2208 R such that g\u03bd = \u2211m \u00b5=1 \u03b1\u03bd,\u00b5 \u00b7 \u03c6\u00b5 and\ng\u2032\u03bd = \u2211m\u2032 \u00b5\u2032=1 \u03b1 \u2032 \u03bd,\u00b5\u2032 \u00b7 \u03c6\u2032\u00b5\u2032 . Plugging this into eq. 26, we obtain:\nh(x1, . . . ,xN ) = \u2211m\n\u00b5=1 \u2211m\u2032 \u00b5\u2032=1 (\u2211R \u03bd=1 \u03b1\u03bd,\u00b5\u03b1 \u2032 \u03bd,\u00b5\u2032 ) \u00b7 \u03c6\u00b5(xi1 , . . . ,xi|I|)\u03c6 \u2032 \u00b5\u2032(xj1 , . . . ,xj|J|)\nThis is an orthonormal separable decomposition of h (eq. 22), with coefficient matrix A = \u2211R \u03bd=1 \u03b1\u03bd(\u03b1 \u2032 \u03bd) >, where \u03b1\u03bd := [\u03b1\u03bd,1 . . . \u03b1\u03bd,m]> and \u03b1\u2032\u03bd := [\u03b1\u2032\u03bd,1 . . . \u03b1\u2032\u03bd,m\u2032 ] > for every \u03bd\u2208R. Obviously the rank of A is no greater than R, implying: \u03c321(A)\n\u03c321(A) + \u00b7 \u00b7 \u00b7+ \u03c32min{m,m\u2032}(A) \u2265 1 R\nwhere as in app. B.1, \u03c31(A) \u2265 \u00b7 \u00b7 \u00b7 \u2265 \u03c3min{m,m\u2032}(A) \u2265 0 stand for the singular values of A. Introducing this inequality into eq. 23 gives:\nD(h; I, J) = \u221a 1\u2212 \u03c3 2 1(A) \u03c321(A) + \u00b7 \u00b7 \u00b7+ \u03c32min{m,m\u2032}(A) \u2264 \u221a 1\u2212 1 R\nThe latter holds for any R \u2208 N that admits eq. 26, so in particular we may take it to be minimal, i.e. to be equal to sep(h; I, J) 12 , bringing forth the sought after upper bound:\nD(h; I, J) \u2264 \u221a 1\u2212 1\nsep(h; I, J) (27)\nBy eq. 27, low separation rank implies proximity (in normalized L2 sense) to a separable function. We may use the inequality to translate the upper bounds on separation ranks established for deep and shallow convolutional arithmetic circuits (sec. 5.2 and 5.3 respectively), into upper bounds on normalized L2 distances from separable functions. To completely frame our analysis in terms of the latter measure, a translation of the lower bound on separation ranks of deep convolutional arithmetic circuits (sec. 5.2) is also required. Eq. 27 does not facilitate such translation, and in fact, it is easy to construct functions hwhose separation ranks are high yet are very close (in normalized L2 sense) to separable functions. 13 However, as we show in app. B.3 below, the specific lower bound of interest can indeed be translated, and our analysis may entirely be framed in terms of normalized L2 distance from separable functions.\nB.3 LOWER BOUND FOR DEEP CONVOLUTIONAL ARITHMETIC CIRCUITS\nLet hy\u2208L2((Rs)N ) be a function realized by a deep convolutional arithmetic circuit (fig. 1(a) with size-4 pooling windows and L = log4 N hidden layers), i.e. hy is given by eq. 2, where f\u03b81 . . .f\u03b8M\u2208L\n2(Rs) are linearly independent representation functions, and Ay is a coefficient tensor of order N and dimension M in each mode, determined by the linear weights of the network ({al,\u03b3}l,\u03b3 ,aL,y) through the hierarchical decomposition in eq. 3. Rearrange eq. 2 by grouping indexes d1. . .dN in accordance with the partition (I, J):\nhy (x1, . . . ,xN ) = \u2211M\ndi1 ...di|I|=1 \u2211M dj1 ...dj|J|=1 Ayd1...dN \u00b7 (\u220f|I| t=1 f\u03b8dit (xit) )(\u220f|J| t=1 f\u03b8djt (xjt) ) (28)\n12 We disregard the trivial case where sep(h; I, J) = 0 (h is identically zero). 13 This will be the case, for example, if h is given by an orthonormal separable decomposition (eq. 22), with coefficient matrix A that has high rank but whose spectral energy is highly concentrated on one singular value.\nLet m = M |I|, and define the following mapping:\n\u00b5 : [M ]|I| \u2192 [m] , \u00b5(di1 , . . . , di|I|) = 1 + \u2211|I|\nt=1 (dit \u2212 1)\u00b7M\n|I|\u2212t\n\u00b5 is a one-to-one correspondence between the index sets [M ]|I| and [m]. We slightly abuse notation, and denote by (di1(\u00b5), . . . , di|I|(\u00b5)) the tuple in [M ]\n|I| that maps to \u00b5 \u2208 [m]. Additionally, we denote the function\u220f|I| t=1 f\u03b8dit (\u00b5) (xit), which according to fact 2 in app. A.1 belongs to L 2((Rs)|I|), by \u03c6\u00b5(xi1 , . . . ,xi|I|). In the exact same manner, we let m\u2032 = M |J|, and define the bijective mapping:\n\u00b5\u2032 : [M ]|J| \u2192 [m\u2032] , \u00b5\u2032(dj1 , . . . , dj|J|) = 1 + \u2211|J|\nt=1 (djt \u2212 1)\u00b7M\n|J|\u2212t\nAs before, (dj1(\u00b5 \u2032), . . . , dj|J|(\u00b5 \u2032)) stands for the tuple in [M ]|J| that maps to \u00b5\u2032 \u2208 [m\u2032], and the function\u220f|J| t=1 f\u03b8djt (\u00b5\u2032)\n(xjt)\u2208L2((Rs)|J|) is denoted by \u03c6\u2032\u00b5\u2032(xj1 , . . . ,xj|J|). Now, recall the definition of matricization given in sec. 2, and consider JAyKI,J \u2013 the matricization of the coefficient tensorAy w.r.t. (I, J). This is a matrix of sizem-by-m\u2032, holdingAd1...dN in row index \u00b5(di1 , . . . , di|I|) and column index \u00b5\n\u2032(dj1 , . . . , dj|J|). Rewriting eq. 28 with the indexes \u00b5 and \u00b5\u2032 instead of (di1 , . . . , di|I|) and (dj1 , . . . , dj|J|), we obtain:\nhy (x1, . . . ,xN ) = \u2211m\n\u00b5=1 \u2211m\u2032 \u00b5\u2032=1 (JAyKI,J)\u00b5,\u00b5\u2032 \u00b7 \u03c6\u00b5(xi1 , . . . ,xi|I|)\u03c6 \u2032 \u00b5\u2032(xj1 , . . . ,xj|J|) (29)\nThis equation has the form of eq. 22. However, for it to qualify as an orthonormal separable decomposition, the sets of functions {\u03c61, . . . , \u03c6m}\u2282L2((Rs)|I|) and {\u03c6\u20321, . . . , \u03c6\u2032m\u2032}\u2282L2((Rs)|J|) must be orthonormal. If the latter holds eq. 23 may be applied, giving an expression for D(hy; I, J) \u2013 the normalized L2 distance of hy from the set of separable functions w.r.t. (I, J), in terms of the singular values of JAyKI,J .\nWe now direct our attention to the special case where f\u03b81 . . .f\u03b8M\u2208L 2(Rs) \u2013 the network\u2019s representation functions, are known to be orthonormal. The general setting, in which only linear independence is known, will be treated thereafter. Orthonormality of representation functions implies that \u03c61 . . . \u03c6m\u2208L2((Rs)|I|) are orthonormal as well:\n\u3008\u03c6\u00b5, \u03c6\u00b5\u0304\u3009 = (1)\n\u222b \u03c6\u00b5(xi1 , . . . ,xi|I|)\u03c6\u00b5\u0304(xi1 , . . . ,xi|I|)dxi1 \u00b7 \u00b7 \u00b7dxi|I|\n= (2) \u222b \u220f|I| t=1 f\u03b8dit (\u00b5) (xit) \u220f|I| t=1 f\u03b8dit (\u00b5\u0304) (xit)dxi1 \u00b7 \u00b7 \u00b7dxi|I|\n= (3) \u220f|I| t=1 \u222b f\u03b8dit (\u00b5) (xit)f\u03b8dit (\u00b5\u0304) (xit)dxit\n= (4) \u220f|I| t=1 \u2329 f\u03b8dit (\u00b5) , f\u03b8dit (\u00b5\u0304) \u232a = (5) \u220f|I| t=1 { 1 , dit(\u00b5) = dit(\u00b5\u0304) 0 , otherwise\n} = (6) { 1 , dit(\u00b5) = dit(\u00b5\u0304) \u2200t \u2208 [|I|] 0 , otherwise\n= (7) { 1 , \u00b5 = \u00b5\u0304 0 , otherwise\n(1) and (4) here follow from the definition of inner product in L2 space, (2) replaces \u03c6\u00b5 and \u03c6\u00b5\u0304 by their definitions, (3) makes use of Fubini\u2019s theorem (see Jones (2001)), (5) relies on the (temporary) assumption that representation functions are orthonormal, (6) is a trivial step, and (7) owes to the fact that \u00b5 7\u2192 (di1(\u00b5), . . . , di|I|(\u00b5)) is an injective mapping. A similar sequence of steps (applied to \u3008\u03c6 \u2032 \u00b5\u2032 , \u03c6 \u2032 \u00b5\u0304\u2032\u3009) shows that in addition to \u03c61 . . . \u03c6m, the functions \u03c6\u20321 . . . \u03c6\u2032m\u2032\u2208L2((Rs)|J|) will also be orthonormal if f\u03b81 . . .f\u03b8M are. We conclude that if representation functions are orthonormal, eq. 29 indeed provides an orthonormal separable decomposition of hy , and the formula in eq. 23 may be applied:\nD(hy; I, J) = \u221a 1\u2212 \u03c3 2 1(JAyKI,J)\n\u03c321(JAyKI,J) + \u00b7 \u00b7 \u00b7+ \u03c32min{m,m\u2032}(JAyKI,J) (30)\nwhere \u03c31(JAyKI,J) \u2265 \u00b7 \u00b7 \u00b7 \u2265 \u03c3min{m,m\u2032}(JAyKI,J) \u2265 0 are the singular values of the coefficient tensor matricization JAyKI,J . In sec. 5.2 we showed that the maximal separation rank realizable by a deep network is greater than or equal to min{r0,M}S , whereM, r0 are the number of channels in the representation and first hidden layers (respectively), and S stands for the number of index quadruplets (sets of the form {4k-3, 4k-2, 4k-1, 4k} for some k \u2208\n[N/4]) that are split by the partition (I, J). To prove this lower bound, we presented in app. A.3 a specific setting for the linear weights of the network ({al,\u03b3}l,\u03b3 ,aL,y) under which rankJAyKI,J = min{r0,M}S . Careful examination of the proof shows that with this particular weight setting, not only is the rank of JAyKI,J equal to min{r0,M}S , but also, all of its non-zero singular values are equal to one another. 14 This implies that \u03c321(JAyKI,J)/(\u03c321(JAyKI,J) + \u00b7 \u00b7 \u00b7 + \u03c32min{m,m\u2032}(JAyKI,J)) = min{r0,M}\u2212S , and since we currently assume that f\u03b81 . . .f\u03b8M are orthonormal, eq. 30 applies and we obtain D(hy; I, J) = \u221a 1\u2212min{r0,M}\u2212S . Maximizing over all possible weight settings, we arrive at the following lower bound for the normalized L2 distance from separable functions brought forth by a deep convolutional arithmetic circuit:\nsup {al,\u03b3}l,\u03b3 , aL,y\nD ( hy|{al,\u03b3}l,\u03b3 ,aL,y ; I, J ) \u2265 \u221a 1\u2212 1\nmin{r0,M}S (31)\nTurning to the general case, we omit the assumption that representation functions f\u03b81 . . .f\u03b8M\u2208L 2(Rs) are orthonormal, and merely rely on their linear independence. The latter implies that the dimension of span{f\u03b81 . . .f\u03b8M } is M , thus there exist orthonormal functions \u03d51. . .\u03d5M\u2208L\n2(Rs) that span it. Let F \u2208 RM\u00d7M be a transition matrix between the bases \u2013 the matrix defined by\u03d5c = \u2211M d=1 Fc,d\u00b7f\u03b8d , \u2200c \u2208 [M ]. Suppose now that we replace the original representation functions f\u03b81 . . .f\u03b8M by the orthonormal ones \u03d51. . .\u03d5M . Using the latter, the lower bound in eq. 31 applies, and there exists a setting for the linear weights of the network \u2013 {al,\u03b3}l,\u03b3 ,aL,y , such that D(hy; I, J)\u2265 \u221a 1\u2212min{r0,M}\u2212S . Recalling the structure of convolutional arithmetic circuits (fig. 1(a)), one readily sees that if we return to the original representation functions f\u03b81 . . .f\u03b8M , while multiplying conv weights in hidden layer 0 by F\n> (i.e. mapping a0,\u03b3 7\u2192F>a0,\u03b3), the overall function hy remains unchanged, and in particular D(hy; I, J)\u2265 \u221a 1\u2212min{r0,M}\u2212S still holds. We conclude that the lower bound in eq. 31 applies, even if representation functions are not orthonormal.\nTo summarize, we translated the lower bound from sec. 5.2 on the maximal separation rank realized by a deep convolutional arithmetic circuit, into a lower bound on the maximal normalized L2 distance from separable functions (eq. 31). This, along with the translation of upper bounds facilitated in app. B.2, implies that the analysis carried out in the paper, which studies correlations modeled by convolutional networks through the notion of separation rank, may equivalently be framed in terms of normalized L2 distance from separable functions. We note however that there is one particular aspect in our original analysis that does not carry through the translation. Namely, in sec. 5.1 it was shown that separation ranks realized by convolutional arithmetic circuits are maximal almost always, i.e. for all linear weight settings but a set of (Lebesgue) measure zero. Put differently, for a given partition (I, J), the maximal separation rank brought forth by a network characterizes almost all functions realized by it. An equivalent statement does not hold with the continuous measure of normalized L2 distance from separable functions. The behavior of this measure across the hypotheses space of a network is non-trivial, and forms a subject for future research.\nC IMPLEMENTATION DETAILS\nIn this appendix we provide implementation details omitted from the description of our experiments in sec. 7. Our implementation, available online at https://github.com/HUJI-Deep/inductive-pooling, is based on the SimNets branch (Cohen et al. (2016a)) of Caffe toolbox (Jia et al. (2014)). The latter realizes convolutional arithmetic circuits in log-space for numerical stability.\nWhen training convolutional arithmetic circuits, we followed the hyper-parameter choices made by Sharir et al. (2016). In particular, our objective function was the cross-entropy loss with no L2 regularization (i.e. with weight decay set to 0), optimized using Adam (Kingma and Ba (2014)) with step-size \u03b1 = 0.003 and moment decay rates \u03b21 = \u03b22 = 0.9. 15000 iterations with batch size 64 (48 epochs) were run, with the step-size \u03b1 decreasing by a factor of 10 after 12000 iterations (38.4 epochs). We did not use dropout (Srivastava et al. (2014)), as the limiting factor in terms of accuracies was the difficulty of fitting training data (as opposed to overfitting) \u2013 see fig. 3.\nFor training the conventional convolutional rectifier networks, we merely switched the hyper-parameters of Adam to the recommended settings specified in Kingma and Ba (2014) (\u03b1 = 0.001, \u03b21 = 0.9, \u03b22 = 0.999), and set weight decay to the standard value of 0.0001.\n14 To see this, note that with the specified weight setting, for every n \u2208 [N/4], J\u03c61,1KI1,n,J1,n has one of two forms: it is either a non-zero (row/column) vector, or it is a matrix holding 1 in several entries and 0 in all the rest, where any two entries holding 1 reside in different rows and different columns. The first of the two forms admits a single non-zero singular value. The second brings forth several singular values equal to 1, possibly accompanied by null singular values. In both cases, all non-zero singular values of J\u03c61,1KI1,n,J1,n are equal to one another. Now, since JAyKI,J = N/4n=1J\u03c61,1KI1,n,J1,n , and since the Kronecker product multiplies singular values (see Bellman (1970)), we have that all non-zero singular values of JAyKI,J are equal, as required.\nD MORPHOLOGICAL CLOSURE\nThe synthetic dataset used in our experiments (sec. 7) consists of binary images displaying different shapes (blobs). One of the tasks facilitated by this dataset is the detection of morphologically closed blobs, i.e. of images that are relatively similar to their morphological closure. The procedure we followed for computing the morphological closure of a binary image is:\n1. Pad the given image with background (0 value) pixels\n2. Morphological dilation: simultaneously turn on (set to 1) all pixels that have a (left, right, top or bottom) neighbor originally active (holding 1)\n3. Morphological erosion: simultaneously turn off (set to 0) all pixels that have a (left, right, top or bottom) neighbor currently inactive (holding 0)\n4. Remove pixels introduced in padding\nIt is not difficult to see that any pixel active in the original image is necessarily active in its closure. Moreover, pixels that are originally inactive yet are surrounded by active ones will also be turned on in the closure, hence the effect of \u201cgap filling\u201d. Finally, we note that the particular sequence of steps described above represents the most basic form of morphological closure. The interested reader is referred to Haralick et al. (1987) for a much more comprehensive introduction.\n", "cats": {"READ": 1.0, "DONTREAD": 0.0}, "reviews": [{"IS_META_REVIEW": true, "comments": "The paper provides a highly complex algebraic machinery to analyze the type of functions covered by convolutional network. As in most attempts  in this direction in the literature, the ideal networks described in paper, which have to be interpretable as polynomials over tensors, do not match the type of CNNs used in practice: for instance the Relu non-linearity is replaced with a product of linear functions (or a sum of logs).\n\nWhile the paper is very technical to read, every concept is clearly stated and mathematical terminology properly introduced. Still, I think some the authors could make some effort to make the key concepts more accessible, and give a more intuitive understanding of what the separation rank means rather before piling up different mathematical interpretation.\nMy SVM-era algebra is quite rusted, and I am not familiar with the separation rank framework: it would have been much easier for me to first fully understand a simple and gentle case (shallow network in section 5.3), than the general deep case.\n\nTo summarize my understanding of the key theorem 1 result:\n- The upper bound of the separation rank is used to show that in the shallow case, this rank grows AT MOST linearly with the network size (as measured by the only hidden layer). So exponential network sizes are caused by this rank needing to grow exponentially, as required by the partition.\n- In the deep case, one also uses the case that the upper bound is linear in the size of the network (as measured by the last hidden layer), however, this situation is caused by the selection of a partition (I^low, J^high), and the maximal rank induced by this partition is only linear anyway, hence the network size can remain linear.\n\nIf tried my best to summarize the key point of this paper and still probably failed at it, which shows how complex is this notion of partition rank, and that its linear growth with network size can either be a good or bad thing depending on the setting. Hopefully, someone will come one day with an explanation that holds in a single slide.\n\n\n\nWhile this is worth publishing as conference paper in its present form, I have two suggestions that, IMHO, would make this work more significant:\n\nOn the theory side, we are still very far from the completeness of the PAC bound papers of the \"shallow era\". In particular, the non-probabilistic lower and upper bound in theorem 1 are probably loose, and there is no PAC-like theory to tell us which one to use and what is the predicted impact on performance (not just the intuition). Also, in the prediction of the inductive bias, the other half is missing. This paper attempts to predict the maximal representation capacity of a DNN under bounded network size constraints, but one of the reason why this size has to be bounded is overfitting (justified by PAC or VC-dim like bounds). If we consider the expected risk as  the sum of the empirical risk and the structural risk, this paper only seems to address fully the empirical risk minimization part, freezing the structural risk. \n\nOn the practice side, an issue is that experiments in this paper mostly confirm what is obvious through intuition, or some simpler form of reasonings. For instance to use convolutions that join pixels which are symmetrical in images to detect symmetry. Basic hand-crafted pattern detectors, as they have been used in computer vision for decades, would just do the job. What would be a great motivation for using this framework is if it answered questions that simple human intuition cannot, and for which we are still in the dark: one example I could think of in the recent use of gated convolutions 'a trous' for 1D speech signal, popularized in Google WaveNet ("}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "The paper uses the notion of separation rank from tensor algebra to analyze the correlations induced through convolution and pooling operations. They show that deep networks have exponentially larger separation ranks compared to shallow ones, and thus, can induce a much richer correlation structure compared to shallow networks. It is argued that this rich inductive bias is crucial for empirical success.\n \n The paper is technically solid. The reviewers note this, and also make a few suggestions on how to make the paper more accessible. The authors have taken this into account. In order to bridge the gap between theory and practice, it is essential for theory papers to be accessible.\n \n The paper covers related work pretty well. One aspect is misses is the recent geometric analysis of deep learning. Can the algebraic analysis be connected to geometric analysis of deep learning, e.g. in the following paper?\n ", "OTHER_KEYS": "ICLR 2017 pcs"}, {"DATE": "09 Jan 2017", "TITLE": "Promising approach to show why deep CNN works well in practice", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer4"}, {"TITLE": "Promising approach to show why deep CNN works well in practice", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer4", "comments": "This paper investigates the fact why deep networks perform well in practice and how modifying the geometry of pooling can make the polynomially\nsized deep network to provide a function with exponentially high separation rank (for certain partitioning.)\n\nIn the authors' previous works, they showed the superiority of deep networks over shallows when the activation function is ReLu and the pooling is max/mean pooling but in the current paper there is no activation function after conv and the pooling is just a multiplication of the node values. Although for the experimental results they've considered both scenarios. \n\nActually, the general reasoning for this problem is hard, therefore, this drawback is not significant and the current contribution adds a reasonable amount of knowledge to the literature. \n\nThis paper studies the convolutional arithmetic circuits and shows how this model can address the inductive biases and how pooling can adjust these biases. \n\nThis interesting contribution gives an intuition about how deep network can capture the correlation between the input variables when its size is polynomial but and correlation is exponential.\n\nIt worth to note that although the authors tried to express their notation and definitions carefully where they were very successful, it would be helpful if they elaborate a bit more on their definitions, expressions, and conclusions in the sense to make them more accessible. \n\n\n\n\n\n\n\n", "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "09 Jan 2017", "REVIEWER_CONFIDENCE": 5}, {"TITLE": "Huge algebraic machinery, so far only used to perform intuitive model selection, but promising direction.", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "The paper provides a highly complex algebraic machinery to analyze the type of functions covered by convolutional network. As in most attempts  in this direction in the literature, the ideal networks described in paper, which have to be interpretable as polynomials over tensors, do not match the type of CNNs used in practice: for instance the Relu non-linearity is replaced with a product of linear functions (or a sum of logs).\n\nWhile the paper is very technical to read, every concept is clearly stated and mathematical terminology properly introduced. Still, I think some the authors could make some effort to make the key concepts more accessible, and give a more intuitive understanding of what the separation rank means rather before piling up different mathematical interpretation.\nMy SVM-era algebra is quite rusted, and I am not familiar with the separation rank framework: it would have been much easier for me to first fully understand a simple and gentle case (shallow network in section 5.3), than the general deep case.\n\nTo summarize my understanding of the key theorem 1 result:\n- The upper bound of the separation rank is used to show that in the shallow case, this rank grows AT MOST linearly with the network size (as measured by the only hidden layer). So exponential network sizes are caused by this rank needing to grow exponentially, as required by the partition.\n- In the deep case, one also uses the case that the upper bound is linear in the size of the network (as measured by the last hidden layer), however, this situation is caused by the selection of a partition (I^low, J^high), and the maximal rank induced by this partition is only linear anyway, hence the network size can remain linear.\n\nIf tried my best to summarize the key point of this paper and still probably failed at it, which shows how complex is this notion of partition rank, and that its linear growth with network size can either be a good or bad thing depending on the setting. Hopefully, someone will come one day with an explanation that holds in a single slide.\n\n\n\nWhile this is worth publishing as conference paper in its present form, I have two suggestions that, IMHO, would make this work more significant:\n\nOn the theory side, we are still very far from the completeness of the PAC bound papers of the \"shallow era\". In particular, the non-probabilistic lower and upper bound in theorem 1 are probably loose, and there is no PAC-like theory to tell us which one to use and what is the predicted impact on performance (not just the intuition). Also, in the prediction of the inductive bias, the other half is missing. This paper attempts to predict the maximal representation capacity of a DNN under bounded network size constraints, but one of the reason why this size has to be bounded is overfitting (justified by PAC or VC-dim like bounds). If we consider the expected risk as  the sum of the empirical risk and the structural risk, this paper only seems to address fully the empirical risk minimization part, freezing the structural risk. \n\nOn the practice side, an issue is that experiments in this paper mostly confirm what is obvious through intuition, or some simpler form of reasonings. For instance to use convolutions that join pixels which are symmetrical in images to detect symmetry. Basic hand-crafted pattern detectors, as they have been used in computer vision for decades, would just do the job. What would be a great motivation for using this framework is if it answered questions that simple human intuition cannot, and for which we are still in the dark: one example I could think of in the recent use of gated convolutions 'a trous' for 1D speech signal, popularized in Google WaveNet (", "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "20 Dec 2016", "REVIEWER_CONFIDENCE": 3}, {"TITLE": "Interesting analysis", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "This paper addresses the question of which functions are well suited to deep networks, as opposed to shallow networks.  The basic intuition is convincing and fairly straightforward.  Pooling operations bring together information.  When information is correlated, it can be more efficiently used if the geometry of pooling regions matches the correlations so that it can be brought together more efficiently.  Shallow networks without layers of localized pooling lack this mechanism to combine correlated information efficiently.\n\nThe theoretical results are focused on convolutional arithmetic circuits, building on prior theoretical results of the authors.  The results make use of the interesting technical notion of separability, which in some sense measures the degree to which a function can be represented as the composition of independent functions.  Because separability is measured relative to a partition of the input, it is an appropriate mechanism for measuring the complexity of functions relative to a particular geometry of pooling operations.  Many of the technical notions are pretty intuitive, although the tensor analysis is pretty terse and not easy to follow without knowledge of the authors\u2019 prior work.\n\nIn some sense the comparison between deep and shallow networks is somewhat misleading, since the shallow networks lack a hierarchical pooling structure.  For example, a shallow convolutional network with RELU and max pooling does not really make sense, since the max occurs over the whole image.  So it seems that the paper is really more of an analysis of the effect of pooling vs. not having pooling.  For example, it is not clear that a deep CNN without pooling would be any more efficient than a shallow network, from this work.\n\nIt is not clear how much the theoretical results depend on the use of a model with product pooling, and how they might be extended to the more common max pooling.  Even if theoretical results are difficult to derive in this case, simple illustrative examples might be helpful.  In fact, if the authors prepare a longer version of the paper for a journal I think the results could be made more intuitive if they could add a simple toy example of a function that can be efficiently represented with a convolutional arithmetic circuit when the pooling structure fits the correlations, and perhaps showing also how this could be represented with a convolutional network with RELU and max pooling.\n\nI would also appreciate a more explicit discussion of how the depth of a deep network affects the separability of functions that can be represented.  A shallow network doesn\u2019t have local pooling, so the difference between deep and shallow if perhaps mostly one of pooling vs. not pooling.  However, practitioners find that very deep networks seem to be more effective than \u201cdeep\u201d networks with only a few convolutional layers and pooling.  The paper does not explicitly discuss whether their results provide insight into this behavior.\n\nOverall, I think that the paper attacks an important problem in an interesting way.  It is not so convincing that this really gets to the heart of why depth is so important, because of the theoretical limitation to arithmetic circuits, and because the comparison is to shallow networks that are without localized pooling. \n", "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 3}, {"DATE": "02 Dec 2016", "TITLE": "Invariances", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2"}, {"IS_META_REVIEW": true, "comments": "The paper provides a highly complex algebraic machinery to analyze the type of functions covered by convolutional network. As in most attempts  in this direction in the literature, the ideal networks described in paper, which have to be interpretable as polynomials over tensors, do not match the type of CNNs used in practice: for instance the Relu non-linearity is replaced with a product of linear functions (or a sum of logs).\n\nWhile the paper is very technical to read, every concept is clearly stated and mathematical terminology properly introduced. Still, I think some the authors could make some effort to make the key concepts more accessible, and give a more intuitive understanding of what the separation rank means rather before piling up different mathematical interpretation.\nMy SVM-era algebra is quite rusted, and I am not familiar with the separation rank framework: it would have been much easier for me to first fully understand a simple and gentle case (shallow network in section 5.3), than the general deep case.\n\nTo summarize my understanding of the key theorem 1 result:\n- The upper bound of the separation rank is used to show that in the shallow case, this rank grows AT MOST linearly with the network size (as measured by the only hidden layer). So exponential network sizes are caused by this rank needing to grow exponentially, as required by the partition.\n- In the deep case, one also uses the case that the upper bound is linear in the size of the network (as measured by the last hidden layer), however, this situation is caused by the selection of a partition (I^low, J^high), and the maximal rank induced by this partition is only linear anyway, hence the network size can remain linear.\n\nIf tried my best to summarize the key point of this paper and still probably failed at it, which shows how complex is this notion of partition rank, and that its linear growth with network size can either be a good or bad thing depending on the setting. Hopefully, someone will come one day with an explanation that holds in a single slide.\n\n\n\nWhile this is worth publishing as conference paper in its present form, I have two suggestions that, IMHO, would make this work more significant:\n\nOn the theory side, we are still very far from the completeness of the PAC bound papers of the \"shallow era\". In particular, the non-probabilistic lower and upper bound in theorem 1 are probably loose, and there is no PAC-like theory to tell us which one to use and what is the predicted impact on performance (not just the intuition). Also, in the prediction of the inductive bias, the other half is missing. This paper attempts to predict the maximal representation capacity of a DNN under bounded network size constraints, but one of the reason why this size has to be bounded is overfitting (justified by PAC or VC-dim like bounds). If we consider the expected risk as  the sum of the empirical risk and the structural risk, this paper only seems to address fully the empirical risk minimization part, freezing the structural risk. \n\nOn the practice side, an issue is that experiments in this paper mostly confirm what is obvious through intuition, or some simpler form of reasonings. For instance to use convolutions that join pixels which are symmetrical in images to detect symmetry. Basic hand-crafted pattern detectors, as they have been used in computer vision for decades, would just do the job. What would be a great motivation for using this framework is if it answered questions that simple human intuition cannot, and for which we are still in the dark: one example I could think of in the recent use of gated convolutions 'a trous' for 1D speech signal, popularized in Google WaveNet ("}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "The paper uses the notion of separation rank from tensor algebra to analyze the correlations induced through convolution and pooling operations. They show that deep networks have exponentially larger separation ranks compared to shallow ones, and thus, can induce a much richer correlation structure compared to shallow networks. It is argued that this rich inductive bias is crucial for empirical success.\n \n The paper is technically solid. The reviewers note this, and also make a few suggestions on how to make the paper more accessible. The authors have taken this into account. In order to bridge the gap between theory and practice, it is essential for theory papers to be accessible.\n \n The paper covers related work pretty well. One aspect is misses is the recent geometric analysis of deep learning. Can the algebraic analysis be connected to geometric analysis of deep learning, e.g. in the following paper?\n ", "OTHER_KEYS": "ICLR 2017 pcs"}, {"DATE": "09 Jan 2017", "TITLE": "Promising approach to show why deep CNN works well in practice", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer4"}, {"TITLE": "Promising approach to show why deep CNN works well in practice", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer4", "comments": "This paper investigates the fact why deep networks perform well in practice and how modifying the geometry of pooling can make the polynomially\nsized deep network to provide a function with exponentially high separation rank (for certain partitioning.)\n\nIn the authors' previous works, they showed the superiority of deep networks over shallows when the activation function is ReLu and the pooling is max/mean pooling but in the current paper there is no activation function after conv and the pooling is just a multiplication of the node values. Although for the experimental results they've considered both scenarios. \n\nActually, the general reasoning for this problem is hard, therefore, this drawback is not significant and the current contribution adds a reasonable amount of knowledge to the literature. \n\nThis paper studies the convolutional arithmetic circuits and shows how this model can address the inductive biases and how pooling can adjust these biases. \n\nThis interesting contribution gives an intuition about how deep network can capture the correlation between the input variables when its size is polynomial but and correlation is exponential.\n\nIt worth to note that although the authors tried to express their notation and definitions carefully where they were very successful, it would be helpful if they elaborate a bit more on their definitions, expressions, and conclusions in the sense to make them more accessible. \n\n\n\n\n\n\n\n", "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "09 Jan 2017", "REVIEWER_CONFIDENCE": 5}, {"TITLE": "Huge algebraic machinery, so far only used to perform intuitive model selection, but promising direction.", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "The paper provides a highly complex algebraic machinery to analyze the type of functions covered by convolutional network. As in most attempts  in this direction in the literature, the ideal networks described in paper, which have to be interpretable as polynomials over tensors, do not match the type of CNNs used in practice: for instance the Relu non-linearity is replaced with a product of linear functions (or a sum of logs).\n\nWhile the paper is very technical to read, every concept is clearly stated and mathematical terminology properly introduced. Still, I think some the authors could make some effort to make the key concepts more accessible, and give a more intuitive understanding of what the separation rank means rather before piling up different mathematical interpretation.\nMy SVM-era algebra is quite rusted, and I am not familiar with the separation rank framework: it would have been much easier for me to first fully understand a simple and gentle case (shallow network in section 5.3), than the general deep case.\n\nTo summarize my understanding of the key theorem 1 result:\n- The upper bound of the separation rank is used to show that in the shallow case, this rank grows AT MOST linearly with the network size (as measured by the only hidden layer). So exponential network sizes are caused by this rank needing to grow exponentially, as required by the partition.\n- In the deep case, one also uses the case that the upper bound is linear in the size of the network (as measured by the last hidden layer), however, this situation is caused by the selection of a partition (I^low, J^high), and the maximal rank induced by this partition is only linear anyway, hence the network size can remain linear.\n\nIf tried my best to summarize the key point of this paper and still probably failed at it, which shows how complex is this notion of partition rank, and that its linear growth with network size can either be a good or bad thing depending on the setting. Hopefully, someone will come one day with an explanation that holds in a single slide.\n\n\n\nWhile this is worth publishing as conference paper in its present form, I have two suggestions that, IMHO, would make this work more significant:\n\nOn the theory side, we are still very far from the completeness of the PAC bound papers of the \"shallow era\". In particular, the non-probabilistic lower and upper bound in theorem 1 are probably loose, and there is no PAC-like theory to tell us which one to use and what is the predicted impact on performance (not just the intuition). Also, in the prediction of the inductive bias, the other half is missing. This paper attempts to predict the maximal representation capacity of a DNN under bounded network size constraints, but one of the reason why this size has to be bounded is overfitting (justified by PAC or VC-dim like bounds). If we consider the expected risk as  the sum of the empirical risk and the structural risk, this paper only seems to address fully the empirical risk minimization part, freezing the structural risk. \n\nOn the practice side, an issue is that experiments in this paper mostly confirm what is obvious through intuition, or some simpler form of reasonings. For instance to use convolutions that join pixels which are symmetrical in images to detect symmetry. Basic hand-crafted pattern detectors, as they have been used in computer vision for decades, would just do the job. What would be a great motivation for using this framework is if it answered questions that simple human intuition cannot, and for which we are still in the dark: one example I could think of in the recent use of gated convolutions 'a trous' for 1D speech signal, popularized in Google WaveNet (", "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "20 Dec 2016", "REVIEWER_CONFIDENCE": 3}, {"TITLE": "Interesting analysis", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "This paper addresses the question of which functions are well suited to deep networks, as opposed to shallow networks.  The basic intuition is convincing and fairly straightforward.  Pooling operations bring together information.  When information is correlated, it can be more efficiently used if the geometry of pooling regions matches the correlations so that it can be brought together more efficiently.  Shallow networks without layers of localized pooling lack this mechanism to combine correlated information efficiently.\n\nThe theoretical results are focused on convolutional arithmetic circuits, building on prior theoretical results of the authors.  The results make use of the interesting technical notion of separability, which in some sense measures the degree to which a function can be represented as the composition of independent functions.  Because separability is measured relative to a partition of the input, it is an appropriate mechanism for measuring the complexity of functions relative to a particular geometry of pooling operations.  Many of the technical notions are pretty intuitive, although the tensor analysis is pretty terse and not easy to follow without knowledge of the authors\u2019 prior work.\n\nIn some sense the comparison between deep and shallow networks is somewhat misleading, since the shallow networks lack a hierarchical pooling structure.  For example, a shallow convolutional network with RELU and max pooling does not really make sense, since the max occurs over the whole image.  So it seems that the paper is really more of an analysis of the effect of pooling vs. not having pooling.  For example, it is not clear that a deep CNN without pooling would be any more efficient than a shallow network, from this work.\n\nIt is not clear how much the theoretical results depend on the use of a model with product pooling, and how they might be extended to the more common max pooling.  Even if theoretical results are difficult to derive in this case, simple illustrative examples might be helpful.  In fact, if the authors prepare a longer version of the paper for a journal I think the results could be made more intuitive if they could add a simple toy example of a function that can be efficiently represented with a convolutional arithmetic circuit when the pooling structure fits the correlations, and perhaps showing also how this could be represented with a convolutional network with RELU and max pooling.\n\nI would also appreciate a more explicit discussion of how the depth of a deep network affects the separability of functions that can be represented.  A shallow network doesn\u2019t have local pooling, so the difference between deep and shallow if perhaps mostly one of pooling vs. not pooling.  However, practitioners find that very deep networks seem to be more effective than \u201cdeep\u201d networks with only a few convolutional layers and pooling.  The paper does not explicitly discuss whether their results provide insight into this behavior.\n\nOverall, I think that the paper attacks an important problem in an interesting way.  It is not so convincing that this really gets to the heart of why depth is so important, because of the theoretical limitation to arithmetic circuits, and because the comparison is to shallow networks that are without localized pooling. \n", "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 3}, {"DATE": "02 Dec 2016", "TITLE": "Invariances", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2"}]}
{"text": "SEMI-SUPERVISED KNOWLEDGE TRANSFER FOR DEEP LEARNING FROM PRIVATE TRAINING DATA\n1 INTRODUCTION\nSome machine learning applications with great benefits are enabled only through the analysis of sensitive data, such as users\u2019 personal contacts, private photographs or correspondence, or even medical records or genetic sequences (Alipanahi et al., 2015; Kannan et al., 2016; Kononenko, 2001; Sweeney, 1997). Ideally, in those cases, the learning algorithms would protect the privacy of users\u2019 training data, e.g., by guaranteeing that the output model generalizes away from the specifics of any individual user. Unfortunately, established machine learning algorithms make no such guarantee; indeed, though state-of-the-art algorithms generalize well to the test set, they continue to overfit on specific training examples in the sense that some of these examples are implicitly memorized.\nRecent attacks exploiting this implicit memorization in machine learning have demonstrated that private, sensitive training data can be recovered from models. Such attacks can proceed directly, by analyzing internal model parameters, but also indirectly, by repeatedly querying opaque models to gather data for the attack\u2019s analysis. For example, Fredrikson et al. (2015) used hill-climbing on the output probabilities of a computer-vision classifier to reveal individual faces from the training data.\n\u2217Work done while the author was at Google. \u2020Work done both at Google Brain and at OpenAI.\nBecause of those demonstrations\u2014and because privacy guarantees must apply to worst-case outliers, not only the average\u2014any strategy for protecting the privacy of training data should prudently assume that attackers have unfettered access to internal model parameters.\nTo protect the privacy of training data, this paper improves upon a specific, structured application of the techniques of knowledge aggregation and transfer (Breiman, 1994), previously explored by Nissim et al. (2007), Pathak et al. (2010), and particularly Hamm et al. (2016). In this strategy, first, an ensemble (Dietterich, 2000) of teacher models is trained on disjoint subsets of the sensitive data. Then, using auxiliary, unlabeled non-sensitive data, a student model is trained on the aggregate output of the ensemble, such that the student learns to accurately mimic the ensemble. Intuitively, this strategy ensures that the student does not depend on the details of any single sensitive training data point (e.g., of any single user), and, thereby, the privacy of the training data is protected even if attackers can observe the student\u2019s internal model parameters.\nThis paper shows how this strategy\u2019s privacy guarantees can be strengthened by restricting student training to a limited number of teacher votes, and by revealing only the topmost vote after carefully adding random noise. We call this strengthened strategy PATE, for Private Aggregation of Teacher Ensembles. Furthermore, we introduce an improved privacy analysis that makes the strategy generally applicable to machine learning algorithms with high utility and meaningful privacy guarantees\u2014in particular, when combined with semi-supervised learning.\nTo establish strong privacy guarantees, it is important to limit the student\u2019s access to its teachers, so that the student\u2019s exposure to teachers\u2019 knowledge can be meaningfully quantified and bounded. Fortunately, there are many techniques for speeding up knowledge transfer that can reduce the rate of student/teacher consultation during learning. We describe several techniques in this paper, the most effective of which makes use of generative adversarial networks (GANs) (Goodfellow et al., 2014) applied to semi-supervised learning, using the implementation proposed by Salimans et al. (2016). For clarity, we use the term PATE-G when our approach is combined with generative, semisupervised methods. Like all semi-supervised learning methods, PATE-G assumes the student has access to additional, unlabeled data, which, in this context, must be public or non-sensitive. This assumption should not greatly restrict our method\u2019s applicability: even when learning on sensitive data, a non-overlapping, unlabeled set of data often exists, from which semi-supervised methods can extract distribution priors. For instance, public datasets exist for text and images, and for medical data.\nIt seems intuitive, or even obvious, that a student machine learning model will provide good privacy when trained without access to sensitive training data, apart from a few, noisy votes from a teacher quorum. However, intuition is not sufficient because privacy properties can be surprisingly hard to reason about; for example, even a single data item can greatly impact machine learning models trained on a large corpus (Chaudhuri et al., 2011). Therefore, to limit the effect of any single sensitive data item on the student\u2019s learning, precisely and formally, we apply the well-established, rigorous standard of differential privacy (Dwork & Roth, 2014). Like all differentially private algorithms, our learning strategy carefully adds noise, so that the privacy impact of each data item can be analyzed and bounded. In particular, we dynamically analyze the sensitivity of the teachers\u2019 noisy votes; for this purpose, we use the state-of-the-art moments accountant technique from Abadi et al. (2016), which tightens the privacy bound when the topmost vote has a large quorum. As a result, for MNIST and similar benchmark learning tasks, our methods allow students to provide excellent utility, while our analysis provides meaningful worst-case guarantees. In particular, we can bound the metric for privacy loss (the differential-privacy \u03b5) to a range similar to that of existing, real-world privacyprotection mechanisms, such as Google\u2019s RAPPOR (Erlingsson et al., 2014).\nFinally, it is an important advantage that our learning strategy and our privacy analysis do not depend on the details of the machine learning techniques used to train either the teachers or their student. Therefore, the techniques in this paper apply equally well for deep learning methods, or any such learning methods with large numbers of parameters, as they do for shallow, simple techniques. In comparison, Hamm et al. (2016) guarantee privacy only conditionally, for a restricted class of student classifiers\u2014in effect, limiting applicability to logistic regression with convex loss. Also, unlike the methods of Abadi et al. (2016), which represent the state-of-the-art in differentiallyprivate deep learning, our techniques make no assumptions about details such as batch selection, the loss function, or the choice of the optimization algorithm. Even so, as we show in experiments on\nMNIST and SVHN, our techniques provide a privacy/utility tradeoff that equals or improves upon bespoke learning methods such as those of Abadi et al. (2016).\nSection 5 further discusses the related work. Building on this related work, our contributions are as follows:\n\u2022 We demonstrate a general machine learning strategy, the PATE approach, that provides differential privacy for training data in a \u201cblack-box\u201d manner, i.e., independent of the learning algorithm, as demonstrated by Section 4 and Appendix C.\n\u2022 We improve upon the strategy outlined in Hamm et al. (2016) for learning machine models that protect training data privacy. In particular, our student only accesses the teachers\u2019 top vote and the model does not need to be trained with a restricted class of convex losses.\n\u2022 We explore four different approaches for reducing the student\u2019s dependence on its teachers, and show how the application of GANs to semi-supervised learning of Salimans et al. (2016) can greatly reduce the privacy loss by radically reducing the need for supervision.\n\u2022 We present a new application of the moments accountant technique from Abadi et al. (2016) for improving the differential-privacy analysis of knowledge transfer, which allows the training of students with meaningful privacy bounds.\n\u2022 We evaluate our framework on MNIST and SVHN, allowing for a comparison of our results with previous differentially private machine learning methods. Our classifiers achieve an (\u03b5, \u03b4) differential-privacy bound of (2.04, 10\u22125) for MNIST and (8.19, 10\u22126) for SVHN, respectively with accuracy of 98.00% and 90.66%. In comparison, for MNIST, Abadi et al. (2016) obtain a looser (8, 10\u22125) privacy bound and 97% accuracy. For SVHN, Shokri & Shmatikov (2015) report approx. 92% accuracy with \u03b5 > 2 per each of 300,000 model parameters, naively making the total \u03b5 > 600,000, which guarantees no meaningful privacy.\n\u2022 Finally, we show that the PATE approach can be successfully applied to other model structures and to datasets with different characteristics. In particular, in Appendix C PATE protects the privacy of medical data used to train a model based on random forests.\nOur results are encouraging, and highlight the benefits of combining a learning strategy based on semi-supervised knowledge transfer with a precise, data-dependent privacy analysis. However, the most appealing aspect of this work is probably that its guarantees can be compelling to both an expert and a non-expert audience. In combination, our techniques simultaneously provide both an intuitive and a rigorous guarantee of training data privacy, without sacrificing the utility of the targeted model. This gives hope that users will increasingly be able to confidently and safely benefit from machine learning models built from their sensitive data.\n2 PRIVATE LEARNING WITH ENSEMBLES OF TEACHERS\nIn this section, we introduce the specifics of the PATE approach, which is illustrated in Figure 1. We describe how the data is partitioned to train an ensemble of teachers, and how the predictions made by this ensemble are noisily aggregated. In addition, we discuss how GANs can be used in training the student, and distinguish PATE-G variants that improve our approach using generative, semi-supervised methods.\n2.1 TRAINING THE ENSEMBLE OF TEACHERS\nData partitioning and teachers: Instead of training a single model to solve the task associated with dataset (X,Y ), where X denotes the set of inputs, and Y the set of labels, we partition the data in n disjoint sets (Xn, Yn) and train a model separately on each set. As evaluated in Section 4.1, assuming that n is not too large with respect to the dataset size and task complexity, we obtain n classifiers fi called teachers. We then deploy them as an ensemble making predictions on unseen inputs x by querying each teacher for a prediction fi(x) and aggregating these into a single prediction.\nAggregation: The privacy guarantees of this teacher ensemble stems from its aggregation. Let m be the number of classes in our task. The label count for a given class j \u2208 [m] and an input ~x is the number of teachers that assigned class j to input ~x: nj(~x) = |{i : i \u2208 [n], fi(~x) = j}|. If we simply apply plurality\u2014use the label with the largest count\u2014the ensemble\u2019s decision may depend on a single teacher\u2019s vote. Indeed, when two labels have a vote count differing by at most one, there is a tie: the aggregated output changes if one teacher makes a different prediction. We add random noise to the vote counts nj to introduce ambiguity:\nf(x) = argmax j\n{ nj(~x) + Lap ( 1\n\u03b3\n)} (1)\nIn this equation, \u03b3 is a privacy parameter and Lap(b) the Laplacian distribution with location 0 and scale b. The parameter \u03b3 influences the privacy guarantee we can prove. Intuitively, a large \u03b3 leads to a strong privacy guarantee, but can degrade the accuracy of the labels, as the noisy maximum f above can differ from the true plurality.\nWhile we could use an f such as above to make predictions, the noise required would increase as we make more predictions, making the model useless after a bounded number of queries. Furthermore, privacy guarantees do not hold when an adversary has access to the model parameters. Indeed, as each teacher fi was trained without taking into account privacy, it is conceivable that they have sufficient capacity to retain details of the training data. To address these limitations, we train another model, the student, using a fixed number of labels predicted by the teacher ensemble.\n2.2 SEMI-SUPERVISED TRANSFER OF THE KNOWLEDGE FROM AN ENSEMBLE TO A STUDENT\nWe train a student on nonsensitive and unlabeled data, some of which we label using the aggregation mechanism. This student model is the one deployed, in lieu of the teacher ensemble, so as to fix the privacy loss to a value that does not grow with the number of user queries made to the student model. Indeed, the privacy loss is now determined by the number of queries made to the teacher ensemble during student training and does not increase as end-users query the deployed student model. Thus, the privacy of users who contributed to the original training dataset is preserved even if the student\u2019s architecture and parameters are public or reverse-engineered by an adversary.\nWe considered several techniques to trade-off the student model\u2019s quality with the number of labels it needs to access: distillation, active learning, semi-supervised learning (see Appendix B). Here, we only describe the most successful one, used in PATE-G: semi-supervised learning with GANs.\nTraining the student with GANs: The GAN framework involves two machine learning models, a generator and a discriminator. They are trained in a competing fashion, in what can be viewed as a two-player game (Goodfellow et al., 2014). The generator produces samples from the data distribution by transforming vectors sampled from a Gaussian distribution. The discriminator is trained to distinguish samples artificially produced by the generator from samples part of the real data distribution. Models are trained via simultaneous gradient descent steps on both players\u2019 costs. In practice, these dynamics are often difficult to control when the strategy set is non-convex (e.g., a DNN). In their application of GANs to semi-supervised learning, Salimans et al. (2016) made the following modifications. The discriminator is extended from a binary classifier (data vs. generator sample) to a multi-class classifier (one of k classes of data samples, plus a class for generated samples). This classifier is then trained to classify labeled real samples in the correct class, unlabeled real samples in any of the k classes, and the generated samples in the additional class.\nAlthough no formal results currently explain why yet, the technique was empirically demonstrated to greatly improve semi-supervised learning of classifiers on several datasets, especially when the classifier is trained with feature matching loss (Salimans et al., 2016).\nTraining the student in a semi-supervised fashion makes better use of the entire data available to the student, while still only labeling a subset of it. Unlabeled inputs are used in unsupervised learning to estimate a good prior for the distribution. Labeled inputs are then used for supervised learning.\n3 PRIVACY ANALYSIS OF THE APPROACH\nWe now analyze the differential privacy guarantees of our PATE approach. Namely, we keep track of the privacy budget throughout the student\u2019s training using the moments accountant (Abadi et al., 2016). When teachers reach a strong quorum, this allows us to bound privacy costs more strictly.\n3.1 DIFFERENTIAL PRIVACY PRELIMINARIES AND A SIMPLE ANALYSIS OF PATE\nDifferential privacy (Dwork et al., 2006b; Dwork, 2011) has established itself as a strong standard. It provides privacy guarantees for algorithms analyzing databases, which in our case is a machine learning training algorithm processing a training dataset. Differential privacy is defined using pairs of adjacent databases: in the present work, these are datasets that only differ by one training example. Recall the following variant of differential privacy introduced in Dwork et al. (2006a). Definition 1. A randomized mechanismM with domain D and rangeR satisfies (\u03b5, \u03b4)-differential privacy if for any two adjacent inputs d, d\u2032 \u2208 D and for any subset of outputs S \u2286 R it holds that:\nPr[M(d) \u2208 S] \u2264 e\u03b5 Pr[M(d\u2032) \u2208 S] + \u03b4. (2)\nIt will be useful to define the privacy loss and the privacy loss random variable. They capture the differences in the probability distribution resulting from runningM on d and d\u2032. Definition 2. LetM : D \u2192 R be a randomized mechanism and d, d\u2032 a pair of adjacent databases. Let aux denote an auxiliary input. For an outcome o \u2208 R, the privacy loss at o is defined as:\nc(o;M,aux, d, d\u2032) \u2206= log Pr[M(aux, d) = o] Pr[M(aux, d\u2032) = o] . (3)\nThe privacy loss random variable C(M,aux, d, d\u2032) is defined as c(M(d);M,aux, d, d\u2032), i.e. the random variable defined by evaluating the privacy loss at an outcome sampled fromM(d).\nA natural way to bound our approach\u2019s privacy loss is to first bound the privacy cost of each label queried by the student, and then use the strong composition theorem (Dwork et al., 2010) to derive the total cost of training the student. For neighboring databases d, d\u2032, each teacher gets the same training data partition (that is, the same for the teacher with d and with d\u2032, not the same across teachers), with the exception of one teacher whose corresponding training data partition differs. Therefore, the label counts nj(~x) for any example ~x, on d and d\u2032 differ by at most 1 in at most two locations. In the next subsection, we show that this yields loose guarantees.\n3.2 THE MOMENTS ACCOUNTANT: A BUILDING BLOCK FOR BETTER ANALYSIS\nTo better keep track of the privacy cost, we use recent advances in privacy cost accounting. The moments accountant was introduced by Abadi et al. (2016), building on previous work (Bun & Steinke, 2016; Dwork & Rothblum, 2016; Mironov, 2016). Definition 3. LetM : D \u2192 R be a randomized mechanism and d, d\u2032 a pair of adjacent databases. Let aux denote an auxiliary input. The moments accountant is defined as:\n\u03b1M(\u03bb) \u2206 = max aux,d,d\u2032 \u03b1M(\u03bb;aux, d, d\u2032) (4)\nwhere \u03b1M(\u03bb;aux, d, d\u2032) \u2206 = logE[exp(\u03bbC(M,aux, d, d\u2032))] is the moment generating function of the privacy loss random variable.\nThe following properties of the moments accountant are proved in Abadi et al. (2016).\nTheorem 1. 1. [Composability] Suppose that a mechanism M consists of a sequence of adaptive mechanismsM1, . . . ,Mk whereMi : \u220fi\u22121 j=1Rj \u00d7 D \u2192 Ri. Then, for any output sequence o1, . . . , ok\u22121 and any \u03bb\n\u03b1M(\u03bb; d, d \u2032) = k\u2211 i=1 \u03b1Mi(\u03bb; o1, . . . , oi\u22121, d, d \u2032) ,\nwhere \u03b1M is conditioned onMi\u2019s output being oi for i < k. 2. [Tail bound] For any \u03b5 > 0, the mechanismM is (\u03b5, \u03b4)-differentially private for\n\u03b4 = min \u03bb\nexp(\u03b1M(\u03bb)\u2212 \u03bb\u03b5) .\nWe write down two important properties of the aggregation mechanism from Section 2. The first property is proved in Dwork & Roth (2014), and the second follows from Bun & Steinke (2016).\nTheorem 2. Suppose that on neighboring databases d, d\u2032, the label counts nj differ by at most 1 in each coordinate. Let M be the mechanism that reports argmaxj { nj + Lap( 1 \u03b3 ) }\n. Then M satisfies (2\u03b3, 0)-differential privacy. Moreover, for any l, aux, d and d\u2032,\n\u03b1(l;aux, d, d\u2032) \u2264 2\u03b32l(l + 1) (5)\nAt each step, we use the aggregation mechanism with noise Lap( 1\u03b3 ) which is (2\u03b3, 0)-DP. Thus over T steps, we get (4T\u03b32 + 2\u03b3 \u221a 2T ln 1\u03b4 , \u03b4)-differential privacy. This can be rather large: plugging in values that correspond to our SVHN result, \u03b3 = 0.05, T = 1000, \u03b4 = 1e\u22126 gives us \u03b5 \u2248 26 or alternatively plugging in values that correspond to our MNIST result, \u03b3 = 0.05, T = 100, \u03b4 = 1e\u22125 gives us \u03b5 \u2248 5.80.\n3.3 A PRECISE, DATA-DEPENDENT PRIVACY ANALYSIS OF PATE\nOur data-dependent privacy analysis takes advantage of the fact that when the quorum among the teachers is very strong, the majority outcome has overwhelming likelihood, in which case the privacy cost is small whenever this outcome occurs. The moments accountant allows us analyze the composition of such mechanisms in a unified framework.\nThe following theorem, proved in Appendix A, provides a data-dependent bound on the moments of any differentially private mechanism where some specific outcome is very likely.\nTheorem 3. LetM be (2\u03b3, 0)-differentially private and q \u2265 Pr[M(d) 6= o\u2217] for some outcome o\u2217. Let l, \u03b3 \u2265 0 and q < e\n2\u03b3\u22121 e4\u03b3\u22121 . Then for any aux and any neighbor d \u2032 of d,M satisfies\n\u03b1(l;aux, d, d\u2032) \u2264 log((1\u2212 q) ( 1\u2212 q 1\u2212 e2\u03b3q )l + q exp(2\u03b3l)).\nTo upper bound q for our aggregation mechanism, we use the following simple lemma, also proved in Appendix A.\nLemma 4. Let n be the label score vector for a database d with nj\u2217 \u2265 nj for all j. Then\nPr[M(d) 6= j\u2217] \u2264 \u2211 j 6=j\u2217 2 + \u03b3(nj\u2217 \u2212 nj) 4 exp(\u03b3(nj\u2217 \u2212 nj))\nThis allows us to upper bound q for a specific score vector n, and hence bound specific moments. We take the smaller of the bounds we get from Theorems 2 and 3. We compute these moments for a few values of \u03bb (integers up to 8). Theorem 1 allows us to add these bounds over successive steps, and derive an (\u03b5, \u03b4) guarantee from the final \u03b1. Interested readers are referred to the script that we used to empirically compute these bounds, which is released along with our code: https://github. com/tensorflow/models/tree/master/differential_privacy/multiple_teachers\nSince the privacy moments are themselves now data dependent, the final \u03b5 is itself data-dependent and should not be revealed. To get around this, we bound the smooth sensitivity (Nissim et al., 2007) of the moments and add noise proportional to it to the moments themselves. This gives us a differentially private estimate of the privacy cost. Our evaluation in Section 4 ignores this overhead and reports the un-noised values of \u03b5. Indeed, in our experiments on MNIST and SVHN, the scale of the noise one needs to add to the released \u03b5 is smaller than 0.5 and 1.0 respectively.\nHow does the number of teachers affect the privacy cost? Recall that the student uses a noisy label computed in (1) which has a parameter \u03b3. To ensure that the noisy label is likely to be the correct one, the noise scale 1\u03b3 should be small compared to the the additive gap between the two largest vales of nj . While the exact dependence of \u03b3 on the privacy cost in Theorem 3 is subtle, as a general principle, a smaller \u03b3 leads to a smaller privacy cost. Thus, a larger gap translates to a smaller privacy cost. Since the gap itself increases with the number of teachers, having more teachers would lower the privacy cost. This is true up to a point. With n teachers, each teacher only trains on a 1n fraction of the training data. For large enough n, each teachers will have too little training data to be accurate.\nTo conclude, we note that our analysis is rather conservative in that it pessimistically assumes that, even if just one example in the training set for one teacher changes, the classifier produced by that teacher may change arbitrarily. One advantage of our approach, which enables its wide applicability, is that our analysis does not require any assumptions about the workings of the teachers. Nevertheless, we expect that stronger privacy guarantees may perhaps be established in specific settings\u2014when assumptions can be made on the learning algorithm used to train the teachers.\n4 EVALUATION\nIn our evaluation of PATE and its generative variant PATE-G, we first train a teacher ensemble for each dataset. The trade-off between the accuracy and privacy of labels predicted by the ensemble is greatly dependent on the number of teachers in the ensemble: being able to train a large set of teachers is essential to support the injection of noise yielding strong privacy guarantees while having a limited impact on accuracy. Second, we minimize the privacy budget spent on learning the student by training it with as few queries to the ensemble as possible.\nOur experiments use MNIST and the extended SVHN datasets. Our MNIST model stacks two convolutional layers with max-pooling and one fully connected layer with ReLUs. When trained on the entire dataset, the non-private model has a 99.18% test accuracy. For SVHN, we add two hidden layers.1 The non-private model achieves a 92.8% test accuracy, which is shy of the state-of-the-art. However, we are primarily interested in comparing the private student\u2019s accuracy with the one of a non-private model trained on the entire dataset, for different privacy guarantees. The source code for reproducing the results in this section is available on GitHub.2\n4.1 TRAINING AN ENSEMBLE OF TEACHERS PRODUCING PRIVATE LABELS\nAs mentioned above, compensating the noise introduced by the Laplacian mechanism presented in Equation 1 requires large ensembles. We evaluate the extent to which the two datasets considered can be partitioned with a reasonable impact on the performance of individual teachers. Specifically, we show that for MNIST and SVHN, we are able to train ensembles of 250 teachers. Their aggregated predictions are accurate despite the injection of large amounts of random noise to ensure privacy. The aggregation mechanism output has an accuracy of 93.18% for MNIST and 87.79% for SVHN, when evaluated on their respective test sets, while each query has a low privacy budget of \u03b5 = 0.05.\nPrediction accuracy: All other things being equal, the number n of teachers is limited by a tradeoff between the classification task\u2019s complexity and the available data. We train n teachers by partitioning the training data n-way. Larger values of n lead to larger absolute gaps, hence potentially allowing for a larger noise level and stronger privacy guarantees. At the same time, a larger n implies a smaller training dataset for each teacher, potentially reducing the teacher accuracy. We empirically find appropriate values of n for the MNIST and SVHN datasets by measuring the test\n1The model is adapted from https://www.tensorflow.org/tutorials/deep_cnn 2 https://github.com/tensorflow/models/tree/master/differential_privacy/multiple_teachers\nset accuracy of each teacher trained on one of the n partitions of the training data. We find that even for n = 250, the average test accuracy of individual teachers is 83.86% for MNIST and 83.18% for SVHN. The larger size of SVHN compensates its increased task complexity.\nPrediction confidence: As outlined in Section 3, the privacy of predictions made by an ensemble of teachers intuitively requires that a quorum of teachers generalizing well agree on identical labels. This observation is reflected by our data-dependent privacy analysis, which provides stricter privacy bounds when the quorum is strong. We study the disparity of labels assigned by teachers. In other words, we count the number of votes for each possible label, and measure the difference in votes between the most popular label and the second most popular label, i.e., the gap. If the gap is small, introducing noise during aggregation might change the label assigned from the first to the second. Figure 3 shows the gap normalized by the total number of teachers n. As n increases, the gap remains larger than 60% of the teachers, allowing for aggregation mechanisms to output the correct label in the presence of noise.\nNoisy aggregation: For MNIST and SVHN, we consider three ensembles of teachers with varying number of teachers n \u2208 {10, 100, 250}. For each of them, we perturb the vote counts with Laplacian noise of inversed scale \u03b3 ranging between 0.01 and 1. This choice is justified below in Section 4.2. We report in Figure 2 the accuracy of test set labels inferred by the noisy aggregation mechanism for these values of \u03b5. Notice that the number of teachers needs to be large to compensate for the impact of noise injection on the accuracy.\n4.2 SEMI-SUPERVISED TRAINING OF THE STUDENT WITH PRIVACY\nThe noisy aggregation mechanism labels the student\u2019s unlabeled training set in a privacy-preserving fashion. To reduce the privacy budget spent on student training, we are interested in making as few label queries to the teachers as possible. We therefore use the semi-supervised training approach described previously. Our MNIST and SVHN students with (\u03b5, \u03b4) differential privacy of (2.04, 10\u22125) and (8.19, 10\u22126) achieve accuracies of 98.00% and 90.66%. These results improve the differential privacy state-of-the-art for these datasets. Abadi et al. (2016) previously obtained 97% accuracy with a (8, 10\u22125) bound on MNIST, starting from an inferior baseline model without privacy. Shokri & Shmatikov (2015) reported about 92% accuracy on SVHN with \u03b5 > 2 per model parameter and a model with over 300,000 parameters. Naively, this corresponds to a total \u03b5 > 600,000.\nWe apply semi-supervised learning with GANs to our problem using the following setup for each dataset. In the case of MNIST, the student has access to 9,000 samples, among which a subset of either 100, 500, or 1,000 samples are labeled using the noisy aggregation mechanism discussed in Section 2.1. Its performance is evaluated on the 1,000 remaining samples of the test set. Note that this may increase the variance of our test set accuracy measurements, when compared to those computed over the entire test data. For the MNIST dataset, we randomly shuffle the test set to ensure that the different classes are balanced when selecting the (small) subset labeled to train the student. For SVHN, the student has access to 10,000 training inputs, among which it labels 500 or 1,000 samples using the noisy aggregation mechanism. Its performance is evaluated on the remaining 16,032 samples. For both datasets, the ensemble is made up of 250 teachers. We use Laplacian scale of 20 to guarantee an individual query privacy bound of \u03b5 = 0.05. These parameter choices are motivated by the results from Section 4.1.\nIn Figure 4, we report the values of the (\u03b5, \u03b4) differential privacy guarantees provided and the corresponding student accuracy, as well as the number of queries made by each student. The MNIST student is able to learn a 98% accurate model, which is shy of 1% when compared to the accuracy of a model learned with the entire training set, with only 100 label queries. This results in a strict differentially private bound of \u03b5 = 2.04 for a failure probability fixed at 10\u22125. The SVHN student achieves 90.66% accuracy, which is also comparable to the 92.80% accuracy of one teacher learned with the entire training set. The corresponding privacy bound is \u03b5 = 8.19, which is higher than for the MNIST dataset, likely because of the larger number of queries made to the aggregation mechanism.\nWe observe that our private student outperforms the aggregation\u2019s output in terms of accuracy, with or without the injection of Laplacian noise. While this shows the power of semi-supervised learning, the student may not learn as well on different kinds of data (e.g., medical data), where categories are not explicitly designed by humans to be salient in the input space. Encouragingly, as Appendix C illustrates, the PATE approach can be successfully applied to at least some examples of such data.\n5 DISCUSSION AND RELATED WORK\nSeveral privacy definitions are found in the literature. For instance, k-anonymity requires information about an individual to be indistinguishable from at least k \u2212 1 other individuals in the dataset (L. Sweeney, 2002). However, its lack of randomization gives rise to caveats (Dwork & Roth, 2014), and attackers can infer properties of the dataset (Aggarwal, 2005). An alternative definition, differential privacy, established itself as a rigorous standard for providing privacy guarantees (Dwork et al., 2006b). In contrast to k-anonymity, differential privacy is a property of the randomized algorithm and not the dataset itself.\nA variety of approaches and mechanisms can guarantee differential privacy. Erlingsson et al. (2014) showed that randomized response, introduced by Warner (1965), can protect crowd-sourced data collected from software users to compute statistics about user behaviors. Attempts to provide differential privacy for machine learning models led to a series of efforts on shallow machine learning models, including work by Bassily et al. (2014); Chaudhuri & Monteleoni (2009); Pathak et al. (2011); Song et al. (2013), and Wainwright et al. (2012).\nA privacy-preserving distributed SGD algorithm was introduced by Shokri & Shmatikov (2015). It applies to non-convex models. However, its privacy bounds are given per-parameter, and the large number of parameters prevents the technique from providing a meaningful privacy guarantee. Abadi et al. (2016) provided stricter bounds on the privacy loss induced by a noisy SGD by introducing the moments accountant. In comparison with these efforts, our work increases the accuracy of a private MNIST model from 97% to 98% while improving the privacy bound \u03b5 from 8 to 1.9. Furthermore, the PATE approach is independent of the learning algorithm, unlike this previous work. Support for a wide range of architecture and training algorithms allows us to obtain good privacy bounds on an accurate and private SVHN model. However, this comes at the cost of assuming that nonprivate unlabeled data is available, an assumption that is not shared by (Abadi et al., 2016; Shokri & Shmatikov, 2015).\nPathak et al. (2010) first discussed secure multi-party aggregation of locally trained classifiers for a global classifier hosted by a trusted third-party. Hamm et al. (2016) proposed the use of knowledge transfer between a collection of models trained on individual devices into a single model guaranteeing differential privacy. Their work studied linear student models with convex and continuously differentiable losses, bounded and c-Lipschitz derivatives, and bounded features. The PATE approach of this paper is not constrained to such applications, but is more generally applicable.\nPrevious work also studied semi-supervised knowledge transfer from private models. For instance, Jagannathan et al. (2013) learned privacy-preserving random forests. A key difference is that their approach is tailored to decision trees. PATE works well for the specific case of decision trees, as demonstrated in Appendix C, and is also applicable to other machine learning algorithms, including more complex ones. Another key difference is that Jagannathan et al. (2013) modified the classic model of a decision tree to include the Laplacian mechanism. Thus, the privacy guarantee does not come from the disjoint sets of training data analyzed by different decision trees in the random forest, but rather from the modified architecture. In contrast, partitioning is essential to the privacy guarantees of the PATE approach.\n6 CONCLUSIONS\nTo protect the privacy of sensitive training data, this paper has advanced a learning strategy and a corresponding privacy analysis. The PATE approach is based on knowledge aggregation and transfer from \u201cteacher\u201d models, trained on disjoint data, to a \u201cstudent\u201d model whose attributes may be made public. In combination, the paper\u2019s techniques demonstrably achieve excellent utility on the MNIST and SVHN benchmark tasks, while simultaneously providing a formal, state-of-the-art bound on users\u2019 privacy loss. While our results are not without limits\u2014e.g., they require disjoint training data for a large number of teachers (whose number is likely to increase for tasks with many output classes)\u2014they are encouraging, and highlight the advantages of combining semi-supervised learning with precise, data-dependent privacy analysis, which will hopefully trigger further work. In particular, such future work may further investigate whether or not our semi-supervised approach will also reduce teacher queries for tasks other than MNIST and SVHN, for example when the discrete output categories are not as distinctly defined by the salient input space features.\nA key advantage is that this paper\u2019s techniques establish a precise guarantee of training data privacy in a manner that is both intuitive and rigorous. Therefore, they can be appealing, and easily explained, to both an expert and non-expert audience. However, perhaps equally compelling are the techniques\u2019 wide applicability. Both our learning approach and our analysis methods are \u201cblackbox,\u201d i.e., independent of the learning algorithm for either teachers or students, and therefore apply, in general, to non-convex, deep learning, and other learning methods. Also, because our techniques do not constrain the selection or partitioning of training data, they apply when training data is naturally and non-randomly partitioned\u2014e.g., because of privacy, regulatory, or competitive concerns\u2014 or when each teacher is trained in isolation, with a different method. We look forward to such further applications, for example on RNNs and other sequence-based models.\nACKNOWLEDGMENTS\nNicolas Papernot is supported by a Google PhD Fellowship in Security. The authors would like to thank Ilya Mironov and Li Zhang for insightful discussions about early drafts of this document.\nA MISSING DETAILS ON THE ANALYSIS\nWe provide missing proofs from Section 3. Theorem 3. LetM be (2\u03b3, 0)-differentially private and q \u2265 Pr[M(d) 6= o\u2217] for some outcome o\u2217. Let l, \u03b3 \u2265 0 and q < e\n2\u03b3\u22121 e4\u03b3\u22121 . Then for any aux and any neighbor d \u2032 of d,M satisfies\n\u03b1(l;aux, d, d\u2032) \u2264 log((1\u2212 q) ( 1\u2212 q 1\u2212 e2\u03b3q )l + q exp(2\u03b3l)).\nProof. Since M is 2\u03b3-differentially private, for every outcome o, Pr[M(d)=o]Pr[M(d\u2032)=o] \u2264 exp(2\u03b3). Let q\u2032 = Pr[M(d) 6= o\u2217]. Then Pr[M(d\u2032) 6= o\u2217] \u2264 exp(2\u03b3)q\u2032. Thus exp(\u03b1(l;aux, d, d\u2032)) = \u2211 o Pr[M(d) = o] ( Pr[M(d) = o] Pr[M(d\u2032) = o]\n)l = Pr[M(d) = o\u2217] ( Pr[M(d) = o\u2217] Pr[M(d\u2032) = o\u2217] )l + \u2211 o6=o\u2217 Pr[M(d) = o] ( Pr[M(d) = o] Pr[M(d\u2032) = o]\n)l \u2264 (1\u2212 q\u2032) ( 1\u2212 q\u2032 1\u2212 e2\u03b3q\u2032 )l + \u2211 o6=o\u2217 Pr[M(d) = o](e2\u03b3)l\n\u2264 (1\u2212 q\u2032) ( 1\u2212 q\u2032 1\u2212 e2\u03b3q\u2032 )l + q\u2032e2\u03b3l.\nNow consider the function\nf(z) = (1\u2212 z) ( 1\u2212 z 1\u2212 e2\u03b3z )l + ze2\u03b3l.\nWe next argue that this function is non-decreasing in (0, e 2\u03b3\u22121 e4\u03b3\u22121 ) under the conditions of the lemma. Towards this goal, define\ng(z, w) = (1\u2212 z) ( 1\u2212 w 1\u2212 e2\u03b3w )l + ze2\u03b3l,\nand observe that f(z) = g(z, z). We can easily verify by differentiation that g(z, w) is increasing individually in z and in w in the range of interest. This implies that f(q\u2032) \u2264 f(q) completing the proof.\nLemma 4. Let n be the label score vector for a database d with nj\u2217 \u2265 nj for all j. Then\nPr[M(d) 6= j\u2217] \u2264 \u2211 j 6=j\u2217 2 + \u03b3(nj\u2217 \u2212 nj) 4 exp(\u03b3(nj\u2217 \u2212 nj))\nProof. The probability that nj\u2217 + Lap( 1\u03b3 ) < nj + Lap( 1 \u03b3 ) is equal to the probability that the sum of two independent Lap(1) random variables exceeds \u03b3(nj\u2217 \u2212 nj). The sum of two independent Lap(1) variables has the same distribution as the difference of twoGamma(2, 1) random variables. Recalling that theGamma(2, 1) distribution has pdf xe\u2212x, we can compute the pdf of the difference via convolution as\u222b \u221e\ny=0\n(y + |x|)e\u2212y\u2212|x|ye\u2212y dy = 1 e|x| \u222b \u221e y=0 (y2 + y|x|)e\u22122y dy = 1 + |x| 4e|x| .\nThe probability mass in the tail can then be computed by integration as 2+\u03b3(nj\u2217\u2212nj)4 exp(\u03b3(nj\u2217\u2212nj) . Taking a union bound over the various candidate j\u2019s gives the claimed bound.\nB APPENDIX: TRAINING THE STUDENT WITH MINIMAL TEACHER QUERIES\nIn this appendix, we describe approaches that were considered to reduce the number of queries made to the teacher ensemble by the student during its training. As pointed out in Sections 3 and 4, this effort is motivated by the direct impact of querying on the total privacy cost associated with student training. The first approach is based on distillation, a technique used for knowledge transfer and model compression (Hinton et al., 2015). The three other techniques considered were proposed in the context of active learning, with the intent of identifying training examples most useful for learning. In Sections 2 and 4, we described semi-supervised learning, which yielded the best results. The student models in this appendix differ from those in Sections 2 and 4, which were trained using GANs. In contrast, all students in this appendix were learned in a fully supervised fashion from a subset of public, labeled examples. Thus, the learning goal was to identify the subset of labels yielding the best learning performance.\nB.1 TRAINING STUDENTS USING DISTILLATION\nDistillation is a knowledge transfer technique introduced as a means of compressing large models into smaller ones, while retaining their accuracy (Bucilua et al., 2006; Hinton et al., 2015). This is for instance useful to train models in data centers before deploying compressed variants in phones. The transfer is accomplished by training the smaller model on data that is labeled with probability vectors produced by the first model, which encode the knowledge extracted from training data. Distillation is parameterized by a temperature parameter T , which controls the smoothness of probabilities output by the larger model: when produced at small temperatures, the vectors are discrete, whereas at high temperature, all classes are assigned non-negligible values. Distillation is a natural candidate to compress the knowledge acquired by the ensemble of teachers, acting as the large model, into a student, which is much smaller with n times less trainable parameters compared to the n teachers.\nTo evaluate the applicability of distillation, we consider the ensemble of n = 50 teachers for SVHN. In this experiment, we do not add noise to the vote counts when aggregating the teacher predictions. We compare the accuracy of three student models: the first is a baseline trained with labels obtained by plurality, the second and third are trained with distillation at T \u2208 {1, 5}. We use the first 10,000 samples from the test set as unlabeled data. Figure 5 reports the accuracy of the student model on the last 16,032 samples from the test set, which were not accessible to the model during training. It is plotted with respect to the number of samples used to train the student (and hence the number of queries made to the teacher ensemble). Although applying distillation yields classifiers that perform more accurately, the increase in accuracy is too limited to justify the increased privacy cost of revealing the entire probability vector output by the ensemble instead of simply the class assigned the largest number of votes. Thus, we turn to an investigation of active learning.\nB.2 ACTIVE LEARNING OF THE STUDENT\nActive learning is a class of techniques that aims to identify and prioritize points in the student\u2019s training set that have a high potential to contribute to learning (Angluin, 1988; Baum, 1991). If the label of an input in the student\u2019s training set can be predicted confidently from what we have learned so far by querying the teachers, it is intuitive that querying it is not worth the privacy budget spent. In our experiments, we made several attempts before converging to a simpler final formulation.\nSiamese networks: Our first attempt was to train a pair of siamese networks, introduced by Bromley et al. (1993) in the context of one-shot learning and later improved by Koch (2015). The siamese networks take two images as input and return 1 if the images are equal and 0 otherwise. They are two identical networks trained with shared parameters to force them to produce similar representations of the inputs, which are then compared using a distance metric to determine if the images are identical or not. Once the siamese models are trained, we feed them a pair of images where the first is unlabeled and the second labeled. If the unlabeled image is confidently matched with a known labeled image, we can infer the class of the unknown image from the labeled image. In our experiments, the siamese networks were able to say whether two images are identical or not, but did not generalize well: two images of the same class did not receive sufficiently confident matches. We also tried a variant of this approach where we trained the siamese networks to output 1 when the two\nimages are of the same class and 0 otherwise, but the learning task proved too complicated to be an effective means for reducing the number of queries made to teachers.\nCollection of binary experts: Our second attempt was to train a collection of binary experts, one per class. An expert for class j is trained to output 1 if the sample is in class j and 0 otherwise. We first trained the binary experts by making an initial batch of queries to the teachers. Using the experts, we then selected available unlabeled student training points that had a candidate label score below 0.9 and at least 4 other experts assigning a score above 0.1. This gave us about 500 unconfident points for 1700 initial label queries. After labeling these unconfident points using the ensemble of teachers, we trained the student. Using binary experts improved the student\u2019s accuracy when compared to the student trained on arbitrary data with the same number of teacher queries. The absolute increases in accuracy were however too limited\u2014between 1.5% and 2.5%.\nIdentifying unconfident points using the student: This last attempt was the simplest yet the most effective. Instead of using binary experts to identify student training points that should be labeled by the teachers, we used the student itself. We asked the student to make predictions on each unlabeled training point available. We then sorted these samples by increasing values of the maximum probability assigned to a class for each sample. We queried the teachers to label these unconfident inputs first and trained the student again on this larger labeled training set. This improved the accuracy of the student when compared to the student trained on arbitrary data. For the same number of teacher queries, the absolute increases in accuracy of the student trained on unconfident inputs first when compared to the student trained on arbitrary data were in the order of 4%\u2212 10%.\nC APPENDIX: ADDITIONAL EXPERIMENTS ON THE UCI ADULT AND DIABETES DATASETS\nIn order to further demonstrate the general applicability of our approach, we performed experiments on two additional datasets. While our experiments on MNIST and SVHN in Section 4 used convolutional neural networks and GANs, here we use random forests to train our teacher and student models for both of the datasets. Our new results on these datasets show that, despite the differing data types and architectures, we are able to provide meaningful privacy guarantees.\nUCI Adult dataset: The UCI Adult dataset is made up of census data, and the task is to predict when individuals make over $50k per year. Each input consists of 13 features (which include the age, workplace, education, occupation\u2014see the UCI website for a full list3). The only pre-processing we apply to these features is to map all categorical features to numerical values by assigning an integer value to each possible category. The model is a random forest provided by the scikit-learn Python package. When training both our teachers and student, we keep all the default parameter values, except for the number of estimators, which we set to 100. The data is split between a training set of 32,562 examples, and a test set of 16,282 inputs.\nUCI Diabetes dataset: The UCI Diabetes dataset includes de-identified records of diabetic patients and corresponding hospital outcomes, which we use to predict whether diabetic patients were readmitted less than 30 days after their hospital release. To the best of our knowledge, no particular classification task is considered to be a standard benchmark for this dataset. Even so, it is valuable to consider whether our approach is applicable to the likely classification tasks, such as readmission, since this dataset is collected in a medical environment\u2014a setting where privacy concerns arise frequently. We select a subset of 18 input features from the 55 available in the dataset (to avoid features with missing values) and form a dataset balanced between the two output classes (see the UCI website for more details4). In class 0, we include all patients that were readmitted in a 30-day window, while class 1 includes all patients that were readmitted after 30 days or never readmitted at all. Our balanced dataset contains 34,104 training samples and 12,702 evaluation samples. We use a random forest model identical to the one described above in the presentation of the Adult dataset.\nExperimental results: We apply our approach described in Section 2. For both datasets, we train ensembles of n = 250 random forests on partitions of the training data. We then use the noisy aggregation mechanism, where vote counts are perturbed with Laplacian noise of scale 0.05 to privately label the first 500 test set inputs. We train the student random forest on these 500 test set inputs and evaluate it on the last 11,282 test set inputs for the Adult dataset, and 6,352 test set inputs for the Diabetes dataset. These numbers deliberately leave out some of the test set, which allowed us to observe how the student performance-privacy trade-off was impacted by varying the number of private labels, as well as the Laplacian scale used when computing these labels.\nFor the Adult dataset, we find that our student model achieves an 83% accuracy for an (\u03b5, \u03b4) = (2.66, 10\u22125) differential privacy bound. Our non-private model on the dataset achieves 85% accuracy, which is comparable to the state-of-the-art accuracy of 86% on this dataset (Poulos & Valle, 2016). For the Diabetes dataset, we find that our privacy-preserving student model achieves a 93.94% accuracy for a (\u03b5, \u03b4) = (1.44, 10\u22125) differential privacy bound. Our non-private model on the dataset achieves 93.81% accuracy.\n3 https://archive.ics.uci.edu/ml/datasets/Adult 4 https://archive.ics.uci.edu/ml/datasets/Diabetes+130-US+hospitals+for+years+1999-2008\n", "cats": {"READ": 1.0, "DONTREAD": 0.0}, "reviews": [{"IS_META_REVIEW": true, "comments": "Altogether a very good paper, a nice read, and interesting. The work advances the state of the art on differentially-private deep learning, is quite well-written, and relatively thorough.\n\nOne caveat is that although the approach is intended to be general, no theoretical guarantees are provided about the learning performance. Privacy-preserving machine learning papers often analyze both the privacy (in the worst case, DP setting) and the learning performance (often under different assumptions). Since the learning performance might depend on the choice of architecture; future experimentation is encouraged, even using the same data sets, with different architectures. If this will not be added, then please justify the choice of architecture used, and/or clarify what can be generalized about the observed learning performance.\n\nAnother caveat is that the reported epsilons are not those that can be privately released; the authors note that their technique for doing so would change the resulting epsilon. However this would need to be resolved in order to have a meaningful comparison to the epsilon-delta values reported in related work.\n\nFinally, as has been acknowledged in the paper, the present approach may not work on other natural data types. Experiments on other data sets is strongly encouraged. Also, please cite the data sets used.\n\nOther comments:\n\nDiscussion of certain parts of the related work are thorough. However, please add some survey/discussion of the related work on differentially-private semi-supervised learning. For example, in the context of random forests, the following paper also proposed differentially-private semi-supervised learning via a teacher-learner approach (although not denoted as \u201cteacher-learner\u201d). The only time the private labeled data is used is when learning the \u201cprimary ensemble.\u201d  A \"secondary ensemble\" is then learned only from the unlabeled (non-private) data, with pseudo-labels generated by the primary ensemble.\n\nG. Jagannathan, C. Monteleoni, and K. Pillaipakkamnatt: A Semi-Supervised Learning Approach to Differential Privacy. Proc. 2013 IEEE International Conference on Data Mining Workshops, IEEE Workshop on Privacy Aspects of Data Mining (PADM), 2013.\n\nSection C. does a nice comparison of approaches. Please make sure the quantitative results here constitute an apples-to-apples comparison with the GAN results. \n\nThe paper is extremely well-written, for the most part. Some places needing clarification include:\n- Last paragraph of 3.1. \u201call teachers\u2026.get the same training data\u2026.\u201d This should be rephrased to make it clear that it is not the same w.r.t. all the teachers, but w.r.t. the same teacher on the neighboring database.\n- 4.1: The authors state: \u201cThe number n of teachers is limited by a trade-off between the classification task\u2019s complexity and the available data.\u201d However, since this tradeoff is not formalized, the statement is imprecise. In particular, if the analysis is done in the i.i.d. setting, the tradeoff would also likely depend on the relation of the target hypothesis to the data distribution.\n- Discussion of figure 3 was rather unclear in the text and caption and should be revised for clarity. In the text section, at first the explanation seems to imply that a larger gap is better (as is also indicated in the caption). However later it is stated that the gap stays under 20%. These sentences seem contradictory, which is likely not what was intended."}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "The paper presents a general teacher-student approach for differentially-private learning in which the student learns to predict a noise vote among a set of teachers. The noise allows the student to be differentially private, whilst maintaining good classification accuracies on MNIST and SVHN. The paper is well-written.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"DATE": "12 Jan 2017", "TITLE": "Simple question related with privacy loss", "IS_META_REVIEW": false, "comments": "I think this paper has great impact.\n\nMy question is what is the \"auxiliary input\" in Definition 2.\n\nCould you explain this term in theoretical view and what is that in your paper?", "OTHER_KEYS": "(anonymous)"}, {"DATE": "11 Jan 2017 (modified: 12 Jan 2017)", "TITLE": "Question for student GAN training", "IS_META_REVIEW": false, "comments": "Thank you for providing an interesting paper.\n\nIn the paper, the student model is trained by semi-supervised fashion as suggested in (Salimans et al., 2016).\n\nAs far as I understand, teacher's ensembles are using for supervised learning and nonsensitive data is for unsupervised learning.\n\nSo, my question is \"Where is the generator ?\".\n\nThe aggregation of teacher network is treated as the generator in GAN framework? ", "OTHER_KEYS": "(anonymous)"}, {"DATE": "10 Jan 2017", "TITLE": "Attacker's Model and Goal?", "IS_META_REVIEW": false, "comments": "Hi,\n\nI have few questions about the paper.\n\n1- What attacker's goal did you consider in your paper? Is it recovering the training data, or checking whether a specific sample has been in the training data? \n\n2- If the attacker's goal is to recover the training data, does the attacker want to recover the exact data or an approximation would be OK?\n\n3- Talking about neural networks:\n- Do you think there is any attack method to recover an exact training data from the learning model?\n- Do you think there is any defense method to prevent an attacker from recovering even an approximate training data?\n\n4- How can we quantify the strength of a learning model (specifically neural networks) without any defensive mechanism?\n\n5- How can we quantify the strength of a learning model which has not been trained on exact training data? For example, some forms of adversarial training methods never train the model on the clean data; instead, at each epoch, the model is trained on different adversarial data derived from the real data. \n- How can the model \"memorize\" the training data, when 1) it has never seen the real data, 2) it has been trained on different data in different epochs?\n\n6- How do you compare the performance of your method with adversarial training?\n\nThanks.", "OTHER_KEYS": "(anonymous)"}, {"IMPACT": 4, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "TITLE": "Nice paper, strong accept", "comments": "This paper addresses the problem of achieving differential privacy in a very general scenario where a set of teachers is trained on disjoint subsets of sensitive data and the student performs prediction based on public data labeled by teachers through noisy voting. I found the approach altogether plausible and very clearly explained by the authors. Adding more discussion of the bound (and its tightness) from Theorem 1 itself would be appreciated. A simple idea of adding perturbation error to the counts, known from differentially-private literature, is nicely re-used by the authors and elegantly applied in a much broader (non-convex setting) and practical context than in a number of differentially-private and other related papers. The generality of the approach, clear improvement over predecessors, and clarity of the writing makes the method worth publishing.", "ORIGINALITY": 5, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "RECOMMENDATION": 9, "DATE": "17 Dec 2016", "CLARITY": 5, "REVIEWER_CONFIDENCE": 4}, {"OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "TITLE": "Good theory", "comments": "This paper discusses how to guarantee privacy for training data. In the proposed approach multiple models trained with disjoint datasets are used as ``teachers'' model, which will train a ``student'' model to predict an output chosen by noisy voting among all of the teachers. \n\nThe theoretical results are nice but also intuitive. Since teachers' result are provided via noisy voting, the student model may not duplicate the teacher's behavior. However, the probabilistic bound has quite a number of  empirical parameters, which makes me difficult to decide whether the security is 100% guaranteed or not.\n\nThe experiments on MNIST and SVHN are good. However, as the paper claims, the proposed approach may be mostly useful for sensitive data like medical histories, it will be nice to conduct one or two experiments on such applications. ", "ORIGINALITY": 5, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "16 Dec 2016", "CLARITY": 5, "REVIEWER_CONFIDENCE": 3}, {"IMPACT": 5, "APPROPRIATENESS": 5, "RECOMMENDATION_UNOFFICIAL": 5, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "Altogether a very good paper, a nice read, and interesting. The work advances the state of the art on differentially-private deep learning, is quite well-written, and relatively thorough.\n\nOne caveat is that although the approach is intended to be general, no theoretical guarantees are provided about the learning performance. Privacy-preserving machine learning papers often analyze both the privacy (in the worst case, DP setting) and the learning performance (often under different assumptions). Since the learning performance might depend on the choice of architecture; future experimentation is encouraged, even using the same data sets, with different architectures. If this will not be added, then please justify the choice of architecture used, and/or clarify what can be generalized about the observed learning performance.\n\nAnother caveat is that the reported epsilons are not those that can be privately released; the authors note that their technique for doing so would change the resulting epsilon. However this would need to be resolved in order to have a meaningful comparison to the epsilon-delta values reported in related work.\n\nFinally, as has been acknowledged in the paper, the present approach may not work on other natural data types. Experiments on other data sets is strongly encouraged. Also, please cite the data sets used.\n\nOther comments:\n\nDiscussion of certain parts of the related work are thorough. However, please add some survey/discussion of the related work on differentially-private semi-supervised learning. For example, in the context of random forests, the following paper also proposed differentially-private semi-supervised learning via a teacher-learner approach (although not denoted as \u201cteacher-learner\u201d). The only time the private labeled data is used is when learning the \u201cprimary ensemble.\u201d  A \"secondary ensemble\" is then learned only from the unlabeled (non-private) data, with pseudo-labels generated by the primary ensemble.\n\nG. Jagannathan, C. Monteleoni, and K. Pillaipakkamnatt: A Semi-Supervised Learning Approach to Differential Privacy. Proc. 2013 IEEE International Conference on Data Mining Workshops, IEEE Workshop on Privacy Aspects of Data Mining (PADM), 2013.\n\nSection C. does a nice comparison of approaches. Please make sure the quantitative results here constitute an apples-to-apples comparison with the GAN results. \n\nThe paper is extremely well-written, for the most part. Some places needing clarification include:\n- Last paragraph of 3.1. \u201call teachers\u2026.get the same training data\u2026.\u201d This should be rephrased to make it clear that it is not the same w.r.t. all the teachers, but w.r.t. the same teacher on the neighboring database.\n- 4.1: The authors state: \u201cThe number n of teachers is limited by a trade-off between the classification task\u2019s complexity and the available data.\u201d However, since this tradeoff is not formalized, the statement is imprecise. In particular, if the analysis is done in the i.i.d. setting, the tradeoff would also likely depend on the relation of the target hypothesis to the data distribution.\n- Discussion of figure 3 was rather unclear in the text and caption and should be revised for clarity. In the text section, at first the explanation seems to imply that a larger gap is better (as is also indicated in the caption). However later it is stated that the gap stays under 20%. These sentences seem contradictory, which is likely not what was intended.", "ORIGINALITY": 5, "IS_ANNOTATED": true, "TITLE": "A nice contribution to differentially-private deep learning", "IS_META_REVIEW": false, "RECOMMENDATION": 9, "DATE": "16 Dec 2016", "CLARITY": 5, "REVIEWER_CONFIDENCE": 4}, {"DATE": "14 Dec 2016", "TITLE": "Input of student model", "IS_META_REVIEW": false, "comments": "Thanks for interesting and well-organized papers. I have a question about teacher-student model. \n\nTeachers are trained on sensitive data, and students are trained on non-sensitive data.\nI wonder how students work on the outputs of teachers.\nSensitive and non-sensitive are different attributes, so I think there are no correlation between teachers and students. \n\nPlease give me some more details. Thanks.  \n", "OTHER_KEYS": "(anonymous)"}, {"IMPACT": 4, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "TITLE": "Clarifications", "comments": "", "ORIGINALITY": 5, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "DATE": "03 Dec 2016", "CLARITY": 5}, {"IMPACT": 5, "APPROPRIATENESS": 5, "RECOMMENDATION_UNOFFICIAL": 5, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "", "ORIGINALITY": 5, "IS_ANNOTATED": true, "TITLE": "What are the challenges involved in your proposed future work?", "IS_META_REVIEW": false, "DATE": "02 Dec 2016", "CLARITY": 5}, {"OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "TITLE": "theory and experiment", "comments": "", "ORIGINALITY": 5, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "DATE": "18 Nov 2016 (modified: 19 Nov 2016)", "CLARITY": 5}, {"IS_META_REVIEW": true, "comments": "Altogether a very good paper, a nice read, and interesting. The work advances the state of the art on differentially-private deep learning, is quite well-written, and relatively thorough.\n\nOne caveat is that although the approach is intended to be general, no theoretical guarantees are provided about the learning performance. Privacy-preserving machine learning papers often analyze both the privacy (in the worst case, DP setting) and the learning performance (often under different assumptions). Since the learning performance might depend on the choice of architecture; future experimentation is encouraged, even using the same data sets, with different architectures. If this will not be added, then please justify the choice of architecture used, and/or clarify what can be generalized about the observed learning performance.\n\nAnother caveat is that the reported epsilons are not those that can be privately released; the authors note that their technique for doing so would change the resulting epsilon. However this would need to be resolved in order to have a meaningful comparison to the epsilon-delta values reported in related work.\n\nFinally, as has been acknowledged in the paper, the present approach may not work on other natural data types. Experiments on other data sets is strongly encouraged. Also, please cite the data sets used.\n\nOther comments:\n\nDiscussion of certain parts of the related work are thorough. However, please add some survey/discussion of the related work on differentially-private semi-supervised learning. For example, in the context of random forests, the following paper also proposed differentially-private semi-supervised learning via a teacher-learner approach (although not denoted as \u201cteacher-learner\u201d). The only time the private labeled data is used is when learning the \u201cprimary ensemble.\u201d  A \"secondary ensemble\" is then learned only from the unlabeled (non-private) data, with pseudo-labels generated by the primary ensemble.\n\nG. Jagannathan, C. Monteleoni, and K. Pillaipakkamnatt: A Semi-Supervised Learning Approach to Differential Privacy. Proc. 2013 IEEE International Conference on Data Mining Workshops, IEEE Workshop on Privacy Aspects of Data Mining (PADM), 2013.\n\nSection C. does a nice comparison of approaches. Please make sure the quantitative results here constitute an apples-to-apples comparison with the GAN results. \n\nThe paper is extremely well-written, for the most part. Some places needing clarification include:\n- Last paragraph of 3.1. \u201call teachers\u2026.get the same training data\u2026.\u201d This should be rephrased to make it clear that it is not the same w.r.t. all the teachers, but w.r.t. the same teacher on the neighboring database.\n- 4.1: The authors state: \u201cThe number n of teachers is limited by a trade-off between the classification task\u2019s complexity and the available data.\u201d However, since this tradeoff is not formalized, the statement is imprecise. In particular, if the analysis is done in the i.i.d. setting, the tradeoff would also likely depend on the relation of the target hypothesis to the data distribution.\n- Discussion of figure 3 was rather unclear in the text and caption and should be revised for clarity. In the text section, at first the explanation seems to imply that a larger gap is better (as is also indicated in the caption). However later it is stated that the gap stays under 20%. These sentences seem contradictory, which is likely not what was intended."}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "The paper presents a general teacher-student approach for differentially-private learning in which the student learns to predict a noise vote among a set of teachers. The noise allows the student to be differentially private, whilst maintaining good classification accuracies on MNIST and SVHN. The paper is well-written.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"DATE": "12 Jan 2017", "TITLE": "Simple question related with privacy loss", "IS_META_REVIEW": false, "comments": "I think this paper has great impact.\n\nMy question is what is the \"auxiliary input\" in Definition 2.\n\nCould you explain this term in theoretical view and what is that in your paper?", "OTHER_KEYS": "(anonymous)"}, {"DATE": "11 Jan 2017 (modified: 12 Jan 2017)", "TITLE": "Question for student GAN training", "IS_META_REVIEW": false, "comments": "Thank you for providing an interesting paper.\n\nIn the paper, the student model is trained by semi-supervised fashion as suggested in (Salimans et al., 2016).\n\nAs far as I understand, teacher's ensembles are using for supervised learning and nonsensitive data is for unsupervised learning.\n\nSo, my question is \"Where is the generator ?\".\n\nThe aggregation of teacher network is treated as the generator in GAN framework? ", "OTHER_KEYS": "(anonymous)"}, {"DATE": "10 Jan 2017", "TITLE": "Attacker's Model and Goal?", "IS_META_REVIEW": false, "comments": "Hi,\n\nI have few questions about the paper.\n\n1- What attacker's goal did you consider in your paper? Is it recovering the training data, or checking whether a specific sample has been in the training data? \n\n2- If the attacker's goal is to recover the training data, does the attacker want to recover the exact data or an approximation would be OK?\n\n3- Talking about neural networks:\n- Do you think there is any attack method to recover an exact training data from the learning model?\n- Do you think there is any defense method to prevent an attacker from recovering even an approximate training data?\n\n4- How can we quantify the strength of a learning model (specifically neural networks) without any defensive mechanism?\n\n5- How can we quantify the strength of a learning model which has not been trained on exact training data? For example, some forms of adversarial training methods never train the model on the clean data; instead, at each epoch, the model is trained on different adversarial data derived from the real data. \n- How can the model \"memorize\" the training data, when 1) it has never seen the real data, 2) it has been trained on different data in different epochs?\n\n6- How do you compare the performance of your method with adversarial training?\n\nThanks.", "OTHER_KEYS": "(anonymous)"}, {"IMPACT": 4, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "TITLE": "Nice paper, strong accept", "comments": "This paper addresses the problem of achieving differential privacy in a very general scenario where a set of teachers is trained on disjoint subsets of sensitive data and the student performs prediction based on public data labeled by teachers through noisy voting. I found the approach altogether plausible and very clearly explained by the authors. Adding more discussion of the bound (and its tightness) from Theorem 1 itself would be appreciated. A simple idea of adding perturbation error to the counts, known from differentially-private literature, is nicely re-used by the authors and elegantly applied in a much broader (non-convex setting) and practical context than in a number of differentially-private and other related papers. The generality of the approach, clear improvement over predecessors, and clarity of the writing makes the method worth publishing.", "ORIGINALITY": 5, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "RECOMMENDATION": 9, "DATE": "17 Dec 2016", "CLARITY": 5, "REVIEWER_CONFIDENCE": 4}, {"OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "TITLE": "Good theory", "comments": "This paper discusses how to guarantee privacy for training data. In the proposed approach multiple models trained with disjoint datasets are used as ``teachers'' model, which will train a ``student'' model to predict an output chosen by noisy voting among all of the teachers. \n\nThe theoretical results are nice but also intuitive. Since teachers' result are provided via noisy voting, the student model may not duplicate the teacher's behavior. However, the probabilistic bound has quite a number of  empirical parameters, which makes me difficult to decide whether the security is 100% guaranteed or not.\n\nThe experiments on MNIST and SVHN are good. However, as the paper claims, the proposed approach may be mostly useful for sensitive data like medical histories, it will be nice to conduct one or two experiments on such applications. ", "ORIGINALITY": 5, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "16 Dec 2016", "CLARITY": 5, "REVIEWER_CONFIDENCE": 3}, {"IMPACT": 5, "APPROPRIATENESS": 5, "RECOMMENDATION_UNOFFICIAL": 5, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "Altogether a very good paper, a nice read, and interesting. The work advances the state of the art on differentially-private deep learning, is quite well-written, and relatively thorough.\n\nOne caveat is that although the approach is intended to be general, no theoretical guarantees are provided about the learning performance. Privacy-preserving machine learning papers often analyze both the privacy (in the worst case, DP setting) and the learning performance (often under different assumptions). Since the learning performance might depend on the choice of architecture; future experimentation is encouraged, even using the same data sets, with different architectures. If this will not be added, then please justify the choice of architecture used, and/or clarify what can be generalized about the observed learning performance.\n\nAnother caveat is that the reported epsilons are not those that can be privately released; the authors note that their technique for doing so would change the resulting epsilon. However this would need to be resolved in order to have a meaningful comparison to the epsilon-delta values reported in related work.\n\nFinally, as has been acknowledged in the paper, the present approach may not work on other natural data types. Experiments on other data sets is strongly encouraged. Also, please cite the data sets used.\n\nOther comments:\n\nDiscussion of certain parts of the related work are thorough. However, please add some survey/discussion of the related work on differentially-private semi-supervised learning. For example, in the context of random forests, the following paper also proposed differentially-private semi-supervised learning via a teacher-learner approach (although not denoted as \u201cteacher-learner\u201d). The only time the private labeled data is used is when learning the \u201cprimary ensemble.\u201d  A \"secondary ensemble\" is then learned only from the unlabeled (non-private) data, with pseudo-labels generated by the primary ensemble.\n\nG. Jagannathan, C. Monteleoni, and K. Pillaipakkamnatt: A Semi-Supervised Learning Approach to Differential Privacy. Proc. 2013 IEEE International Conference on Data Mining Workshops, IEEE Workshop on Privacy Aspects of Data Mining (PADM), 2013.\n\nSection C. does a nice comparison of approaches. Please make sure the quantitative results here constitute an apples-to-apples comparison with the GAN results. \n\nThe paper is extremely well-written, for the most part. Some places needing clarification include:\n- Last paragraph of 3.1. \u201call teachers\u2026.get the same training data\u2026.\u201d This should be rephrased to make it clear that it is not the same w.r.t. all the teachers, but w.r.t. the same teacher on the neighboring database.\n- 4.1: The authors state: \u201cThe number n of teachers is limited by a trade-off between the classification task\u2019s complexity and the available data.\u201d However, since this tradeoff is not formalized, the statement is imprecise. In particular, if the analysis is done in the i.i.d. setting, the tradeoff would also likely depend on the relation of the target hypothesis to the data distribution.\n- Discussion of figure 3 was rather unclear in the text and caption and should be revised for clarity. In the text section, at first the explanation seems to imply that a larger gap is better (as is also indicated in the caption). However later it is stated that the gap stays under 20%. These sentences seem contradictory, which is likely not what was intended.", "ORIGINALITY": 5, "IS_ANNOTATED": true, "TITLE": "A nice contribution to differentially-private deep learning", "IS_META_REVIEW": false, "RECOMMENDATION": 9, "DATE": "16 Dec 2016", "CLARITY": 5, "REVIEWER_CONFIDENCE": 4}, {"DATE": "14 Dec 2016", "TITLE": "Input of student model", "IS_META_REVIEW": false, "comments": "Thanks for interesting and well-organized papers. I have a question about teacher-student model. \n\nTeachers are trained on sensitive data, and students are trained on non-sensitive data.\nI wonder how students work on the outputs of teachers.\nSensitive and non-sensitive are different attributes, so I think there are no correlation between teachers and students. \n\nPlease give me some more details. Thanks.  \n", "OTHER_KEYS": "(anonymous)"}, {"IMPACT": 4, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "TITLE": "Clarifications", "comments": "", "ORIGINALITY": 5, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "DATE": "03 Dec 2016", "CLARITY": 5}, {"IMPACT": 5, "APPROPRIATENESS": 5, "RECOMMENDATION_UNOFFICIAL": 5, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "", "ORIGINALITY": 5, "IS_ANNOTATED": true, "TITLE": "What are the challenges involved in your proposed future work?", "IS_META_REVIEW": false, "DATE": "02 Dec 2016", "CLARITY": 5}, {"OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "TITLE": "theory and experiment", "comments": "", "ORIGINALITY": 5, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "DATE": "18 Nov 2016 (modified: 19 Nov 2016)", "CLARITY": 5}]}
{"text": "DEEP VARIATIONAL CANONICAL CORRELATION ANALYSIS\n1 INTRODUCTION\nIn the multi-view representation learning setting, we have multiple views/measurements of the same underlying signal, and the goal is to learn useful features of each view using complementary information contained in the views. The intuition underlying this setting is that the learned features can help uncover the common sources of variation in the views, which can be helpful for exploratory analysis or for downstream tasks.\nA classical approach in this setting is canonical correlation analysis (CCA, Hotelling, 1936) and its nonlinear extensions, including the kernel extension (Lai and Fyfe, 2000; Akaho, 2001; Melzer et al., 2001; Bach and Jordan, 2002) and the deep neural network (DNN) extension (Andrew et al., 2013; Wang et al., 2015b). CCA projects two random vectors x \u2208 Rdx and y \u2208 Rdy into a lowerdimensional subspace so that the projections are maximally correlated. There is a probabilistic latent variable model interpretation of linear CCA (Bach and Jordan, 2005) as shown in Figure 1 (left). Assume that x and y are linear functions of some lower-dimensional random variable z \u2208 Rdz , where dz \u2264 min(dx, dy). When the prior distribution of the latent variable p(z) and the conditional distributions p(x|z) and p(y|z) are Gaussian, Bach and Jordan (2005) showed that E[z|x] (resp. E[z|y]) lives in the same space as the linear CCA projection for x (resp. y). This generative interpretation of CCA is often lost in nonlinear extensions of CCA. For example, in deep CCA (DCCA, (Andrew et al., 2013)), to extend CCA to nonlinear mappings with greater representation power, one extracts nonlinear features from the original inputs of each view using two DNNs, f for x and g for y, so that the canonical correlation of the DNN outputs (measured by a linear CCA with projection matrices U and V) is maximized. Formally, given a dataset of N pairs of observations (x1,y1), . . . , (xN ,yN ) of the random vectors (x,y), DCCA optimizes\nmax Wf ,Wg\nU,V\ntr ( U>f(X)g(Y)>V ) s.t. U> ( f(X)f(X)> ) U = V> ( g(Y)g(Y)> ) V = NI, (1)\nwhere f(X) = [f(x1), . . . , f(xN )] and g(Y) = [g(y1), . . . ,g(yN )], and Wf denotes all weight parameters of the DNN f (and similarly for g).\nDCCA has achieved good performance in the multi-view representation learning setting across different domains (Wang et al., 2015b,a; Lu et al., 2015; Yan and Mikolajczyk, 2015). However, a disadvantage of DCCA is that it directly looks for DNNs that can map inputs into the low-dimensional space, without a model for generating samples from the latent space. Although Wang et al. (2015b)\u2019s deep canonically correlated autoencoders (DCCAE) model optimizes the combination of the autoencoder objective (reconstruction errors) and the canonical correlation objective, the authors found that in practice, the canonical correlation term tends to dominate the reconstruction error terms in the DCCAE objective when tuning performance for a downstream task (especially when the inputs are noisy), and as a result the inputs are not reconstructed well. At the same time, optimization of the DCCA and DCCAE objectives is challenging due to the constraints that couple all training samples.\nThe main contribution of this paper is the proposal of a new deep multi-view learning model named deep variational CCA (VCCA), which extends the latent variable model interpretation of linear CCA to nonlinear observation models parameterized by DNNs. Computing the marginal data likelihood, as well as inference of the latent variables, are intractable under this model. Inspired by variational autoencoders (VAE, Kingma and Welling, 2014), we parameterize the posterior distribution of the latent variables with another DNN, and derive a variational lower bound of the data likelihood as the objective of VCCA, which is further approximated by Monte Carlo sampling. With the reparameterization trick, sampling for the Monte Carlo approximation is trivial and all DNN weights in VCCA can be optimized jointly via stochastic gradient descent, using unbiased gradient estimates from small minibatches. Interestingly, VCCA is related to multi-view autoencoders (Ngiam et al., 2011), with the key distinctions of additional regularization on the posterior distribution and the sampling procedure at the bottleneck layer.\nWe also propose a variant of VCCA called VCCA-private that can, in addition to the \u201ccommon variables\u201d underlying both views, extract the \u201cprivate variables\u201d within each view. We demonstrate that VCCA-private is able to disentangle the shared and private information for multi-view data without hard supervision. Last but not least, as generative models, VCCA and VCCA-private enable us to obtain high-quality samples for the input of each view.\n2 VARIATIONAL CCA\nThe probabilistic latent variable model of CCA (Bach and Jordan, 2005) defines the following joint distribution over the random variables (x,y):\np(x,y, z) = p(z)p(x|z)p(y|z), p(x,y) = \u222b p(x,y, z)dz. (2)\nThe assumption underlying this model is that, conditioned on the latent variables z \u2208 Rdz , the two views x and y are independent. However, linear observation models (p(x|z) and p(y|z) as shown in Figure 1 (left)) have limited representation power. In this paper, we consider nonlinear\nobservation models p\u03b8(x|z;\u03b8x) and p\u03b8(y|z;\u03b8y), parameterized by \u03b8x and \u03b8y respectively, which can be the collections of weights of DNNs. In this case, the marginal likelihood p\u03b8(x,y) does not have a closed form. In addition, the inference problem p\u03b8(z|x)\u2014the problem of inferring the latent variables given one of the views\u2014is also intractable.\nInspired by Kingma and Welling (2014)\u2019s work on variational autoencoders (VAE), we approximate p\u03b8(z|x) with the conditional density q\u03c6(z|x;\u03c6z), where \u03c6z is the collection of parameters of another DNN.1 We can derive a lower bound on the marginal data likelihood using q\u03c6(z|x):\nlog p\u03b8(x,y) = log p\u03b8(x,y) \u222b q\u03c6(z|x)dz = \u222b log p\u03b8(x,y)q\u03c6(z|x)dz\n= \u222b q\u03c6(z|x) ( log\nq\u03c6(z|x) p\u03b8(z|x,y) + log p\u03b8(x,y, z) q\u03c6(z|x)\n) dz\n= DKL(q\u03c6(z|x)||p\u03b8(z|x,y)) + Eq\u03c6(z|x) [ log p\u03b8(x,y, z)\nq\u03c6(z|x) ] \u2265 Eq\u03c6(z|x) [ log p\u03b8(x,y, z)\nq\u03c6(z|x)\n] =: L(x,y;\u03b8,\u03c6) (3)\nwhere we used the fact that KL divergence is nonnegative in the last step. As a result, L(x,y;\u03b8,\u03c6) is a lower bound on the data log-likelihood log\u03b8 p(x,y). Substituting (2) into (3), we have\nL(x,y;\u03b8,\u03c6) = \u222b q\u03c6(z|x) ( log p(z)\nq\u03c6(z|x) + log p\u03b8(x|z) + log p\u03b8(y|z)\n) dz\n= \u2212DKL(q\u03c6(z|x)||p(z)) + Eq\u03c6(z|x) [log p\u03b8(x|z) + log p\u03b8(y|z)] . (4) VCCA maximizes this variational lower bound on the data likelihood on the training set:\nmax \u03b8,\u03c6\n1\nN N\u2211 i=1 L(xi,yi;\u03b8,\u03c6). (5)\nThe first term in (4) measures the KL divergence between the approximate posterior distribution and the prior distribution of the latent variables z. When the parameterization q\u03c6(z|x) is chosen properly, this term can be computed exactly in closed form. As a concrete example, let the variational approximate posterior be a multivariate Gaussian with diagonal covariance. That is, for a sample pair (xi,yi), we have\nlog q\u03c6(zi|xi) = logN (zi;\u00b5i,\u03a3i), \u03a3i = diag ( \u03c32i1, . . . , \u03c3 2 idz ) , (6)\nwhere the mean \u00b5i and covariance \u03a3i are outputs of an encoding DNN f (and thus [\u00b5i,\u03a3i] = f(xi;\u03c6z) are deterministic nonlinear functions of xi). In this case, we have\nDKL(q\u03c6(zi|xi)||p(zi)) = \u2212 1\n2 dz\u2211 j=1 ( 1 + log \u03c32ij \u2212 \u03c32ij \u2212 \u00b52ij ) .\nThe second term of (4) corresponds to the expected complete data likelihood under the approximate posterior distribution. Though still intractable, this term can be approximated by Monte Carlo sampling. In particular, we draw L samples z(l)i \u223c q\u03c6(zi|xi):\nz (l) i = \u00b5i + \u03a3i (l), where (l) \u223c N (0, I), for l = 1, . . . , L, (7) and have\nEq\u03c6(zi|xi) [log p\u03b8(xi|zi) + log p\u03b8(yi|zi)] \u2248 1\nL L\u2211 l=1 log p\u03b8 ( xi|z(l)i ) + log p\u03b8 ( yi|z(l)i ) . (8)\nNotice that we parameterized q\u03c6(zi|xi) above to obtain the VCCA objective; this is useful when the first view is available for downstream tasks, in which case we can directly apply q\u03c6(zi|xi) to obtain its projection (as features). One could also derive likelihood lower bounds by parameterizing the approximate posteriors q\u03c6(zi|yi) and q\u03c6(zi|xi,yi), and optimize their convex combinations for training. We give a sketch of VCCA in Figure 1 (right).\n1For notational simplicity, we denote by \u03b8 the collection of parameters associated with the model probabilities p\u03b8(\u00b7), and \u03c6 the collection of parameters associated with the variational approximate probabilities q\u03c6(\u00b7), and often omit specific parameters inside the probabilities.\nConnection to multi-view autoencoder (MVAE) If we use the Gaussian observation models\nlog p\u03b8(x|z) = logN (gx(z;\u03b8x), I), log p\u03b8(y|z) = logN (gy(z;\u03b8y), I), we observe that log p\u03b8 ( xi|z(l)i ) and log p\u03b8 ( yi|z(l)i ) measure the reconstruction errors of each\nview\u2019s inputs from samples z(l)i using the two DNNs gx and gy respectively. In this case, maximizing L(x,y;\u03b8,\u03c6) is equivalent to\nmin \u03b8,\u03c6\n1\nN N\u2211 i=1 DKL(q\u03c6(zi|xi)||p(zi)) + 1 2NL N\u2211 i=1 L\u2211 l=1 \u2225\u2225\u2225xi \u2212 gx (z(l)i ;\u03b8x)\u2225\u2225\u22252 + \u2225\u2225\u2225yi \u2212 gy (z(l)i ;\u03b8y)\u2225\u2225\u22252 (9)\ns.t. z(l)i = \u00b5i + \u03a3i (l), where (l) \u223c N (0, I), l = 1, . . . , L.\nNow, consider the case of \u03a3i \u2192 0, for i = 1, . . . , N , and we have z(l)i \u2192 \u00b5i which is a deterministic function of x (and there is no need for sampling). In the limit, the second term of (9) reduces to\n1\n2N N\u2211 i=1 \u2016xi \u2212 gx(f(xi;\u03c6z);\u03b8x)\u2016 2 + \u2016yi \u2212 gy(f(xi;\u03c6z);\u03b8y)\u2016 2 , (10)\nwhich is the objective of the multi-view autoencoder (MVAE, Ngiam et al., 2011). Note, however, that \u03a3i \u2192 0 is prevented by the VCCA objective as it results in a large penalty in DKL(q\u03c6(zi|xi)||p(zi)). Compared with the MVAE objective, in the VCCA objective we are creating L different \u201cnoisy\u201d versions of the latent representation and enforce that these versions reconstruct the original inputs well. The \u201cnoise\u201d distribution (the variances \u03a3i) are also learned and regularized by the KL divergence DKL(q\u03c6(zi|xi)||p(zi)). Using the VCCA objective, we expect to learn different representations from those of MVAE, due to these regularization effects.\n2.1 EXTRACTING PRIVATE VARIABLES\nSo far, VCCA aims at extracting only the latent variables z that are common to both views. A potential disadvantage of this model is that it assumes the common variables are sufficient by themselves to generate the views, which can be too restrictive in practice. Consider the example of audio and articulatory measurements as two views for speech. Although the transcription is a common variable behind the views, it combines with the physical environment and the vocal tract anatomy to generate the individual views. In other words, there might be large variations in the input space that can not be explained by the common variables, making the objective (4) hard to optimize. It may then be beneficial to explicitly model the private variables within each view.\nWe therefore propose a new probabilistic graphical model, shown in Figure 2, that we refer to as VCCA-private. We introduce two sets of hidden variables hx \u2208 Rdhx and hy \u2208 Rdhy to explain the aspects of x and y not captured by the common variables z. Under this model, the data likelihood\nis defined by\np\u03b8(x,y, z,hx,hy) = p(z)p(hx)p(hy)p\u03b8(x|z,hx;\u03b8x)p\u03b8(y|z,hy;\u03b8y), (11)\np\u03b8(x,y) = \u222b \u222b \u222b p\u03b8(x,y, z,hx,hy)dz dhx dhy.\nTo obtain tractable inference, we introduce the following factored variational posterior\nq\u03c6(z,hx,hy|x,y) = q\u03c6(z|x;\u03c6z)q\u03c6(hx|x;\u03c6x)q\u03c6(hy|y;\u03c6y), (12)\nwhere each factor is parameterized by a different DNN. Similarly to VCCA, we can derive a variational lower bound on the data likelihood for VCCA-private as\nlog p\u03b8(x,y) \u2265 \u222b \u222b \u222b q\u03c6(z,hx,hy|x,y) log p\u03b8(x,y, z,hx,hy)\nq\u03c6(z,hx,hy|x,y) dz dhx dhy\n= \u222b \u222b \u222b q\u03c6(z,hx,hy|x,y) [ log p(z)\nq\u03c6(z|x) + log\np(hx)\nq\u03c6(hx|x) + log\np(hy)\nq\u03c6(hy|y) + log p\u03b8(x|z,hx) + log p\u03b8(y|z,hy) ] dz dhx dhy\n= \u2212DKL(q\u03c6(z|x)||p(z))\u2212DKL(q\u03c6(hx|x)||p(hx))\u2212DKL(q\u03c6(hy|y)||p(hy))\n+ \u222b \u222b q\u03c6(z|x)q\u03c6(hx|x) log p\u03b8(x|z,hx)dz dhx + \u222b \u222b q\u03c6(z|x)q\u03c6(hy|y) log p\u03b8(y|z,hy)dz dhy\n=: Lprivate(x,y;\u03b8,\u03c6). (13)\nAs in VCCA, the last two terms of (14) can be approximated by Monte Carlo sampling. For example, we draw samples of z and hx from their corresponding approximate posteriors, and concatenate their samples as inputs to the DNN parameterizing p\u03b8(x|z,hx). In this paper, we use simple Gaussian prior distributions for the private variables, i.e., hx \u223c N (0, I) and hy \u223c N (0, I). We leave to future work to examine the effect of more sophisticated prior distributions for the latent variables.\nVCCA-private maximizes this lower bound on the training set, i.e.,\nmax \u03b8,\u03c6\n1\nN N\u2211 i=1 Lprivate(xi,yi;\u03b8,\u03c6). (14)\nOptimization The objectives (5) and (14) decouple over the training samples and can be trained efficiently using stochastic gradient descent. Enabled by the reparameterization trick, unbiased gradient estimates are obtained by Monte Carlo sampling and the standard backpropagation procedure on minibatches of training samples. We apply the ADAM algorithm (Kingma and Ba, 2015) for optimizing our objectives.\n3 RELATED WORK\nRecently, there has been much interest in unsupervised deep generative models (Kingma and Welling, 2014; Rezende et al., 2014; Goodfellow et al., 2014; Gregor et al., 2015; Makhzani et al., 2016; Burda et al., 2016; Alain et al., 2016). A common motivation behind these models is that, with the expressive power of DNNs, the generative models can capture distributions for complex inputs. Additionally, if we are able to generate realistic samples from the learned distribution, we can infer that we have discovered the underlying structure of the data, which may allow us to reduce the sample complexity for learning for downstream tasks. These previous models have mostly focused on single-view data. Here we focus on the multi-view setting where multiple views of the data are present for feature extraction but only one view is available at test time (in downstream tasks).\nSome recent work has explored deep generative models for (semi-)supervised learning. Kingma et al. (2014) built a generative model based on variational autoencoders (VAEs) for semi-supervised classification, where the authors model the input distribution with two set of latent variables: the class label (if it is missing) and another set that models the intra-class variabilities (styles). Sohn\net al. (2015) proposed a conditional generative model for structured output prediction, where the authors explicitly model the uncertainty in the input/output using Gaussian latent variables. While there are two set of observations (input and output labels) in these work, their graphical models are different from that of VCCA.\nOur work is also related to the deep multi-view probabilistic models based on restricted Boltzmann machines (Srivastava and Salakhutdinov, 2014; Sohn et al., 2014). We note that these are undirected graphical models for which both inference and learning are difficult, and one typically resorts to carefully designed variational approximation and Gibbs sampling procedures for training such models. In contrast, our models only require sampling from simple, standard distributions (such as Gaussians), and all parameters can be learned end-to-end by standard stochastic gradient methods. Therefore, our models are more scalable than the previous multi-view probabilistic models.\nOn the other hand, there is a rich literature in modeling multi-view data using the same or similar graphical models behind VCCA/VCCA-private (Wang, 2007; Jia et al., 2010; Salzmann et al., 2010; Virtanen et al., 2011; Memisevic et al., 2012; Klami et al., 2013). Our methods differ from previous work in parameterizing the probability distributions using DNNs. This makes the model more powerful, while still having tractable objectives and efficient end-to-end training using the local reparameterization technique. We note that, unlike earlier work on probabilistic models of linear CCA (Bach and Jordan, 2005), VCCA does not optimize the same criterion, nor produce the same solution, as any linear or nonlinear CCA. However, we retain the terminology in order to clarify the connection with earlier work on probabilistic models for CCA, which we are extending with DNN models for the observations and for the variational posterior distribution approximation.\n4 EXPERIMENTAL RESULTS\nIn this section, we compare different multi-view representation learning algorithms on three tasks involving several domains: image-image, speech-articulation, and image-text. The algorithms we choose to compare below are closely related to the proposed model or have been shown to have strong empirical performance under similar settings.\n\u2022 Linear CCA: its probabilistic interpretation motivates this work.\n\u2022 Deep CCA (DCCA) (Andrew et al., 2013): see its objective in (1).\n\u2022 Deep canonically correlated autoencoders (DCCAE) (Wang et al., 2015b): combination of the DCCA objective and the reconstruction errors of each view.\n\u2022 Multi-view autoencoder (MVAE) (Ngiam et al., 2011): see its objective in (10).\n\u2022 Multi-view contrastive loss (Hermann and Blunsom, 2014): based on the intuition that the distance between embeddings of paired examples x+ and y+ should be smaller than the distance between embeddings of x+ and an unmatched negative example y\u2212 by a margin:\nmin f,g Lcontrast :=\n1\nN N\u2211 i max ( 0, m+ dis ( f(x+i ), g(y + i ) ) \u2212 dis ( f(x+i ), g(y \u2212 i ) )) ,\nwhere y\u2212i is a randomly sampled view 2 example, and m is a margin hyperparameter. We use the cosine distance dis (a,b) = 1\u2212 \u2329\na \u2016a\u2016 , b \u2016b\u2016\n\u232a .\n4.1 NOISY MNIST DATASET\nWe first demonstrate our algorithms on the noisy MNIST dataset used by Wang et al. (2015b). The dataset is generated using the MNIST dataset (LeCun et al., 1998), which consists of 28 \u00d7 28 grayscale digit images, with 60K/10K images for training/testing. We first linearly rescale the pixel values to the range [0, 1]. Then, we randomly rotate the images at angles uniformly sampled from [\u2212\u03c0/4, \u03c0/4] and the resulting images are used as view 1 inputs. For each view 1 image, we randomly select an image of the same identity (0-9) from the original dataset, add independent random noise uniformly sampled from [0, 1] to each pixel, and truncate the pixel final values to [0, 1] to obtain the corresponding view 2 sample. Selection of input images are given in Figure 3 (left). The original\ntraining set is further split into training/tuning sets of size 50K/10K. The data generation process ensures that the digit identity is the only common variable underlying both views.\nTo evaluate the amount of class information extracted by different methods, after unsupervised learning of latent representations, we reveal the labels and train a linear SVM on the projected view 1 training data (using the one-versus-all scheme), and use it to classify the projected test set. This experiment simulates the typical usage of multi-view learning methods, which is to extract useful representations for downstream discriminative tasks.\nNote that this synthetic dataset perfectly satisfies the multi-view assumption that the two views are independent given the class label, so the latent representation should contain precisely the class information. This is indeed achieved by CCA-based and contrastive loss-based multi-view approaches. In Figure 3 (right), we show 2D t-SNE (van der Maaten and Hinton, 2008) visualizations of the original view 1 inputs and view 1 projections by various deep multi-view methods.\nWe use DNNs with 3 hidden layers of 1024 rectified linear units (ReLUs, Nair and Hinton, 2010) each to parameterize the distributions: q\u03c6(z|x), p\u03b8(x|z), p\u03b8(y|z) in VCCA, and additionally q\u03c6(hx|x) and q\u03c6(hy|y) in VCCA-private. The capacities of these networks are the same as those of their counterparts in DCCA and DCCAE from Wang et al. (2015b). The reconstruction networks p\u03b8(x|z) or p\u03b8(x|z,hx) model each pixel of x as an independent Bernoulli variable and parameterize its mean (using a sigmoid activation); p\u03b8(y|z) and p\u03b8(y|z,hy) model y with diagonal Gaussians and parameterize the mean (using a sigmoid activation) and standard deviation for each pixel dimension. We tune the dimensionality dz over {10, 20, 30, 40, 50}, and fix dhx = dhy = 30 for VCCA-private. We select the hyperparameter combination that yields the best SVM classification accuracy on the projected tuning set, and report the corresponding accuracy on the projected test set.\nThe effect of dropout We add dropout (Srivastava et al., 2014) to all intermediate layers and the input layers and find it to be very useful in our models, with most of the gain coming from dropout applied to the samples of z, hx and hy . This is because dropout encourages each latent dimension to reconstruct the inputs well in the absence of other dimensions, and therefore avoids learning coadapted features. Intuitively, in VCCA-private dropout also helps to prevent the degenerate situation where the pathways x \u2192 hx \u2192 x and y \u2192 hy \u2192 y achieve good reconstruction while ignoring z (e.g., by setting it to a constant). We use the same dropout rate for all layers and tune it over {0, 0.1, 0.2, 0.3, 0.4}. We show the 2D t-SNE embeddings of the common variables z learned by VCCA and VCCA-private on test set in Figure 4. We observe that in general, VCCA/VCCA-private tend to separate the classes\nin the projection well; dropout significantly improves the performance of both VCCA and VCCAprivate, with the latter slightly outperforming the former. While such class separation can also be achieved by DCCA/contrastive loss as well, these methods can not naturally generate samples in the input space. On the other hand, such separation is not achieved by multi-view autoencoders.\nThe effect of private variables on reconstructions We show sample reconstructions (mean and standard deviation) by VCCA for the view 2 images from the test set in Figure 5 (columns 2 and 3). We observe that for each input, the mean reconstruction of yi by VCCA is a prototypical image of the same digit, regardless of the individual style in yi. This is to be expected, as yi contains an arbitrary image of the same digit as xi, and the variation in background noise in yi does not appear in xi and can not be reflected in q\u03c6(z|x); thus the best way for p\u03b8(y|z) to model yi is to output a prototypical image of that class to achieve on average small reconstruction error. On the other hand, since yi contains little rotation of the digits, this variation is suppressed to a large extent in q\u03c6(z|x) (it is no longer the major variation in z as in the original inputs).\nWe show sample reconstructions by VCCA-private for the same set of view 2 images in Figure 5 (columns 4 and 5). With the help of private variables hy (as part of the input to p\u03b8(y|z,hy)), the model does a much better job in reconstructing the styles of y. And by disentangling the private variables from the shared variables, q\u03c6(z|x) achieves even better class separation than VCCA does.\nWe also note that the standard deviation of the reconstruction is low within the digit and high outside the digit, implying that p\u03b8(y|z,hy) is able to separate the background noise from the digit image.\nDisentanglement of private/shared variables In Figure 6 (in Appendix) we provide the 2D tSNE embeddings of the shared variables z (top row) and the private variables hx (bottom row) learned by VCCA-private. In the embedding of hx, digits with different identities but the same rotation are mapped close together, and the rotation varies smoothly from left to right, confirming that the private variables contain little class information but mainly style information.\nFinally, we give the test error rates of linear SVMs applied to the features learned with different models in Table 1. VCCA-private is comparable in performance to the best previous approach (DCCAE), while having the advantage that it can also generate.\n4.2 XRMB SPEECH-ARTICULATION DATASET\nWe now consider the task of learning acoustic features for speech recognition. We use data from the Wisconsin X-ray microbeam (XRMB) corpus (Westbury, 1994), which contains simultaneously recorded speech and articulatory measurements from 47 American English speakers. We follow the setup of Wang et al. (2015a,b) and use the learned features for speaker-independent phonetic recognition.2 The two input views are standard 39D acoustic features (13 mel frequency cepstral coefficients (MFCCs) and their first and second derivatives) and 16D articulatory features (horizontal/vertical displacement of 8 pellets attached to several parts of the vocal tract), each then concatenated over a 7-frame window around each frame to incorporate context. The speakers are split into disjoint sets of 35/8/2/2 speakers for feature learning/recognizer training/tuning/testing. The 35 speakers for feature learning are fixed; the remaining 12 are used in a 6-fold experiment (recognizer training on 8 speakers, tuning on 2 speakers, and testing on the remaining 2 speakers). Each speaker has roughly 50K frames. We remove the per-speaker mean and variance of the articulatory measurements for each training speaker, and remove the mean of the acoustic measurements for each utterance. All learned feature types are used in a \u201ctandem\u201d speech recognizer (Hermansky et al., 2000), i.e., they are appended to the original 39D features and used in a standard hidden Markov model (HMM)-based recognizer with Gaussian mixture observation distributions.\nEach algorithm uses up to 3 ReLU hidden layers, each of 1500 units, for the projection and reconstruction mappings. For VCCA/VCCA-private, we use Gaussian observation models as the inputs are real-valued. In contrast to the MNIST experiments, we do not learn the standard deviations of each output dimension on training data, as this leads to poor downstream task performance. Instead, we use isotropic covariances for each view, and tune the standard deviations by grid search. The best model uses a smaller standard deviation (0.1) for the view 2 than for view 1 (1.0), effectively putting more emphasis on the reconstruction of articulatory measurements. Our best performing VCCA model uses dz = 70, while the best performing VCCA-private model uses dz = 70 and dhx = dhy = 10.\n2As in Wang and Livescu (2016), we use the Kaldi toolkit (Povey et al., 2011) for feature extraction and recognition with hidden Markov models. Our results do not match Wang et al. (2015a,b) (who instead used the HTK toolkit (Young et al., 1999)) for the same types of features, but the relative results are consistent.\nThe mean phone error rates (PER) over 6 folds obtained by different algorithms are given in Table 1. Our methods achieve competitive performance in comparison to previous deep multi-view methods.\n4.3 MIR-FLICKR DATASET\nFinally, we consider the task of learning cross-modality features for topic classification on the MIRFlickr database (Huiskes and Lew, 2008). The Flickr database contains 1 million images accompanied by user tags, among which 25000 images are labeled with 38 topic classes (each image may be categorized as multiple topics). We use the same image and text features as in previous work (Srivastava and Salakhutdinov, 2014; Sohn et al., 2014): the image feature is 3857 dimensional real-valued vector, composed of Pyramid Histogram of Words (PHOW) (Bosch et al., 2007), GIST (Oliva and Torralba, 2001), and MPEG-7 descriptors (Manjunath et al., 2001), while the text feature is a 2000-dimensional binary vector of frequent tags.\nFollowing the same protocol as Sohn et al. (2014), we train multi-view representations using the unlabelled data,3 and use projected image features of the labeled data (further divided into splits of 10000/5000/10000 samples for training/tuning/testing) for training and evaluating a classifier that predicts the topic labels, corresponding to the unimodal query task in Srivastava and Salakhutdinov (2014); Sohn et al. (2014). For each algorithm, we select the model which achieves the highest mean average precision (mAP) on the validation set, and report its performance on the test set.\nEach algorithm uses up to 4 ReLU hidden layers, each of 1024 units, for the projection and reconstruction mappings. For VCCA/VCCA-private, we use Gaussian observation models with isotropic covariance for image features, with standard deviation tuned by grid search, and a Bernoulli model for text features. In this experiment, we also found it helpful to tune an additional trade-off parameter for the text-view likelihood (cross-entropy); the best VCCA/VCCA-private models prefer a large trade-off parameter of the level 104, emphasizing the reconstruction of the sparse text-view inputs. Our best performing VCCA model uses dz = 1024, while the best performing VCCA-private model uses dz = 1024 and dhx = dhy = 16.\nAs shown in Table 1, VCCA/VCCA-private achieve significantly higher mAPs than other methods considered here. Being much easier to train, the performance of our methods are competitive with the previous state-of-the-art mAP result of 0.607 achieved by the multi-view RBMs of Sohn et al. (2014) under the same setting.\n5 CONCLUSIONS\nWe have proposed variational canonical correlation analysis (VCCA), a deep generative method for multi-view representation learning. Our method embodies a natural idea for multi-view learning: the multiple views can be generated from a small set of shared latent variables. VCCA is parameterized by DNNs and can be trained efficiently by backpropagation, and is therefore scalable. We have also shown that, by modeling the private variables that are specific to each view, the VCCA-private variant can disentangle shared/private variables and provide higher-quality reconstructions.\nIn the future, we will explore other prior distributions such as mixtures of Gaussians or discrete random variables, which may enforce clustering in the latent space and in turn work better for discriminative tasks. We will also explore other observation models, including replacing the autoencoder objective with that of adversarial networks (Goodfellow et al., 2014; Makhzani et al., 2016; Chen et al., 2016). Another direction is to explicitly incorporate the structure of the inputs, such as the sequence structure of speech and text and the spatial structure of images.\nACKOWLEDGEMENTS\nThis research was supported by NSF grant IIS-1321015. The opinions expressed in this work are those of the authors and do not necessarily reflect the views of the funding agency. This research used GPUs donated by NVIDIA Corporation.\n3As in Sohn et al. (2014), we exclude about 250000 samples which contain fewer than two tags.\nA ADDITIONAL T-SNE VISUALIZATION OF NOISY MNIST\n", "cats": {"READ": 1.0, "DONTREAD": 0.0}, "reviews": [{"IS_META_REVIEW": true, "comments": "7\n\nSummary:\nThis paper describes the use of variational autoencoders for multi-view representation learning as an alternative to canonical correlation analysis (CCA), deep CCA (DCCA), and multi-view autoencoders (MVAE). Two variants of variational autoencoders (which the authors call VCCA and VCCA-private) are investigated. The method\u2019s performances are compared on a synthetic MNIST dataset, the XRMB speech-articulation dataset, and the MIR-Flickr dataset.\n\nReview:\nVariational autoencoders are widely used and their performance for multi-view representation learning should be of interest to the ICLR community. The paper is well written and clear. The experiments are thorough. It is interesting that the performance of MVAE and VCCA is quite different given the similarity of their objective functions. I further find the analyses of the effects of dropout and private variables useful.\n\nAs the authors point out, \u201cVCCA does not optimize the same criterion, nor produce the same solution, as any linear or nonlinear CCA\u201d. It would have been interesting to discuss the differences of a linear variant of VCCA and linear CCA, and to compare it quantitatively. While it might not make sense to use variational inference in the linear case, it would nevertheless help to understand the differences better.\n\nThe derivations in Equation 3 and Equation 13 seem unnecessarily detailed given that VCCA and VCCA-p are special cases of VAE, only with certain architectural constraints. Perhaps move to the Appendix?\n\nIn Section 3 the authors claim that \u201cif we are able to generate realistic samples from the learned distribution, we can infer that we have discovered the underlying structure of the data\u201d. This is not correct, a model which hasn\u2019t learned a thing can have perfectly realistic samples (see Theis et al., 2016). Please remove or revise the sentence.\n\nMinor:\nIn the equation between Equation 8 and 9, using notation N(x; g(z, theta), I) as in Equation 6 would make it clearer."}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "The application of VAEs to multiview settings, snd the general problem of learning representations over multiple modalities is of increasing importance. After discussion and reviewing the rebuttals, the reviewers felt that the paper is not yet ready. The application is strong, but that more can be done in terms of praticality, generalisation using other posteriors, and the comparisons made. For this reason, the paper is unfortunately not yet ready for inclusion in this year's proceedings.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"DATE": "18 Jan 2017", "TITLE": "updated response", "IS_META_REVIEW": false, "comments": "Dear reviewers,\n\nWe have modified our response to your review, acknowledging the related work pointed out by several reviewers, and clarifying our main contributions. \n\nThanks!", "OTHER_KEYS": "Weiran Wang"}, {"TITLE": "No Title", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "This paper considers the case where multiple views of data are learned through a probabilistic deep neural network formulation. This makes the model non-linear (unlike e.g. CCA) but makes inference difficult. Therefore, the VAE framework is invoked for inference.\n\nIn [Ref 1] the authors show that maximum likelihood estimation based on their linear latent model leads to the canonical correlation directions. But in the non-linear case with DNNs it's not clear (at least with the present analysis) what the solution is wrt to the canonical directions. There's no such analysis in the paper, hence I find it a stretch to refer to this model as a CCA type of model. In contrast, e.g. DCCA / DCCAE are taking the canonical correlation between features into account inside the objective and provide interpretations.\n\n[Ref 1] F. R. Bach and M. I. Jordan. A probabilistic interpretation of canonical correlation analysis. Technical Report 688, 2005.\n\nThere is also a significant body of very related work on non-linear multi-view models which is not discussed in this paper. For example, there's been probabilistic non-linear multi-view models [Ref 2, 3], also extended to the Bayesian case with common/private spaces [Ref 4] and the variational / deep learning case [Ref 5].\n\n[Ref 2] Ek et al. Gaussian process latent variable models for human pose estimation. MLMI, 2007.\n[Ref 3] Shon et al. Learning shared latent structure for image synthesis and robotic imitation. NIPS, 2006.\n[Ref 4] Damianou et al. Manifold relevance determination. ICML, 2012.\n[Ref 5] Damianou and Lawrence. Deep Gaussian processes. AISTATS, 2013.\n\nI can see the utility of this model as bringing together two elements: multi-view modeling and VAEs. This seems like an obvious idea but to the best of my knowledge it hasn't been done before and is actually a potentially very useful model.\n\nHowever, the question is, what is the proper way of extending VAE to multiple views? The paper didn't convince me that VAE can work well with multiple views using the shown straightforward construction. Specifically, VCCA doesn't seem to promote the state of the art in terms of results (it actually is overall below the SOA), while the VCCA-private seems a quite ill-posed model: the dimensionalities d have to be manually tuned with exhaustive search; further, the actual model does not provide a consinstent way of encouraging the private and common variables to avoid learning redundant information. Relying only on dropout for this seems a quite ad-hoc solution (in fact, from Fig. 4 (ver2) it seems that the dropout rate is quite crucial). Perhaps good performance might be achieved with a lot of tuning (which might be why the FLICKR results got better in ver2 without changing the model), but it seems quite difficult to optimize for the above reasons. From a purely experimental point of view, VCCA-private doesn't seem to promote the SOA either. Of course one wouldn't expect any new published paper to beat all previous baselines, but it seems that extension of VAE to multiple views is a very interesting idea which deserves some more investigation of how to do it efficiently.\n\nAnother issue is the approximate posterior being parameterized only from one of the views. This makes the model less useful as a generic multi-view model, since it will misbehave in tasks other than classification. But if classification is the main objective, then one should compare to a proper classification model, e.g. a feedforward neural network.\n\nThe plots of Fig. 8 are very nice. Overall, the paper convinced me that there is merit in attaching multiple views to VAE. However, it didn't convince me a) that the proposed way to achieve this is practical b) that there is a connection to CCA (other than being a method for multiple views). The bottom line is that, although the paper is interesting, it needs a little more work.\n", "IS_META_REVIEW": false, "RECOMMENDATION": 5, "DATE": "17 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "Review: Deep Variational Canonical Correlation Analysis", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "UPDATE: I have read the replies on this thread. My opinion has not changed.\n\nThe authors propose deep VCCA, a deep version of the probabilistic CCA model by using likelihoods parameterized by nonlinear functions (neural nets). Variational inference is applied with an inference network and reparameterization gradients. An additional variant, termed VCCA-private, is also introduced, which includes local latent variables for each data point (view). A connection to the multi view auto encoder is also shown.\n\nSince the development of black box variational inference and variational auto-encoders, the methodology in model-specific papers like this one are arguably not very interesting. The model is a straightforward extension of probabilistic CCA with neural net parameterized likelihoods. Inference is mechanically the same as any black box approach using the reparameterization gradient and inference networks. The approach also uses a mean-field approximation, which is quite old given the many recent developments in more expressive approximations (see, e.g., Rezende and Mohamed (2015); Tran et al. (2016)).\n\nThe connection to multi-view auto encoders is at first insightful, but no more than the difference between MAP and variational inference. This is a well-known insight: in the abstract, the authors argue that the key distinction is the additional sampling, but ultimately what matters is the KL regularizer. Even with noisy samples, the variances of a normal variational approximation would collapse to zero and thus become a point mass approximation, equivalent to optimizing a point estimate from the MVAE objective. (I suspect the authors know this to some degree due to their remarks in the paper, but it is unclear.)\n\nThat said, I think the paper has strong merits in application. The experiments are strong, comparing to alternative multi-view approaches under a number of interesting data sets. While the use of \"private variables\" is simple, they demonstrate how it can successfully disentangle the per-view latent representation from the shared view. It would have been preferable to compare to methods using probabilistic inference, such as full Bayes for the linear CCA. \n\nThere are also a number of approximations taken to almost be standard in the paper which may not be necessary, such as the use of a mean-field family or the use of an inference network. To separate out how much the approximate inference is influencing the fit of the model, I strongly recommend using MCMC and non-amortized variational inference on at least one experiment. \n\n+ Rezende, D. J., & Mohamed, S. (2015). Variational Inference with Normalizing Flows. Presented at the International Conference on Machine Learning.\n+ Tran, D., Ranganath, R., & Blei, D. M. (2016). The Variational Gaussian Process. Presented at the International Conference on Learning Representations.", "IS_META_REVIEW": false, "RECOMMENDATION": 5, "DATE": "16 Dec 2016 (modified: 21 Dec 2016)", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "Linear VCCA", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "7\n\nSummary:\nThis paper describes the use of variational autoencoders for multi-view representation learning as an alternative to canonical correlation analysis (CCA), deep CCA (DCCA), and multi-view autoencoders (MVAE). Two variants of variational autoencoders (which the authors call VCCA and VCCA-private) are investigated. The method\u2019s performances are compared on a synthetic MNIST dataset, the XRMB speech-articulation dataset, and the MIR-Flickr dataset.\n\nReview:\nVariational autoencoders are widely used and their performance for multi-view representation learning should be of interest to the ICLR community. The paper is well written and clear. The experiments are thorough. It is interesting that the performance of MVAE and VCCA is quite different given the similarity of their objective functions. I further find the analyses of the effects of dropout and private variables useful.\n\nAs the authors point out, \u201cVCCA does not optimize the same criterion, nor produce the same solution, as any linear or nonlinear CCA\u201d. It would have been interesting to discuss the differences of a linear variant of VCCA and linear CCA, and to compare it quantitatively. While it might not make sense to use variational inference in the linear case, it would nevertheless help to understand the differences better.\n\nThe derivations in Equation 3 and Equation 13 seem unnecessarily detailed given that VCCA and VCCA-p are special cases of VAE, only with certain architectural constraints. Perhaps move to the Appendix?\n\nIn Section 3 the authors claim that \u201cif we are able to generate realistic samples from the learned distribution, we can infer that we have discovered the underlying structure of the data\u201d. This is not correct, a model which hasn\u2019t learned a thing can have perfectly realistic samples (see Theis et al., 2016). Please remove or revise the sentence.\n\nMinor:\nIn the equation between Equation 8 and 9, using notation N(x; g(z, theta), I) as in Equation 6 would make it clearer.", "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"DATE": "14 Dec 2016", "TITLE": "revision notes", "IS_META_REVIEW": false, "comments": "Dear reviewers,\n\nWe just uploaded a slightly modified version. This version \n\n-- contains the updated results on Flickr and the proposed methods significantly outperform others\n-- reorganized the figures and tables to make the paper more compact\n\nThanks!\n", "OTHER_KEYS": "Weiran Wang"}, {"DATE": "03 Dec 2016", "TITLE": "VCCA and DCCAE", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1"}, {"DATE": "02 Dec 2016", "TITLE": "Lower bound and using q(z | x) instead of q(z | x, y)", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3"}, {"IS_META_REVIEW": true, "comments": "7\n\nSummary:\nThis paper describes the use of variational autoencoders for multi-view representation learning as an alternative to canonical correlation analysis (CCA), deep CCA (DCCA), and multi-view autoencoders (MVAE). Two variants of variational autoencoders (which the authors call VCCA and VCCA-private) are investigated. The method\u2019s performances are compared on a synthetic MNIST dataset, the XRMB speech-articulation dataset, and the MIR-Flickr dataset.\n\nReview:\nVariational autoencoders are widely used and their performance for multi-view representation learning should be of interest to the ICLR community. The paper is well written and clear. The experiments are thorough. It is interesting that the performance of MVAE and VCCA is quite different given the similarity of their objective functions. I further find the analyses of the effects of dropout and private variables useful.\n\nAs the authors point out, \u201cVCCA does not optimize the same criterion, nor produce the same solution, as any linear or nonlinear CCA\u201d. It would have been interesting to discuss the differences of a linear variant of VCCA and linear CCA, and to compare it quantitatively. While it might not make sense to use variational inference in the linear case, it would nevertheless help to understand the differences better.\n\nThe derivations in Equation 3 and Equation 13 seem unnecessarily detailed given that VCCA and VCCA-p are special cases of VAE, only with certain architectural constraints. Perhaps move to the Appendix?\n\nIn Section 3 the authors claim that \u201cif we are able to generate realistic samples from the learned distribution, we can infer that we have discovered the underlying structure of the data\u201d. This is not correct, a model which hasn\u2019t learned a thing can have perfectly realistic samples (see Theis et al., 2016). Please remove or revise the sentence.\n\nMinor:\nIn the equation between Equation 8 and 9, using notation N(x; g(z, theta), I) as in Equation 6 would make it clearer."}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "The application of VAEs to multiview settings, snd the general problem of learning representations over multiple modalities is of increasing importance. After discussion and reviewing the rebuttals, the reviewers felt that the paper is not yet ready. The application is strong, but that more can be done in terms of praticality, generalisation using other posteriors, and the comparisons made. For this reason, the paper is unfortunately not yet ready for inclusion in this year's proceedings.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"DATE": "18 Jan 2017", "TITLE": "updated response", "IS_META_REVIEW": false, "comments": "Dear reviewers,\n\nWe have modified our response to your review, acknowledging the related work pointed out by several reviewers, and clarifying our main contributions. \n\nThanks!", "OTHER_KEYS": "Weiran Wang"}, {"TITLE": "No Title", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "This paper considers the case where multiple views of data are learned through a probabilistic deep neural network formulation. This makes the model non-linear (unlike e.g. CCA) but makes inference difficult. Therefore, the VAE framework is invoked for inference.\n\nIn [Ref 1] the authors show that maximum likelihood estimation based on their linear latent model leads to the canonical correlation directions. But in the non-linear case with DNNs it's not clear (at least with the present analysis) what the solution is wrt to the canonical directions. There's no such analysis in the paper, hence I find it a stretch to refer to this model as a CCA type of model. In contrast, e.g. DCCA / DCCAE are taking the canonical correlation between features into account inside the objective and provide interpretations.\n\n[Ref 1] F. R. Bach and M. I. Jordan. A probabilistic interpretation of canonical correlation analysis. Technical Report 688, 2005.\n\nThere is also a significant body of very related work on non-linear multi-view models which is not discussed in this paper. For example, there's been probabilistic non-linear multi-view models [Ref 2, 3], also extended to the Bayesian case with common/private spaces [Ref 4] and the variational / deep learning case [Ref 5].\n\n[Ref 2] Ek et al. Gaussian process latent variable models for human pose estimation. MLMI, 2007.\n[Ref 3] Shon et al. Learning shared latent structure for image synthesis and robotic imitation. NIPS, 2006.\n[Ref 4] Damianou et al. Manifold relevance determination. ICML, 2012.\n[Ref 5] Damianou and Lawrence. Deep Gaussian processes. AISTATS, 2013.\n\nI can see the utility of this model as bringing together two elements: multi-view modeling and VAEs. This seems like an obvious idea but to the best of my knowledge it hasn't been done before and is actually a potentially very useful model.\n\nHowever, the question is, what is the proper way of extending VAE to multiple views? The paper didn't convince me that VAE can work well with multiple views using the shown straightforward construction. Specifically, VCCA doesn't seem to promote the state of the art in terms of results (it actually is overall below the SOA), while the VCCA-private seems a quite ill-posed model: the dimensionalities d have to be manually tuned with exhaustive search; further, the actual model does not provide a consinstent way of encouraging the private and common variables to avoid learning redundant information. Relying only on dropout for this seems a quite ad-hoc solution (in fact, from Fig. 4 (ver2) it seems that the dropout rate is quite crucial). Perhaps good performance might be achieved with a lot of tuning (which might be why the FLICKR results got better in ver2 without changing the model), but it seems quite difficult to optimize for the above reasons. From a purely experimental point of view, VCCA-private doesn't seem to promote the SOA either. Of course one wouldn't expect any new published paper to beat all previous baselines, but it seems that extension of VAE to multiple views is a very interesting idea which deserves some more investigation of how to do it efficiently.\n\nAnother issue is the approximate posterior being parameterized only from one of the views. This makes the model less useful as a generic multi-view model, since it will misbehave in tasks other than classification. But if classification is the main objective, then one should compare to a proper classification model, e.g. a feedforward neural network.\n\nThe plots of Fig. 8 are very nice. Overall, the paper convinced me that there is merit in attaching multiple views to VAE. However, it didn't convince me a) that the proposed way to achieve this is practical b) that there is a connection to CCA (other than being a method for multiple views). The bottom line is that, although the paper is interesting, it needs a little more work.\n", "IS_META_REVIEW": false, "RECOMMENDATION": 5, "DATE": "17 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "Review: Deep Variational Canonical Correlation Analysis", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "UPDATE: I have read the replies on this thread. My opinion has not changed.\n\nThe authors propose deep VCCA, a deep version of the probabilistic CCA model by using likelihoods parameterized by nonlinear functions (neural nets). Variational inference is applied with an inference network and reparameterization gradients. An additional variant, termed VCCA-private, is also introduced, which includes local latent variables for each data point (view). A connection to the multi view auto encoder is also shown.\n\nSince the development of black box variational inference and variational auto-encoders, the methodology in model-specific papers like this one are arguably not very interesting. The model is a straightforward extension of probabilistic CCA with neural net parameterized likelihoods. Inference is mechanically the same as any black box approach using the reparameterization gradient and inference networks. The approach also uses a mean-field approximation, which is quite old given the many recent developments in more expressive approximations (see, e.g., Rezende and Mohamed (2015); Tran et al. (2016)).\n\nThe connection to multi-view auto encoders is at first insightful, but no more than the difference between MAP and variational inference. This is a well-known insight: in the abstract, the authors argue that the key distinction is the additional sampling, but ultimately what matters is the KL regularizer. Even with noisy samples, the variances of a normal variational approximation would collapse to zero and thus become a point mass approximation, equivalent to optimizing a point estimate from the MVAE objective. (I suspect the authors know this to some degree due to their remarks in the paper, but it is unclear.)\n\nThat said, I think the paper has strong merits in application. The experiments are strong, comparing to alternative multi-view approaches under a number of interesting data sets. While the use of \"private variables\" is simple, they demonstrate how it can successfully disentangle the per-view latent representation from the shared view. It would have been preferable to compare to methods using probabilistic inference, such as full Bayes for the linear CCA. \n\nThere are also a number of approximations taken to almost be standard in the paper which may not be necessary, such as the use of a mean-field family or the use of an inference network. To separate out how much the approximate inference is influencing the fit of the model, I strongly recommend using MCMC and non-amortized variational inference on at least one experiment. \n\n+ Rezende, D. J., & Mohamed, S. (2015). Variational Inference with Normalizing Flows. Presented at the International Conference on Machine Learning.\n+ Tran, D., Ranganath, R., & Blei, D. M. (2016). The Variational Gaussian Process. Presented at the International Conference on Learning Representations.", "IS_META_REVIEW": false, "RECOMMENDATION": 5, "DATE": "16 Dec 2016 (modified: 21 Dec 2016)", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "Linear VCCA", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "7\n\nSummary:\nThis paper describes the use of variational autoencoders for multi-view representation learning as an alternative to canonical correlation analysis (CCA), deep CCA (DCCA), and multi-view autoencoders (MVAE). Two variants of variational autoencoders (which the authors call VCCA and VCCA-private) are investigated. The method\u2019s performances are compared on a synthetic MNIST dataset, the XRMB speech-articulation dataset, and the MIR-Flickr dataset.\n\nReview:\nVariational autoencoders are widely used and their performance for multi-view representation learning should be of interest to the ICLR community. The paper is well written and clear. The experiments are thorough. It is interesting that the performance of MVAE and VCCA is quite different given the similarity of their objective functions. I further find the analyses of the effects of dropout and private variables useful.\n\nAs the authors point out, \u201cVCCA does not optimize the same criterion, nor produce the same solution, as any linear or nonlinear CCA\u201d. It would have been interesting to discuss the differences of a linear variant of VCCA and linear CCA, and to compare it quantitatively. While it might not make sense to use variational inference in the linear case, it would nevertheless help to understand the differences better.\n\nThe derivations in Equation 3 and Equation 13 seem unnecessarily detailed given that VCCA and VCCA-p are special cases of VAE, only with certain architectural constraints. Perhaps move to the Appendix?\n\nIn Section 3 the authors claim that \u201cif we are able to generate realistic samples from the learned distribution, we can infer that we have discovered the underlying structure of the data\u201d. This is not correct, a model which hasn\u2019t learned a thing can have perfectly realistic samples (see Theis et al., 2016). Please remove or revise the sentence.\n\nMinor:\nIn the equation between Equation 8 and 9, using notation N(x; g(z, theta), I) as in Equation 6 would make it clearer.", "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"DATE": "14 Dec 2016", "TITLE": "revision notes", "IS_META_REVIEW": false, "comments": "Dear reviewers,\n\nWe just uploaded a slightly modified version. This version \n\n-- contains the updated results on Flickr and the proposed methods significantly outperform others\n-- reorganized the figures and tables to make the paper more compact\n\nThanks!\n", "OTHER_KEYS": "Weiran Wang"}, {"DATE": "03 Dec 2016", "TITLE": "VCCA and DCCAE", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1"}, {"DATE": "02 Dec 2016", "TITLE": "Lower bound and using q(z | x) instead of q(z | x, y)", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3"}]}
{"text": "EXPLORING UNDER-APPRECIATED REWARDS\n1 INTRODUCTION\nHumans can reason about symbolic objects and solve algorithmic problems. After learning to count and then manipulate numbers via simple arithmetic, people eventually learn to invent new algorithms and even reason about their correctness and efficiency. The ability to invent new algorithms is fundamental to artificial intelligence (AI). Although symbolic reasoning has a long history in AI (Russell et al., 2003), only recently have statistical machine learning and neural network approaches begun to make headway in automated algorithm discovery (Reed & de Freitas, 2016; Kaiser & Sutskever, 2016; Neelakantan et al., 2016), which would constitute an important milestone on the path to AI. Nevertheless, most of the recent successes depend on the use of strong supervision to learn a mapping from a set of training inputs to outputs by maximizing a conditional log-likelihood, very much like neural machine translation systems (Sutskever et al., 2014; Bahdanau et al., 2015). Such a dependence on strong supervision is a significant limitation that does not match the ability of people to invent new algorithmic procedures based solely on trial and error.\nBy contrast, reinforcement learning (RL) methods (Sutton & Barto, 1998) hold the promise of searching over discrete objects such as symbolic representations of algorithms by considering much weaker feedback in the form of a simple verifier that tests the correctness of a program execution on a given problem instance. Despite the recent excitement around the use of RL to tackle Atari games (Mnih et al., 2015) and Go (Silver et al., 2016), standard RL methods are not yet able to consistently and reliably solve algorithmic tasks in all but the simplest cases (Zaremba & Sutskever, 2014). A key property of algorithmic problems that makes them challenging for RL is reward sparsity, i.e., a policy usually has to get a long action sequence exactly right to obtain a non-zero reward.\n\u2217Work done as a member of the Google Brain Residency program (g.co/brainresidency) \u2020Also at the Department of Computing Science, University of Alberta, daes@ualberta.ca\nWe believe one of the key factors limiting the effectiveness of current RL methods in a sparse reward setting is the use of undirected exploration strategies (Thrun, 1992), such as -greedy and entropy regularization (Williams & Peng, 1991). For long action sequences with delayed sparse reward, it is hopeless to explore the space uniformly and blindly. Instead, we propose a formulation to encourage exploration of action sequences that are under-appreciated by the current policy. Our formulation considers an action sequence to be under-appreciated if the model\u2019s log-probability assigned to an action sequence under-estimates the resulting reward from the action sequence. Exploring underappreciated states and actions encourages the policy to have a better calibration between its logprobabilities and observed reward values, even for action sequences with negligible rewards. This effectively increases exploration around neglected action sequences.\nWe term our proposed technique under-appreciated reward exploration (UREX). We show that the objective given by UREX is a combination of a mode seeking objective (standard REINFORCE) and a mean seeking term, which provides a well motivated trade-off between exploitation and exploration. To empirically evaluate our method, we take a set of algorithmic tasks such as sequence reversal, multi-digit addition, and binary search. We choose to focus on these tasks because, although simple, they present a difficult sparse reward setting which has limited the success of standard RL approaches. The experiments demonstrate that UREX significantly outperforms baseline RL methods, such as entropy regularized REINFORCE and one-step Q-learning, especially on the more difficult tasks, such as multi-digit addition. Moreover, UREX is shown to be more robust to changes of hyper-parameters, which makes hyper-parameter tuning less tedious in practice. In addition to introducing a new variant of policy gradient with improved performance, our paper is the first to demonstrate strong results for an RL method on algorithmic tasks. To our knowledge, the addition task has not been solved by any model-free reinforcement learning approach. We observe that some of the policies learned by UREX can successfully generalize to long sequences; e.g., in 2 out of 5 random restarts, the policy learned by UREX for the addition task correctly generalizes to addition of numbers with 2000 digits with no mistakes, even though training sequences are at most 33 digits long.\n2 NEURAL NETWORKS FOR LEARNING ALGORITHMS\nAlthough research on using neural networks to learn algorithms has witnessed a surge of recent interest, the problem of program induction from examples has a long history in many fields, including program induction, inductive logic programming (Lavrac & Dzeroski, 1994), relational learning (Kemp et al., 2007) and regular language learning (Angulin, 1987). Rather than presenting a comprehensive survey of program induction here, we focus on neural network approaches to algorithmic tasks and highlight the relative simplicity of our neural network architecture.\nMost successful applications of neural networks to algorithmic tasks rely on strong supervision, where the inputs and target outputs are completely known a priori. Given a dataset of examples, one learns the network parameters by maximizing the conditional likelihood of the outputs via backpropagation (e.g., Reed & de Freitas (2016); Kaiser & Sutskever (2016); Vinyals et al. (2015)). However, target outputs may not be available for novel tasks, for which no prior algorithm is known to be available. A more desirable approach to inducing algorithms, followed in this paper, advocates using self-driven learning strategies that only receive reinforcement based on the outputs produced. Hence, just by having access to a verifier for an algorithmic problem, one can aim to learn an algorithm. For example, if one does not know how to sort an array, but can check the extent to which an array is sorted, then one can provide the reward signal necessary for learning sorting algorithms.\nWe formulate learning algorithms as an RL problem and make use of model-free policy gradient methods to optimize a set parameters associated with the algorithm. In this setting, the goal is to learn a policy \u03c0\u03b8 that given an observed state st at step t, estimates a distribution over the next action at, denoted \u03c0\u03b8(at | st). Actions represent the commands within the algorithm and states represent the joint state of the algorithm and the environment. Previous work in this area has focused on augmenting a neural network with additional structure and increased capabilities (Zaremba & Sutskever, 2015; Graves et al., 2016). In contrast, we utilize a simple architecture based on a standard recurrent neural network (RNN) with LSTM cells (Hochreiter & Schmidhuber, 1997) as depicted in Figure 1. At each episode, the environment is initialized with a latent state h, unknown to the agent, which determines s1 and the subsequent state transition and reward functions. Once the agent observes s1\nas the input to the RNN, the network outputs a distribution \u03c0\u03b8(a1 | s1), from which an action a1 is sampled. This action is applied to the environment, and the agent receives a new state observation s2. The state s2 and the previous action a1 are then fed into the RNN and the process repeats until the end of the episode. Upon termination, a reward signal is received.\n3 LEARNING A POLICY BY MAXIMIZING EXPECTED REWARD\nWe start by discussing the most common form of policy gradient, REINFORCE (Williams, 1992), and its entropy regularized variant (Williams & Peng, 1991). REINFORCE has been applied to model-free policy-based learning with neural networks and algorithmic domains (Zaremba & Sutskever, 2015; Graves et al., 2016).\nThe goal is to learn a policy \u03c0\u03b8 that, given an observed state st at step t, estimates a distribution over the next action at, denoted \u03c0\u03b8(at |st). The environment is initialized with a latent vector, h, which determines the initial observed state s1 = g(h), and the transition function st+1 = f(st,at | h). Note that the use of nondeterministic transitions f as in Markov decision processes (MDP) may be recovered by assuming that h includes the random seed for the any nondeterministic functions. Given a latent state h, and s1:T \u2261 (s1, . . . , sT ), the model probability of an action sequence a1:T \u2261 (a1, . . . ,aT ) is expressed as,\n\u03c0\u03b8(a1:T | h) = T\u220f t=1 \u03c0\u03b8(at | st) , where s1 = g(h), st+1 = f(st,at | h) for 1 \u2264 t < T .\nThe environment provides a reward at the end of the episode, denoted r(a1:T | h). For ease of readability we drop the subscript from a1:T and simply write \u03c0\u03b8(a | h) and r(a | h). The objective used to optimize the policy parameters, \u03b8, consists of maximizing expected reward under actions drawn from the policy, plus an optional maximum entropy regularizer. Given a distribution over initial latent environment states p(h), we express the regularized expected reward as,\nORL(\u03b8; \u03c4) = Eh\u223cp(h) {\u2211\na\u2208A \u03c0\u03b8(a | h)\n[ r(a | h)\u2212 \u03c4 log \u03c0\u03b8(a | h) ]} . (1)\nWhen \u03c0\u03b8 is a non-linear function defined by a neural network, finding the global optimum of \u03b8 is challenging, and one often resorts to gradient-based methods to find a local optimum of ORL(\u03b8; \u03c4). Given that dd\u03b8\u03c0\u03b8(a) = \u03c0\u03b8(a) d d\u03b8 log \u03c0\u03b8(a) for any a such that \u03c0\u03b8(a) > 0, one can verify that,\nd\nd\u03b8 ORL(\u03b8; \u03c4 | h) = \u2211 a\u2208A \u03c0\u03b8(a | h) d d\u03b8 log \u03c0\u03b8(a | h) [ r(a | h)\u2212 \u03c4 log \u03c0\u03b8(a | h)\u2212 \u03c4 ] . (2)\nBecause the space of possible actionsA is large, enumerating over all of the actions to compute this gradient is infeasible. Williams (1992) proposed to compute the stochastic gradient of the expected\nreward by using Monte Carlo samples. Using Monte Carlo samples, one first drawsN i.i.d. samples from the latent environment states {h(n)}Nn=1, and then draws K i.i.d. samples {a(k)}Kk=1 from \u03c0\u03b8(a | h(n)) to approximate the gradient of (1) by using (2) as,\nd\nd\u03b8 ORL(\u03b8; \u03c4) \u2248\n1\nNK N\u2211 n=1 K\u2211 k=1 d d\u03b8 log \u03c0\u03b8(a (k) | h(n)) [ r(a(k) | h(n))\u2212 \u03c4 log \u03c0\u03b8(a(k) | h(n))\u2212 \u03c4 ] .\n(3) This reparametrization of the gradients is the key to the REINFORCE algorithm. To reduce the variance of (3), one uses rewards r\u0302 that are shifted by some offset values,\nr\u0302 (a(k) | h) = r(a(k) | h)\u2212 b(h) , (4) where b is known as a baseline or sometimes called a critic. Note that subtracting any offset from the rewards in (1) simply results in shifting the objective ORL by a constant. Unfortunately, directly maximizing expected reward (i.e., when \u03c4 = 0) is prone to getting trapped in a local optimum. To combat this tendency, Williams & Peng (1991) augmented the expected reward objective by including a maximum entropy regularizer (\u03c4 > 0) to promote greater exploration. We will refer to this variant of REINFORCE as MENT (maximum entropy exploration).\n4 UNDER-APPRECIATED REWARD EXPLORATION (UREX)\nTo explain our novel form of policy gradient, we first note that the optimal policy \u03c0\u2217\u03c4 , which globally maximizes ORL(\u03b8; \u03c4 | h) in (1) for any \u03c4 > 0, can be expressed as,\n\u03c0\u2217\u03c4 (a | h) = 1\nZ(h) exp {1 \u03c4 r(a | h) } , (5)\nwhere Z(h) is a normalization constant making \u03c0\u2217\u03c4 a distribution over the space of action sequences A. One can verify this by first acknowledging that,\nORL(\u03b8; \u03c4 | h) = \u2212\u03c4 DKL (\u03c0\u03b8(\u00b7 | h) \u2016 \u03c0\u2217\u03c4 (\u00b7 | h)) . (6) Since DKL (p \u2016 q) is non-negative and zero iff p = q, then \u03c0\u2217\u03c4 defined in (5) maximizes ORL. That said, given a particular form of \u03c0\u03b8, finding \u03b8 that exactly characterizes \u03c0\u2217\u03c4 may not be feasible.\nThe KL divergence DKL (\u03c0\u03b8 \u2016 \u03c0\u2217\u03c4 ) is known to be mode seeking (Murphy, 2012, Section 21.2.2) even with entropy regularization (\u03c4 > 0). Learning a policy by optimizing this direction of the KL is prone to falling into a local optimum resulting in a sub-optimal policy that omits some of the modes of \u03c0\u2217\u03c4 . Although entropy regularization helps mitigate the issues as confirmed in our experiments, it is not an effective exploration strategy as it is undirected and requires a small regularization coefficient \u03c4 to avoid too much random exploration. Instead, we propose a directed exploration strategy that improves the mean seeking behavior of policy gradient in a principled way.\nWe start by considering the alternate mean seeking direction of the KL divergence, DKL (\u03c0\u2217\u03c4 \u2016 \u03c0\u03b8). Norouzi et al. (2016) considered this direction of the KL to directly learn a policy by optimizing\nORAML(\u03b8; \u03c4) = Eh\u223cp(h) { \u03c4 \u2211 a\u2208A \u03c0\u2217\u03c4 (a | h) log \u03c0\u03b8(a | h) } , (7)\nfor structured prediction. This objective has the same optimal solution \u03c0\u2217\u03c4 as ORL since, ORAML(\u03b8; \u03c4 | h) = \u2212\u03c4 DKL (\u03c0\u2217\u03c4 (\u00b7 | h) \u2016 \u03c0\u03b8(\u00b7 | h)) + const . (8)\nNorouzi et al. (2016) argue that in some structured prediction problems when one can draw samples from \u03c0\u2217\u03c4 , optimizing (7) is more effective than (1), since no sampling from a non-stationary policy \u03c0\u03b8 is required. If \u03c0\u03b8 is a log-linear model of a set of features,ORAML is convex in \u03b8 whereasORL is not, even in the log-linear case. Unfortunately, in scenarios that the reward landscape is unknown or computing the normalization constant Z(h) is intractable, sampling from \u03c0\u2217\u03c4 is not straightforward.\nIn RL problems, the reward landscape is completely unknown, hence sampling from \u03c0\u2217\u03c4 is intractable. This paper proposes to approximate the expectation with respect to \u03c0\u2217\u03c4 by using selfnormalized importance sampling (Owen, 2013), where the proposal distribution is \u03c0\u03b8 and the reference distribution is \u03c0\u2217\u03c4 . For importance sampling, one draws K i.i.d. samples {a(k)}Kk=1 from\n\u03c0\u03b8(a | h) and computes a set of normalized importance weights to approximate ORAML(\u03b8; \u03c4 | h) as,\nORAML(\u03b8; \u03c4 | h) \u2248 \u03c4 K\u2211 k=1 w\u03c4 (a (k) | h)\u2211K m=1 w\u03c4 (a (m) | h) log \u03c0\u03b8(a (k) | h) , (9)\nwhere w\u03c4 (a(k) | h) \u221d \u03c0\u2217\u03c4/\u03c0\u03b8 denotes an importance weight defined by,\nw\u03c4 (a (k) | h) = exp {1 \u03c4 r(a(k) | h)\u2212 log \u03c0\u03b8(a(k) | h) } . (10)\nOne can view these importance weights as evaluating the discrepancy between scaled rewards r/\u03c4 and the policy\u2019s log-probabilities log \u03c0\u03b8. Among the K samples, a sample that is least appreciated by the model, i.e., has the largest r/\u03c4 \u2212 log \u03c0\u03b8, receives the largest positive feedback in (9). In practice, we have found that just using the importance sampling RAML objective in (9) does not always yield promising solutions. Particularly, at the beginning of training, when \u03c0\u03b8 is still far away from \u03c0\u2217\u03c4 , the variance of importance weights is too large, and the self-normalized importance sampling procedure results in poor approximations. To stabilize early phases of training and ensure that the model distribution \u03c0\u03b8 achieves large expected reward scores, we combine the expected reward and RAML objectives to benefit from the best of their mode and mean seeking behaviors. Accordingly, we propose the following objective that we call under-appreciated reward exploration (UREX),\nOUREX(\u03b8; \u03c4) = Eh\u223cp(h) {\u2211\na\u2208A\n[ \u03c0\u03b8(a | h) r(a | h) + \u03c4 \u03c0\u2217\u03c4 (a | h) log \u03c0\u03b8(a | h) ]} , (11)\nwhich is the sum of the expected reward and RAML objectives. In our preliminary experiments, we considered a composite objective of ORL + ORAML, but we found that removing the entropy term is beneficial. Hence, theOUREX objective does not include entropy regularization. Accordingly, the optimum policy for OUREX is no longer \u03c0\u2217\u03c4 , as it was for ORL and ORAML. Appendix A derives the optimal policy for OUREX as a function of the optimal policy for ORL. We find that the optimal policy of UREX is more sharply concentrated on the high reward regions of the action space, which may be an advantage for UREX, but we leave more analysis of this behavior to future work.\nTo compute the gradient ofOUREX(\u03b8; \u03c4), we use the self-normalized importance sampling estimate outlined in (9). We assume that the importance weights are constant and contribute no gradient to d d\u03b8OUREX(\u03b8; \u03c4). To approximate the gradient, one draws N i.i.d. samples from the latent environment states {h(n)}Nn=1, and then draws K i.i.d. samples {a(k)}Kk=1 from \u03c0\u03b8(a |h (n)) to obtain\nd\nd\u03b8 OUREX(\u03b8; \u03c4) \u2248\n1\nN N\u2211 n=1 K\u2211 k=1 d d\u03b8 log \u03c0\u03b8(a (k) |h(n)) [ 1 K r\u0302 (a(k) | h(n))+\u03c4 w\u03c4 (a (k) |h(n))\u2211K m=1w\u03c4 (a (m) |h(n)) ] .\n(12) As with REINFORCE, the rewards are shifted by an offset b(h). In this gradient, the model logprobability of a sample action sequence a(k) is reinforced if the corresponding reward is large, or the corresponding importance weights are large, meaning that the action sequence is under-appreciated. The normalized importance weights are computed using a softmax operator softmax(r/\u03c4 \u2212 log \u03c0\u03b8).\n5 RELATED WORK\nBefore presenting the experimental results, we briefly review some pieces of previous work that closely relate to the UREX approach.\nReward-Weighted Regression. Both RAML and UREX objectives bear some similarity to a method in continuous control known as Reward-Weighted Regression (RWR) (Peters & Schaal, 2007; Wierstra et al., 2008). Using our notation, the RWR objective is expressed as,\nORWR(\u03b8; \u03c4 | h) = log \u2211 a\u2208A \u03c0\u2217\u03c4 (a | h)\u03c0\u03b8(a | h) (13)\n\u2265 \u2211 a\u2208A q(a | h) log \u03c0 \u2217 \u03c4 (a | h)\u03c0\u03b8(a | h) q(a | h) . (14)\nTo optimize ORWR, Peters & Schaal (2007) propose a technique inspired by the EM algorithm to maximize a variational lower bound in (14) based on a variational distribution q(a | h). The RWR objective can be interpreted as a log of the correlation between \u03c0\u2217\u03c4 and \u03c0\u03b8. By contrast, the RAML and UREX objectives are both based on a KL divergence between \u03c0\u2217\u03c4 and \u03c0\u03b8.\nTo optimize the RWR objective, one formulates the gradient as, d\nd\u03b8 ORWR(\u03b8; \u03c4 | h) = \u2211 a\u2208A \u03c0\u2217\u03c4 (a | h)\u03c0\u03b8(a | h) C d d\u03b8 log \u03c0\u03b8(a | h), (15)\nwhere C denotes the normalization factor, i.e., C = \u2211\na\u2208A \u03c0 \u2217 \u03c4 (a | h)\u03c0\u03b8(a | h). The expectation\nwith respect to \u03c0\u2217\u03c4 (a | h)\u03c0\u03b8(a | h)/C on the RHS can be approximated by self-normalized importance sampling,1 where the proposal distribution is \u03c0\u03b8. Accordingly, one draws K Monte Carlo samples {a(k)}Kk=1 i.i.d. from \u03c0\u03b8(a |h) and formulates the gradient as,\nd\nd\u03b8 ORWR(\u03b8; \u03c4 | h) \u2248\n1\nK K\u2211 k=1 u(a(k) | h)\u2211K m=1 u(a (m) | h) d d\u03b8 log \u03c0\u03b8(a (k) | h), (16)\nwhere u(a(k) | h) = exp{ 1\u03c4 r(a (k) | h)}. There is some similarity between (16) and (9) in that they both use self-normalized importance sampling, but note the critical difference that (16) and (9) estimate the gradients of two different objectives, and hence the importance weights in (16) do not correct for the sampling distribution \u03c0\u03b8(a |h) as opposed to (9). Beyond important technical differences, the optimal policy of ORWR is a one hot distribution with all probability mass concentrated on an action sequence with maximal reward, whereas the optimal policies for RAML and UREX are everywhere nonzero, with the probability of different action sequences being assigned proportionally to their exponentiated reward (with UREX introducing an additional re-scaling; see Appendix A). Further, the notion of under-appreciated reward exploration evident in OUREX, which is key to UREX\u2019s performance, is missing in the RWR formulation. Exploration. The RL literature contains many different attempts at incorporating exploration that may be compared with our method. The most common exploration strategy considered in valuebased RL is -greedy Q-learning, where at each step the agent either takes the best action according to its current value approximation or with probability takes an action sampled uniformly at random. Like entropy regularization, such an approach applies undirected exploration, but it has achieved recent success in game playing environments (Mnih et al., 2013; Van Hasselt et al., 2016; Mnih et al., 2016).\nProminent approaches to improving exploration beyond -greedy in value-based or model-based RL have focused on reducing uncertainty by prioritizing exploration toward states and actions where the agent knows the least. This basic intuition underlies work on counter and recency methods (Thrun, 1992), exploration methods based on uncertainty estimates of values (Kaelbling, 1993; Tokic, 2010), methods that prioritize learning environment dynamics (Kearns & Singh, 2002; Stadie et al., 2015), and methods that provide an intrinsic motivation or curiosity bonus for exploring unknown states (Schmidhuber, 2006; Bellemare et al., 2016).\nIn contrast to value-based methods, exploration for policy-based RL methods is often a by-product of the optimization algorithm itself. Since algorithms like REINFORCE and Thompson sampling choose actions according to a stochastic policy, sub-optimal actions are chosen with some non-zero probability. The Q-learning algorithm may also be modified to sample an action from the softmax of the Q values rather than the argmax (Sutton & Barto, 1998).\nAsynchronous training has also been reported to have an exploration effect on both value- and policy-based methods. Mnih et al. (2016) report that asynchronous training can stabilize training by reducing the bias experienced by a single trainer. By using multiple separate trainers, an agent is less likely to become trapped at a policy found to be locally optimal only due to local conditions. In the same spirit, Osband et al. (2016) use multiple Q value approximators and sample only one to act for each episode as a way to implicitly incorporate exploration.\nBy relating the concepts of value and policy in RL, the exploration strategy we propose tries to bridge the discrepancy between the two. In particular, UREX can be viewed as a hybrid combination of value-based and policy-based exploration strategies that attempts to capture the benefits of each.\n1Bornschein & Bengio (2014) apply the same trick to optimize the log-likelihood of latent variable models.\nPer-step Reward. Finally, while we restrict ourselves to episodic settings where a reward is associated with an entire episode of states and actions, much work has been done to take advantage of environments that provide per-step rewards. These include policy-based methods such as actor-critic (Mnih et al., 2016; Schulman et al., 2016) and value-based approaches based on Qlearning (Van Hasselt et al., 2016; Schaul et al., 2016). Some of these value-based methods have proposed a softening of Q-values which can be interpreted as adding a form of maximum-entropy regularizer (Asadi & Littman, 2016; Azar et al., 2012; Fox et al., 2016; Ziebart, 2010). The episodic total-reward setting that we consider is naturally harder since the credit assignment to individual actions within an episode is unclear.\n6 SIX ALGORITHMIC TASKS\nWe assess the effectiveness of the proposed approach on five algorithmic tasks from the OpenAI Gym (Brockman et al., 2016), as well as a new binary search problem. Each task is summarized below, with further details available on the Gym website2 or in the corresponding open-source code.3 In each case, the environment has a hidden tape and a hidden sequence. The agent observes the sequence via a pointer to a single character, which can be moved by a set of pointer control actions. Thus an action at is represented as a tuple (m,w, o) where m denotes how to move, w is a boolean denoting whether to write, and o is the output symbol to write.\n1. Copy: The agent should emit a copy of the sequence. The pointer actions are move left and right. 2. DuplicatedInput: In the hidden tape, each character is repeated twice. The agent must dedupli-\ncate the sequence and emit every other character. The pointer actions are move left and right.\n3. RepeatCopy: The agent should emit the hidden sequence once, then emit the sequence in the reverse order, then emit the original sequence again. The pointer actions are move left and right.\n4. Reverse: The agent should emit the hidden sequence in the reverse order. As before, the pointer actions are move left and right.\n5. ReversedAddition: The hidden tape is a 2\u00d7n grid of digits representing two numbers in base 3 in little-endian order. The agent must emit the sum of the two numbers, in little-endian order. The allowed pointer actions are move left, right, up, or down.\nThe OpenAI Gym provides an additional harder task called ReversedAddition3, which involves adding three numbers. We omit this task, since none of the methods make much progress on it.\nFor these tasks, the input sequences encountered during training range from a length of 2 to 33 characters. A reward of 1 is given for each correct emission. On an incorrect emission, a small penalty of\u22120.5 is incurred and the episode is terminated. The agent is also terminated and penalized with a reward of \u22121 if the episode exceeds a certain number of steps. For the experiments using UREX and MENT, we associate an episodic sequence of actions with the total reward, defined as the sum of the per-step rewards. The experiments using Q-learning, on the other hand, used the per-step rewards. Each of the Gym tasks has a success threshold, which determines the required average reward over 100 episodes for the agent to be considered successful.\nWe also conduct experiments on an additional algorithmic task described below: 6. BinarySearch: Given an integer n, the environment has a hidden array of n distinct numbers\nstored in ascending order. The environment also has a query number x unknown to the agent that is contained somewhere in the array. The goal of the agent is to find the query number in the array in a small number of actions. The environment has three integer registers initialized at (n, 0, 0). At each step, the agent can interact with the environment via the four following actions: \u2022 INC(i): increment the value of the register i for i \u2208 {1, 2, 3}. \u2022 DIV(i): divide the value of the register i by 2 for i \u2208 {1, 2, 3}. \u2022 AVG(i): replace the value of the register i with the average of the two other registers. \u2022 CMP(i): compare the value of the register i with x and receive a signal indicating which\nvalue is greater. The agent succeeds when it calls CMP on an array cell holding the value x.\n2gym.openai.com 3github.com/openai/gym\nWe set the maximum number of steps to 2n+1 to allow the agent to perform a full linear search. A policy performing full linear search achieves an average reward of 5, because x is chosen uniformly at random from the elements of the array. A policy employing binary search can find the number x in at most 2 log2 n + 1 steps. If n is selected uniformly at random from the range 32 \u2264 n \u2264 512, binary search yields an optimal average reward above 9.55. We set the success threshold for this task to an average reward of 9.\n7 EXPERIMENTS\nWe compare our policy gradient method using under-appreciated reward exploration (UREX) against two main RL baselines: (1) REINFORCE with entropy regularization termed MENT (Williams & Peng, 1991), where the value of \u03c4 determines the degree of regularization. When \u03c4 = 0, standard REINFORCE is obtained. (2) one-step double Q-learning based on bootstrapping one step future rewards.\n7.1 ROBUSTNESS TO HYPER-PARAMETERS\nHyper-parameter tuning is often tedious for RL algorithms. We found that the proposed UREX method significantly improves robustness to changes in hyper-parameters when compared to MENT. For our experiments, we perform a careful grid search over a set of hyper-parameters for both MENT and UREX. For any hyper-parameter setting, we run the MENT and UREX methods 5 times with different random restarts. We explore the following main hyper-parameters:\n\u2022 The learning rate denoted \u03b7 chosen from a set of 3 possible values \u03b7 \u2208 {0.1, 0.01, 0.001}. \u2022 The maximum L2 norm of the gradients, beyond which the gradients are clipped. This parame-\nter, denoted c, matters for training RNNs. The value of c is selected from c \u2208 {1, 10, 40, 100}. \u2022 The temperature parameter \u03c4 that controls the degree of exploration for both MENT and UREX.\nFor MENT, we use \u03c4 \u2208 {0, 0.005, 0.01, 0.1}. For UREX, we only consider \u03c4 = 0.1, which consistently performs well across the tasks.\nIn all of the experiments, both MENT and UREX are treated exactly the same. In fact, the change of implementation is just a few lines of code. Given a value of \u03c4 , for each task, we run 60 training jobs comprising 3 learning rates, 4 clipping values, and 5 random restarts. We run each algorithm for a maximum number of steps determined based on the difficulty of the task. The training jobs for Copy, DuplicatedInput, RepeatCopy, Reverse, ReversedAddition, and BinarySearch are run for 2K, 500, 50K, 5K, 50K, and 2K stochastic gradient steps, respectively. We find that running a trainer job longer does not result in a better performance. Our policy network comprises a single LSTM layer with 128 nodes. We use the Adam optimizer (Kingma & Ba, 2015) for the experiments.\nTable 1 shows the percentage of 60 trials on different hyper-parameters (\u03b7, c) and random restarts which successfully solve each of the algorithmic tasks. It is clear that UREX is more robust than\nMENT to changes in hyper-parameters, even though we only report the results of UREX for a single temperature. See Appendix B for more detailed tables on hyper-parameter robustness.\n7.2 RESULTS\nTable 2 presents the number of successful attempts (out of 5 random restarts) and the expected reward values (averaged over 5 trials) for each RL algorithm given the best hyper-parameters. Onestep Q-learning results are also included in the table. We also present the training curves for MENT and UREX in Figure 2. It is clear that UREX outperforms the baselines on these tasks. On the more difficult tasks, such as Reverse and ReverseAddition, UREX is able to consistently find an appropriate algorithm, but MENT and Q-learning fall behind. Importantly, for the BinarySearch task, which exhibits many local maxima and necessitates smart exploration, UREX is the only method that can solve it consistently. The Q-learning baseline solves some of the simple tasks, but it makes little headway on the harder tasks. We believe that entropy regularization for policy gradient and - greedy for Q-learning are relatively weak exploration strategies in long episodic tasks with delayed rewards. On such tasks, one random exploratory step in the wrong direction can take the agent off the optimal policy, hampering its ability to learn. In contrast, UREX provides a form of adaptive and smart exploration. In fact, we observe that the variance of the importance weights decreases as the agent approaches the optimal policy, effectively reducing exploration when it is no longer necessary; see Appendix E.\n7.3 GENERALIZATION TO LONGER SEQUENCES\nTo confirm whether our method is able to find the correct algorithm for multi-digit addition, we investigate its generalization to longer input sequences than provided during training. We evaluate the trained models on inputs up to a length of 2000 digits, even though training sequences were at most 33 characters. For each length, we test the model on 100 randomly generated inputs, stopping when the accuracy falls below 100%. Out of the 60 models trained on addition with UREX, we find that 5 models generalize to numbers up to 2000 digits without any observed mistakes. On the best UREX hyper-parameters, 2 out of the 5 random restarts are able to generalize successfully. For more detailed results on the generalization performance on 3 different tasks including Copy,\nDuplicatedInput, and ReversedAddition, see Appendix C. During these evaluations, we take the action with largest probability from \u03c0\u03b8(a | h) at each time step rather than sampling randomly. We also looked into the generalization of the models trained on the BinarySearch task. We found that none of the agents perform proper binary search. Rather, those that solved the task perform a hybrid of binary and linear search: first actions follow a binary search pattern, but then the agent switches to a linear search procedure once it narrows down the search space; see Appendix D for some execution traces for BinarySearch and ReversedAddition. Thus, on longer input sequences, the agent\u2019s running time complexity approaches linear rather than logarithmic. We hope that future work will make more progress on this task. This task is especially interesting because the reward signal should incorporate both correctness and efficiency of the algorithm.\n7.4 IMPLEMENTATION DETAILS\nIn all of the experiments, we make use of curriculum learning. The environment begins by only providing small inputs and moves on to longer sequences once the agent achieves close to maximal reward over a number of steps. For policy gradient methods including MENT and UREX, we only provide the agent with a reward at the end of the episode, and there is no notion of intermediate reward. For the value-based baseline, we implement one-step Q-learning as described in Mnih et al. (2016)-Alg. 1, employing double Q-learning with -greedy exploration. We use the same RNN in our policy-based approaches to estimate the Q values. A grid search over exploration rate, exploration rate decay, learning rate, and sync frequency (between online and target network) is conducted to find the best hyper-parameters. Unlike our other methods, the Q-learning baseline uses intermediate rewards, as given by the OpenAI Gym on a per-step basis. Hence, the Q-learning baseline has a slight advantage over the policy gradient methods.\nIn all of the tasks except Copy, our stochastic optimizer uses mini-batches comprising 400 policy samples from the model. These 400 samples correspond to 40 different random sequences drawn from the environment, and 10 random policy trajectories per sequence. In other words, we set K = 10 and N = 40 as defined in (3) and (12). For MENT, we use the 10 samples to subtract the mean of the coefficient of dd\u03b8 log \u03c0\u03b8(a | h) which includes the contribution of the reward and entropy regularization. For UREX, we use the 10 trajectories to subtract the mean reward and normalize the importance sampling weights. We do not subtract the mean of the normalized importance weights. For the Copy task, we use mini-batches with 200 samples using K = 10 and N = 20. Experiments are conducted using Tensorflow (Abadi et al., 2016).\n8 CONCLUSION\nWe present a variant of policy gradient, called UREX, which promotes the exploration of action sequences that yield rewards larger than what the model expects. This exploration strategy is the result of importance sampling from the optimal policy. Our experimental results demonstrate that UREX significantly outperforms other value and policy based methods, while being more robust\nto changes of hyper-parameters. By using UREX, we can solve algorithmic tasks like multi-digit addition from only episodic reward, which other methods cannot reliably solve even given the best hyper-parameters. We introduce a new algorithmic task based on binary search to advocate more research in this area, especially when the computational complexity of the solution is also of interest. Solving these tasks is not only important for developing more human-like intelligence in learning algorithms, but also important for generic reinforcement learning, where smart and efficient exploration is the key to successful methods.\n9 ACKNOWLEDGMENT\nWe thank Sergey Levine, Irwan Bello, Corey Lynch, George Tucker, Kelvin Xu, Volodymyr Mnih, and the Google Brain team for insightful comments and discussions.\nA OPTIMAL POLICY FOR THE UREX OBJECTIVE\nTo derive the form of the optimal policy for the UREX objective (11), note that for each h one would like to maximize \u2211\na\u2208A\n[ \u03c0\u03b8(a) r(a) + \u03c4 \u03c0 \u2217 \u03c4 (a) log \u03c0\u03b8(a) ] , (17)\nsubject to the constraint \u2211\na\u2208A \u03c0\u03b8(a) = 1. To enforce the constraint, we introduce a Lagrange multiplier \u03b1 and aim to maximize\u2211\na\u2208A\n[ \u03c0\u03b8(a) r(a) + \u03c4 \u03c0 \u2217 \u03c4 (a) log \u03c0\u03b8(a)\u2212 \u03b1\u03c0\u03b8(a) ] + \u03b1 . (18)\nSince the gradient of the Lagrangian (18) with respect to \u03b8 is given by\u2211 a\u2208A d\u03c0\u03b8(a) d\u03b8 [ r(a) + \u03c4 \u03c0\u2217\u03c4 (a) \u03c0\u03b8(a) \u2212 \u03b1 ] , (19)\nthe optimal choice for \u03c0\u03b8 is achieved by setting\n\u03c0\u03b8(a) = \u03c4 \u03c0\u2217\u03c4 (a)\n\u03b1\u2212 r(a) for all a \u2208 A , (20)\nforcing the gradient to be zero. The Lagrange multiplier \u03b1 can then be chosen so that \u2211\na\u2208A \u03c0\u03b8(a) = 1 while also satisfying \u03b1 > maxa\u2208A r(a); see e.g. (Golub, 1987).\nB ROBUSTNESS TO HYPER-PARAMETERS\nTables 3\u20138 provide more details on different cells of Table 1. Each table presents the results of MENT using the best temperature \u03c4 vs. UREX with \u03c4 = 0.1 on a variety of learning rates and clipping values. Each cell is the number of trials out of 5 random restarts that succeed at solving the task using a specific \u03b7 and c.\nC GENERALIZATION TO LONGER SEQUENCES\nTable 9 provides a more detailed look into the generalization performance of the trained models on Copy, DuplicatedInput, and ReversedAddition. The tables show how the number of models which can solve the task correctly drops off as the length of the input increases.\nD EXAMPLE EXECUTION TRACES\nWe provide the traces of two trained agents on the ReversedAddition task (Figure 3) and the BinarySearch task (Table 10).\nE VARIANCE OF IMPORTANCE WEIGHTS\nF A SIMPLE BANDIT TASK\n", "cats": {"READ": 1.0, "DONTREAD": 0.0}, "reviews": [{"DATE": "23 Feb 2017", "TITLE": "Same objective as reward-weighted regression", "IS_META_REVIEW": false, "comments": "\nIt has recently come to my attention that the objective proposed by the authors in this paper has in fact already been studied in the literature under the name 'reward-weighted regression', from e.g. ICML 2007 [1]. This has spawned several other works using the same objective (e.g. [2]). One can examine for instance the objective proposed in Section 3.4 from [2], from ICANN 2008.\n\nWhile this paper has already been accepted to ICLR, it would be beneficial for the authors to at least cite these works (and other related works) so that readers are aware of the previous origins of this idea.\n\n[1] ", "OTHER_KEYS": "(anonymous)"}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "This paper proposes a nice algorithm for improved exploration in policy search RL settings. The method essentially optimizes a weighted combination of expected reward (essentially the REINFORCE objective without an entropy regularizer) plus a term from the reward augmented maximum likelihood objective (from a recent NIPS paper), and show that the resulting update can be made with a fairly small modification to the REINFORCE algorithm. The authors show improved performance on several sequential \"program-like\" domains like copying a string, adding, etc.\n \n I'm recommending this paper for acceptance, as I think the contribution here is a good one, and the basic approach very nicely offers a better exploration policy than the typical Boltzmann policy, using a fairly trivial modification to REINFORCE. But after re-reading I'm less enthusiastic, simply because the delta over previous work (namely the RAML paper) doesn't seem incredibly substantial. None of the domains in the experiments seem substantially challenges, and the fact that it can improve over REINFORCE isn't necessarily amazing.\n \n Pros:\n + Well-motivated (and simple) modification to REINFORCE to get better exploration\n + Demonstrably better performance with seemingly less hyperparameter tunies\n \n Cons:\n - Delta over RAML work isn't that clear, essentially is just a weighted combination between REINFORCE and RAML (though in RL context)\n - Experiments are good, but not outstanding relative to simple baselines", "OTHER_KEYS": "ICLR 2017 pcs"}, {"DATE": "13 Jan 2017", "TITLE": "Paper Updates", "IS_META_REVIEW": false, "comments": "We thank all the reviewers for their thoughtful comments.  We have made several updates to the paper in response:\n\n-- We have made adjustments to the introduction to make it clear that the motivation of the paper is twofold: (1) to introduce an RL algorithm that improves the performance and the exploration behavior of REINFORCE (2) to improve the behavior of RL methods on algorithmic tasks.\n\n-- In Section 6, we have made it clear that we only use total-episode reward for UREX and MENT.\n\n-- For more results, we have revised the paper to include an appendix with a simple bandit-like task with a large action space. \n", "OTHER_KEYS": "Ofir Nachum"}, {"IMPACT": 3, "SUBSTANCE": 3, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "TITLE": "Review", "comments": "This paper proposes a novel exploration strategy that promotes exploration of under-appreciated reward regions. Proposed importance sampling based approach is a simple modification to REINFORCE and experiments in several algorithmic toy tasks show that the proposed model is performing better than REINFORCE and Q-learning.\n\nThis paper shows promising results in automated algorithm discovery using reinforcement learning. However it is not very clear what is the main motivation of the paper. Is the main motivation better exploration for policy gradient methods? If so, authors should have benchmarked their algorithm with standard reinforcement learning tasks. While there is a huge body of literature on improving REINFORCE, authors have considered a simple version of REINFORCE on a non-standard task and say that UREX is better. If the main motivation is improving the performance in algorithm learning tasks, then the baselines are still weak. Authors should make it clear which is the main motivation.\n\nAlso the action space is too small. In the beginning authors raise the concern that entropy regularization might not scale to larger action spaces. So a comparison of MENT and UREX in a large action space problem would give more insights on whether UREX is not affected by large action space.\n\n--------------------------\nAfter rebuttal:\nI missed the action sequences argument when I pointed about small action space issue.\n\nFor question regarding weak baseline, there are several tricks used in the literature to tackle high-variance issue for REINFORCE. For example, see Mnih & Gregor, 2014.\n\nI have increased my rating from 6 to 7. I still encourage the authors to improve their baseline.\n", "IS_ANNOTATED": true, "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "20 Dec 2016 (modified: 24 Jan 2017)", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "Review", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "\n\noverview:\nThis work proposes to link trajectory log-probabilities and rewards by defining under-appreciated rewards. This suggests that there is a linear relationship between trajectory rewards and their log-probability which can be exploited by measuring the resulting mismatch.\nThat is, when an action sequence under-appreciates its reward, its log-probability is increased.\nThis method is a simple modification to the well-known REINFORCE method, requiring only one extra hyperparameter \\tau, and intuitively provides us with a better exploration mechanism than \\epsilon-greedy or random exploration.\nThe method is tested on algorithmic environments, and compared to entropy-regularized REINFORCE and double Q-learning, and performs equally or better than those two baselines, especially in more complex environments.\n\n\nremarks:\n- the focus in the introduction on algorithmic tasks may be a double-edged sword. It is an interesting domain to test your hypothesis and benchmark your method. At the same time, it distracts the reader from the (IMO) generality of the proposed method.\n- in the introduction you say the reward is sparse, in section 6, on tasks 1-5, you then say there is a reward at each correct emission, i.e. each time step. This is only 'corrected' to end-of-episode-reward in section 7.4, after having discussed results. I'd move or mention this in section 6.\n- approach seems quite sensible to tau being in the same range as logpi(a|h), but you only try tau=0.1 for UREX. I'm not sure I understand nor agree with this experimentation choice.\n- an alternative to grid search is random search (Bergstra&Bengio, 2012). It may illustrate better hyperparameter robustness, and allow you to explore more in the same number of experiment.\n\nopinion:\n- An interesting approach to policy-gradient, to be sure. It tackles the very important question of \"how should agents explore?\"\n- I'm ambivalent to claiming that an algorithm is robust to hyperparmeters simply because it performs better on the selected hyperparameter range. All you really show is that it performs well some amount of time when the hyperparams lay in that range. Couldn't it be that MENT needs different hyperparameters? (Just being devil's advocate here)\n- I see why matching 1/tau with logpi is the obvious choice, but it implies a very strong prior: that the reward (to a factor of 1/tau) lies in the same space as the log policy. One point of failure I see (but correct me if I'm wrong) is that as the length of the trajectory grows the reward is expected to grow linearly, so short ways to get some reward will be less explored than long ways of getting the same reward, creating an imbalance unless the reward is shaped such that shorter trajectories get more reward (which is only the case in task 6).\n- It might have been good to also compare with methods explicitly trying to explore better with value-functions (e.g. prioritized experience replay, Schaul et al 2015)\n- At the risk of repeating myself, tau plays a major role in this method, but there is little analysis on its effect on experiments.\n\n\nThe methodology and reasoning is clearly explained and I think this paper communicates its message very well.\nThat message is novel, albeit a minor modification to a well-known algorithm, it is well motivated and, I think, a welcome addition to literature concerning exploration in RL.\nThe experiments are chosen accordingly, and results seem to reflect the hypothesis of the authors.\n\nI realize the tyranny of extensive experimentation and the scarcity of time, but I do think that this paper would benefit from more (or cleverer) experimentation, as well as demonstrating more explicitly the impact of the method on exploration.\nReading this paper convinced me that measuring mismatch between a trajectory's observed reward and its probability given the current policy is a clever (and well motivated) thing to do. Yet, I think that the paper could have a more convincing empirical argument, even if it is for toy tasks.", "IS_META_REVIEW": false, "RECOMMENDATION": 8, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"SUBSTANCE": 2, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "TITLE": "Review", "comments": "The paper proposes a new algorithm based on REINFORCE which aims at exploring under-appreciate action sequences. The idea is to compare the probability of a sequence of actions under the current policy with the estimated reward. Actions where the current policy under-estimate the reward will provide a higher feedback, thus encouraging exploration of particular sequences of actions. The UREX model is tested on 6 algortihmic RL problems and show interesting properties in comparison to the standard regularized REINFORCE (MENT) model and to Q-Learning.\n\nThe model is interesting, well defined and well explained. As far as I know, the UREX model is an original model which will certainly be useful for the RL community. The only drawback of the paper is to restrict the evaluation of this algortihm to algorithmic problems that are specific while it would be easy to test the proposed model onto other standard RL problems. This would clearly help to make the article stronger and I greatly encourage the authors to add some other tasks in their paper. ", "IS_ANNOTATED": true, "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "16 Dec 2016", "CLARITY": 4, "REVIEWER_CONFIDENCE": 4}, {"DATE": "07 Dec 2016", "TITLE": "Optimal Reward vs. Actual Reward", "IS_META_REVIEW": false, "comments": "Hi,\n\nI feel the weight \"w_\\tau(a^k|h)\" proposed in Equation (10) is quite crucial to model the *under appreciation*. Equation (10) is linked to the optimal policy distribution proposed by Equation (5). The weight should be proportional  to \u03c0*/\u03c0_\\theta, and \u03c0* is proposed in Equation (5) based on the rewards given optimal policy. But apparently, in Equation (10), the reward term \"r\" used is from the underlying policy, which is not optimal. Otherwise, it's not quite possible to compute that for gradient update. \n\nHope you could clarify on this. Thank you!", "OTHER_KEYS": "(anonymous)"}, {"SUBSTANCE": 2, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "TITLE": "pre-review questions", "comments": "", "IS_ANNOTATED": true, "IS_META_REVIEW": false, "DATE": "02 Dec 2016", "CLARITY": 4}, {"DATE": "29 Nov 2016", "TITLE": "A few questions", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1"}, {"DATE": "23 Feb 2017", "TITLE": "Same objective as reward-weighted regression", "IS_META_REVIEW": false, "comments": "\nIt has recently come to my attention that the objective proposed by the authors in this paper has in fact already been studied in the literature under the name 'reward-weighted regression', from e.g. ICML 2007 [1]. This has spawned several other works using the same objective (e.g. [2]). One can examine for instance the objective proposed in Section 3.4 from [2], from ICANN 2008.\n\nWhile this paper has already been accepted to ICLR, it would be beneficial for the authors to at least cite these works (and other related works) so that readers are aware of the previous origins of this idea.\n\n[1] ", "OTHER_KEYS": "(anonymous)"}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "This paper proposes a nice algorithm for improved exploration in policy search RL settings. The method essentially optimizes a weighted combination of expected reward (essentially the REINFORCE objective without an entropy regularizer) plus a term from the reward augmented maximum likelihood objective (from a recent NIPS paper), and show that the resulting update can be made with a fairly small modification to the REINFORCE algorithm. The authors show improved performance on several sequential \"program-like\" domains like copying a string, adding, etc.\n \n I'm recommending this paper for acceptance, as I think the contribution here is a good one, and the basic approach very nicely offers a better exploration policy than the typical Boltzmann policy, using a fairly trivial modification to REINFORCE. But after re-reading I'm less enthusiastic, simply because the delta over previous work (namely the RAML paper) doesn't seem incredibly substantial. None of the domains in the experiments seem substantially challenges, and the fact that it can improve over REINFORCE isn't necessarily amazing.\n \n Pros:\n + Well-motivated (and simple) modification to REINFORCE to get better exploration\n + Demonstrably better performance with seemingly less hyperparameter tunies\n \n Cons:\n - Delta over RAML work isn't that clear, essentially is just a weighted combination between REINFORCE and RAML (though in RL context)\n - Experiments are good, but not outstanding relative to simple baselines", "OTHER_KEYS": "ICLR 2017 pcs"}, {"DATE": "13 Jan 2017", "TITLE": "Paper Updates", "IS_META_REVIEW": false, "comments": "We thank all the reviewers for their thoughtful comments.  We have made several updates to the paper in response:\n\n-- We have made adjustments to the introduction to make it clear that the motivation of the paper is twofold: (1) to introduce an RL algorithm that improves the performance and the exploration behavior of REINFORCE (2) to improve the behavior of RL methods on algorithmic tasks.\n\n-- In Section 6, we have made it clear that we only use total-episode reward for UREX and MENT.\n\n-- For more results, we have revised the paper to include an appendix with a simple bandit-like task with a large action space. \n", "OTHER_KEYS": "Ofir Nachum"}, {"IMPACT": 3, "SUBSTANCE": 3, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "TITLE": "Review", "comments": "This paper proposes a novel exploration strategy that promotes exploration of under-appreciated reward regions. Proposed importance sampling based approach is a simple modification to REINFORCE and experiments in several algorithmic toy tasks show that the proposed model is performing better than REINFORCE and Q-learning.\n\nThis paper shows promising results in automated algorithm discovery using reinforcement learning. However it is not very clear what is the main motivation of the paper. Is the main motivation better exploration for policy gradient methods? If so, authors should have benchmarked their algorithm with standard reinforcement learning tasks. While there is a huge body of literature on improving REINFORCE, authors have considered a simple version of REINFORCE on a non-standard task and say that UREX is better. If the main motivation is improving the performance in algorithm learning tasks, then the baselines are still weak. Authors should make it clear which is the main motivation.\n\nAlso the action space is too small. In the beginning authors raise the concern that entropy regularization might not scale to larger action spaces. So a comparison of MENT and UREX in a large action space problem would give more insights on whether UREX is not affected by large action space.\n\n--------------------------\nAfter rebuttal:\nI missed the action sequences argument when I pointed about small action space issue.\n\nFor question regarding weak baseline, there are several tricks used in the literature to tackle high-variance issue for REINFORCE. For example, see Mnih & Gregor, 2014.\n\nI have increased my rating from 6 to 7. I still encourage the authors to improve their baseline.\n", "IS_ANNOTATED": true, "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "20 Dec 2016 (modified: 24 Jan 2017)", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "Review", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "\n\noverview:\nThis work proposes to link trajectory log-probabilities and rewards by defining under-appreciated rewards. This suggests that there is a linear relationship between trajectory rewards and their log-probability which can be exploited by measuring the resulting mismatch.\nThat is, when an action sequence under-appreciates its reward, its log-probability is increased.\nThis method is a simple modification to the well-known REINFORCE method, requiring only one extra hyperparameter \\tau, and intuitively provides us with a better exploration mechanism than \\epsilon-greedy or random exploration.\nThe method is tested on algorithmic environments, and compared to entropy-regularized REINFORCE and double Q-learning, and performs equally or better than those two baselines, especially in more complex environments.\n\n\nremarks:\n- the focus in the introduction on algorithmic tasks may be a double-edged sword. It is an interesting domain to test your hypothesis and benchmark your method. At the same time, it distracts the reader from the (IMO) generality of the proposed method.\n- in the introduction you say the reward is sparse, in section 6, on tasks 1-5, you then say there is a reward at each correct emission, i.e. each time step. This is only 'corrected' to end-of-episode-reward in section 7.4, after having discussed results. I'd move or mention this in section 6.\n- approach seems quite sensible to tau being in the same range as logpi(a|h), but you only try tau=0.1 for UREX. I'm not sure I understand nor agree with this experimentation choice.\n- an alternative to grid search is random search (Bergstra&Bengio, 2012). It may illustrate better hyperparameter robustness, and allow you to explore more in the same number of experiment.\n\nopinion:\n- An interesting approach to policy-gradient, to be sure. It tackles the very important question of \"how should agents explore?\"\n- I'm ambivalent to claiming that an algorithm is robust to hyperparmeters simply because it performs better on the selected hyperparameter range. All you really show is that it performs well some amount of time when the hyperparams lay in that range. Couldn't it be that MENT needs different hyperparameters? (Just being devil's advocate here)\n- I see why matching 1/tau with logpi is the obvious choice, but it implies a very strong prior: that the reward (to a factor of 1/tau) lies in the same space as the log policy. One point of failure I see (but correct me if I'm wrong) is that as the length of the trajectory grows the reward is expected to grow linearly, so short ways to get some reward will be less explored than long ways of getting the same reward, creating an imbalance unless the reward is shaped such that shorter trajectories get more reward (which is only the case in task 6).\n- It might have been good to also compare with methods explicitly trying to explore better with value-functions (e.g. prioritized experience replay, Schaul et al 2015)\n- At the risk of repeating myself, tau plays a major role in this method, but there is little analysis on its effect on experiments.\n\n\nThe methodology and reasoning is clearly explained and I think this paper communicates its message very well.\nThat message is novel, albeit a minor modification to a well-known algorithm, it is well motivated and, I think, a welcome addition to literature concerning exploration in RL.\nThe experiments are chosen accordingly, and results seem to reflect the hypothesis of the authors.\n\nI realize the tyranny of extensive experimentation and the scarcity of time, but I do think that this paper would benefit from more (or cleverer) experimentation, as well as demonstrating more explicitly the impact of the method on exploration.\nReading this paper convinced me that measuring mismatch between a trajectory's observed reward and its probability given the current policy is a clever (and well motivated) thing to do. Yet, I think that the paper could have a more convincing empirical argument, even if it is for toy tasks.", "IS_META_REVIEW": false, "RECOMMENDATION": 8, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"SUBSTANCE": 2, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "TITLE": "Review", "comments": "The paper proposes a new algorithm based on REINFORCE which aims at exploring under-appreciate action sequences. The idea is to compare the probability of a sequence of actions under the current policy with the estimated reward. Actions where the current policy under-estimate the reward will provide a higher feedback, thus encouraging exploration of particular sequences of actions. The UREX model is tested on 6 algortihmic RL problems and show interesting properties in comparison to the standard regularized REINFORCE (MENT) model and to Q-Learning.\n\nThe model is interesting, well defined and well explained. As far as I know, the UREX model is an original model which will certainly be useful for the RL community. The only drawback of the paper is to restrict the evaluation of this algortihm to algorithmic problems that are specific while it would be easy to test the proposed model onto other standard RL problems. This would clearly help to make the article stronger and I greatly encourage the authors to add some other tasks in their paper. ", "IS_ANNOTATED": true, "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "16 Dec 2016", "CLARITY": 4, "REVIEWER_CONFIDENCE": 4}, {"DATE": "07 Dec 2016", "TITLE": "Optimal Reward vs. Actual Reward", "IS_META_REVIEW": false, "comments": "Hi,\n\nI feel the weight \"w_\\tau(a^k|h)\" proposed in Equation (10) is quite crucial to model the *under appreciation*. Equation (10) is linked to the optimal policy distribution proposed by Equation (5). The weight should be proportional  to \u03c0*/\u03c0_\\theta, and \u03c0* is proposed in Equation (5) based on the rewards given optimal policy. But apparently, in Equation (10), the reward term \"r\" used is from the underlying policy, which is not optimal. Otherwise, it's not quite possible to compute that for gradient update. \n\nHope you could clarify on this. Thank you!", "OTHER_KEYS": "(anonymous)"}, {"SUBSTANCE": 2, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "TITLE": "pre-review questions", "comments": "", "IS_ANNOTATED": true, "IS_META_REVIEW": false, "DATE": "02 Dec 2016", "CLARITY": 4}, {"DATE": "29 Nov 2016", "TITLE": "A few questions", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1"}]}
{"text": "MULTILAYER RECURRENT NETWORK MODELS OF PRI- MATE RETINAL GANGLION CELL RESPONSES\n1 INTRODUCTION\nOur understanding of sensory processing in the brain is most straightforwardly reflected in our ability to model the process by which stimuli presented at the sensory periphery are transformed into the spiking activity of populations of neurons. For decades, researchers have interrogated stimulus-response\n\u2217These authors contributed equally.\nneural properties using simplified targeted stimuli, such as bars, spots, or gratings. While these types of stimuli uncovered many interesting aspects of visual computation, they have several limitations (Barlow & Levick, 1965). These stimuli may not fully drive important components of neural response, and modeling efforts have often assumed a quasi-linear mapping from stimulus to firing rate. Subsequent efforts to characterize cells relied on white noise stimulation and building models through reverse correlation (de Boer R & Kuyper, 1968; Marmarelis & Naka, 1972; Chichilnisky, 2001). A standard model used to relate white noise to spiking responses is the linear-nonlinear-Poisson (LN) or generalized linear model (GLM) which consists of a spatiotemporal linear filtering of the stimulus followed by a nonlinearity and probabilistic spike generation (Chichilnisky, 2001; Simoncelli et al., 2004; Schwartz et al., 2006). Although this family of models have advanced our understanding, they do not optimally capture neural responses, especially to natural scenes which can lead to more complex responses than white noise stimuli (David et al., 2004). Even in the retina, early in the visual processing stream, these commonly-used models capture retinal ganglion cell (RGC) responses to natural stimuli less accurately than to white noise (Heitman et al., 2016).\nRecently, deep neural networks have been used to dramatically improve performance on a diverse array of machine learning tasks (Krizhevsky et al., 2012; LeCun et al., 2015). Furthermore, these networks bear a loose resemblance to real neural networks, and provide a sufficiently rich model class that can still be roughly constrained to match the biological architecture (Kriegeskorte, 2015). Most previous research at this intersection of neuroscience and artificial neural networks has focused on training networks on a certain task, such as object recognition, and then comparing the computations performed in different layers of the artificial network to those performed by real neurons (Yamins et al., 2014). Here we take a different approach: we fit multilayer models directly to the spiking responses of neurons, an approach that has not been explored in detail (but see (McIntosh et al., 2016) for some recent independent parallel developments).\n2 APPROACH\nWe fit a range of models, detailed below, to spiking responses of primate RGCs. Our baseline comparisons are the GLM architectures that have been widely used to construct previous neural models (Pillow et al., 2008), though here we focus on individual neuronal responses (we leave modeling of correlations between neurons for future work). We focused on RNNs as a flexible framework in which to model more complex temporal and spatial nonlinearities. We also explored a number of network architectures involving features or weights shared across observed neurons. Given the complexity of the network architectures, we reasoned that sharing statistical strength across neurons by learning a shared feature space might improve predictive performance. This is conceptually a form of multitask learning - we are using a shared representation to achieve better generalization (Baxter, 2000). Motivated by previous research showing significant differences in the processing properties of the two cell types examined, ON and OFF parasol retinal ganglion cells, we fit separate models for each of these cell types (Chichilnisky & Kalmar, 2002).\n3 METHODS\n3.1 DATA COLLECTION\nWe fit spiking responses of OFF and ON parasol retinal ganglion cells to natural scenes. Recordings were performed on isolated retina using a large-scale multi-electrode recording system (Litke et al., 2004; Frechette et al., 2005; Field et al., 2007). A standard spike sorting algorithm was used to identify spikes from different cells from the voltage signals on each electrode during visual stimulation (Litke et al., 2004). We focus on two separate experiments (the same experimental procedure in two separate retinas) here; analyses of other datasets yielded similar results. Models were fit separately for the two experiments due to animal to animal variability in cell properties, such as receptive field size and firing rate. Almost all spike sorted cells were used for training (exp 1 = 118 OFF cells, 66 ON cells; exp 2 = 142 OFF cells, 103 ON cells): two cells were removed due to data quality issues (see sec 3.3). Performance metrics in this paper are reported for the same subset of cells used in a previous study (Heitman et al., 2016). These cells passed passed a manual screen for spike sorting accuracy, demonstrated stable light responses, and met a convergence criteria in prior linear-nonlinear modeling (exp 1 = 10 OFF cells, 18 ON cells; exp 2 = 65 OFF cells, 14 ON cells). The naturalistic movie\nstimulus consisted of images from the Van Hateren database shown for one second each, with spatial jitter based on eye movements during fixation by awake macaque monkeys (Z.M. Hafed and R.J. Krauzlis, personal communication), (van Hateren & van der Schaaf, 1998). An example stimulus can be found at https://youtu.be/sG_18Uz_6OE. 59 distinct natural scenes movies of length one minute (the training data) were interleaved with 59 repetitions of a 30 second movie (the test data). Interleaving ensured that the test movie repetitions spanned the same period of time as the training data and therefore experienced the same range of experimental conditions (in case of neural response drifts over time). The first 4 movies shown (2 training movies and 2 repetitions of the test movie) were excluded to avoid initial transients. Test metrics are reported for the last 29 seconds of the 30 second test movie for the same reason. For further details on the experimental set-up, data preprocessing, and visual stimulation, see Heitman et al. (2016).\n3.2 MODEL TRAINING\nAll models were implemented in Theano and trained on a combination of CPUs and GPUs (Theano Development Team, 2016). Training was performed using the Adam optimizer on the mean squared error (MSE) between predicted firing rate and true spikes (Kingma & Ba, 2014). We also experimented with optimizing a Poisson likelihood; this led to qualitatively similar results but occasionally less stable fits, so we focus on the MSE results here. All recurrent dynamics and temporal filters operated on time bins of 8.33 ms (the frame rate of the movie). Spike history terms and performance metrics were calculated for 0.833 ms bins. We used the same split of training and validation data for both experiments: 104 thirty-second movies as training data and 10 thirty-second movies as a held-out validation set.\nDuring training, the performance on the held-out validation set is checked after every pass through the training data. After each iteration through the training data, if the model exhibits significantly better validation performance than our previous best, we reset the minimum number of iterations to be twice the current iteration number. If we make it through those iterations without another significant improvement, we stop. We train for a maximum of 150 epochs, where we define one epoch as one pass through all the training data. The model with the best validation performance is saved and used to assess test performance. All models with shared parameters were trained on a combined MSE over\nall neurons and the parameters picked were those which minimized validation MSE for all neurons. For individual LNs/GLMs/RNNs, the validation MSE was minimized for each neuron separately.\n3.3 RECEPTIVE FIELD CENTER ESTIMATION\nIn all models used in this paper, we estimate the receptive field (RF) center of each neuron in order to identify the appropriate portion of the image to use as input. We calculate a 250 ms long spike triggered average (STA) using reverse correlation of the neuron\u2019s spikes with a white noise stimulus. We reduce the noise in this STA by using a rank 1 approximation (singular value decomposition followed by reconstruction using the primary temporal and spatial components). We then smooth each frame of the STA via convolution with a Gaussian spatial filter. The center location is defined as the pixel location that has the maximum absolute magnitude over time. The center locations were visually assessed to check accuracy of the algorithm. Rare cases where the algorithm failed to identify the correct center indicated neurons that responded to very little of the image as their receptive field was more than half-way displaced out of the image. These two neurons (two Exp 1 ON cells) were removed from further analysis. If the receptive field center is close to the edge of the image, the image patch is padded with the average training stimulus value.\n3.4 PERFORMANCE EVALUATION\nTo quantitatively evaluate the accuracy of model spike predictions, we used the fraction of explainable variance, which has been described in previous literature (Heitman et al., 2016). Average firing rates over time are obtained after generating spikes from the model in 0.833 ms bins and smoothing with a Gaussian temporal filter (SD=10ms). The fraction of variance is computed as\nF (r, rs) = 1\u2212 \u2211\nt(r(t)\u2212 rs(t))2\u2211 t(r(t)\u2212 \u00b5)2\n(1)\nwhere r(t) is the smoothed recorded firing rate, rs(t) is the smoothed predicted firing rate, and \u00b5 is the average recorded rate. Finally, to account for the reproducibility of responses over repeated trials, we normalize by the fraction of variance captured by using the average firing rate on the odd (ro) trials of the repeated test movie to predict responses on the even (re) trials:\nFV = F (r, rs)\nF (re, ro) . (2)\n4 MODEL ANALYSIS\n4.1 NETWORK ARCHITECTURES\nIndividual LNs and GLMs: The linear-nonlinear model (LN) consists of a spatiotemporal filtering of the 31x31x30 movie patch (Xt, width by height by time) surrounding the estimated center of the neuron\u2019s receptive field plus a bias term (b), followed by a sigmoid nonlinearity (f ), and Poisson spike generation to produce the responses rt. The generalized linear model (GLM), given by\nrt \u223c Poiss [ f ( ~wTs (Xt ~wt) + b+ \u2211 i hirt\u2212i )] , (3)\nhas the same architecture with the addition of a post-spike history filter h before the nonlinearity f (Pillow et al., 2008). We used a rank 1 approximation of the full spatiotemporal filter (higher rank models did not significantly improve fits on a subset of examined neurons), resulting in a vectorized 31x31 spatial filter (~ws) and a 30 bin temporal filter (~wt) which spans 250 ms (Heitman et al., 2016). The post-spike history filter consists of a weighted sum of a basis of 20 raised cosines spanning approximately 100 ms (Pillow et al., 2008). The models with spike history were fit by initializing with the model fit without spike history. The filter either operates on the recorded spikes (training and validation) or the spikes generated by the model (testing). The nonlinearity is the logistic sigmoid: f = L/(1 + exp(\u2212x)), which has been shown to improve fitting over an exponential nonlinearity for modeling RGC responses to natural scenes (Heitman et al., 2016).\nShared LN: In this model, the architecture is similar to the individual LNs but all cells of a given type (OFF or ON) share the same temporal and spatial filters (Figure 1A; note that the spatial filters are displaced to the RF center of each individual RGC). All other parameters are individually tuned for each observed neuron. There is an additional gain term that weights the output of the filtering individually for each observed neuron.\nTwo-layer RNN, 50 units: In this architecture, there are two recurrent neural network (RNN) layers between the image patch and Poisson neural unit:\n~h (1) j,t = max(0, U1~sj,t + V1 ~h (1) j,t\u22121 + ~c) (4)\n~h (2) j,t = max(0, U2 ~h (1) j,t + V2 ~h (2) j,t\u22121 + ~d) (5) rj,t \u223c Poiss [ f(~wTj ~h (2) j,t + bj) ] . (6)\nThe activity of the 50 units in the first RNN layer at time t is given by ~h(1)j,t in Eqn. 4. These units are rectified linear, and receive input from the vectorized 31x31 image patch surrounding the center of neuron j\u2019s receptive field, ~sj,t, with weights U1, along with input from the other units in the layer with weights V1 and a bias ~c. The output of the first RNN is then fed into a second RNN with similar architecture. The firing rate for each observed neuron in the final layer is then given by Eqn. 6, and is a weighted sum of the recurrent units plus a bias bj , followed by a softplus nonlinearity f = log(1 + exp(\u2212x)). Note that all parameters are shared across neurons except for the weights to the final layer and the final bias terms (~wj and bj).\nGLM-RNN Hybrid: The GLM-RNN hybrid model consists of a spatial filter followed by a two-layer RNN. The architecture resembles that of the full two-layer RNN with 50 units, except the input to the first layer is a scalar (post multiplication with the spatial filter) at each time step instead of the full image patch; thus the RNN in this model is responsible for shaping the temporal properties of the output, but does not affect spatial processing after the first linear spatial filtering stage. All weights\nare shared across neurons except for weights to the final layer (~wj) and the final bias terms (bj):\nyj,t = ~w T s ~sj,t (7)\n~h (1) j,t = max(0, ~u1yj,t + V1 ~h (1) j,t\u22121 + ~c) (8)\n~h (2) j,t = max(0, U2h (1) j,t + V2 ~h (2) j,t\u22121 + ~d) (9) rj,t \u223c Poiss [ f(~wTj ~h (2) t + bj) ] . (10)\n4.2 MODEL PERFORMANCE\nRNNs of varying architectures consistently outperformed LNs and GLMs in predicting neural spiking responses to a novel natural scene movie for both OFF and ON parasol retinal ganglion cells in both experiments (Figure 2). A shared two-layer recurrent network consistently captures around 80% of the explainable variance across experiments and cell types. Other recurrent architectures (1-3 layer RNNs and a 2 layer LSTM) led to similar levels of performance (Supplementary Figure 6). The increase in performance according to the fraction of explainable variance metric was not an average effect: almost all neurons were significantly better predicted by the RNN (Figure 2B). A 2 layer RNN model with additional trained spike history filters outperformed GLMs and LNs according to a normalized log likelihood metric (Supplementary Figure 7).\nInspection of the mean predicted firing rate traces for LNs and RNNs in Figure 3 reveals that the recurrent network seems to be capturing the timing of firing more precisely. The LN often predicts a general increase in firing rate at the correct times, but the RNN captures the sudden increase in firing rate followed by decay which often occurs when the image changes. On the other hand, the LN models sometimes predict modest increases or decreases in firing rate that the recurrent nets miss.\nUnderstanding why the recurrent models improve performance is a challenging task due to the black-box nature of deep networks. The first layer filters (U1, from image patches to recurrent units) have an interpretable structure resembling traditional receptive fields expected in the retina\n(Supplementary Figure 8). However, the computations performed by the recurrent units are difficult to tease apart, because the weights are less interpretable. Thus, instead of attempting a mechanistic explanation of the internals of the RNN, we focused on what additional captured information resulted in the improved RNN performance.\nOne possibility is that capturing nonlinear effects in parts of the image far from the receptive field center improved predictions (McIlwain, 1964; Passaglia et al., 2009). We restricted the size of the image patch surrounding each receptive field center from 31x31 to 15x15 (Supplementary Figure 9). Shared RNNs trained on the smaller image patch size did as well, or better, than those trained on the larger patch across almost all combinations of cell type and experiment. (We see a similar small improvement when training the LN models on the small patch.) Thus we concluded that long-range nonlinear spatial interactions do not contribute to the increased performance produced by the RNNs.\nWe also investigated whether nonlinear spatial interactions or nonlinear temporal processing primarily contributed to better predictions. To accomplish this, we constructed a GLM-RNN hybrid, described previously, in which a single spatial filter precedes a two-layer RNN - effectively allowing only temporal nonlinearities to be captured. This model improved prediction over the LNs and GLMs but did not reach full RNN performance. The amount by which this model closed the gap differed for different experiments and cell types. We quantified this by computing the difference between multitask RNN and multitask LN performance for each neuron and the difference between multitask hybrid and multitask LN performance. We divide the latter by the former (on a cell-by-cell basis) to obtain the ratios summarized in Figure 2C. The hybrid model closed greater than half of the gap on average between multitask LN and RNN performance, indicating that the richer temporal dynamics of the RNN model account for a large part of the difference between RNN and LN performance, though spatial nonlinearities play a role too.\n5 MODEST TRAINING DATA LENGTH SUFFICES FOR GOOD PERFORMANCE\nDeep networks can be complex and often require large amounts of data to adequately train: convolutional neural networks used for object recognition are trained on over a million images (Krizhevsky et al., 2012). Standard neuroscience experiments yield limited data sets, so it is crucial to assess whether we have enough data to adequately fit our network architectures. We trained the RNN on varying amounts of data, and ran several different iterations of the network to explore variation over random initializations and randomly chosen training sets. These results are shown for both ON and OFF cells in Figure 4. Surprisingly small amounts of training data resulted in good predictive abilities. For larger amounts of training data, different iterations resulted in very similar mean fraction of variance values, indicating fairly robust fitting in these models. See Supplementary Figure 10 for further details.\n6 BENEFITS OF MULTITASK FRAMEWORK\nWe investigated whether the multitask framework with shared parameters across neurons actually helps to improve predictive performance with reasonable amounts of experimental data. First, we quantified the benefits of parameter-sharing in the simple LN model. This is a highly constrained\nframework: every cell has the same spatial and temporal filter. The shared LN does not improve performance for most neurons (Figure 5A).\nWe expected the multitask framework to be more helpful applied to the RNN model because in this case we are sharing features but not all parameters across neurons. Indeed, the multitask RNN consistently outperformed RNNs trained individually on single neurons (Figure 5B); individuallytrained RNNs also had much more variable losses than did the multitask-trained RNNs. In a realistic experimental setting with limited data, the multitask framework is a useful way to leverage all of the data collected for all neurons.\n7 CONCLUSION\nUsing retinal neurons responding to natural scenes as an example, we showed that: using deep networks to model neural spiking responses can significantly improve prediction over current state-ofthe-art models; sharing information across neurons in a multi-task framework leads to better and more stable predictions; and these models work well even given relatively small amounts of experimental data. We believe that the multitask RNN framework presented here will enable new, richer models of complex nonlinear spiking computations in other brain areas.\nWhile one could argue that we have merely exchanged the black box of the brain for another black box, just having a more predictive model is an important tool for research: these predictive models of the primate retina can be used in retinal prosthetics research, to probe decoding, and as a first stage of processing in the modeling of higher visual areas. Additionally, the recurrent network is more accessible and available for experimentation and quantitative analysis. For example, the trained neural network models may guide choices for more accurate simpler models by identifying key computational features that are important to include. Training smaller models on the denoised compression of spiking data (the predicted firing rate) may help them to learn features they otherwise would not (Ba & Caruana, 2014). The deep network approach allows one to determine types of information important to the neuron without having to build an exact mechanistic model of how such information is incorporated, as demonstrated by our finding that both spatial and temporal nonlinearities are not fully captured by the standard pseudo-linear models. We hope in future work to gain a more thorough and quantitative understanding of the dynamics captured by the recurrent networks and to extend this approach to higher sensory areas.\nACKNOWLEDGMENTS\nFunding for this research was provided by the National Science Foundation Graduate Research Fellowship Program under grant No. DGE-114747 (NB), Grant Number No. DGE-16-44869 (EB), the National Science Foundation IGERT Training Grant No. 0801700 (NB), the National Institutes of Health Grant EY017992 (EJC), NSF CRCNS IIS-1430239 (LP, EJC) and Google Faculty Research awards (LP, EJC); in addition, this work was supported by the Intelligence Advanced Research Projects Activity (IARPA) via Department of Interior/ Interior Business Center (DoI/IBC) contract number D16PC00003 (LP). The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright annotation thereon. Disclaimer: The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of IARPA, DoI/IBC, or the U.S. Government.\n", "cats": {"READ": 1.0, "DONTREAD": 0.0}, "reviews": [{"IS_META_REVIEW": true, "comments": "This paper explores the ability of nonlinear recurrent neural networks to account for neural response properties that have otherwise eluded the ability of other models.  A multilayer rnn is trained to imitate the stimulus-response mapping measured from actual retinal ganglion cells in response to a sequence of natural images.  The rnn performs significantly better, especially in accounting for transient responses, than conventional LN/GLM models.\n\nThis work is an important step in understanding the nonlinear response properties of visual neurons.  Recent results have shown that the responses of even retinal ganglion cells in response to natural movies are difficult to explain in terms of standard receptive field models.  So this presents an important challenge to the field.  If we even had *a* model that works, it would be a starting point.  So this work should be seen in that light.  The challenge now of course is to tease apart what the rnn is doing.  Perhaps it could now be pruned and simplified to see what parts are critical to performance.  It would have been nice to see such an analysis.   Nevertheless this result is a good first start and I think important for people to know about.\n\nI am a bit confused about what is being called a \"movie.\"  My understanding is that it is essentially a sequence of unrelated images shown for 1 sec. each.  But then it is stated that the \"frame rate\" is 1/8.33 ms.  I think this must refer to the refresh rate of the monitor, right?   \n\nI would guess that the deviations from the LN model are even stronger when you show actual dynamic natural scenes - i.e., real movies.  Here I would expect the rnn to have an even more profound effect, and potentially be much more informative."}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "This work is an important step in developing the tools for understanding the nonlinear response properties of visual neurons. The methods are sound and the results are meaningful. Reviewer 3 gave a much lower score than the other two reviewers because Rev 3 does not appreciate the improvement of prediction performance as an advance in itself. For understanding of the visual algorithms in the brain, however, prediction performance is the most critical success criterion. The paper provides convincing evidence that the approach is promising and likely to facilitate further advances towards achieving this long-term goal.\n \n I am confident enough to defend acceptance of this paper for a poster.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"DATE": "15 Jan 2017", "TITLE": "Revised Manuscript", "IS_META_REVIEW": false, "comments": "We have posted a new revision of the manuscript in which we changed the subset of cells for which we show results to better align with a previous study. The results remain unchanged. One of our example cells in Figure 3 was no longer in the criteria-passing subset so we are showing responses from a different OFF example cell. Additionally, we added a supplementary figure showing comparisons using a normalized log-likelihood metric, posted a link to a video of the stimulus, added \u201cResponses\u201d to the title, and made minor cosmetic changes to the figures.", "OTHER_KEYS": "Eleanor Batty"}, {"OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "TITLE": "Review of \"Multilayer Recurrent Network Models of Primate Retinal Ganglion Cells\"", "comments": "This paper explores the ability of nonlinear recurrent neural networks to account for neural response properties that have otherwise eluded the ability of other models.  A multilayer rnn is trained to imitate the stimulus-response mapping measured from actual retinal ganglion cells in response to a sequence of natural images.  The rnn performs significantly better, especially in accounting for transient responses, than conventional LN/GLM models.\n\nThis work is an important step in understanding the nonlinear response properties of visual neurons.  Recent results have shown that the responses of even retinal ganglion cells in response to natural movies are difficult to explain in terms of standard receptive field models.  So this presents an important challenge to the field.  If we even had *a* model that works, it would be a starting point.  So this work should be seen in that light.  The challenge now of course is to tease apart what the rnn is doing.  Perhaps it could now be pruned and simplified to see what parts are critical to performance.  It would have been nice to see such an analysis.   Nevertheless this result is a good first start and I think important for people to know about.\n\nI am a bit confused about what is being called a \"movie.\"  My understanding is that it is essentially a sequence of unrelated images shown for 1 sec. each.  But then it is stated that the \"frame rate\" is 1/8.33 ms.  I think this must refer to the refresh rate of the monitor, right?   \n\nI would guess that the deviations from the LN model are even stronger when you show actual dynamic natural scenes - i.e., real movies.  Here I would expect the rnn to have an even more profound effect, and potentially be much more informative.\n", "SOUNDNESS_CORRECTNESS": 4, "ORIGINALITY": 4, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "RECOMMENDATION": 8, "DATE": "17 Dec 2016", "REVIEWER_CONFIDENCE": 5}, {"TITLE": "a clearly written paper with nice, if straightforward, results", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer4", "comments": "This is a clearly written paper with a nice, if straightforward, result: RNNs can be good predictive models of neuron firing rates in the retina.\n\nOn the one hand, the primary scientific contribution seems to just be to confirm that this approach works. On this particular stimulus locked task the gains from using the RNN seemed relatively modest, and it hasn't yet taught us anything new about the biology.\n\nOn the other hand, this (along with the concurrent work of McIntosh et al.) is introducing neural network modeling to a field that isn't currently using it, and where it should prove very effective.\n\nI think it would be very interesting to see the results of applying a framework like this one with LFP and other neurons as input and on a shorter discretization time scale.\n\nI suspect followup work building on this proof of concept will be increasingly exciting.\n\nMinor comments:\nSec 3.2:\nI didn't understand the role of the 0.833 ms bins.\nUse \"epoch\" throughout, rather than alternating between \"epoch\" and \"pass through data\".\n\nFig. 4 would be better with the x-axis on a log scale.", "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"IMPACT": 4, "RECOMMENDATION_UNOFFICIAL": 5, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "This paper fits models to spike trains of retinal ganglion cells that are driven by natural images. I think the title should thus include the word \u201cactivity\u201d at the end for otherwise it is actually formally incorrect.\n\nAnyhow, this paper proposes more specifically a recurrent network for this time series prediction and compares it to what seems to be the previous approach of a generalized linear model. Overall the stated paradigm is that when one can predict the spikes well then one can look into the model and learn how nature does it. \n\nIn general the paper sounds plausible, though I am not convinced that I learned a lot. The results in figure 2 show that the RNN model can predict the spikes a bit better. So this is nice. But now what? You have shown that a more complicated model can produce better fits to the data, though there are of course still some variations to the real data. Your initial outline was that a better predictive model helps you to better understand the neural processing in the retina. So tell us what you learned. I am not a specialist of the retina, but I know that there are several layers and recurrencies in the retina, so I am not so surprised that the new model is better than the GLM. \n\nIt seems that more complicated recurrent models such as LSTM do not improve the performance according to a statement in the paper. However, comparisons on this level are also difficult as a more complex models needs more data. Hence, I would actually expect that more layers and even a more detailed model of the retina could eventually improve the prediction even further. \nI was also a bit puzzled that all the neurons in the network share all the same parameters (weights). While the results show that these simplified models can capture a lot of the spike train characteristics, couldn\u2019t a model with free parameters eventually outperform this one (with correspondingly more training data)?\n", "IS_ANNOTATED": true, "TITLE": "What do we learn from the model?", "IS_META_REVIEW": false, "RECOMMENDATION": 4, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "TITLE": "Performance metric", "comments": "", "SOUNDNESS_CORRECTNESS": 4, "ORIGINALITY": 4, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "DATE": "11 Dec 2016"}, {"DATE": "03 Dec 2016", "TITLE": "preliminary questions", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer4"}, {"IMPACT": 4, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "RECOMMENDATION_UNOFFICIAL": 5, "comments": "", "IS_ANNOTATED": true, "TITLE": "Experimental setup", "IS_META_REVIEW": false, "DATE": "28 Nov 2016"}, {"IS_META_REVIEW": true, "comments": "This paper explores the ability of nonlinear recurrent neural networks to account for neural response properties that have otherwise eluded the ability of other models.  A multilayer rnn is trained to imitate the stimulus-response mapping measured from actual retinal ganglion cells in response to a sequence of natural images.  The rnn performs significantly better, especially in accounting for transient responses, than conventional LN/GLM models.\n\nThis work is an important step in understanding the nonlinear response properties of visual neurons.  Recent results have shown that the responses of even retinal ganglion cells in response to natural movies are difficult to explain in terms of standard receptive field models.  So this presents an important challenge to the field.  If we even had *a* model that works, it would be a starting point.  So this work should be seen in that light.  The challenge now of course is to tease apart what the rnn is doing.  Perhaps it could now be pruned and simplified to see what parts are critical to performance.  It would have been nice to see such an analysis.   Nevertheless this result is a good first start and I think important for people to know about.\n\nI am a bit confused about what is being called a \"movie.\"  My understanding is that it is essentially a sequence of unrelated images shown for 1 sec. each.  But then it is stated that the \"frame rate\" is 1/8.33 ms.  I think this must refer to the refresh rate of the monitor, right?   \n\nI would guess that the deviations from the LN model are even stronger when you show actual dynamic natural scenes - i.e., real movies.  Here I would expect the rnn to have an even more profound effect, and potentially be much more informative."}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "This work is an important step in developing the tools for understanding the nonlinear response properties of visual neurons. The methods are sound and the results are meaningful. Reviewer 3 gave a much lower score than the other two reviewers because Rev 3 does not appreciate the improvement of prediction performance as an advance in itself. For understanding of the visual algorithms in the brain, however, prediction performance is the most critical success criterion. The paper provides convincing evidence that the approach is promising and likely to facilitate further advances towards achieving this long-term goal.\n \n I am confident enough to defend acceptance of this paper for a poster.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"DATE": "15 Jan 2017", "TITLE": "Revised Manuscript", "IS_META_REVIEW": false, "comments": "We have posted a new revision of the manuscript in which we changed the subset of cells for which we show results to better align with a previous study. The results remain unchanged. One of our example cells in Figure 3 was no longer in the criteria-passing subset so we are showing responses from a different OFF example cell. Additionally, we added a supplementary figure showing comparisons using a normalized log-likelihood metric, posted a link to a video of the stimulus, added \u201cResponses\u201d to the title, and made minor cosmetic changes to the figures.", "OTHER_KEYS": "Eleanor Batty"}, {"OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "TITLE": "Review of \"Multilayer Recurrent Network Models of Primate Retinal Ganglion Cells\"", "comments": "This paper explores the ability of nonlinear recurrent neural networks to account for neural response properties that have otherwise eluded the ability of other models.  A multilayer rnn is trained to imitate the stimulus-response mapping measured from actual retinal ganglion cells in response to a sequence of natural images.  The rnn performs significantly better, especially in accounting for transient responses, than conventional LN/GLM models.\n\nThis work is an important step in understanding the nonlinear response properties of visual neurons.  Recent results have shown that the responses of even retinal ganglion cells in response to natural movies are difficult to explain in terms of standard receptive field models.  So this presents an important challenge to the field.  If we even had *a* model that works, it would be a starting point.  So this work should be seen in that light.  The challenge now of course is to tease apart what the rnn is doing.  Perhaps it could now be pruned and simplified to see what parts are critical to performance.  It would have been nice to see such an analysis.   Nevertheless this result is a good first start and I think important for people to know about.\n\nI am a bit confused about what is being called a \"movie.\"  My understanding is that it is essentially a sequence of unrelated images shown for 1 sec. each.  But then it is stated that the \"frame rate\" is 1/8.33 ms.  I think this must refer to the refresh rate of the monitor, right?   \n\nI would guess that the deviations from the LN model are even stronger when you show actual dynamic natural scenes - i.e., real movies.  Here I would expect the rnn to have an even more profound effect, and potentially be much more informative.\n", "SOUNDNESS_CORRECTNESS": 4, "ORIGINALITY": 4, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "RECOMMENDATION": 8, "DATE": "17 Dec 2016", "REVIEWER_CONFIDENCE": 5}, {"TITLE": "a clearly written paper with nice, if straightforward, results", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer4", "comments": "This is a clearly written paper with a nice, if straightforward, result: RNNs can be good predictive models of neuron firing rates in the retina.\n\nOn the one hand, the primary scientific contribution seems to just be to confirm that this approach works. On this particular stimulus locked task the gains from using the RNN seemed relatively modest, and it hasn't yet taught us anything new about the biology.\n\nOn the other hand, this (along with the concurrent work of McIntosh et al.) is introducing neural network modeling to a field that isn't currently using it, and where it should prove very effective.\n\nI think it would be very interesting to see the results of applying a framework like this one with LFP and other neurons as input and on a shorter discretization time scale.\n\nI suspect followup work building on this proof of concept will be increasingly exciting.\n\nMinor comments:\nSec 3.2:\nI didn't understand the role of the 0.833 ms bins.\nUse \"epoch\" throughout, rather than alternating between \"epoch\" and \"pass through data\".\n\nFig. 4 would be better with the x-axis on a log scale.", "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"IMPACT": 4, "RECOMMENDATION_UNOFFICIAL": 5, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "This paper fits models to spike trains of retinal ganglion cells that are driven by natural images. I think the title should thus include the word \u201cactivity\u201d at the end for otherwise it is actually formally incorrect.\n\nAnyhow, this paper proposes more specifically a recurrent network for this time series prediction and compares it to what seems to be the previous approach of a generalized linear model. Overall the stated paradigm is that when one can predict the spikes well then one can look into the model and learn how nature does it. \n\nIn general the paper sounds plausible, though I am not convinced that I learned a lot. The results in figure 2 show that the RNN model can predict the spikes a bit better. So this is nice. But now what? You have shown that a more complicated model can produce better fits to the data, though there are of course still some variations to the real data. Your initial outline was that a better predictive model helps you to better understand the neural processing in the retina. So tell us what you learned. I am not a specialist of the retina, but I know that there are several layers and recurrencies in the retina, so I am not so surprised that the new model is better than the GLM. \n\nIt seems that more complicated recurrent models such as LSTM do not improve the performance according to a statement in the paper. However, comparisons on this level are also difficult as a more complex models needs more data. Hence, I would actually expect that more layers and even a more detailed model of the retina could eventually improve the prediction even further. \nI was also a bit puzzled that all the neurons in the network share all the same parameters (weights). While the results show that these simplified models can capture a lot of the spike train characteristics, couldn\u2019t a model with free parameters eventually outperform this one (with correspondingly more training data)?\n", "IS_ANNOTATED": true, "TITLE": "What do we learn from the model?", "IS_META_REVIEW": false, "RECOMMENDATION": 4, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "TITLE": "Performance metric", "comments": "", "SOUNDNESS_CORRECTNESS": 4, "ORIGINALITY": 4, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "DATE": "11 Dec 2016"}, {"DATE": "03 Dec 2016", "TITLE": "preliminary questions", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer4"}, {"IMPACT": 4, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "RECOMMENDATION_UNOFFICIAL": 5, "comments": "", "IS_ANNOTATED": true, "TITLE": "Experimental setup", "IS_META_REVIEW": false, "DATE": "28 Nov 2016"}]}
{"text": "NEURO-SYMBOLIC PROGRAM SYNTHESIS\n1 INTRODUCTION\nThe act of programming, i.e., developing a procedure to accomplish a task, is a remarkable demonstration of the reasoning abilities of the human mind. Expectedly, Program Induction is considered as one of the fundamental problems in Machine Learning and Artificial Intelligence. Recent progress on deep learning has led to the proposal of a number of promising neural architectures for this problem. Many of these models are inspired from computation modules (CPU, RAM, GPU) (Graves et al., 2014; Kurach et al., 2015; Reed & de Freitas, 2015; Neelakantan et al., 2015) or common data structures used in many algorithms (stack) (Joulin & Mikolov, 2015). A common thread in this line of work is to specify the atomic operations of the network in some differentiable form, allowing efficient end-to-end training of a neural controller, or to use reinforcement learning to make hard choices about which operation to perform. While these results are impressive, these approaches have a number of important limitations: (a) they are computationally expensive and hard to train, (b) a model has to be trained for each task (program) separately, and (c) it is hard to interpret or verify the correctness of the learnt mapping (as it is defined by a neural network). While some recently proposed methods (Kurach et al., 2015; Gaunt et al., 2016; Riedel et al., 2016; Bunel et al., 2016) do learn interpretable programs, they still need to learn a separate neural network model for each individual task.\nMotivated by the need for model interpretability and scalability to multiple tasks, we address the problem of Program Synthesis. Program Synthesis, the problem of automatically constructing programs that are consistent with a given specification, has long been a subject of research in Computer Science (Biermann, 1978; Summers, 1977). This interest has been reinvigorated in recent years on\nthe back of the development of methods for learning programs in various domains, ranging from low-level bit manipulation code (Solar-Lezama et al., 2005) to data structure manipulations (Singh & Solar-Lezama, 2011) and regular expression based string transformations (Gulwani, 2011).\nMost of the recently proposed methods for program synthesis operate by searching the space of programs in a Domain-Specific Language (DSL) instead of arbitrary Turing-complete languages. This hypothesis space of possible programs is huge (potentially infinite) and searching over it is a challenging problem. Several search techniques including enumerative (Udupa et al., 2013), stochastic (Schkufza et al., 2013), constraint-based (Solar-Lezama, 2008), and version-space algebra based algorithms (Gulwani et al., 2012) have been developed to search over the space of programs in the DSL, which support different kinds of specifications (examples, partial programs, natural language etc.) and domains. These techniques not only require significant engineering and research effort to develop carefully-designed heuristics for efficient search, but also have limited applicability and can only synthesize programs of limited sizes and types.\nIn this paper, we present a novel technique called Neuro-Symbolic Program Synthesis (NSPS) that learns to generate a program incrementally without the need for an explicit search. Once trained, NSPS can automatically construct computer programs that are consistent with any set of input-output examples provided at test time. Our method is based on two novel module neural architectures. The first module, called the cross correlation I/O network, produces a continuous representation of any given set of input-output examples. The second module, the Recursive-Reverse-Recursive Neural Network (R3NN), given the continuous representation of the input-output examples, synthesizes a program by incrementally expanding partial programs. R3NN employs a tree-based neural architecture that sequentially constructs a parse tree by selecting which non-terminal symbol to expand using rules from a context-free grammar (i.e., the DSL).\nWe demonstrate the efficacy of our method by applying it to the rich and complex domain of regularexpression-based syntactic string transformations, using a DSL based on the one used by FlashFill (Gulwani, 2011; Gulwani et al., 2012), a Programming-By-Example (PBE) system in Microsoft Excel 2013. Given a few input-output examples of strings, the task is to synthesize a program built on regular expressions to perform the desired string transformation. An example task that can be expressed in this DSL is shown in Figure 1, which also shows the DSL.\nOur evaluation shows that NSPS is not only able to construct programs for known tasks from new input-output examples, but it is also able to construct completely new programs that it had not observed during training. Specifically, the proposed system is able to synthesize string transformation programs for 63% of tasks that it had not observed at training time, and for 94% of tasks when 100 program samples are taken from the model. Moreover, our system is able to learn 38% of 238 real-world FlashFill benchmarks.\nTo summarize, the key contributions of our work are:\n\u2022 A novel Neuro-Symbolic program synthesis technique to encode neural search over the space of programs defined using a Domain-Specific Language (DSL).\n\u2022 The R3NN model that encodes and expands partial programs in the DSL, where each node has a global representation of the program tree.\n\u2022 A novel cross-correlation based neural architecture for learning continuous representation of sets of input-output examples.\n\u2022 Evaluation of the NSPS approach on the complex domain of regular expression based string transformations.\n2 PROBLEM DEFINITION\nIn this section, we formally define the DSL-based program synthesis problem that we consider in this paper. Given a DSL L, we want to automatically construct a synthesis algorithm A such that given a set of input-output example, {(i1, o1), \u00b7 \u00b7 \u00b7 , (in, on)}, A returns a program P \u2208 L that conforms to the input-output examples, i.e.,\n\u2200j : 1 \u2264 j \u2264 n P (ij) = oj . (1)\nThe syntax and semantics of the DSL for string transformations is shown in Figure 1(b) and Figure 8 respectively. The DSL corresponds to a large subset of FlashFill DSL (except conditionals), and allows for a richer class of substring operations than FlashFill. A DSL program takes as input a string v and returns an output string o. The top-level string expression e is a concatenation of a finite list of substring expressions f1, \u00b7 \u00b7 \u00b7 , fn. A substring expression f can either be a constant string s or a substring expression, which is defined using two position logics pl (left) and pr (right). A position logic corresponds to a symbolic expression that evaluates to an index in the string. A position logic p can either be a constant position k or a token match expression (r, k,Dir), which denotes the Start or End of the kth match of token r in input string v. A regex token can either be a constant string s or one of 8 regular expression tokens: p (ProperCase), C (CAPS), l (lowercase), d (Digits), \u03b1 (Alphabets), \u03b1n (Alphanumeric), \u2227 (StartOfString), and $ (EndOfString). The semantics of the DSL programs is described in the appendix.\nA DSL program for the name transformation task shown in Figure 1(a) that is consistent with the examples is: Concat(f1,ConstStr(\u201c, \u201d), f2,ConstStr(\u201c.\u201d)), where f1 \u2261 SubStr(v, (\u201c \u201d,\u22121,End),ConstPos(\u22121)) and f2 \u2261 SubStr(v,ConstPos(0),ConstPos(1)). The program concatenates the following 4 strings: i) substring between the end of last whitespace and end of string, ii) constant string \u201c, \u201d, iii) first character of input string, and iv) constant string \u201c.\u201d.\n3 OVERVIEW OF OUR APPROACH\nWe now present an overview of our approach. Given a DSL L, we learn a generative model of programs in the DSL L that is conditioned on input-output examples to efficiently search for consistent programs. The workflow of our system is shown in Figure 2, which is trained end-to-end using a large training set of programs in the DSL together with their corresponding input-output examples. To generate a large training set, we uniformly sample programs from the DSL and then use a rule-based strategy to compute well-formed input strings. Given a program P (sampled from the DSL), the rule-based strategy generates input strings for the program P ensuring that the preconditions of P are met (i.e. P doesn\u2019t throw an exception on the input strings). It collects the pre-conditions of all Substring expressions present in the sampled program P and then generates inputs conforming to them. For example, let\u2019s assume the sampled program is SubStr(v,(CAPS, 2, Start), (\u201c \u201d, 3, Start)), which extracts the substring between the start of 2nd capital letter and start of 3rd whitespace. The rule-based strategy would ensure that all the generated input strings consist of at least 2 capital letters and 3 whitespaces in addition to other randomly generated characters. The corresponding output strings are obtained by running the programs on the input strings.\nA DSL can be considered as a context-free grammar with a start symbol S and a set of non-terminals with corresponding expansion rules. The (partial) grammar derivations or trees correspond to (partial) programs. A na\u0131\u0308ve way to perform a search over the programs in a DSL is to start from the start symbol S and then randomly choose non-terminals to expand with randomly chosen expansion rules until reaching a derivation with only terminals. We, instead, learn a generative model over partial derivations in the DSL that assigns probabilities to different non-terminals in a partial derivation and corresponding expansions to guide the search for complete derivations.\nOur generative model uses a Recursive-Reverse-Recursive Neural Network (R3NN) to encode partial trees (derivations) in L, where each node in the partial tree encodes global information about every other node in the tree. The model assigns a vector representation for every symbol and every expansion rule in the grammar. Given a partial tree, the model first assigns a vector representation to each leaf node, and then performs a recursive pass going up in the tree to assign a global tree representation to the root. It then performs a reverse-recursive pass starting from the root to assign a global tree representation to each node in the tree.\nThe generative process is conditioned on a set of input-output examples to learn a program that is consistent with this set of examples. We experiment with multiple input-output encoders including an LSTM encoder that concatenates the hidden vectors of two deep bidirectional LSTM networks for input and output strings in the examples, and a Cross Correlation encoder that computes the cross correlation between the LSTM tensor representations of input and output strings in the examples. This vector is then used as an additional input in the R3NN model to condition the generative model.\n4 TREE-STRUCTURED GENERATION MODEL\nWe define a program t-steps into construction as a partial program tree (PPT) (see Figure 3 for a visual depiction). A PPT has two types of nodes: leaf (symbol) nodes and inner non-leaf (rule) nodes. A leaf node represents a symbol, whether non-terminal or terminal. An inner non-leaf node represents a particular production rule of the DSL, where the number of children of the non-leaf node is equivalent to the arity of the RHS of the rule it represents. A PPT is called a program tree (PT) whenever all the leaves of the tree are terminal symbols. Such a tree represents a completed program under the DSL and can be executed. We define an expansion as the valid application of a specific production rule (e \u2192 e op2 e) to a specific non-terminal leaf node within a PPT (leaf with symbol e). We refer to the specific production rule that an expansion is derived from as the expansion type. It can be seen that if there exist two leaf nodes (l1 and l2) with the same symbol then for every expansion specific to l1 there exists an expansion specific to l2 with the same type.\n4.1 RECURSIVE-REVERSE-RECURSIVE NEURAL NETWORK\nIn order to define a generation model over PPTs, we need an efficient way of assigning probabilities to every valid expansion in the current PPT. A valid expansion has two components: first the production rule used, and second the position of the expanded leaf node relative to every other node in the tree. To account for the first component, a separate distributed representation for each production rule is maintained. The second component is handled using an architecture where the forward propagation resembles belief propagation on trees, allowing a notion of global tree state at every node within the tree. A given expansion probability is then calculated as being proportional to the inner product between the production rule representation and the global-tree representation of the leaf-level non-terminal node. We now describe the design of this architecture in more detail.\nThe R3NN has the following parameters for the grammar described by a DSL (see Figure 3):\n1. For every symbol s \u2208 S, an M\u2212dimensional representation \u03c6(s) \u2208 RM . 2. For every production rule r \u2208 R, an M\u2212dimensional representation \u03c9(r) \u2208 RM .\n3. For every production rule r \u2208 R, a deep neural network fr which takes as input a vector x \u2208 RQ\u00b7M , with Q being the number of symbols on the RHS of the production rule r, and outputs a vector y \u2208 RM . Therefore, the production-rule network fr takes as input a concatenation of the distributed representations of each of its RHS symbols and produces a distributed representation for the LHS symbol.\n4. For every production rule r \u2208 R, an additional deep neural network gr which takes as input a vector x\u2032 \u2208 RM and outputs a vector y\u2032 \u2208 RQ\u00b7M . We can think of gr as a reverse production-rule network that takes as input a vector representation of the LHS and produces a concatenation of the distributed representations of each of the rule\u2019s RHS symbols.\nLet E be the set of all valid expansions in a PPT T , let L be the current leaf nodes of T and N be the current non-leaf (rule) nodes of T . Let S(l) be the symbol of leaf l \u2208 L and R(n) represent the production rule of non-leaf node n \u2208 N .\n4.1.1 GLOBAL TREE INFORMATION AT THE LEAVES\nTo compute the probability distribution over the set E, the R3NN first computes a distributed representation for each leaf node that contains global tree information. To accomplish this, for every leaf node l \u2208 L in the tree we retrieve its distributed representation \u03c6(S(l)) . We now do a standard recursive bottom-to-top, RHS\u2192LHS pass on the network, by going up the tree and applying fR(n) for every non-leaf node n \u2208 N on its RHS node representations (see Figure 3(a)). These networks fR(n) produce a node representation which is input into the parent\u2019s rule network and so on until we reach the root node.\nOnce at the root node, we effectively have a fixed-dimensionality global tree representation \u03c6(root) for the start symbol. The problem is that this representation has lost any notion of tree position. To solve this problem R3NN now does what is effectively a reverse-recursive pass which starts at the root node with \u03c6(root) as input and moves towards the leaf nodes (see Figure 3(b)).\nMore concretely, we start with the root node representation \u03c6(root) and use that as input into the rule network gR(root) where R(root) is the production rule that is applied to the start symbol in T . This produces a representation \u03c6\u2032(c) for each RHS node c of R(root). If c is a non-leaf node, we iteratively apply this procedure to c, i.e., process \u03c6\u2032(c) using gR(c) to get representations \u03c6\u2032(cc) for every RHS node cc of R(c), etc. If c is a leaf node, we now have a leaf representation \u03c6\u2032(c) which has an information path to \u03c6(root) and thus to every other leaf node in the tree. Once the reverse-recursive process is complete, we now have a distributed representation \u03c6\u2032(l) for every leaf node l which contains global tree information. While \u03c6(l1) and \u03c6(l2) could be equal for leaf nodes which have the same symbol type, \u03c6\u2032(l1) and \u03c6\u2032(l2) will not be equal even if they have the same symbol type because they are at different positions in the tree.\n4.1.2 EXPANSION PROBABILITIES\nGiven the global leaf representations \u03c6\u2032(l), we can now straightforwardly acquire scores for each e \u2208 E. For expansion e, let e.r be the expansion type (production rule r \u2208 R that e applies) and let e.l be the leaf node l that e.r is applied to. ze = \u03c6\u2032(e.l) \u00b7 \u03c9(e.r) The score of an expansion is calculated using ze = \u03c6\u2032(e.l) \u00b7 \u03c9(e.r). The probability of expansion e is simply the exponentiated normalized sum over all scores: \u03c0(e) = e\nze\u2211 e\u2032\u2208E e z e\u2032 .\nAn additional improvement that was found to help was to add a bidirectional LSTM (BLSTM) to process the global leaf representations right before calculating the scores. To do this, we first order the global leaf representations sequentially from left-most leaf node to right-mode leaf node. We then treat each leaf node as a time step for a BLSTM to process. This provides a sort of skip connection between leaf nodes, which potentially reduces the path length that information needs to travel between leaf nodes in the tree. The BLSTM hidden states are then used in the score calculation rather than the leaves themselves.\nThe R3NN can be seen as an extension and combination of several previous tree-based models, which were mainly developed in the context of natural language processing (Le & Zuidema, 2014; Paulus et al., 2014; Irsoy & Cardie, 2013).\n5 CONDITIONING WITH INPUT/OUTPUT EXAMPLES\nNow that we have defined a generation process over tree-structured programs, we need a way of conditioning this generation process on a set of input/output examples. The set of input/output examples provide a nearly complete specification for the desired output program, and so a good encoding of the examples is crucial to the success of our program generator. For the most part, this example encoding needs to be domain-specific, since different DSLs have different inputs (some may operate over integers, some over strings, etc.). Therefore, in our case, we use an encoding adapted to the input-output strings that our DSL operates over. We also investigate different ways of conditioning program search on the learnt example input-output encodings.\n5.1 ENCODING INPUT/OUTPUT EXAMPLES\nThere are two types of information that string manipulation programs need to extract from inputoutput examples: 1) constant strings, such as \u201c@domain.com\u201d or \u201c.\u201d, which appear in all output examples; 2) substring indices in input where the index might be further defined by a regular expression. These indices determine which parts of the input are also present in the output. To simplify the DSL, we assume that there is a fixed finite universe of possible constant strings that could appear in programs. Therefore we focus on extracting the second type of information, the substring indices.\nIn earlier hand-engineered systems such as FlashFill, this information was extracted from the inputoutput strings by running the Longest Common Substring algorithm, a dynamic programming algorithm that efficiently finds matching substrings in string pairs. To extract substrings, FlashFill runs LCS on every input-output string pair in the I/O set to get a set of substring candidates. It then takes the entire set of substring candidates and simply tries every possible regex and constant index that can be used at substring boundaries, exhaustively searching for the one which is the most \u201cgeneral\u201d, where generality is specified by hand-engineered heuristics.\nIn contrast to these previous methods, instead of hand-designing a complicated algorithm to extract regex-based substrings, we develop neural network based architectures that are capable of learning to extract and produce continuous representations of the likely regular expressions given I/O examples.\n5.1.1 BASELINE LSTM ENCODER\nOur first I/O encoding network involves running two separate deep bidirectional LSTM networks for processing the input and the output string in each example pair. For each pair, it then concatenates the topmost hidden representation at every time step to produce a 4HT -dimensional feature vector per I/O pair, where T is the maximum string length for any input or output string, and H is the topmost LSTM hidden dimension.\nWe then concatenate the encoding vectors across all I/O pairs to get a vector representation of the entire I/O set. This encoding is conceptually straightforward and has very little prior knowledge about what operations are being performed over the strings, i.e., substring, constant, etc., which might make it difficult to discover substring indices, especially the ones based on regular expressions.\n5.1.2 CROSS CORRELATION ENCODER\nTo help the model discover input substrings that are copied to the output, we designed an novel I/O example encoder to compute the cross correlation between each input and output example representation. We used the two output tensors of the LSTM encoder (discussed above) as inputs to this encoder. For each example pair, we first slide the output feature block over the input feature block and compute the dot product between the respective position representation. Then, we sum over all overlapping time steps. Features of all pairs are then concatenated to form a 2\u2217 (T \u22121)-dimensional vector encoding for all example pairs. There are 2 \u2217 (T \u2212 1) possible alignments in total between input and output feature blocks. An illustration of the cross-correlation encoder is shown in Figure 9. We also designed the following variants of this encoder.\nDiffused Cross Correlation Encoder: This encoder is identical to the Cross Correlation encoder except that instead of summing over overlapping time steps after the element-wise dot product, we simply concatenate the vectors corresponding to all time steps, resulting in a final representation that contains 2 \u2217 (T \u2212 1) \u2217 T features for each example pair. LSTM-Sum Cross Correlation Encoder: In this variant of the Cross Correlation encoder, instead of doing an element-wise dot product, we run a bidirectional LSTM over the concatenated feature blocks of each alignment. We represent each alignment by the LSTM hidden representation of the final time step leading to a total of 2 \u2217H \u2217 2 \u2217 (T \u2212 1) features for each example pair. Augmented Diffused Cross Correlation Encoder: For this encoder, the output of each character position of the Diffused Cross Correlation encoder is combined with the character embedding at this position, then a basic LSTM encoder is run over the combined features to extract a 4\u2217H-dimensional vector for both the input and the output streams. The LSTM encoder output is then concatenated with the output of the Diffused Cross Correlation encoder forming a (4\u2217H+T \u2217(T\u22121))-dimensional feature vector for each example pair.\n5.2 CONDITIONING PROGRAM SEARCH ON EXAMPLE ENCODINGS\nOnce the I/O example encodings have been computed, we can use them to perform conditional generation of the program tree using the R3NN model. There are a number of ways in which the PPT generation model can be conditioned using the I/O example encodings depending on where the I/O example information is inserted in the R3NN model. We investigated three locations to inject example encodings:\n1) Pre-conditioning: where example encodings are concatenated to the encoding of each tree leaf, and then passed to a conditioning network before the bottom-up recursive pass over the program tree. The conditioning network can be either a multi-layer feedforward network, or a bidirectional LSTM network running over tree leaves. Running an LSTM over tree leaves allows the model to learn more about the relative position of each leaf node in the tree.\n2) Post-conditioning: After the reverse-recursive pass, example encodings are concatenated to the updated representation of each tree leaf and then fed to a conditioning network before computing the expansion scores.\n3) Root-conditioning: After the recursive pass over the tree, the root encoding is concatenated to the example encodings and passed to a conditioning network. The updated root representation is then used to drive the reverse-recursive pass.\nEmpirically, pre-conditioning worked better than either root- or post- conditioning. In addition, conditioning at all 3 places simultaneously did not cause a significant improvement over just pre-conditioning. Therefore, for the experimental section, we report models which only use preconditioning.\n6 EXPERIMENTS\nIn order to evaluate and compare variants of the previously described models, we generate a dataset randomly from the DSL. To do so, we first enumerate all possible programs under the DSL up to a specific number of instructions, which are then partitioned into training, validation and test sets. In order to have a tractable number of programs, we limited the maximum number of instructions for programs to be 13. Length 13 programs are important for this specific DSL because all larger programs can be written as compositions of sub-programs of length at most 13. The semantics of length 13 programs therefore constitute the \u201catoms\u201d of this particular DSL.\nIn testing our model, there are two different categories of generalization. The first is input/output generalization, where we are given a new set of input/output examples as well as a program with a specific tree that we have seen during training. This represents the model\u2019s capacity to be applied on new data. The second category is program generalization, where we are given both a previously unseen program tree in addition to unseen input/output examples. Therefore the model needs to have a sufficient enough understanding of the semantics of the DSL that it can construct novel combinations of operations. For all reported results, training sets correspond to the first type of generalization since we have seen the program tree but not the input/output pairs. Test sets represent the second type of generalization, as they are trees which have not been seen before on input/output pairs that have also not been seen before.\nIn this section, we compare several different variants of our model. We first evaluate the effect of each of the previously described input/output encoders. We then evaluate the R3NN model against a simple recurrent model called io2seq, which is basically an LSTM that takes as input the input/output conditioning vector and outputs a sequence of DSL symbols that represents a linearized program tree. Finally, we report the results of the best model on the length 13 training and testing sets, as well as on a set of 238 benchmark functions.\n6.1 SETUP AND HYPERPARAMETERS SETTINGS\nFor training the R3NN, two hyperparameters that were crucial for stabilizing training were the use of hyperbolic tangent activation functions in both R3NN (other activations such as ReLU more consistently diverged during our initial experiments) and cross-correlation I/O encoders and the use of minibatches of length 8. Additionally, for all results, the program tree generation is conditioned on a set of 10 input/output string pairs. We used ADAM (Kingma & Ba, 2014) to optimize the networks with a learning rate of 0.001. Network weights used the default torch initializations.\nDue to the difficulty of batching tree-based neural networks since each sample in a batch has a potentially different tree structure, we needed to do batching sequentially. Therefore for each minibatch of size N , we accumulated the gradients for each sample. After all N sample gradients were accumulated, we updated the parameters and reset the accumulated gradients. Due to this sequential processing, in order to train models in a reasonable time, we limited our batch sizes to between 8-12. Despite the computational inefficiency, batching was critical to successfully train an R3NN, as online learning often caused the network to diverge.\nFor each latent function and set of input/output examples that we test on, we report whether we had a success after sampling 100 functions from the model and testing all 100 to see if one of these functions is equivalent to the latent function. Here we consider two functions to be equivalent with respect to a specific input/output example set if the functions output the same strings when run on the inputs. Under this definition, two functions can have a different set of operations but still be equivalent with respect to a specific input-output set.\nWe restricted the maximum size of training programs to be 13 because of two computational considerations. As described earlier, one difficulty was in batching tree-based neural networks of different structure and the computational cost of batching increases with the increase in size of the program trees. The second issue is that valid I/O strings for programs often grow with the program length, in the sense that for programs of length 40 a minimal valid I/O string will typically be much longer than a minimal valid I/O string for length 20 programs. For example, for a program such as (Concat (ConstStr \u201clongstring\u201d) (Concat (ConstStr \u201clongstring\u201d) (Concat (ConstStr \u201clongstring\u201d) ...))), the valid output string would be \u201clongstringlongstringlongstring...\u201d which could be many\nhundreds of characters long. Because of limited GPU memory, the I/O encoder models can quickly run out of memory.\n6.2 EXAMPLE ENCODING\nIn this section, we evaluate the effect of several different input/output example encoders. To control for the effect of the tree model, all results here used an R3NN with fixed hyperparameters to generate the program tree. Table 1 shows the performance of several of these input/output example encoders. We can see that the summed cross-correlation encoder did not perform well, which can be due to the fact that the sum destroys positional information that might be useful for determining specific substring indices. The LSTM-sum and the augmented diffused cross-correlation models did the best. Surprisingly, the LSTM encoder was capable of finding nearly 88% of all programs without having any prior knowledge explicitly built into the architecture. We use 100 samples for evaluating the Train and Test sets. The training performance is sometimes slightly lower because there are close to 5 million training programs but we only look at less than 2 million of these programs during training. We sample a subset of only 1000 training programs from the 5 million program set to report the training results in the tables. The test sets also consist of 1000 programs.\n6.3 IO2SEQ\nIn this section, we motivate the use of the R3NN by testing whether a simpler model can also be used to generate programs. The io2seq model is an LSTM whose initial hidden and cell states are a function of the input/output encoding vector. The io2seq model then generates a linearized tree of a program symbol-by-symbol. An example of what a linearized program tree looks like is (S(e(f (ConstStr\u201c@\u201d)ConstStr)f )e)S , which represents the program tree that returns the constant string \u201c@\u201d. Predicting a linearized tree using an LSTM was also done in the context of parsing (Vinyals et al., 2015). For the io2seq model, we used the LSTM-sum cross-correlation I/O conditioning model.\nThe results in Table 2 show that the performance of the io2seq model at 100 samples per latent test function is far worse than the R3NN, at around 42% versus 91%, respectively. The reasons for that could be that the io2seq model needs to perform far more decisions than the R3NN, since the io2seq model has to predict the parentheses symbols that determine at which level of the tree a particular symbol is at. For example, the io2seq model requires on the order of 100 decisions for length 13 programs, while the R3NN requires no more than 13.\n6.4 EFFECT OF SAMPLING MULTIPLE PROGRAMS\nFor the best R3NN model that we trained, we also evaluated the effect that a different number of samples per latent function had on performance. The results are shown in Table 3. The increase of the model\u2019s performance as the sample size increases hints that the model has a notion of what type of program satisfies a given I/O pair, but it might not be that certain about the details such as which regex to use, etc. By 300 samples, the model is nearing perfect accuracy on the test sets.\n6.5 EFFECT OF NUMBER OF INPUT-OUTPUT EXAMPLES\nWe evaluate the effect of varying the number of input-output examples used to train the Input-output encoders. The 1-best accuracy for train and test data for models trained for 74 epochs is shown in Figure 4. As expected, the accuracy increases with increase in number of input-output examples, since more examples add more information to the encoder and constrain the space of consistent programs in the DSL.\n6.6 FLASHFILL BENCHMARKS\nWe also evaluate our learnt models on 238 real-world FlashFill benchmarks obtained from the Microsoft Excel team and online help-forums. These benchmarks involve string manipulation tasks described using input-output examples. We evaluate two models \u2013 one with a cross correlation encoder trained on 5 input-output examples and another trained on 10 input-output examples. Both the models were trained on randomly sampled programs from the DSL upto size 13 with randomly generated input-output examples.\nThe distribution of the size of smallest DSL programs needed to solve the benchmark tasks is shown in Figure 5(a), which varies from 4 to 63. The figure also shows the number of benchmarks for which our model was able to learn the program using 5 input-output examples using samples of top-2000 learnt programs. In total, the model is able to learn programs for 91 tasks (38.2%). Since the model was trained for programs upto size 13, it is not surprising that it is not able to solve tasks that need larger program size. There are 110 FlashFill benchmarks that require programs upto size 13, out of which the model is able to solve 82.7% of them.\nThe effect of sampling multiple learnt programs instead of only top program is shown in Figure 5(b). With only 10 samples, the model can already learn about 13% of the benchmarks. We observe a steady increase in performance upto about 2000 samples, after which we do not observe any significant improvement. Since there are more than 2 million programs in the DSL of length 11 itself, the enumerative techniques with uniform search do not scale well (Alur et al., 2015).\nWe also evaluate a model that is learnt with 10 input-output examples per benchmark. This model can only learn programs for about 29% of the FlashFill benchmarks. Since the FlashFill benchmarks contained only 5 input-output examples for each task, to run the model that took 10 examples as input, we duplicated the I/O examples. Our models are trained on the synthetic training dataset\nthat is generated uniformly from the DSL. Because of the discrepancy between the training data distribution (uniform) and auxiliary task data distribution, the model with 10 input/output examples might not perform the best on the FlashFill benchmark distribution, even though it performs better on the synthetic data distribution (on which it is trained) as shown in Figure 4.\nOur model is able to solve majority of FlashFill benchmarks that require learning programs with upto 3 Concat operations. We now describe a few of these benchmarks, also shown in Figure 6. An Excel user wanted to clean a set of medical billing records by adding a missing \u201c]\u201d to medical codes as shown in Figure 6(a). Our system learns the following program given these 5 input-output examples: Concat(SubStr(v,ConstPos(0),(d,-1,End)), ConstStr(\u201c]\u201d)). The program concatenates the substring between the start of the input string and the position of the last digit regular expression with the constant string \u201c]\u201d. Another task that required user to transform some numbers into a hex format is shown in Figure 6(b). Our system learns the following program: Concat(ConstStr(\u201c0x\u201d),SubStr(v,ConstPos(0),ConstPos(2))). For some benchmarks with long input strings, it is still able to learn regular expressions to extract the desired substring, e.g. it learns a program to extract \u201cNancyF\u201d from the string \u201c123456789,freehafer ,drew ,nancy,19700101,11/1/2007,NancyF@north.com,1230102,123 1st Avenue,Seattle,wa,09999\u201d.\nOur system is currently not able to learn programs for benchmarks that require 4 or more Concat operations. Two such benchmarks are shown in Figure 7. The task of combining names in Figure 7(a) requires 6 Concat arguments, whereas the phone number transformation task in Figure 7(b) requires 5 Concat arguments. This is mainly because of the scalability issues in training with programs of larger size. There are also a few interesting benchmarks where the R3NN models gets very close to learning the desired program. For example, for the task \u201cBill Gates\u201d \u2192 \u201cMr. Bill Gates\u201d, it learns a program that generates \u201cMr.Bill Gates\u201d (without the whitespace), and for the task \u201c617-444-5454\u201d \u2192 \u201c(617) 444-5454\u201d, it learns a program that generates the string \u201c(617 444-5454\u201d.\n7 RELATED WORK\nWe have seen a renewed interest in recent years in the area of Program Induction and Synthesis.\nIn the machine learning community, a number of promising neural architectures have been proposed to perform program induction. These methods have employed architectures inspired from computation modules (Turing Machines, RAM) (Graves et al., 2014; Kurach et al., 2015; Reed & de Freitas, 2015; Neelakantan et al., 2015) or common data structures such as stacks used in many algorithms (Joulin & Mikolov, 2015). These approaches represent the atomic operations of the network in a differentiable form, which allows for efficient end-to-end training of a neural controller. However, unlike our approach that learns comprehensible complete programs, many of these approaches learn only the program behavior (i.e., they produce desired outputs on new input data). Some recently proposed methods (Kurach et al., 2015; Gaunt et al., 2016; Riedel et al., 2016; Bunel et al., 2016) do learn interpretable programs but these techniques require learning a separate neural network model for each individual task, which is undesirable in many synthesis settings where we would like to learn programs in real-time for a large number of tasks. Liang et al. (2010) restrict the problem space with a probabilistic context-free grammar and introduce a new representation of programs based on combinatory logic, which allows for sharing sub-programs across multiple tasks. They then take a hierarchical Bayesian approach to learn frequently occurring substructures of programs. Our approach, instead, uses neural architectures to condition the search space of programs, and does not require additional step of representing program space using combinatory logic for allowing sharing.\nThe DSL-based program synthesis approach has also seen a renewed interest recently (Alur et al., 2015). It has been used for many applications including synthesizing low-level bitvector implementations (Solar-Lezama et al., 2005), Excel macros for data manipulation (Gulwani, 2011; Gulwani et al., 2012), superoptimization by finding smaller equivalent loop bodies (Schkufza et al., 2013), protocol synthesis from scenarios (Udupa et al., 2013), synthesis of loop-free programs (Gulwani et al., 2011), and automated feedback generation for programming assignments (Singh et al., 2013). The synthesis techniques proposed in the literature generally employ various search techniques including enumeration with pruning, symbolic constraint solving, and stochastic search, while supporting different forms of specifications including input-output examples, partial programs, program invariants, and reference implementation.\nIn this paper, we consider input-output example based specification over the hypothesis space defined by a DSL of string transformations, similar to that of FlashFill (without conditionals) (Gulwani, 2011). The key difference between our approach over previous techniques is that our system is trained completely in an end-to-end fashion, while previous techniques require significant manual effort to design heuristics for efficient search. There is some work on guiding the program search using learnt clues that suggest likely DSL expansions, but the clues are learnt over hand-coded textual features of examples (Menon et al., 2013). Moreover, their DSL consists of composition of about 100 high-level text transformation functions such as count and dedup, whereas our DSL consists of tree structured programs over richer regular expression based substring constructs.\nThere is also a recent line of work on learning probabilistic models of code from a large number of code repositories (big code) (Raychev et al., 2015; Bielik et al., 2016; Hindle et al., 2016), which are then used for applications such as auto-completion of partial programs, inference of variable and method names, program repair, etc. These language models typically capture only the syntactic\nproperties of code, unlike our approach that also tries to capture the semantics to learn the desired program. The work by Maddison & Tarlow (2014) addresses the problem of learning structured generative models of source code but both their model and application domain are different from ours. Piech et al. (2015) use an NPM-RNN model to embed program ASTs, where a subtree of the AST rooted at a node n is represented by a matrix obtained by combining representations of the children of node n and the embedding matrix of the node n itself (which corresponds to its functional behavior). The forward pass in our R3NN architecture from leaf nodes to the root node is, at a high-level, similar, but we use a distributed representation for each grammar symbol that leads to a different root representation. Moreover, R3NN also performs a reverse-recursive pass to ensure all nodes in the tree encode global information about other nodes in the tree. Finally, the R3NN network is then used to incrementally build a tree to synthesize a program.\nThe R3NN model employed in our work is related to several tree and graph structured neural networks present in the NLP literature (Le & Zuidema, 2014; Paulus et al., 2014; Irsoy & Cardie, 2013). The Inside-Outside Recursive Neural Network (Le & Zuidema, 2014) in particular is most similar to the R3NN, where they generate a parse tree incrementally by using global leaf-level representations to determine which expansions in the parse tree to take next.\n8 CONCLUSION\nWe have proposed a novel technique called Neuro-Symbolic Program Synthesis that is able to construct a program incrementally based on given input-output examples. To do so, a new neural architecture called Recursive-Reverse-Recursive Neural Network is used to encode and expand a partial program tree into a full program tree. Its effectiveness at example-based program synthesis is demonstrated, even when the program has not been seen during training.\nThese promising results open up a number of interesting directions for future research. For example, we took a supervised-learning approach here, assuming availability of target programs during training. In some scenarios, we may only have access to an oracle that returns the desired output given an input. In this case, reinforcement learning is a promising framework for program synthesis.\nA DOMAIN-SPECIFIC LANGUAGE FOR STRING TRANSFORMATIONS\nThe semantics of the DSL programs is shown in Figure 8. The semantics of a Concat expression is to concatenate the results of recursively evaluating the constituent substring expressions fi. The semantics of ConstStr(s) is to simply return the constant string s. The semantics of a substring expression is to first evaluate the two position logics pl and pr to p1 and p2 respectively, and then return the substring corresponding to v[p1..p2]. We denote s[i..j] to denote the substring of string s starting at index i (inclusive) and ending at index j (exclusive), and len(s) denotes its length. The semantics of ConstPos(k) expression is to return k if k > 0 or return len + k (if k < 0). The semantics of position logic (r, k, Start) is to return the Start of kth match of r in v from the beginning (if k > 0) or from the end (if k < 0).\n", "cats": {"READ": 1.0, "DONTREAD": 0.0}, "reviews": [{"IS_META_REVIEW": true, "comments": "This paper proposes a model that is able to infer a program from input/output example pairs, focusing on a restricted domain-specific language that captures a fairly wide variety of string transformations, similar to that used by Flash Fill in Excel.  The approach is to model successive \u201cextensions\u201d of a program tree conditioned on some embedding of the input/output pairs.  Extension probabilities are computed as a function of leaf and production rule embeddings \u2014 one of the main contributions is the so-called \u201cRecursive-Reverse-Recursive Neural Net\u201d which computes a globally aware embedding of a leaf by doing something that looks like belief propagation on a tree (but training this operation in an end-to-end differentiable way).\n\nThere are many strong points about this paper.  In contrast with some of the related work in the deep learning community, I can imagine this being used in an actual application in the near future.  The R3NN idea is a good one and the authors motivate it quite well.  Moreover, the authors have explored many variants of this model to understand what works well and what does not.  Finally, the exposition is clear (even if it is a long paper), which made this paper a pleasure to read.  Some weaknesses of this paper: the results are still not super accurate, perhaps because the model has only been trained on small programs but is being asked to infer programs that should be much longer.  And it\u2019s unclear why the authors did not simply train on longer programs\u2026  It also seems that the number of I/O pairs is fixed?  So if I had more I/O pairs, the model might not be able to use those additional pairs (and based on the experiments, more pairs can hurt\u2026).  Overall however, I would certainly like to see this paper accepted at ICLR.\n\nOther miscellaneous comments:\n* Too many e\u2019s in the expansion probability expression \u2014 might be better just to write \u201cSoftmax\u201d.\n* There is a comment about adding a bidirectional LSTM to process the global leaf representations before calculating scores, but no details are given on how this is done (as far as I can see).\n* The authors claim that using hyperbolic tangent activation functions is important \u2014 I\u2019d be interested in some more discussion on this and why something like ReLU would not be good.\n* It\u2019s unclear to me how batching was done in this setting since each program has a different tree topology.  More discussion on this would be appreciated.  Related to this, it would be good to add details on optimization algorithm (SGD?  Adagrad?  Adam?), learning rate schedules and how weights were initialized.  At the moment, the results are not particularly reproducible.\n* In Figure 6 (unsolved benchmarks), it would be great to add the program sizes for these harder examples (i.e., did the approach fail because these benchmarks require long programs?  Or was it some other reason?)\n* There is a missing related work by Piech et al (Learning Program Embeddings\u2026) where the authors trained a recursive neural network (that matched abstract syntax trees for programs submitted to an online course) to predict program output (but did not synthesize programs)."}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "There is a bit of a spread in the reviewer scores and unfortunately it wasn't possible to entice the reviewers to participate in a discussion. The area chair therefore discounts the late review of reviewer3, who seems to have had a misunderstanding that was successfully rebutted by the authors. The other reviewers are supportive of the paper.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"DATE": "16 Jan 2017", "TITLE": "Reactions to author response?", "IS_META_REVIEW": false, "comments": "Dear reviewers, do you have any reactions after the authors responded to your reviews?", "OTHER_KEYS": "ICLR 2017 conference"}, {"IMPACT": 2, "MEANINGFUL_COMPARISON": 2, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "The paper presents a method to synthesize string manipulation programs based on a set of input output pairs. The paper focuses on a restricted class of programs based on a simple context free grammar sufficient to solve string manipulation tasks from the FlashFill benchmark. A probabilistic generative model called Recursive-Reverse-Recursive Neural Network (R3NN) is presented that assigns a probability to each program's parse tree after a bottom-up and a top-down pass. Results are presented on a synthetic dataset and a Microsoft Excel benchmark called FlashFill.\n\nThe problem of program synthesis is important with a lot of recent interest from the deep learning community. The approach taken in the paper based on parse trees and recursive neural networks seems interesting and promising. However, the model seems too complicated and unclear at several places (details below). On the negative side, the experiments are particularly weak, and the paper does not seem ready for publication based on its experimental results. I was positive about the paper until I realized that the method obtains an accuracy of 38% on FlashFill benchmark when presented with only 5 input-output examples but the performance degrades to 29% when 10 input-output examples are used. This was surprising to the authors too, and they came up with some hypothesis to explain this phenomenon. To me, this is a big problem indicating either a bug in the code or a severe shortcoming of the model. Any model useful for program synthesis needs to be applicable to many input-output examples because most complicated programs require many examples to disambiguate the details of the program.\n\nGiven the shortcoming of the experiments, I am not convinced that the paper is ready for publication. Thus, I recommend weak reject. I encourage the authors to address the comments below and resubmit as the general idea seems promising.\n\nMore comments:\n\nI am unclear about the model at several places:\n- How is the probability distribution normalized? Given the nature of bottom-up top-down evaluation of the potentials, should one enumerate over different completions of a program and the compare their exponentiated potentials? If so, does this restrict the applicability of the model to long programs as the enumeration of the completions gets prohibitively slow?\n- What if you only use 1 input-output pair for each program instead of 5? Do the results get better?\n- Section 5.1.2 is not clear to me. Can you elaborate by potentially including some examples? Does your input-output representation pre-supposes a fixed number of input-output examples across tasks (e.g. 5 or 10 for all of the tasks)?\n\nRegarding the experiments,\n- Could you present some baseline results on FlashFill benchmark based on previous work?\n- Is your method only applicable to short programs? (based on the choice of 13 for the number of instructions)\n- Does a program considered correct when it is identical to a test program, or is it considered correct when it succeeds on a set of held-out input-output pairs?\n- When using 100 or more program samples, do you report the accuracy of the best program out of 100 (i.e. recall) or do you first filter the programs based on training input-output pairs and then evaluate a program that is selected?\n\nYour paper is well beyond the recommended limit of 8 pages. please consider making it shorter.", "ORIGINALITY": 2, "IS_ANNOTATED": true, "TITLE": "Review", "IS_META_REVIEW": false, "RECOMMENDATION": 5, "DATE": "02 Jan 2017", "REVIEWER_CONFIDENCE": 4}, {"IMPACT": 4, "SUBSTANCE": 2, "RECOMMENDATION_UNOFFICIAL": 2, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "This paper sets out to tackle the program synthesis problem: given a set of input/output pairs discover the program that generated them. The authors propose a bipartite model, with one component that is a generative model of tree-structured programs and the other component an input/output pair encoder for conditioning. They consider applying many variants of this basic model to a FlashFill DSL. The experiments explore a practical dataset and achieve fine numbers. The range of models considered, carefulness of the exposition, and basic experimental setup make this a valuable paper for an important area of research. I have a few questions, which I think would strengthen the paper, but think it's worth accepting as is.\n\nQuestions/Comments:\n\n- The dataset is a good choice, because it is simple and easy to understand. What is the effect of the \"rule based strategy\" for computing well formed input strings?\n\n- Clarify what \"backtracking search\" is? I assume it is the same as trying to generate the latent function? \n\n- In general describing the accuracy as you increase the sample size could be summarize simply by reporting the log-probability of the latent function. Perhaps it's worth reporting that? Not sure if I missed something.", "ORIGINALITY": 4, "IS_ANNOTATED": true, "TITLE": "Strong ideas for an important problem", "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "20 Dec 2016", "CLARITY": 4, "REVIEWER_CONFIDENCE": 3}, {"IMPACT": 2, "SUBSTANCE": 1, "RECOMMENDATION_UNOFFICIAL": 1, "MEANINGFUL_COMPARISON": 2, "comments": "This paper proposes a model that is able to infer a program from input/output example pairs, focusing on a restricted domain-specific language that captures a fairly wide variety of string transformations, similar to that used by Flash Fill in Excel.  The approach is to model successive \u201cextensions\u201d of a program tree conditioned on some embedding of the input/output pairs.  Extension probabilities are computed as a function of leaf and production rule embeddings \u2014 one of the main contributions is the so-called \u201cRecursive-Reverse-Recursive Neural Net\u201d which computes a globally aware embedding of a leaf by doing something that looks like belief propagation on a tree (but training this operation in an end-to-end differentiable way).\n\nThere are many strong points about this paper.  In contrast with some of the related work in the deep learning community, I can imagine this being used in an actual application in the near future.  The R3NN idea is a good one and the authors motivate it quite well.  Moreover, the authors have explored many variants of this model to understand what works well and what does not.  Finally, the exposition is clear (even if it is a long paper), which made this paper a pleasure to read.  Some weaknesses of this paper: the results are still not super accurate, perhaps because the model has only been trained on small programs but is being asked to infer programs that should be much longer.  And it\u2019s unclear why the authors did not simply train on longer programs\u2026  It also seems that the number of I/O pairs is fixed?  So if I had more I/O pairs, the model might not be able to use those additional pairs (and based on the experiments, more pairs can hurt\u2026).  Overall however, I would certainly like to see this paper accepted at ICLR.\n\nOther miscellaneous comments:\n* Too many e\u2019s in the expansion probability expression \u2014 might be better just to write \u201cSoftmax\u201d.\n* There is a comment about adding a bidirectional LSTM to process the global leaf representations before calculating scores, but no details are given on how this is done (as far as I can see).\n* The authors claim that using hyperbolic tangent activation functions is important \u2014 I\u2019d be interested in some more discussion on this and why something like ReLU would not be good.\n* It\u2019s unclear to me how batching was done in this setting since each program has a different tree topology.  More discussion on this would be appreciated.  Related to this, it would be good to add details on optimization algorithm (SGD?  Adagrad?  Adam?), learning rate schedules and how weights were initialized.  At the moment, the results are not particularly reproducible.\n* In Figure 6 (unsolved benchmarks), it would be great to add the program sizes for these harder examples (i.e., did the approach fail because these benchmarks require long programs?  Or was it some other reason?)\n* There is a missing related work by Piech et al (Learning Program Embeddings\u2026) where the authors trained a recursive neural network (that matched abstract syntax trees for programs submitted to an online course) to predict program output (but did not synthesize programs).\n", "ORIGINALITY": 4, "IS_ANNOTATED": true, "TITLE": "Nice program synthesis approach to a practical Excel flash-fill like application", "IS_META_REVIEW": false, "RECOMMENDATION": 8, "DATE": "16 Dec 2016", "APPROPRIATENESS": 1, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "REVIEWER_CONFIDENCE": 4}, {"DATE": "15 Dec 2016", "TITLE": "Training?", "IS_META_REVIEW": false, "comments": "Few remarks:\n\nThe paper explains what the tree is and how examples are encoded, but its missing important explanation on:\n\n- how the I/O examples are encoded in the tree during training exactly.\n- it is not well explained how the prediction works during testing when the examples are given.\n\nIt looks like the DSL is restricted to know all constant strings that will be used, which seems difficult in realistic scenarios.\n", "OTHER_KEYS": "(anonymous)"}, {"IS_META_REVIEW": true, "comments": "This paper proposes a model that is able to infer a program from input/output example pairs, focusing on a restricted domain-specific language that captures a fairly wide variety of string transformations, similar to that used by Flash Fill in Excel.  The approach is to model successive \u201cextensions\u201d of a program tree conditioned on some embedding of the input/output pairs.  Extension probabilities are computed as a function of leaf and production rule embeddings \u2014 one of the main contributions is the so-called \u201cRecursive-Reverse-Recursive Neural Net\u201d which computes a globally aware embedding of a leaf by doing something that looks like belief propagation on a tree (but training this operation in an end-to-end differentiable way).\n\nThere are many strong points about this paper.  In contrast with some of the related work in the deep learning community, I can imagine this being used in an actual application in the near future.  The R3NN idea is a good one and the authors motivate it quite well.  Moreover, the authors have explored many variants of this model to understand what works well and what does not.  Finally, the exposition is clear (even if it is a long paper), which made this paper a pleasure to read.  Some weaknesses of this paper: the results are still not super accurate, perhaps because the model has only been trained on small programs but is being asked to infer programs that should be much longer.  And it\u2019s unclear why the authors did not simply train on longer programs\u2026  It also seems that the number of I/O pairs is fixed?  So if I had more I/O pairs, the model might not be able to use those additional pairs (and based on the experiments, more pairs can hurt\u2026).  Overall however, I would certainly like to see this paper accepted at ICLR.\n\nOther miscellaneous comments:\n* Too many e\u2019s in the expansion probability expression \u2014 might be better just to write \u201cSoftmax\u201d.\n* There is a comment about adding a bidirectional LSTM to process the global leaf representations before calculating scores, but no details are given on how this is done (as far as I can see).\n* The authors claim that using hyperbolic tangent activation functions is important \u2014 I\u2019d be interested in some more discussion on this and why something like ReLU would not be good.\n* It\u2019s unclear to me how batching was done in this setting since each program has a different tree topology.  More discussion on this would be appreciated.  Related to this, it would be good to add details on optimization algorithm (SGD?  Adagrad?  Adam?), learning rate schedules and how weights were initialized.  At the moment, the results are not particularly reproducible.\n* In Figure 6 (unsolved benchmarks), it would be great to add the program sizes for these harder examples (i.e., did the approach fail because these benchmarks require long programs?  Or was it some other reason?)\n* There is a missing related work by Piech et al (Learning Program Embeddings\u2026) where the authors trained a recursive neural network (that matched abstract syntax trees for programs submitted to an online course) to predict program output (but did not synthesize programs)."}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "There is a bit of a spread in the reviewer scores and unfortunately it wasn't possible to entice the reviewers to participate in a discussion. The area chair therefore discounts the late review of reviewer3, who seems to have had a misunderstanding that was successfully rebutted by the authors. The other reviewers are supportive of the paper.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"DATE": "16 Jan 2017", "TITLE": "Reactions to author response?", "IS_META_REVIEW": false, "comments": "Dear reviewers, do you have any reactions after the authors responded to your reviews?", "OTHER_KEYS": "ICLR 2017 conference"}, {"IMPACT": 2, "MEANINGFUL_COMPARISON": 2, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "The paper presents a method to synthesize string manipulation programs based on a set of input output pairs. The paper focuses on a restricted class of programs based on a simple context free grammar sufficient to solve string manipulation tasks from the FlashFill benchmark. A probabilistic generative model called Recursive-Reverse-Recursive Neural Network (R3NN) is presented that assigns a probability to each program's parse tree after a bottom-up and a top-down pass. Results are presented on a synthetic dataset and a Microsoft Excel benchmark called FlashFill.\n\nThe problem of program synthesis is important with a lot of recent interest from the deep learning community. The approach taken in the paper based on parse trees and recursive neural networks seems interesting and promising. However, the model seems too complicated and unclear at several places (details below). On the negative side, the experiments are particularly weak, and the paper does not seem ready for publication based on its experimental results. I was positive about the paper until I realized that the method obtains an accuracy of 38% on FlashFill benchmark when presented with only 5 input-output examples but the performance degrades to 29% when 10 input-output examples are used. This was surprising to the authors too, and they came up with some hypothesis to explain this phenomenon. To me, this is a big problem indicating either a bug in the code or a severe shortcoming of the model. Any model useful for program synthesis needs to be applicable to many input-output examples because most complicated programs require many examples to disambiguate the details of the program.\n\nGiven the shortcoming of the experiments, I am not convinced that the paper is ready for publication. Thus, I recommend weak reject. I encourage the authors to address the comments below and resubmit as the general idea seems promising.\n\nMore comments:\n\nI am unclear about the model at several places:\n- How is the probability distribution normalized? Given the nature of bottom-up top-down evaluation of the potentials, should one enumerate over different completions of a program and the compare their exponentiated potentials? If so, does this restrict the applicability of the model to long programs as the enumeration of the completions gets prohibitively slow?\n- What if you only use 1 input-output pair for each program instead of 5? Do the results get better?\n- Section 5.1.2 is not clear to me. Can you elaborate by potentially including some examples? Does your input-output representation pre-supposes a fixed number of input-output examples across tasks (e.g. 5 or 10 for all of the tasks)?\n\nRegarding the experiments,\n- Could you present some baseline results on FlashFill benchmark based on previous work?\n- Is your method only applicable to short programs? (based on the choice of 13 for the number of instructions)\n- Does a program considered correct when it is identical to a test program, or is it considered correct when it succeeds on a set of held-out input-output pairs?\n- When using 100 or more program samples, do you report the accuracy of the best program out of 100 (i.e. recall) or do you first filter the programs based on training input-output pairs and then evaluate a program that is selected?\n\nYour paper is well beyond the recommended limit of 8 pages. please consider making it shorter.", "ORIGINALITY": 2, "IS_ANNOTATED": true, "TITLE": "Review", "IS_META_REVIEW": false, "RECOMMENDATION": 5, "DATE": "02 Jan 2017", "REVIEWER_CONFIDENCE": 4}, {"IMPACT": 4, "SUBSTANCE": 2, "RECOMMENDATION_UNOFFICIAL": 2, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "This paper sets out to tackle the program synthesis problem: given a set of input/output pairs discover the program that generated them. The authors propose a bipartite model, with one component that is a generative model of tree-structured programs and the other component an input/output pair encoder for conditioning. They consider applying many variants of this basic model to a FlashFill DSL. The experiments explore a practical dataset and achieve fine numbers. The range of models considered, carefulness of the exposition, and basic experimental setup make this a valuable paper for an important area of research. I have a few questions, which I think would strengthen the paper, but think it's worth accepting as is.\n\nQuestions/Comments:\n\n- The dataset is a good choice, because it is simple and easy to understand. What is the effect of the \"rule based strategy\" for computing well formed input strings?\n\n- Clarify what \"backtracking search\" is? I assume it is the same as trying to generate the latent function? \n\n- In general describing the accuracy as you increase the sample size could be summarize simply by reporting the log-probability of the latent function. Perhaps it's worth reporting that? Not sure if I missed something.", "ORIGINALITY": 4, "IS_ANNOTATED": true, "TITLE": "Strong ideas for an important problem", "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "20 Dec 2016", "CLARITY": 4, "REVIEWER_CONFIDENCE": 3}, {"IMPACT": 2, "SUBSTANCE": 1, "RECOMMENDATION_UNOFFICIAL": 1, "MEANINGFUL_COMPARISON": 2, "comments": "This paper proposes a model that is able to infer a program from input/output example pairs, focusing on a restricted domain-specific language that captures a fairly wide variety of string transformations, similar to that used by Flash Fill in Excel.  The approach is to model successive \u201cextensions\u201d of a program tree conditioned on some embedding of the input/output pairs.  Extension probabilities are computed as a function of leaf and production rule embeddings \u2014 one of the main contributions is the so-called \u201cRecursive-Reverse-Recursive Neural Net\u201d which computes a globally aware embedding of a leaf by doing something that looks like belief propagation on a tree (but training this operation in an end-to-end differentiable way).\n\nThere are many strong points about this paper.  In contrast with some of the related work in the deep learning community, I can imagine this being used in an actual application in the near future.  The R3NN idea is a good one and the authors motivate it quite well.  Moreover, the authors have explored many variants of this model to understand what works well and what does not.  Finally, the exposition is clear (even if it is a long paper), which made this paper a pleasure to read.  Some weaknesses of this paper: the results are still not super accurate, perhaps because the model has only been trained on small programs but is being asked to infer programs that should be much longer.  And it\u2019s unclear why the authors did not simply train on longer programs\u2026  It also seems that the number of I/O pairs is fixed?  So if I had more I/O pairs, the model might not be able to use those additional pairs (and based on the experiments, more pairs can hurt\u2026).  Overall however, I would certainly like to see this paper accepted at ICLR.\n\nOther miscellaneous comments:\n* Too many e\u2019s in the expansion probability expression \u2014 might be better just to write \u201cSoftmax\u201d.\n* There is a comment about adding a bidirectional LSTM to process the global leaf representations before calculating scores, but no details are given on how this is done (as far as I can see).\n* The authors claim that using hyperbolic tangent activation functions is important \u2014 I\u2019d be interested in some more discussion on this and why something like ReLU would not be good.\n* It\u2019s unclear to me how batching was done in this setting since each program has a different tree topology.  More discussion on this would be appreciated.  Related to this, it would be good to add details on optimization algorithm (SGD?  Adagrad?  Adam?), learning rate schedules and how weights were initialized.  At the moment, the results are not particularly reproducible.\n* In Figure 6 (unsolved benchmarks), it would be great to add the program sizes for these harder examples (i.e., did the approach fail because these benchmarks require long programs?  Or was it some other reason?)\n* There is a missing related work by Piech et al (Learning Program Embeddings\u2026) where the authors trained a recursive neural network (that matched abstract syntax trees for programs submitted to an online course) to predict program output (but did not synthesize programs).\n", "ORIGINALITY": 4, "IS_ANNOTATED": true, "TITLE": "Nice program synthesis approach to a practical Excel flash-fill like application", "IS_META_REVIEW": false, "RECOMMENDATION": 8, "DATE": "16 Dec 2016", "APPROPRIATENESS": 1, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "REVIEWER_CONFIDENCE": 4}, {"DATE": "15 Dec 2016", "TITLE": "Training?", "IS_META_REVIEW": false, "comments": "Few remarks:\n\nThe paper explains what the tree is and how examples are encoded, but its missing important explanation on:\n\n- how the I/O examples are encoded in the tree during training exactly.\n- it is not well explained how the prediction works during testing when the examples are given.\n\nIt looks like the DSL is restricted to know all constant strings that will be used, which seems difficult in realistic scenarios.\n", "OTHER_KEYS": "(anonymous)"}]}
{"text": "SPEECH RECOGNITION SYSTEM\n1 INTRODUCTION\nWe present an end-to-end system to speech recognition, going from the speech signal (e.g. MelFrequency Cepstral Coefficients (MFCC), power spectrum, or raw waveform) to the transcription. The acoustic model is trained using letters (graphemes) directly, which take out the need for an intermediate (human or automatic) phonetic transcription. Indeed, the classical pipeline to build state of the art systems for speech recognition consists in first training an HMM/GMM model to force align the units on which the final acoustic model operates (most often context-dependent phone states). This approach takes its roots in HMM/GMM training (Woodland & Young, 1993). The improvements brought by deep neural networks (DNNs) (Mohamed et al., 2012; Hinton et al., 2012) and convolutional neural networks (CNNs) (Sercu et al., 2015; Soltau et al., 2014) for acoustic modeling only extend this training pipeline.\nThe current state of the art on Librispeech (the dataset that we used for our evaluations) uses this approach too (Panayotov et al., 2015; Peddinti et al., 2015b), with an additional step of speaker adaptation (Saon et al., 2013; Peddinti et al., 2015a). Recently, Senior et al. (2014) proposed GMMfree training, but the approach still requires to generate a force alignment. An approach that cut ties with the HMM/GMM pipeline (and with force alignment) was to train with a recurrent neural network (RNN) (Graves et al., 2013) for phoneme transcription. There are now competitive end-to-end approaches of acoustic models toppled with RNNs layers as in (Hannun et al., 2014; Miao et al., 2015; Saon et al., 2015; Amodei et al., 2015), trained with a sequence criterion (Graves et al., 2006). However these models are computationally expensive, and thus take a long time to train.\nCompared to classical approaches that need phonetic annotation (often derived from a phonetic dictionary, rules, and generative training), we propose to train the model end-to-end, using graphemes directly. Compared to sequence criterion based approaches that train directly from speech signal to graphemes (Miao et al., 2015), we propose a simple(r) architecture (23 millions of parameters for our best model, vs. 100 millions of parameters in (Amodei et al., 2015)) based on convolutional networks\nfor the acoustic model, toppled with a graph transformer network (Bottou et al., 1997), trained with a simpler sequence criterion. Our word-error-rate on clean speech is slightly better than (Hannun et al., 2014), and slightly worse than (Amodei et al., 2015), in particular factoring that they train on 12,000 hours while we only train on the 960h available in LibriSpeech\u2019s train set. Finally, some of our models are also trained on the raw waveform, as in (Palaz et al., 2013; 2015; Sainath et al., 2015). The rest of the paper is structured as follows: the next section presents the convolutional networks used for acoustic modeling, along with the automatic segmentation criterion. The following section shows experimental results comparing different features, the criterion, and our current best word error rates on LibriSpeech.\n2 ARCHITECTURE\nOur speech recognition system is a standard convolutional neural network (LeCun & Bengio, 1995) fed with various different features, trained through an alternative to the Connectionist Temporal Classification (CTC) (Graves et al., 2006), and coupled with a simple beam search decoder. In the following sub-sections, we detail each of these components.\n2.1 FEATURES\nWe consider three types of input features for our model: MFCCs, power-spectrum, and raw wave. MFCCs are carefully designed speech-specific features, often found in classical HMM/GMM speech systems (Woodland & Young, 1993) because of their dimensionality compression (13 coefficients are often enough to span speech frequencies). Power-spectrum features are found in most recent deep learning acoustic modeling features (Amodei et al., 2015). Raw wave has been somewhat explored in few recent work (Palaz et al., 2013; 2015). ConvNets have the advantage to be flexible enough to be used with either of these input feature types. Our acoustic models output letter scores (one score per letter, given a dictionary L).\n2.2 CONVNET ACOUSTIC MODEL\nThe acoustic models we considered in this paper are all based on standard 1D convolutional neural networks (ConvNets). ConvNets interleave convolution operations with pointwise non-linearity operations. Often ConvNets also embark pooling layers: these type of layers allow the network to \u201csee\u201d a larger context, without increasing the number of parameters, by locally aggregating the previous convolution operation output. Instead, our networks leverage striding convolutions. Given (xt)t=1...Tx an input sequence with Tx frames of dx dimensional vectors, a convolution with kernel width kw, stride dw and dy frame size output computes the following:\nyit = bi + dx\u2211 j=1 kw\u2211 k=1 wi,j,k x j dw\u00d7(t\u22121)+k \u22001 \u2264 i \u2264 dy, (1)\nwhere b \u2208 Rdy and w \u2208 Rdy\u00d7dx\u00d7kw are the parameters of the convolution (to be learned).\nPointwise non-linear layers are added after convolutional layers. In our experience, we surprisingly found that using hyperbolic tangents, their piecewise linear counterpart HardTanh (as in (Palaz et al., 2015)) or ReLU units lead to similar results.\nThere are some slight variations between the architectures, depending on the input features. MFCC-based networks need less striding, as standard MFCC filters are applied with large strides on the input\nraw sequence. With power spectrum-based and raw wave-based networks, we observed that the overall stride of the network was more important than where the convolution with strides were placed. We found thus preferrable to set the strided convolutions near the first input layers of the network, as it leads to the fastest architectures: with power spectrum features or raw wave, the input sequences are very long and the first convolutions are thus the most expensive ones.\nThe last layer of our convolutional network outputs one score per letter in the letter dictionary (dy = |L|). Our architecture for raw wave is shown in Figure 1 and is inspired by (Palaz et al., 2015). The architectures for both power spectrum and MFCC features do not include the first layer. The full network can be seen as a non-linear convolution, with a kernel width of size 31280 and stride equal to 320; given the sample rate of our data is 16KHz, label scores are produced using a window of 1955 ms, with steps of 20ms.\n2.3 INFERRING SEGMENTATION WITH AUTOSEGCRITERION\nMost large labeled speech databases provide only a text transcription for each audio file. In a classification framework (and given our acoustic model produces letter predictions), one would need the segmentation of each letter in the transcription to train properly the model. Unfortunately, manually labeling the segmentation of each letter would be tedious. Several solutions have been explored in the speech community to alleviate this issue: HMM/GMM models use an iterative EM procedure: (i) during the Estimation step, the best segmentation is inferred, according to the current model, by maximizing the joint probability of the letter (or any sub-word unit) transcription and input sequence. (ii) During the Maximization step the model is optimized by minimizing a frame-level criterion, based on the (now fixed) inferred segmentation. This approach is also often used to boostrap the training of neural network-based acoustic models.\nOther alternatives have been explored in the context of hybrid HMM/NN systems, such as the MMI criterion (Bahl et al., 1986) which maximizes the mutual information between the acoustic sequence and word sequences or the Minimum Bayse Risk (MBR) criterion (Gibson & Hain, 2006).\nMore recently, standalone neural network architectures have been trained using criterions which jointly infer the segmentation of the transcription while increase the overall score of the right transcription (Graves et al., 2006; Palaz et al., 2014). The most popular one is certainly the Connectionist Temporal Classification (CTC) criterion, which is at the core of Baidu\u2019s Deep Speech architecture (Amodei et al., 2015). CTC assumes that the network output probability scores, normalized at the frame level. It considers all possible sequence of letters (or any sub-word units), which can lead to a to a given transcription. CTC also allow a special \u201cblank\u201d state to be optionally inserted between each letters. The rational behind the blank state is two-folds: (i) modeling \u201cgarbage\u201d frames which might occur between each letter and (ii) identifying the separation between two identical consecutive letters in a transcription. Figure 2a shows an example of the sequences accepted by CTC for a given transcription. In practice, this graph is unfolded as shown in Figure 2b, over the available frames output by the acoustic model. We denote Gctc(\u03b8, T ) an unfolded graph over T frames for a given transcription \u03b8, and \u03c0 = \u03c01, . . . , \u03c0T \u2208 Gctc(\u03b8, T ) a path in this graph representing a (valid) sequence of letters for this transcription. At each time step t, each node of the graph is assigned with the corresponding log-probability letter (that we denote ft(\u00b7)) output by the acoustic model. CTC aims at maximizing the \u201coverall\u201d score of paths in Gctc(\u03b8, T ); for that purpose, it minimizes the Forward score:\nCTC(\u03b8, T ) = \u2212 logadd \u03c0\u2208Gctc(\u03b8,T ) T\u2211 t=1 f\u03c0t(x) , (2)\nwhere the \u201clogadd\u201d operation, also often called \u201clog-sum-exp\u201d is defined as logadd(a, b) = exp(log(a) + log(b)). This overall score can be efficiently computed with the Forward algorithm. To put things in perspective, if one would replace the logadd(\u00b7) by a max(\u00b7) in (2) (which can be then efficiently computed by the Viterbi algorithm, the counterpart of the Forward algorithm), one would then maximize the score of the best path, according to the model belief. The logadd(\u00b7) can be seen as a smooth version of the max(\u00b7): paths with similar scores will be attributed the same weight in the overall score (and hence receive the same gradient), and paths with much larger score will have much more overall weight than paths with low scores. In practice, using the logadd(\u00b7) works much better than the max(\u00b7). It is also worth noting that maximizing (2) does not diverge, as the acoustic model is assumed to output normalized scores (log-probabilities) fi(\u00b7).\nIn this paper, we explore an alternative to CTC, with three differences: (i) there are no blank labels, (ii) un-normalized scores on the nodes (and possibly un-normalized transition scores on the edges) (iii) global normalization instead of per-frame normalization:\n\u2022 The advantage of (i) is that it produces a much simpler graph (see Figure 3a and Figure 3b). We found that in practice there was no advantage of having a blank class to model the possible \u201cgarbage\u201d frames between letters. Modeling letter repetitions (which is also an important quality of the blank label in CTC) can be easily replaced by repetition character labels (we used two extra labels for two and three repetitions). For example \u201ccaterpillar\u201d could be written as \u201ccaterpil2ar\u201d, where \u201c2\u201d is a label to represent the repetition of the previous letter. Not having blank labels also simplifies the decoder.\n\u2022 With (ii) one can easily plug an external language model, which would insert transition scores on the edges of the graph. This could be particularly useful in future work, if one wanted to model representations more high-level than letters. In that respect, avoiding normalized transitions is important to alleviate the problem of \u201clabel bias\u201d Bottou (1991); Lafferty et al. (2001). In this work, we limited ourselves to transition scalars, which are learned together with the acoustic model.\n\u2022 The normalization evoked in (iii) is necessary when using un-normalized scores on nodes or edges; it insures incorrect transcriptions will have a low confidence.\nIn the following, we name our criterion \u201cAuto Segmentation Criterion\u201d (ASG). Considering the same notations than for CTC in (2), and an unfolded graph Gasg(\u03b8, T ) over T frames for a given transcription \u03b8 (as in Figure 3b), as well as a fully connected graph Gfull(\u03b8, T ) over T frames (representing all possible sequence of letters, as in Figure 3c), ASG aims at minimizing:\nASG(\u03b8, T ) = \u2212 logadd \u03c0\u2208Gasg(\u03b8,T ) T\u2211 t=1 (f\u03c0t(x) + g\u03c0t\u22121,\u03c0t(x)) + logadd \u03c0\u2208Gfull(\u03b8,T ) T\u2211 t=1 (f\u03c0t(x) + g\u03c0t\u22121,\u03c0t(x)) , (3) where gi,j(\u00b7) is a transition score model to jump from label i to label j. The left-hand part of 3 promotes sequences of letters leading to the right transcription, and the right-hand part demotes all sequences of letters. As for CTC, these two parts can be efficiently computed with the Forward algorithm. Derivatives with respect to fi(\u00b7) and gi,j(\u00b7) can be obtained (maths are a bit tedious) by applying the chain rule through the Forward recursion.\n2.4 BEAM-SEARCH DECODER\nWe wrote our own one-pass decoder, which performs a simple beam-search with beam threholding, histogram pruning and language model smearing Steinbiss et al. (1994). We kept the decoder as\nsimple as possible (under 1000 lines of C code). We did not implement any sort of model adaptation before decoding, nor any word graph rescoring. Our decoder relies on KenLM Heafield et al. (2013) for the language modeling part. It also accepts un-normalized acoustic scores (transitions and emissions from the acoustic model) as input. The decoder attempts to maximize the following:\nL(\u03b8) = logadd \u03c0\u2208Gasg(\u03b8,T ) T\u2211 t=1 (f\u03c0t(x) + g\u03c0t\u22121,\u03c0t(x)) + \u03b1 logPlm(\u03b8) + \u03b2|\u03b8| , (4)\nwhere Plm(\u03b8) is the probability of the language model given a transcription \u03b8, \u03b1 and \u03b2 are two hyper-parameters which control the weight of the language model and the word insertion penalty respectively.\n3 EXPERIMENTS\n3.1 SETUP\nWe implemented everything using Torch71. The ASG criterion as well as the decoder were implemented in C (and then interfaced into Torch).\nWe consider as benchmark LibriSpeech, a large speech database freely available for download (Panayotov et al., 2015). LibriSpeech comes with its own train, validation and test sets. Except when specified, we used all the available data (about 1000h of audio files) for training and validating our models. We use the original 16 KHz sampling rate. The vocabulary L contains 30 graphemes: the standard English alphabet plus the apostrophe, silence, and two special \u201crepetition\u201d graphemes which encode the duplication (once or twice) of the previous letter (see Section 2.3).\nThe architecture hyper-parameters, as well the decoder ones were tuned using the validation set. In the following, we either report letter-error-rates (LERs) or word-error-rates (WERs). WERs have been obtained by using our own decoder (see Section 2.4), with the standard 4-gram language model provided with LibriSpeech2.\n1http://www.torch.ch. 2http://www.openslr.org/11.\nMFCC features are computed with 13 coefficients, a 25 ms sliding window and 10 ms stride. We included first and second order derivatives. Power spectrum features are computed with a 25 ms window, 10 ms stride, and have 257 components. All features are normalized (mean 0, std 1) per input sequence.\n3.2 RESULTS\nTable 1 reports a comparison between CTC and ASG, in terms of LER and speed. Our ASG criterion is implemented in C (CPU only), leveraging SSE instructions when possible. Our batching is done with an OpenMP parallel for. We picked the CTC criterion implementation provided by Baidu3. Both criteria lead to the same LER. For comparing the speed, we report performance for sequence sizes as reported initially by Baidu, but also for longer sequence sizes, which corresponds to our average use case. ASG appears faster on long sequences, even though it is running on CPU only. Baidu\u2019s GPU CTC implementation seems more aimed at larger vocabularies (e.g. 5000 Chinese characters).\nWe also investigated the impact of the training size on the dataset, as well as the effect of a simple data augmentation procedure, where shifts were introduced in the input frames, as well as stretching. For that purpose, we tuned the size of our architectures (given a particular size of the dataset), to avoid over-fitting. Figure 4a shows the augmentation helps for small training set size. However, with enough training data, the effect of data augmentation vanishes, and both type of features appear to perform similarly. Figure 4b reports the WER with respect to the available training data size. We observe that we compare very well against Deep Speech 1 & 2 which were trained with much more data Hannun et al. (2014); Amodei et al. (2015).\nFinally, we report in Table 2 the best results of our system so far, trained on 1000h of speech, for each type of features. The overall stride of architectures is 320 (see Figure 1), which produces a label every 20 ms. We found that one could squeeze out about 1% in performance by refining the precision of the output. This is efficiently achieved by shifting the input sequence, and feeding it to the network several times. Results in Table 2 were obtained by a single extra shift of 10 ms. Both power spectrum and raw features are performing slightly worse than MFCCs. One could expect, however, that with enough data (see Figure 4) the gap would vanish.\n3https://github.com/baidu-research/warp-ctc.\n4 CONCLUSION\nWe have introduced a simple end-to-end automatic speech recognition system, which combines a standard 1D convolutional neural network, a sequence criterion which can infer the segmentation, and a simple beam-search decoder. The decoding results are competitive on the LibriSpeech corpus with MFCC features (7.2% WER), and promising with power spectrum and raw speech (9.4% WER and 10.1% WER respectively). We showed that our AutoSegCriterion can be faster than CTC (Graves et al., 2006), and as accurate (table 1). Our approach breaks free from HMM/GMM pre-training and force-alignment, as well as not being as computationally intensive as RNN-based approaches (Amodei et al., 2015) (on average, one LibriSpeech sentence is processed in less than 60ms by our ConvNet, and the decoder runs at 8.6x on a single thread).\n", "cats": {"READ": 1.0, "DONTREAD": 0.0}, "reviews": [{"IS_META_REVIEW": true, "comments": "This submission proposes a letter-level decoder with a variation of the CTC approach they call ASG, where the blank symbol is dropped and replaced by letter repetition symbols, and where explicit normalization is dropped. Both the description of a letter-level model (though not novel), as well as the CTC-variant are interesting. \n\nThe approach is evaluated on the LibriSpeech task. The authors claim that their approach is competitive. They compare their modelling variant ASG to CTC, but a comparison of the letter-level approach to available word-level results are missing. Compared to the results obtained in Panayotov et al. 2015, the performance obtained here seems only comparable to word-level GMM/HMM models, but worse than word-level hybrid DNN/HMM models, though Panayotov et al. also appled speaker adaptation, which was not done, as far as I can see. I suggest to add a comparison to Panyotov's results (in addition to mentioning Baidu's results on Librispeech, which are not comparable due to much larger amounts of training data), to allow readers to get a quantitative idea. As pointed out by the authors in the text, Baidu's GPU implementation for CTC is more aimed at larger vocabularies, therefore the comparison to GPU in Tables 1a-c do not seem to be helpful for this work, without further discussing the implementations.\n\nYou are using quite a huge analysis window (nearly 2s). Even though other authors also use windows up to 0.5s to 1s (e.g. MRASTA features), some comments on how you arrive at such a large window, and what advantages you observe for it, would be interesting.\n\nThe submission is well written, though more details on the experiences with using non-normalized (transition) scores and beam pruning would be desirable. Table 1 would be better readable if the units of the numbers shown in a/b/c would be shown within the tables, and not only in the caption.\n\nPrior (partial) publications of this work (your NIPS end-to-end workshop paper) should clearly be mentioned/referenced.\n\nWhat do you mean by transition \"scalars\"?\n\nI do not repeat further comments here, which were already given in the pre-review period.\n\nMinor comments:\n - Sec. 2.3, end of 2nd sentence: train properly the model -> train the model properly\n   End of same paragraph: boostrap -> bootstrap (such errors should be avoided by performing an automatic spell check)\n - Sec. 2.3: Bayse -> Bayes\n - definition of logadd is wrong (see comment) - (applies also for your NIPS end-to-end workshop paper).\n - line before Eq. (3): all possible sequence of letters -> all possible sequences of letters (plural)\n - Sec. 2.4, first line: threholding -> thresholding (spell check..)\n - Figure 4: mention the corpus used here - dev?"}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "Without revisions to this paper or a rebuttal from the authors, it is hard to accept this paper. The main contribution of the paper is removing the blank from CTC to create a somewhat different criterion, but this is not particularly novel (see, for example, ", "OTHER_KEYS": "ICLR 2017 pcs"}, {"TITLE": "A promising start, but a lot more context is needed and the current results are not so competitive", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "This paper describes an end-to-end system for speech recognition that uses a linear conditional random field framework.  A convnet estimates node potentials, while transition scores are provided by trained scalar values.  The convnet acoustic model computes scores for letters, not phones, which reduces the need for expert knowledge in training the system.  At test time, scores from a word-level language model, the convnet node potentials, learned letter-to-letter transition scores, and a word insertion penalty are combined to find the best-scoring word hypothesis.  The model may be trained from the raw audio waveform, power spectra, or MFCC features using conditional maximum likelihood estimation.  Experiments on the Librispeech corpus show that the model achieves a 7.2% WER on the test-clean set from Librispeech using MFCC features, a 9.4% WER using power spectral features, and a 10.1% WER using the raw waveform.\n\nPros\n+ It is interesting to see that a convnet trained from scratch using conditional maximum likelihood can perform reasonably well in a speech recognition system for English that uses graphemic (letter-based) acoustic models instead of phonetic models.  This is a promising research direction.\n\nCons\n- The paper is missing a lot of context / prior work that deserves to be cited.  In addition to the papers I already mentioned in various comments, the authors should also be aware of another 2016 Interspeech paper: Zhang et al., \"Towards End-to-End Speech Recognition with Deep Convolutional Neural Networks\", ", "IS_META_REVIEW": false, "RECOMMENDATION": 4, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 5}, {"TITLE": "Review", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "\u200bThere have been numerous works \u200bon learning from raw waveforms and training letter-based CTC networks for speech recognition, however, there are very few works on combining both of them with purely ConvNet as it is done in this paper. It is interesting to see results on a large scale corpus such as Librispeech that is used in this paper, though some baseline results from hybrid NN/HMM systems should be provided. To readers, it is unclear how this system is close to state-of-the-art only from Table 2.\n\nThe key contribution of this paper may be the end-to-end sequence training criterion for their CTC variant (where the blank symbol is dropped), which may be viewed as sequence training of CTC as H. Sak, et al. \"Learning acoustic frame labeling for speech recognition with recurrent neural networks\", 2015. However, instead of generating the denominator lattices using a frame-level trained CTC model first, this paper directly compute the sequence-level loss by considering all the competing hypothesis in the normalizer. Therefore, the model is trained end-to-end. From this perspective, it is closely related to D. Povey's LF-MMI for sequence-training of HMMs. As another reviewer has pointed out, references and discussions on that should be provided. \n\nThis approach should be more expensive than frame-level training of CTCs, however, from Table 1, the authors' implementation is much faster. Did the systems there use the same sampling rate? You said at the end of 2.2 that the step size for your model is 20ms. Is it also the same for Baidu's CTC system. Also, have you tried increasing the step size, e.g. to 30ms or 40ms, as people have found that it may work (equally) better, while significantly cut down the computational cost.", "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "09 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"DATE": "09 Dec 2016", "TITLE": "citations missing", "IS_META_REVIEW": false, "comments": "Dear authors,\n\nHere are some missing relevant citations.\n\nYou should definitely cite the original paper that used CTC with characters.\nGraves et al., \"Towards End-to-End Speech Recognition with Recurrent Neural Networks\", in ICML 2014.\n\nYou should probably also cite and have a related work section with attention-based models such as:\nChan et al., \"Listen, Attend and Spell: A Neural Network for Large Vocabulary Conversational Speech Recognition\", in ICASSP 2016.\nBahdanau et al., \"End-to-End Attention-based Large Vocabulary Speech Recognition\", in ICASSP 2016.\n\nboth of which are highly relevant to end-to-end ASR.\n\nQuestion:\nWhy did you use Librispeech as opposed to WSJ and/or SWBD. Most end-to-end ASR papers publish on WSJ, especially since there is an established benchmark for comparison (i.e., Graves et al., 2014, Bahdanau et al., 2016, Chan et al., 2016). SWBD also has much stronger benchmarks from the general speech community, and even for end-to-end ASR (i.e., see MSR's CTC paper by Zweig et., \"Advances in All-Neural Speech Recognition\", 2016). You should also definitely comment and compare to Zweig's paper, since they used a similar encoding mechanism.\n\nQuestion:\nIs \"Letter Error Rate\" (LER) the common terminology? From Alex Graves papers and others I see \"Character Error Rate\" (CER). What is the difference?\n\nQuestion:\nVery cool that you can combine wav+cnns+ctc->ASR, but still a little bit disappointed that handcrafted features perform better. Do you expect this to change with more data?", "OTHER_KEYS": "(anonymous)"}, {"TITLE": "Review", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "This submission proposes a letter-level decoder with a variation of the CTC approach they call ASG, where the blank symbol is dropped and replaced by letter repetition symbols, and where explicit normalization is dropped. Both the description of a letter-level model (though not novel), as well as the CTC-variant are interesting. \n\nThe approach is evaluated on the LibriSpeech task. The authors claim that their approach is competitive. They compare their modelling variant ASG to CTC, but a comparison of the letter-level approach to available word-level results are missing. Compared to the results obtained in Panayotov et al. 2015, the performance obtained here seems only comparable to word-level GMM/HMM models, but worse than word-level hybrid DNN/HMM models, though Panayotov et al. also appled speaker adaptation, which was not done, as far as I can see. I suggest to add a comparison to Panyotov's results (in addition to mentioning Baidu's results on Librispeech, which are not comparable due to much larger amounts of training data), to allow readers to get a quantitative idea. As pointed out by the authors in the text, Baidu's GPU implementation for CTC is more aimed at larger vocabularies, therefore the comparison to GPU in Tables 1a-c do not seem to be helpful for this work, without further discussing the implementations.\n\nYou are using quite a huge analysis window (nearly 2s). Even though other authors also use windows up to 0.5s to 1s (e.g. MRASTA features), some comments on how you arrive at such a large window, and what advantages you observe for it, would be interesting.\n\nThe submission is well written, though more details on the experiences with using non-normalized (transition) scores and beam pruning would be desirable. Table 1 would be better readable if the units of the numbers shown in a/b/c would be shown within the tables, and not only in the caption.\n\nPrior (partial) publications of this work (your NIPS end-to-end workshop paper) should clearly be mentioned/referenced.\n\nWhat do you mean by transition \"scalars\"?\n\nI do not repeat further comments here, which were already given in the pre-review period.\n\nMinor comments:\n - Sec. 2.3, end of 2nd sentence: train properly the model -> train the model properly\n   End of same paragraph: boostrap -> bootstrap (such errors should be avoided by performing an automatic spell check)\n - Sec. 2.3: Bayse -> Bayes\n - definition of logadd is wrong (see comment) - (applies also for your NIPS end-to-end workshop paper).\n - line before Eq. (3): all possible sequence of letters -> all possible sequences of letters (plural)\n - Sec. 2.4, first line: threholding -> thresholding (spell check..)\n - Figure 4: mention the corpus used here - dev?\n", "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "09 Dec 2016", "REVIEWER_CONFIDENCE": 5}, {"DATE": "09 Dec 2016", "TITLE": "Publication at NIPS/end-to-end workshop", "IS_META_REVIEW": false, "comments": "A slightly more compressed version of this submission will be presented at the NIPS end-to-end workshop on Dec. 10, 2016. The NIPS submission seems to be a clear subset of this submission and should at least be mentioned in this paper.", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3"}, {"DATE": "02 Dec 2016", "TITLE": "Normalization in beam search", "IS_META_REVIEW": false, "comments": "When dropping the normalization of acoustic model scores, the range of scores obtained might vary and would have an effect on beam pruning and on its relation to the normalized LM scores. Did you analyse this?", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3"}, {"DATE": "02 Dec 2016", "TITLE": "Modified CTC/blank replacement", "IS_META_REVIEW": false, "comments": "Sec. 2.3: you use digits to label character repetitions. How do you handle numbers?", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3"}, {"DATE": "02 Dec 2016", "TITLE": "Notation", "IS_META_REVIEW": false, "comments": "It seems that you use inconsistent notation - the variable 't' is used for different time scales: in Eq. (1) t represents strided time frames, whereas in x_t above it enumerates frames directly.", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3"}, {"DATE": "02 Dec 2016", "TITLE": "logadd?", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3"}, {"DATE": "29 Nov 2016", "TITLE": "How is this model different from conventional HMM based model?", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2"}, {"DATE": "29 Nov 2016", "TITLE": "Is the ASG criterion really new?", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1"}, {"IS_META_REVIEW": true, "comments": "This submission proposes a letter-level decoder with a variation of the CTC approach they call ASG, where the blank symbol is dropped and replaced by letter repetition symbols, and where explicit normalization is dropped. Both the description of a letter-level model (though not novel), as well as the CTC-variant are interesting. \n\nThe approach is evaluated on the LibriSpeech task. The authors claim that their approach is competitive. They compare their modelling variant ASG to CTC, but a comparison of the letter-level approach to available word-level results are missing. Compared to the results obtained in Panayotov et al. 2015, the performance obtained here seems only comparable to word-level GMM/HMM models, but worse than word-level hybrid DNN/HMM models, though Panayotov et al. also appled speaker adaptation, which was not done, as far as I can see. I suggest to add a comparison to Panyotov's results (in addition to mentioning Baidu's results on Librispeech, which are not comparable due to much larger amounts of training data), to allow readers to get a quantitative idea. As pointed out by the authors in the text, Baidu's GPU implementation for CTC is more aimed at larger vocabularies, therefore the comparison to GPU in Tables 1a-c do not seem to be helpful for this work, without further discussing the implementations.\n\nYou are using quite a huge analysis window (nearly 2s). Even though other authors also use windows up to 0.5s to 1s (e.g. MRASTA features), some comments on how you arrive at such a large window, and what advantages you observe for it, would be interesting.\n\nThe submission is well written, though more details on the experiences with using non-normalized (transition) scores and beam pruning would be desirable. Table 1 would be better readable if the units of the numbers shown in a/b/c would be shown within the tables, and not only in the caption.\n\nPrior (partial) publications of this work (your NIPS end-to-end workshop paper) should clearly be mentioned/referenced.\n\nWhat do you mean by transition \"scalars\"?\n\nI do not repeat further comments here, which were already given in the pre-review period.\n\nMinor comments:\n - Sec. 2.3, end of 2nd sentence: train properly the model -> train the model properly\n   End of same paragraph: boostrap -> bootstrap (such errors should be avoided by performing an automatic spell check)\n - Sec. 2.3: Bayse -> Bayes\n - definition of logadd is wrong (see comment) - (applies also for your NIPS end-to-end workshop paper).\n - line before Eq. (3): all possible sequence of letters -> all possible sequences of letters (plural)\n - Sec. 2.4, first line: threholding -> thresholding (spell check..)\n - Figure 4: mention the corpus used here - dev?"}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "Without revisions to this paper or a rebuttal from the authors, it is hard to accept this paper. The main contribution of the paper is removing the blank from CTC to create a somewhat different criterion, but this is not particularly novel (see, for example, ", "OTHER_KEYS": "ICLR 2017 pcs"}, {"TITLE": "A promising start, but a lot more context is needed and the current results are not so competitive", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "This paper describes an end-to-end system for speech recognition that uses a linear conditional random field framework.  A convnet estimates node potentials, while transition scores are provided by trained scalar values.  The convnet acoustic model computes scores for letters, not phones, which reduces the need for expert knowledge in training the system.  At test time, scores from a word-level language model, the convnet node potentials, learned letter-to-letter transition scores, and a word insertion penalty are combined to find the best-scoring word hypothesis.  The model may be trained from the raw audio waveform, power spectra, or MFCC features using conditional maximum likelihood estimation.  Experiments on the Librispeech corpus show that the model achieves a 7.2% WER on the test-clean set from Librispeech using MFCC features, a 9.4% WER using power spectral features, and a 10.1% WER using the raw waveform.\n\nPros\n+ It is interesting to see that a convnet trained from scratch using conditional maximum likelihood can perform reasonably well in a speech recognition system for English that uses graphemic (letter-based) acoustic models instead of phonetic models.  This is a promising research direction.\n\nCons\n- The paper is missing a lot of context / prior work that deserves to be cited.  In addition to the papers I already mentioned in various comments, the authors should also be aware of another 2016 Interspeech paper: Zhang et al., \"Towards End-to-End Speech Recognition with Deep Convolutional Neural Networks\", ", "IS_META_REVIEW": false, "RECOMMENDATION": 4, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 5}, {"TITLE": "Review", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "\u200bThere have been numerous works \u200bon learning from raw waveforms and training letter-based CTC networks for speech recognition, however, there are very few works on combining both of them with purely ConvNet as it is done in this paper. It is interesting to see results on a large scale corpus such as Librispeech that is used in this paper, though some baseline results from hybrid NN/HMM systems should be provided. To readers, it is unclear how this system is close to state-of-the-art only from Table 2.\n\nThe key contribution of this paper may be the end-to-end sequence training criterion for their CTC variant (where the blank symbol is dropped), which may be viewed as sequence training of CTC as H. Sak, et al. \"Learning acoustic frame labeling for speech recognition with recurrent neural networks\", 2015. However, instead of generating the denominator lattices using a frame-level trained CTC model first, this paper directly compute the sequence-level loss by considering all the competing hypothesis in the normalizer. Therefore, the model is trained end-to-end. From this perspective, it is closely related to D. Povey's LF-MMI for sequence-training of HMMs. As another reviewer has pointed out, references and discussions on that should be provided. \n\nThis approach should be more expensive than frame-level training of CTCs, however, from Table 1, the authors' implementation is much faster. Did the systems there use the same sampling rate? You said at the end of 2.2 that the step size for your model is 20ms. Is it also the same for Baidu's CTC system. Also, have you tried increasing the step size, e.g. to 30ms or 40ms, as people have found that it may work (equally) better, while significantly cut down the computational cost.", "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "09 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"DATE": "09 Dec 2016", "TITLE": "citations missing", "IS_META_REVIEW": false, "comments": "Dear authors,\n\nHere are some missing relevant citations.\n\nYou should definitely cite the original paper that used CTC with characters.\nGraves et al., \"Towards End-to-End Speech Recognition with Recurrent Neural Networks\", in ICML 2014.\n\nYou should probably also cite and have a related work section with attention-based models such as:\nChan et al., \"Listen, Attend and Spell: A Neural Network for Large Vocabulary Conversational Speech Recognition\", in ICASSP 2016.\nBahdanau et al., \"End-to-End Attention-based Large Vocabulary Speech Recognition\", in ICASSP 2016.\n\nboth of which are highly relevant to end-to-end ASR.\n\nQuestion:\nWhy did you use Librispeech as opposed to WSJ and/or SWBD. Most end-to-end ASR papers publish on WSJ, especially since there is an established benchmark for comparison (i.e., Graves et al., 2014, Bahdanau et al., 2016, Chan et al., 2016). SWBD also has much stronger benchmarks from the general speech community, and even for end-to-end ASR (i.e., see MSR's CTC paper by Zweig et., \"Advances in All-Neural Speech Recognition\", 2016). You should also definitely comment and compare to Zweig's paper, since they used a similar encoding mechanism.\n\nQuestion:\nIs \"Letter Error Rate\" (LER) the common terminology? From Alex Graves papers and others I see \"Character Error Rate\" (CER). What is the difference?\n\nQuestion:\nVery cool that you can combine wav+cnns+ctc->ASR, but still a little bit disappointed that handcrafted features perform better. Do you expect this to change with more data?", "OTHER_KEYS": "(anonymous)"}, {"TITLE": "Review", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "This submission proposes a letter-level decoder with a variation of the CTC approach they call ASG, where the blank symbol is dropped and replaced by letter repetition symbols, and where explicit normalization is dropped. Both the description of a letter-level model (though not novel), as well as the CTC-variant are interesting. \n\nThe approach is evaluated on the LibriSpeech task. The authors claim that their approach is competitive. They compare their modelling variant ASG to CTC, but a comparison of the letter-level approach to available word-level results are missing. Compared to the results obtained in Panayotov et al. 2015, the performance obtained here seems only comparable to word-level GMM/HMM models, but worse than word-level hybrid DNN/HMM models, though Panayotov et al. also appled speaker adaptation, which was not done, as far as I can see. I suggest to add a comparison to Panyotov's results (in addition to mentioning Baidu's results on Librispeech, which are not comparable due to much larger amounts of training data), to allow readers to get a quantitative idea. As pointed out by the authors in the text, Baidu's GPU implementation for CTC is more aimed at larger vocabularies, therefore the comparison to GPU in Tables 1a-c do not seem to be helpful for this work, without further discussing the implementations.\n\nYou are using quite a huge analysis window (nearly 2s). Even though other authors also use windows up to 0.5s to 1s (e.g. MRASTA features), some comments on how you arrive at such a large window, and what advantages you observe for it, would be interesting.\n\nThe submission is well written, though more details on the experiences with using non-normalized (transition) scores and beam pruning would be desirable. Table 1 would be better readable if the units of the numbers shown in a/b/c would be shown within the tables, and not only in the caption.\n\nPrior (partial) publications of this work (your NIPS end-to-end workshop paper) should clearly be mentioned/referenced.\n\nWhat do you mean by transition \"scalars\"?\n\nI do not repeat further comments here, which were already given in the pre-review period.\n\nMinor comments:\n - Sec. 2.3, end of 2nd sentence: train properly the model -> train the model properly\n   End of same paragraph: boostrap -> bootstrap (such errors should be avoided by performing an automatic spell check)\n - Sec. 2.3: Bayse -> Bayes\n - definition of logadd is wrong (see comment) - (applies also for your NIPS end-to-end workshop paper).\n - line before Eq. (3): all possible sequence of letters -> all possible sequences of letters (plural)\n - Sec. 2.4, first line: threholding -> thresholding (spell check..)\n - Figure 4: mention the corpus used here - dev?\n", "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "09 Dec 2016", "REVIEWER_CONFIDENCE": 5}, {"DATE": "09 Dec 2016", "TITLE": "Publication at NIPS/end-to-end workshop", "IS_META_REVIEW": false, "comments": "A slightly more compressed version of this submission will be presented at the NIPS end-to-end workshop on Dec. 10, 2016. The NIPS submission seems to be a clear subset of this submission and should at least be mentioned in this paper.", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3"}, {"DATE": "02 Dec 2016", "TITLE": "Normalization in beam search", "IS_META_REVIEW": false, "comments": "When dropping the normalization of acoustic model scores, the range of scores obtained might vary and would have an effect on beam pruning and on its relation to the normalized LM scores. Did you analyse this?", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3"}, {"DATE": "02 Dec 2016", "TITLE": "Modified CTC/blank replacement", "IS_META_REVIEW": false, "comments": "Sec. 2.3: you use digits to label character repetitions. How do you handle numbers?", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3"}, {"DATE": "02 Dec 2016", "TITLE": "Notation", "IS_META_REVIEW": false, "comments": "It seems that you use inconsistent notation - the variable 't' is used for different time scales: in Eq. (1) t represents strided time frames, whereas in x_t above it enumerates frames directly.", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3"}, {"DATE": "02 Dec 2016", "TITLE": "logadd?", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3"}, {"DATE": "29 Nov 2016", "TITLE": "How is this model different from conventional HMM based model?", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2"}, {"DATE": "29 Nov 2016", "TITLE": "Is the ASG criterion really new?", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1"}]}
{"text": "HOST-BASED INTRUSION DETECTION SYSTEMS\nIn computer security, designing a robust intrusion detection system is one of the most fundamental and important problems. In this paper, we propose a system-call language-modeling approach for designing anomaly-based host intrusion detection systems. To remedy the issue of high false-alarm rates commonly arising in conventional methods, we employ a novel ensemble method that blends multiple thresholding classifiers into a single one, making it possible to accumulate \u2018highly normal\u2019 sequences. The proposed system-call language model has various advantages leveraged by the fact that it can learn the semantic meaning and interactions of each system call that existing methods cannot effectively consider. Through diverse experiments on public benchmark datasets, we demonstrate the validity and effectiveness of the proposed method. Moreover, we show that our model possesses high portability, which is one of the key aspects of realizing successful intrusion detection systems.\n1 INTRODUCTION\nAn intrusion detection system (IDS) refers to a hardware/software platform for monitoring network or system activities to detect malicious signs therefrom. Nowadays, practically all existing computer systems operate in a networked environment, which continuously makes them vulnerable to a variety of malicious activities. Over the years, the number of intrusion events is significantly increasing across the world, and intrusion detection systems have already become one of the most critical components in computer security. With the explosive growth of logging data, the role of machine learning in effective discrimination between malicious and benign system activities has never been more important.\nA survey of existing IDS approaches needs a multidimensional consideration. Depending on the scope of intrusion monitoring, there exist two main types of intrusion detection systems: networkbased (NIDS) and host-based (HIDS). The network-based intrusion detection systems monitor communications between hosts, while the host-based intrusion detection systems monitor the activity on a single system. From a methodological point of view, intrusion detection systems can also be classified into two classes (Jyothsna et al., 2011): signature-based and anomaly-based. The signaturebased approaches match the observed behaviors against templates of known attack patterns, while the anomaly-based techniques compare the observed behaviors against an extensive baseline of normal behaviors constructed from prior knowledge, declaring each of anomalous activities to be an attack. The signature-based methods detect already known and learned attack patterns well but have an innate difficulty in detecting unfamiliar attack patterns. On the other hand, the anomaly-based methods can potentially detect previously unseen attacks but may suffer from making a robust baseline of normal behavior, often yielding high false alarm rates. The ability to detect a \u2018zero-day\u2019 attack (i.e., vulnerability unknown to system developers) in a robust manner is becoming an important requirement of an anomaly-based approach. In terms of this two-dimensional taxonomy, we can classify our proposed method as an anomaly-based host intrusion detection system.\n\u2217To whom correspondence should be addressed.\nIt was Forrest et al. (1996) who first started to use system-call traces as the raw data for hostbased anomaly intrusion detection systems, and system-call traces have been widely used for IDS research and development since their seminal work (Forrest et al., 2008). Recently, Creech & Hu (2014) proposed to use neural networks on top of a sequence of system calls in the context of HIDS. System calls represent low-level interactions between programs and the kernel in the system, and many researchers consider system-call traces as the most accurate source useful for detecting intrusion in an anomaly-based HIDS. From a data acquisition point of view, system-call traces are easy to collect in a large quantity in real-time. Our approach described in this paper also utilizes system-call traces as input data.\nFor nearly two decades, various research has been conducted based on analyzing system-call traces. Most of the existing anomaly-based host intrusion detection methods typically aim to identify meaningful features using the frequency of individual calls and/or windowed patterns of calls from sequences of system calls. However, such methods have limited ability to capture call-level features and phrase-level features simultaneously. As will be detailed shortly, our approach tries to address this limitation by generating a language model of system calls that can jointly learn the semantics of individual system calls and their interactions (that can collectively represent a new meaning) appearing in call sequences.\nIn natural language processing (NLP), a language model represents a probability distribution over sequences of words, and language modeling has been a very important component of many NLP applications, including machine translation (Cho et al., 2014; Bahdanau et al., 2014), speech recognition (Graves et al., 2013), question answering (Hermann et al., 2015), and summarization (Rush et al., 2015). Recently, deep recurrent neural network (RNN)-based language models are showing remarkable performance in various tasks (Zaremba et al., 2014; Jozefowicz et al., 2016). It is expected that such neural language models will be applicable to not only NLP applications but also signal processing, bioinformatics, economic forecasting, and other tasks that require effective temporal modeling.\nMotivated by this performance advantage and versatility of deep RNN-based language modeling, we propose an application of neural language modeling to host-based introduction detection. We consider system-call sequences as a language used for communication between users (or programs) and the system. In this view, system calls and system-call sequences correspond to words and sentences in natural languages, respectively. Based on this system-call language model, we can perform various tasks that comprise our algorithm to detect anomalous system-call sequences: e.g., estimation of the relative likelihood of different words (i.e., system calls) and phrases (i.e., a window of system calls) in different contexts.\nThe idea of using artificial neural networks for IDSs has been popular (Debar et al., 1992; Ryan et al., 1998; Mukkamala et al., 2002; Wang et al., 2010; Creech & Hu, 2014). For more recent deep learning-based techniques, there exists an example that utilized LSTM for improving intrusion detection performance (Staudemeyer & Omlin, 2013; Staudemeyer, 2015). However, the work by Staudemeyer & Omlin (2013); Staudemeyer (2015) was in essence a feature-based supervised classifier (rather than an anomaly detector) requiring heavy annotation efforts to create labels. As such, their work required explicitly labeled attack data and possessed an inherent limitation that it could not detect new types of attacks. In addition, their approach was not an end-to-end framework and needed careful feature engineering to extract salient features for the classification task. Only one binary label was given per sequence to train their model, unlike our proposed method that is trained to predict the next call, effectively capturing contextual information needed for classification.\nOur specific contributions can be summarized as follows: First, to model sequences of system calls, we propose a neural language modeling technique that utilizes long short-term memory (LSTM) (Hochreiter & Schmidhuber, 1997) units for enhanced long-range dependence learning. The present work is one of the first end-to-end frameworks to model system-call sequences as a natural language for effectively detecting anomalous patterns therefrom. Second, to reduce false-alarm rates of anomaly-based intrusion detection, we propose a leaky rectified linear units (ReLU) (Maas et al., 2013) based ensemble method that constructs an integrative classifier using multiple (relatively weak) thresholding classifiers. Each of the component classifiers is trained to detect different types of \u2018highly normal\u2019 sequences (i.e., system call sequences with very high probability of being normal), and our ensemble method blends them to produce a robust classifier that delivers significantly lower false-alarm rates than other commonly used ensemble methods. As shown in Figure 1, these\ntwo aspects of our contributions can seamlessly be combined into a single framework. Note that the ensemble method we propose is not limited to our language-model based front-end but also applicable to other types of front-ends.\nIn the rest of this paper, we will explain more details of our approach and then present our experimental results that demonstrate the effectiveness of our proposed method.\n2 PROPOSED METHOD\nFigure 1 shows the overview of our proposed approach to designing an intrusion detection system. Our method consists of two parts: the front-end is for language modeling of system calls in various settings, and the back-end is for anomaly prediction based on an ensemble of thresholding classifiers derived from the front-end. In this section, we describe details of each component in our pipeline.\n2.1 LANGUAGE MODELING OF SYSTEM CALLS\nFigure 2 illustrates the architecture of our system-call language model. The system call language model estimates the probability distribution of the next call in a sequence given the sequence of previous calls. We assume that the host system generates a finite number of system calls. We index each system call by using an integer starting from 1 and denote the fixed set of all possible system calls in the system as S = {1, \u00b7 \u00b7 \u00b7 ,K}. Let x = x1x2 \u00b7 \u00b7 \u00b7xl(xi \u2208 S) denote a sequence of l system calls.\nAt the input layer, the call at each time step xi is fed into the model in the form of one-hot encoding, in other words, a K dimensional vector with all elements zero except position xi. At the embedding layer, incoming calls are embedded to continuous space by multiplying embedding matrix W , which should be learned. At the hidden layer, the LSTM unit has an internal state, and this state is updated recurrently at each time step. At the output layer, a softmax activation function is used to produce the estimation of normalized probability values of possible calls coming next in the sequence, P (xi|x1:i\u22121). According to the chain rule, we can estimate the sequence probability by the following formula:\nP (x) = l\u220f i=1 P (xi|x1:i\u22121) (1)\nGiven normal training system call sequence data, we can train this LSTM-based system call language model using the back-propagation through time (BPTT) algorithm. The training criterion minimizes the cross-entropy loss, which is equivalent to maximizing the likelihood of the system call sequence. A standard RNN often suffers from the vanishing/exploding gradient problem, and when training with BPTT, gradient values tend to blow up or vanish exponentially. This makes it difficult to learn long-term dependency in RNNs (Bengio et al., 1994). LSTM, a well-designed RNN architecture component, is equipped with an explicit memory cell and tends to be more effective to cope with this problem, resulting in numerous successes in recent RNN applications.\n(a) language model architecture (b) estimation of sequence probability\nembedding layer\nhidden layer\noutput layer\ninput layer \ud835\udfce \u2219 \u2219 \u2219 \ud835\udfce \ud835\udfcf \ud835\udfce \u2219 \u2219 \u2219 \ud835\udfce\n\u22ef\n\ud835\udc43(\ud835\udc651) \ud835\udc43(\ud835\udc652|\ud835\udc651) \ud835\udc43(\ud835\udc653|\ud835\udc651:2) \ud835\udc43(\ud835\udc65\ud835\udc5b|\ud835\udc651:\ud835\udc5b\u22121)\n\ud835\udc651\nfork\n\ud835\udc652\nsetgid\n\ud835\udc65\ud835\udc5b\u22121\nioctl\n\ud835\udc65\ud835\udc5b\nclose\n[GO]\nFigure 2: System-call language model.\nBecause typical processes in the system execute a long chain of system calls, the number of system calls required to fully understand the meaning of a system-call sequence is quite large. In addition, the system calls comprising a process are intertwined with each other in a complicated way. The boundaries between system-call sequences are also vague. In this regard, learning long-term dependence is crucial for devising effective intrusion detection systems.\nMarkov chains and hidden Markov models are widely used probabilistic models that can estimate the probability of the next call given a sequence of previous calls. There has been previous work on using Markov models in intrusion detection systems (Hofmeyr et al., 1998; Hoang et al., 2003; Hu et al., 2009; Yolacan et al., 2014). However, these methods have an inherent limitation in that the probability of the next call is decided by only a finite number of previous calls. Moreover, LSTM can model exponentially more complex functions than Markov models by using continuous space representations. This property alleviates the data sparsity issue that occurs when a large number of previous states are used in Markov models. In short, the advantages of LSTM models compared to Markov models are two folds: the ability to capture long-term dependency and enhanced expressive power.\nGiven a new query system-call sequence, on the assumption that abnormal call patterns deviate from learned normal patterns, yielding significantly lower probabilities than those of normal call patterns, a sequence with an average negative log-likelihood above a threshold is classified as abnormal, while a sequence with an average negative log-likelihood below the threshold is classified as normal. By changing the threshold value, we can draw a receiver operating characteristic (ROC) curve, which is the most widely used measure to evaluate intrusion detection systems.\nCommonly, IDS is evaluated by the ROC curve rather than a single point corresponding to a specific threshold on the curve. Sensitivity to the threshold is shown on the curve. The x-axis of the curve represents false alarm rates, and the y-axis of the curve represents detection rates.1 If the threshold is too low, the IDS is able to detect attacks well, but users would be annoyed due to false alarms. Conversely, if the threshold is too high, false alarm rates becomes lower, but it is easy for IDS to miss attacks. ROC curves closer to (0, 1) means a better classifier (i.e., a better intrusion detection system). The area under curve (AUC) summarizes the ROC curve into a single value in the range [0, 1] (Bradley, 1997).\n2.2 ENSEMBLE METHOD TO MINIMIZE FALSE ALARM RATES\nBuilding a \u2018strong normal\u2019 model (a model representing system-call sequences with high probabilities of being normal) is challenging because of over-fitting issues. In other words, a lower training loss does not necessarily imply better generalization performance. We can consider two reasons for encountering this issue.\nFirst, it is possible that only normal data were used for training the IDS without any attack data. Learning discriminative features that can separate normal call sequences from abnormal sequences is thus hard without seeing any abnormal sequences beforehand. This is a common obstacle for\n1A false alarm rate is the ratio of validation normal data classified as abnormal. A detection rate is the ratio of detected attacks in the real attack data.\nalmost every anomaly detection problem. In particular, malicious behaviors are frequently hidden and account for only a small part of all the system call sequences.\nSecond, in theory, we need a huge amount of data to cover all possible normal patterns to train the model satisfactorily. However, doing so is often impossible in a realistic situation because of the diverse and dynamic nature of system call patterns. Gathering live system-call data is harder than generating synthetic system-call data. The generation of normal training data in an off-line setting can create artifacts, because these data are made in fixed conditions for the sake of convenience in data generation. This setting may cause normal patterns to have some bias.\nAll these situations make it more difficult to choose a good set of hyper-parameters for LSTM architecture. To cope with this challenge, we propose a new ensemble method. Due to the lack of data, different models with different parameters capture slightly different normal patterns. If function f \u2208 S\u2217 7\u2192 R, which maps a system call sequence to a real value, is given, we can define a thresholding classifier as follows:\nCf (x; \u03b8) = { normal forf(x) \u2264 \u03b8; abnormal otherwise.\n(2)\nMost of the intrusion detection algorithms, including our proposed method, employ a thresholding classifier. For the sake of explanation, we define a term \u2018highly normal\u2019 sequence for the classifier Cf as a system call sequence having an extremely low f value so it will be classified as normal even when the threshold \u03b8 is sufficiently low to discriminate true abnormals. Highly normal sequences are represented as a flat horizontal line near (1, 1) in the ROC curve. The more the classifier finds highly normal sequences, the longer this line is. Note that a highly normal sequence is closely related to the false alarm rate.\nOur goal is to minimize the false alarm rate through the composition of multiple classifiers Cf1 , Cf2 , . . . , Cfm into a single classifier Cf , resulting in accumulated \u2018highly normal\u2019 data (here m is the number of classifiers used in the ensemble). This is due to the fact that a low false alarm rate is an important requisite in computer security, especially in intrusion detection systems. Our ensemble method can be represented by a simple formula:\nf(x) = m\u2211 i=1 wi\u03c3(fi(x)\u2212 bi). (3)\nAs activation function \u03c3, we used a leaky ReLU function, namely \u03c3(x) = max(x, 0.001x). Intuitively, the activation function forces potential \u2018highly normal\u2019 sequences having f values lower than bi to keep their low f values to the final f value. If we use the regular ReLU function instead, the degree of \u2018highly normal\u2019 sequences could not be differentiated. We set the bias term bi to the median of f values of the normal training data. In (3), wi indicates the importance of each classifier fi. Because we do not know the performance of each classifier before evaluation, we set wi to 1/m. Mathematically, this appears to be a degenerated version of a one-layer neural network. The basic philosophy of the ensemble method is that when the classification results from various classifiers are slightly different, we can make a better decision by composing them well. Still, including bad classifiers could degrade the overall performance. By choosing classifiers carefully, we can achieve satisfactory results in practice, as will be shown in Section 3.2.\n2.3 BASELINE CLASSIFIERS\nDeep neural networks are an excellent representation learning method. We exploit the sequence representation learned from the final state vector of the LSTM layer after feeding all the sequences of calls. For comparison with our main classifier, we use two baseline classifiers that are commonly used for anomaly detection exploiting vectors corresponding to each sequence: k-nearest neighbor (kNN) and k-means clustering (kMC). Examples of previous work for mapping sequences into vectors of fixed-dimensional hand-crafted features include normalized frequency and tf-idf (Liao & Vemuri, 2002; Xie et al., 2014).\nLet T be a normal training set, and let lstm(x) denotes a learned representation of call sequence x from the LSTM layer. kNN classifiers search for k nearest neighbors in T of query sequence x\non the embedded space and measure the minimum radius to cover them all. The minimum radius g(x; k) is used to classify query sequence x. Alternatively, we can count the number of vectors within the fixed radius, g(x; r). In this paper, we used the former. Because the computational cost of a kNN classifier is proportional to the size of T , using a kNN classifier would be intolerable when the normal training dataset becomes larger.\ng(x; k) = min r s.t. \u2211 y\u2208T [ d(lstm(x), lstm(y)) \u2264 r ] \u2265 k (4)\ng(x; r) = 1\u2212 1 |T | \u2211 y\u2208T [ d(lstm(x), lstm(y)) \u2264 r ] (5)\nThe kMC algorithm partitions T on the new vector space into k clusters G1, G2, . . . , Gk in which each vector belongs to the cluster with the nearest mean so as to minimize the within-cluster sum of squares. They are computed by Lloyd\u2019s algorithm and converge quickly to a local optimum. The minimum distance from each center of clusters \u00b5i, h(x; k), is used to classify the new query sequence.\nh(x; k) = min i=1,\u00b7\u00b7\u00b7 ,k d(lstm(x), \u00b5i) (6)\nThe two classifiers Cg and Ch are closely related in that the kMC classifier is equivalent to the 1-nearest neighbor classifier on the set of centers. In both cases of kNN and kMC, we need to choose parameter k empirically, depending on the distribution of vectors. In addition, we need to choose a distance metric on the embedding space; we used the Euclidean distance measure in our experiments.\n3 EXPERIMENTAL RESULTS AND DISCUSSION\n3.1 DATASETS\nThough system call traces themselves might be easy to acquire, collecting or generating a sufficient amount of meaningful traces for the evaluation of intrusion detection systems is a nontrivial task. In order to aid researchers in this regard, the following datasets were made publicly available from prior work: ADFA-LD (Creech & Hu, 2013), KDD98 (Lippmann et al., 2000) and UNM (of New Mexico, 2012). The KDD98 and UNM datasets were released in 1998 and 2004, respectively. Although these two received continued criticism about their applicability to modern systems (Brown et al., 2009; McHugh, 2000; Tan & Maxion, 2003), we include them as the results would show how our model fares against early works in the field, which were mostly evaluated on these datasets. As the ADFALD dataset was generated around 2012 to reflect contemporary systems and attacks, we have done our evaluation mainly on this dataset.\nThe ADFA-LD dataset was captured on an x86 machine running Ubuntu 11.04 and consists of three groups: normal training traces, normal validation traces, and attack traces. The KDD98 dataset was audited on a Solaris 2.5.1 server. We processed the audit data into system call traces per session. Each session trace was marked as normal or attack depending on the information provided in the accompanied bsm.list file, which is available alongside the dataset. Among the UNM process set, we tested our model with lpr that was collected from SunOS 4.1.4 machines. We merged the live lpr set and the synthetic lpr set. This combined dataset is further categorized into two groups: normal traces and attack traces. To maintain consistency with ADFA-LD, we divided the normal data of KDD98 and UNM into training and validation data in a ratio of 1:5, which is the ratio of the ADFA-LD dataset. The numbers of system-call sequences in each dataset we used are summarized in Table 1.\n3.2 PERFORMANCE EVALUATION\nWe used ADFA-LD and built three independent system-call language models by changing the hyperparameters of the LSTM layer: (1) one layer with 200 cells, (2) one layer with 400 cells, and (3) two layers with 400 cells. We matched the number of cells and the dimension of the embedding\nvector. Our parameters were uniformly initialized in [\u22120.1, 0.1]. For computational efficiency, we adjusted all system-call sequences in a mini-batch to be of similar lengths. We used the Adam optimizer (Kingma & Ba, 2014) for stochastic gradient descent with a learning rate of 0.0001. The normalized gradient was rescaled whenever its norm exceeded 5 (Pascanu et al., 2013), and we used dropout (Srivastava et al., 2014) with probability 0.5. We show the ROC curves obtained from the experiment in Figure 3.\nFor the two baseline classifiers, we used the Euclidean distance measure. Changing the distance measure to another metric did not perform well on average. In case of kNN, using k = 11 achieved the best performance empirically. For kMC, using k = 1 gave the best performance. Increasing the value of k produced similar but poorer results. We speculate the reason why a single cluster suffices as follows: learned representation vectors of normal training sequence are symmetrically distributed. The kNN classifier Cg and the kMC classifier Ch achieved similar performance. Compared to Liao & Vemuri (2002); Xie et al. (2014), our baseline classifiers easily returned \u2018highly normal\u2019 calls. This result was leveraged by the better representation obtained from the proposed system-call language modeling.\nAs shown in the left plot of Figure 3, three LSTM classifiers performed better than Cg and Ch. We assume that the three LSTM classifiers we trained are strong enough by themselves, and their classification results would be different from each other. By applying ensemble methods, we would expect to improve the performance. The first one was averaging, the second one was voting, and lastly we used our ensemble method as we explained in Section 2.2. The proposed ensemble method gave a better AUC value (0.928) with a large margin than that of the averaging ensemble method (0.890) and the voting ensemble method (0.859). Moreover, the curve obtained from the proposed ensemble method was placed above individual single curves, while other ensemble methods did not show this property.\nIn the setting of anomaly detection where attack data are unavailable, learning ensemble parameters is infeasible. If we exploit partial attack data, the assumption breaks down and the zero-day attack issue remains. Our ensemble method is appealing in that it performs remarkably well without learning.\nTo be clear, we applied ensemble methods to three LSTM classifiers learned independently using different hyper-parameters, not with the baseline classifiers, Cg or Ch. Applying ensemble methods to each type of baseline classifier gave unsatisfactory results since changing parameters or initialization did not result in complementary and reasonable classifiers that were essential for ensemble methods. Alternatively, we could do ensemble our LSTM classifiers and baseline classifiers together. However, this would also be a wrong idea because their f values differ in scale. The value of f in our LSTM classifier is an average negative log-likelihood, whereas g and h indicate distances in a continuous space.\nAccording to Creech & Hu (2014), the extreme learning machine (ELM) model, sequence timedelay embedding (STIDE), and the hidden Markov model (HMM) (Forrest et al., 1996; Warrender et al., 1999) achieved about 13%, 23%, and 42% false alarm rates (FAR) for 90% detection rate (DR), respectively. We achieved 16% FAR for 90% DR, which is comparable to the result of ELM and outperforms those of STIDE and HMM. The ROC curves for ELM, HMM, and STIDE can be found, but we could not draw those curves on the same plot with ours because the authors provided no specific details of their results. Creech & Hu (2014) classified ELM as a semantic approach and other two as syntactic approaches which treat each call as a basic unit. To be fair, our proposed method should be compared with those approaches that use system calls only as a basic unit in that we watch the sequence call-by-call. Furthermore, our method is end-to-end while ELM relies on hand-crafted features.\nIn Creech & Hu (2014), the authors reported that there was significant overhead for training the models mentioned above, and the overhead would inevitably increase for handling larger data. Longer phrases tend to be more informative, but handling them typically requires larger dictionaries. For this reason, Creech & Hu (2014) had to put an empirical upper bound to limit the lengths of phrases, which then might lower the performance of the models to handle various attacks. By contrast, our approach can learn in continuous space semantically meaningful representations of calls, phrases, and sequences of arbitrary lengths. Moreover, our method can relieve the burden of preprocessing (potentially massive) logging data. We expect that incorporating prior knowledge into our model can further boost its performance.\n3.3 PORTABILITY EVALUATION\nWe carried out experiments similar to those presented in Section 3.2 using the KDD98 dataset and the UNM dataset. First, we trained our system-call language model with LSTM having one layer of 200 cells and built our classifier using the normal training traces of the KDD98 dataset. The same model was used to evaluate the UNM dataset to examine the portability of the LSTM models trained with data from a different but similar system. The results of our experiments are represented in Figure 4. For comparison, we display the ROC curve of the UNM dataset by using the model from training the normal traces therein. To examine portability, the system calls in test datasets need to be included or matched to those of training datasets. UNM was generated using an earlier version of OS than that of KDD98, but ADFA-LD was audited on a fairly different OS. This made our experiments with other combinations difficult.\nThrough a quantitative analysis, for the KDD98 dataset, we earned an almost perfect ROC curve with an AUC value of 0.994 and achieved 2.3% FAR for 100% DR. With the same model, we tested the UNM datset and obtained a ROC curve with an AUC value of 0.969 and 5.5% FAR for 99.8% DR. This result was close to the result earned by using the model trained on normal training traces of the UNM dataset itself, as shown in the right plot of Figure 4.\nThis result is intriguing because it indicates that system-call language models have a strong portability. In other words, after training one robust and extensive model, the model can then be deployed to other similar host systems. By doing so, we can mitigate the burden of training cost. This paradigm is closely related to the concept of transfer learning, or zero-shot learning. It is well known that neural networks can learn abstract features and that they can be used successfully for unseen data.\n3.4 VISUALIZATION OF LEARNED REPRESENTATIONS\nIt is well-known that neural network based-language models can learn semantically meaningful embeddings to continuous space (Bengio et al., 2003; Mikolov et al., 2013; Cho et al., 2014). We\nexpected to see a similar characteristic with the proposed system-call language model. The 2D projection of the calls using the embedding matrix W learned from the system-call language model was done by t-SNE (Van der Maaten & Hinton, 2008) and shown in Figure 5. Just as the natural language model, we can expect that calls having similar co-occurrence patterns are positioned in similar locations in the embedded space after training the system call language model. We can clearly see that calls having alike functionality are clustered with each other.\nThe first obvious cluster would be the read-write call pair and the open-close pair. The calls of each pair were located in close proximity in the space, meaning that our model learned to associate them together. At the same time, the difference between the calls of each pair appears to be almost the same in the space, which in turn would mean our model learned that the relationship of each pair somewhat resembles.\nAnother notable cluster would be the group of select, pselect6, ppoll, epoll wait and nanosleep. The calls select, pselect6 and ppoll all have nearly identical functions in that they wait for some file descriptors to become ready for some class of I/O operation or for signals. The other two calls also have similar characteristics in that they wait for a certain event or signal as well. This could be interpreted as our model learning that these \u2018waiting\u2019 calls share similar characteristics.\nOther interesting groups would be: readlink and lstat64 which are calls related to symbolic links; fstatat64 and fstat64 which are calls related to stat calls using file descriptors; pipe and pipe2 which are nearly identical and appear almost as one on the embedding layer. These cases show that our model is capable of learning similar characteristics among the great many system calls.\nSimilarly to the call representations, we expected that attack sequences with the same type would cluster to each other, and we tried to visualize them. However, for various reasons including the lack of data, we were not able to observe this phenomenon. Taking the fact that detecting abnormal patterns from normal patterns well would be sufficiently hard into consideration, learning representation to separate different abnormal patterns with only seen normal patterns would also be an extremely difficult task.\n4 CONCLUSION\nOur main contributions for designing intrusion detection systems as described in this paper have two parts: the introduction of a system-call language modeling approach and a new ensemble method. To the best of the authors\u2019 knowledge, our method is the first to introduce the concept of a language model, especially using LSTM, to anomaly-based IDS. The system-call language model can capture the semantic meaning of each call and its relation to other system calls. Moreover, we proposed an innovative and simple ensemble method that can better fit to IDS design by focusing on lowering false alarm rates. We showed its outstanding performance by comparing it with existing state-of-theart methods and demonstrated its robustness and generality by experiments on diverse benchmarks.\nAs discussed earlier, the proposed method also has excellent portability. In contrast to alternative methods, our proposed method incurs significant smaller training overhead because it does not need to build databases or dictionaries to keep a potentially exponential amount of patterns. Our method is compact and light in that the size of the space required to save parameters is small. The overall training and inference processes are also efficient and fast, as our methods can be implemented using efficient sequential matrix multiplications.\nAs part of our future work, we are planning to tackle the task of detecting elaborate contemporary attacks including mimicry attacks (Wagner & Soto, 2002; Shu et al., 2015) by more advanced methods. Our proposed method allows us to estimate the likelihood of arbitrary sections in a given system-call sequence, which may be helpful for analyzing the capability of handling mimicry attacks. For instance, it is possible to determine if there exists a sufficiently long section (rather than the whole sequence) with the average log-likelihood below the threshold. In addition, we are considering designing a new framework to build a robust model in on-line settings by collecting large-scale data generated from distributed environments. For optimization of the present work, we would be able to alter the structure of RNNs used in our system-call language model and ensemble algorithm. Finally, we anticipate that a hybrid method that combines signature-based approaches and feature engineering will allow us to create more accurate intrusion detection systems.\nACKNOWLEDGMENTS\nThis work was supported by BK21 Plus Project in 2016 (Electrical and Computer Engineering, Seoul National University).\n", "cats": {"READ": 1.0, "DONTREAD": 0.0}, "reviews": [{"IS_META_REVIEW": true, "comments": "In this paper a novel approach for anomaly detection is considered for the task of intrusion detection based on system call sequence.\nThe system call sequence is regarded as a language, and multiple lstm-rnn language models are trained and ensembled. \nDiversity in the ensemble is achieved by choosing different hyper parameters for each lstm-LM.\nThe combination of the LMs is done by averaging transformations of the likelihoods. \n\nI really like the fact that no attack data is used during training, and I like the LM and ensemble approach. \nThe only high level drawbacks I have are the following, which might have a simple answer as I'm not an expert in this field:\n\n- Relaying of system calls seems weak: If the attacker has access to some \"normal\" sequences of system calls, all she can fool the system by interleaving its malicious system calls with normal ones, in a way that will artificially raise the likelihood of the sequence. \n- A few lines covering other anomaly detection tasks, where RNNs are used, can be added to the introduction, to give a better idea about the novelty of the approach."}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "This is a pure application paper: an application of LSTMs to host intrusion detection systems based upon observed system calls. And from the application standpoint, I don't believe this is a bad paper, the authors seem to achieve reasonable results from the method (though admittedly, I can't really judge the quality of these results without a lot more familiarity based upon current work in intrusion detection systems). However, in terms of the ICLR audience, I simply don't believe there is enough here to warrant substantial interest. As an example, the authors highlighted the distinction between network intrusion detection systems (NIDS) and host intrusion detection systems (HIDS), and felt that one review was completely unsuitable because they didn't realize this distinction when reading the paper. This is of course a crucial distinction from the security application side, but from the algorithmic/ML side, it's simply not that important, and the fact that there _has_ been previous work exactly on LSTMs for NIDS makes this paper unlikely to have a huge impact in the ICLR community. It would be much better suited to a security conference, where the application could be judged on its own merits, and the community would likely understand much better how significant these results were.\n \n Pros:\n + Nice application of LSTMs to HIDS task\n \n Cons:\n - Nothing really novel from the algorithmic/ML side\n - The significance of the results are difficult to assess without more formal understanding of the problem domains\n \n The PCs have thus decided that this paper isn't ready to appear at the conference.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"DATE": "19 Jan 2017", "TITLE": "revision", "IS_META_REVIEW": false, "comments": "In order to reflect the comments from reviewers, the revised manuscript now includes additional references to related work (Introduction) and more in-depth comparison with existing methods (Section 3.2) as well as more details of our future work (Conclusion).", "OTHER_KEYS": "Sungroh Yoon"}, {"TITLE": "No Title", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "This paper presents an anomaly-based host intrusion detection method. LSTM RNN is used to model the system-call sequences and the averaged sequence likelihood is then used to determine anomaly, which is the attack. This paper also compares an ensemble method with two baselines as classification model.\n+This is is well written and more of ideas are clearly presented.\n+It demonstrates an interesting application of LSTM sequential modeling to HIDS problem\n-The overall novelty is limited considering the major technical components like LSTM RNN and ensemble method are already established. \n-The contribution of the proposed ensemble method needs further evaluation because it is also possible to use ensemble ideas in kNN and kMC baselines.", "IS_META_REVIEW": false, "RECOMMENDATION": 5, "DATE": "02 Jan 2017", "REVIEWER_CONFIDENCE": 3}, {"TITLE": "Incremental advance, not well-suited for ICLR", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "The authors propose using an LSTM on a sequence of system calls to perform network intrusion detection (NIDS). The idea of using neural networks (in general) for NIDS is old [1]. The idea of using some sort of NN on top of a sequence of system calls for NIDS is published [2]. The idea of using LSTMs for NIDS is published [2]. The paper in [2] operates on counts of N-grams of system calls, rather than on the raw sequence, but that pre-processing does not seem \"heavy\" to me.\n\nOverall, the proposed system works as well as other proposed NIDS system, and the paper checks portability (which is good).\n\nBut, on the con side, I don't see this paper as adding a lot to the state-of-the-art in NIDS. Nor does is the paper well-matched to ICLR. I didn't learn a lot about representations from this paper: many people have thrown LSTM at sequence problems.\n\nTherefore, I think it's below threshold for ICLR. The authors may wish to submit to a security conference.\n\nReferences:\n1. Debar, Herve, Monique Becker, and Didier Siboni. \"A neural network component for an intrusion detection system.\" Research in Security and Privacy, 1992. Proceedings., 1992 IEEE Computer Society Symposium on. IEEE, 1992.\n2. Creech, Gideon, and Jiankun Hu. \"A semantic approach to host-based intrusion detection systems using contiguousand discontiguous system call patterns.\" IEEE Transactions on Computers 63.4 (2014): 807-819.\n3. Staudemeyer, Ralf C. \"Applying long short-term memory recurrent neural networks to intrusion detection.\" South African Computer Journal 56.1 (2015).", "IS_META_REVIEW": false, "RECOMMENDATION": 5, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "neat idea, clearly written", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "In this paper a novel approach for anomaly detection is considered for the task of intrusion detection based on system call sequence.\nThe system call sequence is regarded as a language, and multiple lstm-rnn language models are trained and ensembled. \nDiversity in the ensemble is achieved by choosing different hyper parameters for each lstm-LM.\nThe combination of the LMs is done by averaging transformations of the likelihoods. \n\nI really like the fact that no attack data is used during training, and I like the LM and ensemble approach. \nThe only high level drawbacks I have are the following, which might have a simple answer as I'm not an expert in this field:\n\n- Relaying of system calls seems weak: If the attacker has access to some \"normal\" sequences of system calls, all she can fool the system by interleaving its malicious system calls with normal ones, in a way that will artificially raise the likelihood of the sequence. \n- A few lines covering other anomaly detection tasks, where RNNs are used, can be added to the introduction, to give a better idea about the novelty of the approach. \n", "IS_META_REVIEW": false, "RECOMMENDATION": 8, "DATE": "12 Dec 2016", "REVIEWER_CONFIDENCE": 3}, {"DATE": "03 Dec 2016", "TITLE": "Comparison to other methods?", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3"}, {"IS_META_REVIEW": true, "comments": "In this paper a novel approach for anomaly detection is considered for the task of intrusion detection based on system call sequence.\nThe system call sequence is regarded as a language, and multiple lstm-rnn language models are trained and ensembled. \nDiversity in the ensemble is achieved by choosing different hyper parameters for each lstm-LM.\nThe combination of the LMs is done by averaging transformations of the likelihoods. \n\nI really like the fact that no attack data is used during training, and I like the LM and ensemble approach. \nThe only high level drawbacks I have are the following, which might have a simple answer as I'm not an expert in this field:\n\n- Relaying of system calls seems weak: If the attacker has access to some \"normal\" sequences of system calls, all she can fool the system by interleaving its malicious system calls with normal ones, in a way that will artificially raise the likelihood of the sequence. \n- A few lines covering other anomaly detection tasks, where RNNs are used, can be added to the introduction, to give a better idea about the novelty of the approach."}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "This is a pure application paper: an application of LSTMs to host intrusion detection systems based upon observed system calls. And from the application standpoint, I don't believe this is a bad paper, the authors seem to achieve reasonable results from the method (though admittedly, I can't really judge the quality of these results without a lot more familiarity based upon current work in intrusion detection systems). However, in terms of the ICLR audience, I simply don't believe there is enough here to warrant substantial interest. As an example, the authors highlighted the distinction between network intrusion detection systems (NIDS) and host intrusion detection systems (HIDS), and felt that one review was completely unsuitable because they didn't realize this distinction when reading the paper. This is of course a crucial distinction from the security application side, but from the algorithmic/ML side, it's simply not that important, and the fact that there _has_ been previous work exactly on LSTMs for NIDS makes this paper unlikely to have a huge impact in the ICLR community. It would be much better suited to a security conference, where the application could be judged on its own merits, and the community would likely understand much better how significant these results were.\n \n Pros:\n + Nice application of LSTMs to HIDS task\n \n Cons:\n - Nothing really novel from the algorithmic/ML side\n - The significance of the results are difficult to assess without more formal understanding of the problem domains\n \n The PCs have thus decided that this paper isn't ready to appear at the conference.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"DATE": "19 Jan 2017", "TITLE": "revision", "IS_META_REVIEW": false, "comments": "In order to reflect the comments from reviewers, the revised manuscript now includes additional references to related work (Introduction) and more in-depth comparison with existing methods (Section 3.2) as well as more details of our future work (Conclusion).", "OTHER_KEYS": "Sungroh Yoon"}, {"TITLE": "No Title", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "This paper presents an anomaly-based host intrusion detection method. LSTM RNN is used to model the system-call sequences and the averaged sequence likelihood is then used to determine anomaly, which is the attack. This paper also compares an ensemble method with two baselines as classification model.\n+This is is well written and more of ideas are clearly presented.\n+It demonstrates an interesting application of LSTM sequential modeling to HIDS problem\n-The overall novelty is limited considering the major technical components like LSTM RNN and ensemble method are already established. \n-The contribution of the proposed ensemble method needs further evaluation because it is also possible to use ensemble ideas in kNN and kMC baselines.", "IS_META_REVIEW": false, "RECOMMENDATION": 5, "DATE": "02 Jan 2017", "REVIEWER_CONFIDENCE": 3}, {"TITLE": "Incremental advance, not well-suited for ICLR", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "The authors propose using an LSTM on a sequence of system calls to perform network intrusion detection (NIDS). The idea of using neural networks (in general) for NIDS is old [1]. The idea of using some sort of NN on top of a sequence of system calls for NIDS is published [2]. The idea of using LSTMs for NIDS is published [2]. The paper in [2] operates on counts of N-grams of system calls, rather than on the raw sequence, but that pre-processing does not seem \"heavy\" to me.\n\nOverall, the proposed system works as well as other proposed NIDS system, and the paper checks portability (which is good).\n\nBut, on the con side, I don't see this paper as adding a lot to the state-of-the-art in NIDS. Nor does is the paper well-matched to ICLR. I didn't learn a lot about representations from this paper: many people have thrown LSTM at sequence problems.\n\nTherefore, I think it's below threshold for ICLR. The authors may wish to submit to a security conference.\n\nReferences:\n1. Debar, Herve, Monique Becker, and Didier Siboni. \"A neural network component for an intrusion detection system.\" Research in Security and Privacy, 1992. Proceedings., 1992 IEEE Computer Society Symposium on. IEEE, 1992.\n2. Creech, Gideon, and Jiankun Hu. \"A semantic approach to host-based intrusion detection systems using contiguousand discontiguous system call patterns.\" IEEE Transactions on Computers 63.4 (2014): 807-819.\n3. Staudemeyer, Ralf C. \"Applying long short-term memory recurrent neural networks to intrusion detection.\" South African Computer Journal 56.1 (2015).", "IS_META_REVIEW": false, "RECOMMENDATION": 5, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "neat idea, clearly written", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "In this paper a novel approach for anomaly detection is considered for the task of intrusion detection based on system call sequence.\nThe system call sequence is regarded as a language, and multiple lstm-rnn language models are trained and ensembled. \nDiversity in the ensemble is achieved by choosing different hyper parameters for each lstm-LM.\nThe combination of the LMs is done by averaging transformations of the likelihoods. \n\nI really like the fact that no attack data is used during training, and I like the LM and ensemble approach. \nThe only high level drawbacks I have are the following, which might have a simple answer as I'm not an expert in this field:\n\n- Relaying of system calls seems weak: If the attacker has access to some \"normal\" sequences of system calls, all she can fool the system by interleaving its malicious system calls with normal ones, in a way that will artificially raise the likelihood of the sequence. \n- A few lines covering other anomaly detection tasks, where RNNs are used, can be added to the introduction, to give a better idea about the novelty of the approach. \n", "IS_META_REVIEW": false, "RECOMMENDATION": 8, "DATE": "12 Dec 2016", "REVIEWER_CONFIDENCE": 3}, {"DATE": "03 Dec 2016", "TITLE": "Comparison to other methods?", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3"}]}
{"text": "RENDERGAN: GENERATING REALISTIC LABELED DATA\nDeep Convolutional Neuronal Networks (DCNNs) are showing remarkable performance on many computer vision tasks. Due to their large parameter space, they require many labeled samples when trained in a supervised setting. The costs of annotating data manually can render the use of DCNNs infeasible. We present a novel framework called RenderGAN that can generate large amounts of realistic, labeled images by combining a 3D model and the Generative Adversarial Network framework. In our approach, image augmentations (e.g. lighting, background, and detail) are learned from unlabeled data such that the generated images are strikingly realistic while preserving the labels known from the 3D model. We apply the RenderGAN framework to generate images of barcode-like markers that are attached to honeybees. Training a DCNN on data generated by the RenderGAN yields considerably better performance than training it on various baselines.\n1 INTRODUCTION\nWhen an image is taken from a real world scene, many factors determine the final appearance: background, lighting, object shape, position and orientation of the object, the noise of the camera sensor, and more. In computer vision, high-level information such as class, shape, or pose is reconstructed from raw image data. Most real-world applications require the reconstruction to be invariant to noise, background, and lighting changes.\nIn recent years, deep convolutional neural networks (DCNNs) advanced to the state of the art in many computer vision tasks (Krizhevsky et al., 2012; He et al., 2015; Razavian et al., 2014). More training data usually increases the performance of DCNNs. While image data is mostly abundant, labels for supervised training must often be created manually \u2013 a time-consuming and tedious activity. For complex annotations such as human joint angles, camera viewpoint or image segmentation, the costs of labeling can be prohibitive.\nIn this paper, we propose a method to drastically reduce the costs of labeling such that we can train a model to predict even complex sets of labels. We present a generative model that can sample from the joint distribution of labels and data. The training procedure of our model does not require any manual labeling. We show that the generated data is of high quality and can be used to train a model in a supervised setting, i.e. a model that maps from real samples to labels, without using any manually labeled samples.\nWe propose two modifications to the recently introduced GAN framework (Goodfellow et al., 2014). First, a simple 3D model is embedded into the generator network to produce samples from corresponding input labels. Second, the generator learns to add missing image characteristics to the model output using a number of parameterized augmentation functions. In the adversarial training we leverage large amounts of unlabeled image data to learn the particular form of blur, lighting, background and image detail. By constraining the augmentation functions we ensure that the resulting image still represents the given set of labels. The resulting images are hard to distinguish from real samples and can be used to train a DCNN to predict the labels from real input data.\nThe RenderGAN framework was developed to solve the scarcity of labeled data in the BeesBook project (Wario et al., 2015) in which we analyze the social behavior of honeybees. A barcode-like marker is attached to the honeybees\u2019 backs for identification (see Fig. 1). Annotating this data is tedious, and therefore only a limited amount of labeled data exists. A 3D model (see the upper row of Fig. 2) generates a simple image of the tag based on position, orientation, and bit configuration. The RenderGAN then learns from unlabeled data to add lighting, background, and image details.\nTraining a DCNN on data generated by the RenderGAN yields considerably better performance compared to various baselines. We furthermore include a previously used computer vision pipeline in the evaluation. The networks\u2019 detections are used as feature to track the honeybees over time. When we use detections from the DCNN instead of the computer vision pipeline, the accuracy of assigning the true id increases from 55% to 96%.\nOur contributions are as follows. We present an extension of the GAN framework that allows to sample from the joint distribution of data and labels. The generated samples are nearly indistinguishable from real data for a human observer and can be used to train a DCNN end-to-end to classify real samples. In a real-world use case, our approach significantly outperforms several baselines. Our approach requires no manual labeling. The simple 3D model is the only form of supervision.\n2 RELATED WORK\nThere exists multiple approaches to reduce the costs associated with labeling.\nA common approach to deal with limited amount of labels is data augmentation (Goodfellow et al., 2016, Chapter 7.4). Translation, noise, and other deformations can often be applied without changing the labels, thereby effectively increasing the number of training samples and reducing overfitting.\nDCNNs learn a hierarchy of features \u2013 many of which are applicable to related domains (Yosinski et al., 2014). Therefore, a common technique is to pre-train a model on a larger dataset such as ImageNet (Deng et al., 2009) and then fine-tune its parameters to the task at hand (Girshick et al., 2014; Long et al., 2015; Razavian et al., 2014). This technique only works in cases where a large enough related dataset exists. Furthermore, labeling enough data for fine-tuning might still be costly.\nIf a basic model of the data exists (for example, a 3D model of the human body), it can be used to generate labeled data. Peng et al. (2015) generated training data for a DCNN with 3D-CAD models.\nSu et al. (2015) used 3D-CAD models from large online repositories to generate large amounts of training images for the viewpoint estimation task on the PASCAL 3D+ dataset (Xiang et al., 2014). Massa et al. (2015) are matching natural images to 3D-CAD models with features extracted from a DCNN. Richter et al. (2016) and Ros et al. (2016) used 3D game engines to collect labeled data for image segmentation. However, the explicit modeling of the image acquisition physics (scene lighting, reflections, lense distortions, sensor noise, etc.) is cumbersome and might still not be able to fully reproduce the particularities of the imaging process such as unstructured background or object specific noise. Training a DCNN on generated data that misses certain features will result in overfitting and poor performance on the real data.\nGenerative Adversarial Networks (GAN) (see Fig. 3) can learn to generate high-quality samples (Goodfellow et al., 2014), i.e. sample from the data distribution p(x). Denton et al. (2015) synthesized images with a GAN on the CIFAR dataset (Krizhevsky, 2009), which were hard for humans to distinguish from real images. While a GAN implicitly learns a meaningful latent embedding of the data (Radford et al., 2015), there is no simple relationship between the latent dimensions and the labels of interest. Therefore, high-level information can\u2019t be inferred from generated samples. cGANs are an extension of GANs to sample from a conditional distribution given some labels, i.e. p(x|l). However, training cGANs requires a labeled dataset. Springenberg (2015) showed that GANs can be used in a semi-supervised setting but restricted their analysis to categorical labels. Wang & Gupta (2016) trained two separate GANs, one to model the object normals and another one for the texture conditioned on the normals. As they rely on conditional GANs, they need large amounts of labeled data. Chen et al. (2016) used an information theoretic to disentangle the representation. They decomposed the representation into a structured and unstructured part. And successfully related on a qualitative level the structured part to high-level concepts such as camera viewpoint or hair style. However, explicitly controlling the relationship between the latent space and generated samples without using labeled data is an open problem, i.e. sampling from p(x, l) without requiring labels for training.\n3 RENDERGAN\nMost supervised learning tasks can be modeled as a regression problem, i.e. approximating a function f\u0302 : Rn 7\u2192 L that maps from data space R to label space L. We consider f\u0302 to be the best available function on this particular task. Analogous to ground truth data, one could call f\u0302 the ground truth function.\nIn the RenderGAN framework, we aim to solve the inverse problem to this regression task: generate data given the labels. This is achieved by embedding a simple 3D model into the generator of a GAN. The samples generated by the simple model must correspond to the given labels but may lack many factors of the real data such as background or lightning. Through a cascade of augmentation functions, the generator can adapt the images from the 3D model to match the real data.\nWe formalize image augmentation as a function \u03c6(x, d), which modifies the image x based on the augmentation parameter d (a tensor of any rank). The augmentation must preserve the labels of the image x. Therefore, it must hold for all images x and all augmentations parameters d:\nf\u0302 (\u03c6(x, d)) = f\u0302(x) (1)\nThe augmentation function must furthermore be differentiable w.r.t. x and d as the gradient will be back-propagated through \u03c6 to the generator. Image augmentations such as lighting, surrounding, and noise do preserve the labels and fit this definition. We will provide appropriate definitions of \u03c6 for the mentioned augmentations in the following section.\nIf appropriate augmentation functions are found that can model the missing factors and are differentiable, we can use the GAN framework to find parameters that result in realistic output images. Multiple augmentation functions can be combined to perform a more complex augmentation. Here, we will consider multiple augmentation functions applied sequentially, i.e. we have k augmentation functions \u03c6i and k corresponding outputs Gi from the generator. The output of the previous augmentation function is the input to the next one. Thus, we can write the generator given some labels l as:\ng(z, l) = \u03c6k(\u03c6k\u22121(. . . \u03c60(M(l), G0(z)) . . . , Gk\u22121(z)), Gk(z)) (2)\nwhere M is the 3D model. We can furthermore learn the label distribution with the generator. As the discriminator loss must be backpropagated through the 3D model M, it must be differentiable. This can be achieved by emulating the 3D model with a neural network (Dosovitskiy et al., 2015). The resulting generator g(z) can be written as (see Fig. 4 for a visual interpretation):\ng(z) = \u03c6k(\u03c6k\u22121(. . . \u03c60(M(Gl(z)), G0(z)) . . . , Gk\u22121(z)), Gk(z)) (3)\nAs any differentiable function approximator can be employed in the GAN framework, the theoretical properties still hold. The training is carried out as in the conventional GAN framework. In a real application, the augmentation functions might restrict the generator from converging to the data distribution.\nIf the training converges, we can collect generated realistic data with g(z) and the high-level information captured in the 3D model with Gl(z). We can now train a supervised learning algorithm on the labeled generated data (Gl (z) , g (z)) and solve the regression task of approximating f\u0302 without depending on manual labels.\n4 APPLICATION TO THE BEESBOOK PROJECT\nIn the BeesBook project, we aim to understand the complex social behavior of honey bees. For identification, a tag with a binary code is attached to the back of the bees.\nThe orientations in space, position, and bits of the tags are required to track the bees over time. Decoding this information is not trivial: the bees are oriented in any direction in space. The tag might be partially occluded. Moreover, spotlights on the tag can sometimes even confuse humans. A previously used computer vision pipeline did not perform reliably. Although we invested a substantial amount of time on labeling, a DCNN trained on this data did not perform significantly better (see Sec. 3). We therefore wanted to synthesize labeled images which are realistic enough to train an improved decoder network.\nFollowing the idea outlined in section 3, we created a simple 3D model of a bee marker. The 3D model comprises a mesh which represents the structure of the marker and a simple camera model to project the mesh to the image plane. The model is parameterized by its position, its pitch, yaw and roll, and its ID. Given a parameter set, we obtain a marker image, a background segmentation mask\nand a depth map. The generated images lack many important factors: blur, lighting, background, and image detail (see Fig. 2). A DCNN trained on this data does not generalize well (see Sec. 5).\nOver the last years we collected a large amount of unlabeled image data. We successfully augmented the 3D model using this dataset, as described below.\nWe trained a neural network to emulate the 3D model. Its outputs are indistinguishable from the images of the 3D model. The discriminator error can now be backpropagated through the 3D model which allows the generator to also learn the distributions of positions and orientations of the bee marker. The IDs are sampled uniformly during training. The weights of the 3D model network are fixed during the RenderGAN training.\nWe apply different augmentation functions that account for blur, lighting, background, and image detail. The output of the 3D model and of each augmentation function is of shape (64, 64) and in the range [\u22121, 1]. In Fig. 5, the structure of the generator is shown. Blurriness: The 3D model produces hard edges, but the images of the real tags show a broad range of blur. The generator produces a scalar \u03b1 \u2208 [0, 1] per image that controls the blur.\n\u03c6blur(x, \u03b1) = (1\u2212 \u03b1) (x\u2212 b\u03c3 (x)) + b\u03c3(x) (4)\nwhere b\u03c3(x) = x \u2217 k\u03c3 denotes convolving the image x with a Gaussian kernel k\u03c3 of scale \u03c3. The implementation of the blur function is inspired by Laplacian pyramids (Burt & Adelson, 1983). As required for augmentation functions, the labels are preserved, because we limit the maximum amount of blur by picking \u03c3 = 2. \u03c6blur is also differentiable w.r.t the inputs \u03b1 and x.\nLighting of the tag: The images from the 3D model are binary. In real images, tags exhibit different shades of gray. We model the lighting by a smooth scaling and shifting of the pixel intensities. The generator provides three outputs for the lighting: scaling of black parts sb, scaling of white parts sw and a shift t. All outputs have the same dimensions as the image x. An important invariant is that the black bits of the tag must stay darker than the white bits. Otherwise, a bit could flip, and the label would change. By restricting the scaling sw and sb to be between 0.10 and 1, we ensure that this invariant holds. The lighting is locally corrolated and should cause smooth changes in the image. Hence, Gaussian blur b(x) is applied to sb, sw, and t.\n\u03c6lighting(x, sw, sb, t) = x \u00b7 b(sw) \u00b7W (x) + x \u00b7 b(sb) \u00b7 (1\u2212W (x)) + b(t) (5)\nThe segmentation mask W (x) is one for white parts and zero for the black part of the image. As the intensity of the input is distributed around -1 and 1, we can use thresholding to differentiate between black and white parts.\nBackground: The background augmentation can change the background pixels arbitrarily. A segmentation mask Bx marks the background pixels of the image x which are replaced by the pixels from the generated image d.\n\u03c6bg(x, d) = x \u00b7 (1\u2212Bx) + d \u00b7Bx (6)\nThe 3D model provides the segmentation mask. As \u03c6bg can only change background pixels, the labels remain unchanged.\nDetails: In this stage, the generator can add small details to the whole image including the tag. The output of the generator d is passed through a high-pass filter to ensure that the added details are small enough not to flip a bit. Furthermore, d is restricted to be in [\u22122, 2] to make sure the generator cannot avoid the highpass filter by producing huge values. With the range [\u22122, 2], the generator has the possibility to change black pixels to white, which is needed to model spotlights.\n\u03c6detail(x, d) = x+ highpass(d) (7) The high-pass is implemented by taking the difference between the image and a blurred version of the image (\u03c3 = 3.5). As the spotlights on the tags are only a little smaller than the bits, we increase its slope after the cutoff frequency by repeating the high-pass filter three times.\nThe image augmentations are applied in the order as listed above: \u03c6detail \u25e6\u03c6background \u25e6\u03c6lighting \u25e6 \u03c6blur. Please note that there exist parameters to the augmentation functions that could change the labels. As long as it is guaranteed that such augmentations will result in unrealistic looking images, the generator network will learn to avoid them. For example, even though the detail augmentation could be used to add high-frequency noise to obscure the tag, this artifact would be detected by the discriminator.\nArchitecture of the generator: The generator network has to produce outputs for each augmentation function. We will outline only the most important parts. See our code available online for all the details of the networks1. The generator starts with a small network consisting of dense layers, which predicts the parameters for the 3D model (position, orientations). The output of another dense layer is reshaped and used as starting block for a chain of convolution and upsampling layers. We found it advantageous to merge a depth map of the 3D model into the generator as especially the lighting depends on the orientation of the tag in space. The input to the blur augmentation is predicted by reducing an intermediate convolutional feature map to a single scalar. An additional network is branched off to predict the input to the lighting augmentation. For the background generation, the output of the lighting network is merged back into the main generator network together with the actual image from the 3D model.\nFor the discriminator architecture, we mostly rely on the architecture given by Radford et al. (2015), but doubled the number of convolutional layers and added a final dense layer. This change improved the quality of the generated images.\nClip layer: Some of the augmentation parameters have to be restricted to a range of values to ensure that the labels remain valid. The training did not converge when using functions like tanh or sigmoid due to vanishing gradients. We are using a combination of clipping and activity regularization to keep the output in a given interval [a, b]. If the input x is out of bounds, it is clipped and a regularization loss r depending on the distance between x and the appropriate bound is added.\nr(x) =  \u03b3||x\u2212 a||1 if x < a 0 if a \u2264 x \u2264 b \u03b3||x\u2212 b||1 if x > b\n(8)\nf(x) = min(max(a, x), b) (9) With the scalar \u03b3, the weight of the loss can be adapted. For us \u03b3 = 15 worked well. If \u03b3 is chosen too small, the regularization loss might not be big enough to move the output of the previous layer towards the interval [a, b]. During training, we observe that the loss decreases to a small value but never vanishes.\nTraining: We train generator and discriminator as in the normal GAN setting. We use 2.4M unlabeled images of tags to train the RenderGAN. We use Adam (Kingma & Ba, 2014) as an optimizer with a starting learning rate of 0.0002, which we reduce in epoch 200, 250, and 300 by a factor of 0.25. In Fig. 6b the training loss of the GAN is shown. The GAN does not converge to the point where the discriminator can no longer separate generated from real samples. The augmentation functions might restrict the generator too much such that it cannot model certain properties. Nevertheless, it is hard for a human to distinguish the generated from real images. In some cases, the\n1https://github.com/berleon/deepdecoder\ngenerator creates unrealistic high-frequencies artifacts. The discriminator unfailingly assigns a low score to theses images. We can therefore discard them for the training of the supervised algorithm. More generated images are shown in Appendix A. In Fig. 7, we show random points in the latent space, while fixing the tag parameters. The generator indeed learned to model the various lighting conditions, noise intensities, and backgrounds.\n5 RESULTS\nWe constructed the RenderGAN to generate labeled data. But does a DCNN trained with the RenderGAN data perform better than one trained on the limited amounts of real data? And are learned augmentations indeed needed or do simple hand-designed augmentation achieve the same result? The following paragraphs describe the different datasets used in the evaluation. We focus on the performance of a DCNN on the generated data. Thus, we do not compare our method to conventional GANs as those do not provide labels and are generally hard to evaluate.\nData from the RenderGAN: We generate 5 million tags with the RenderGAN framework. Due to the abundance, one training sample is only used twice during training. It is not further augmented.\nReal Data: The labels of the real data are extracted from ground truth data that was originally collected to evaluate bee trajectories. This ground truth data contains the path and id of each bee over multiple consecutive frames. Data from five different time spans was annotated \u2013 in total 66K tags. As the data is correlated in time (same ids, similar lighting conditions), we assign the data from one time span completely to either the train or test set. The data from three time spans forms the train set (40K). The test set (26K) contains data from the remaining two time spans. The ground truth data lacks the orientation of the tags, which is therefore omitted for this evaluation. Due to the smaller\nsize of the real training set, we augment it with random translation, rotation, shear transformation, histogram scaling, and noise (see Appendix C for exact parameters).\nRenderGAN + Real: We also train a DCNN on generated and real data which is mixed at a 50:50 ratio.\nHandmade augmentations: We tried to emulate the augmentations learned by the RenderGAN by hand. For example, we generate the background by an image pyramid where the pixel intensities are drawn randomly. We model all effects, i.e. blur, lighting, background, noise and spotlights (see Appendix B for details on their implementation). We apply the handmade augmentation to different learned representations of the RenderGAN, e.g. we use the learned lighting representation and add the remaining effects such as background and noise with handmade augmentations (HM LI). See Table 1 for the different combinations of learned representations and hand designed augmentations.\nComputer vision pipeline CV : The previously used computer vision pipeline (Wario et al., 2015) is based on manual feature extraction. For example, a modified Hough transformation to find ellipses. The MHD obtained by this model is only a rough estimate given that the computer vision pipeline had to be evaluated and fine-tuned on the same data set due to label scarcity.\nTraining setup: An epoch consists of 1000 batches a\u0301 128 samples. We use early stopping to select the best parameters of the networks. For the training with generated data, we use the real training set as the validation set. When training on real data, the test set is also used for validation. We could alternatively reduce the real training set further and form an extra validation set, but this would harm the performance of the DCNN trained on the real data. We use the 34-layer ResNet architecture (He et al., 2015) but start with 16 feature maps instead of 64. The DCNNs are evaluated on the mean Hamming distance (MHD) i.e. the expected value of bits decoded wrong. Human experts can decode tags with a MHD of around 0.23.\nResults: In Table 2, we present the results of the evaluation. The training losses of the networks are plotted in Fig. 8. The model trained with the data generated by the RenderGAN has an MHD of 0.424. The performance can furthermore be slightly improved by combining the generated with real data. The small gap in performance when adding real data is a further indicator of the quality of the generated samples.\nIf we use predictions from this DCNN instead of the computer vision pipeline, the accuracy of the tracking improves from 55% of the ids assigned correctly to 96%. At this quality, it is possible to analyze the social behavior of the honeybees reliably.\nCompared to the handmade augmentations (HM 3D), data from the RenderGAN leads to considerably better performance. The large gap in performance between the HM 3D and HM LI data highlights the importance of the learned lighting augmentation.\n6 DISCUSSION\nWe proposed a novel extension to the GAN framework that is capable of rendering samples from a basic 3D model more realistic. Compared to computer graphics pipelines, the RenderGAN can learn complex effects from unlabeled data that would be otherwise hard to model with explicit rules.\nContrary to conventional GANs, the generator provides explicit information about the synthesized images, which can be used as labels for a supervised algorithm. The training of the RenderGAN requires no labels.\nWe showed an application of the RenderGAN framework to the BeesBook project, in which the generator adds blur, lighting, background, and details to images from a basic 3D model. The generated data looks strikingly real and includes fine details such as spotlights, compression artifacts, and sensor noise.\nIn contrast to previous work that applied 3D models to produce training samples for DCNNs (Su et al., 2015; Richter et al., 2016; Ros et al., 2016), we were able to train a DCNN from scratch with only generated data that still generalizes to unseen real data.\nWhile some work is required to adapt the RenderGAN to a specific domain, once set up, arbitrary amounts of labeled data can be acquired cheaply, even if the data distribution changes. For example, if the tag design changes to include more bits, small adaptions to the 3D model\u2019s source code and eventually the hyperparameters of the augmentation functions would be sufficient. However, if we had labeled the data by hand, then we would have to annotate data again.\nWhile the proposed augmentations represent common image characteristics, a disadvantage of the RenderGAN framework is that these augmentation functions must be carefully customized for the application at hand to ensure that high-level information is preserved. Furthermore, a suitable 3D model must be available.\n7 FUTURE WORK\nFor future work, it would be interesting to see the RenderGAN framework used on other tasks where basic 3D models exist e.g. human faces, pose estimation, or viewpoint prediction. In this context, one could come up with different augmentation functions e.g. colorization, affine transformations, or diffeomorphism. The RenderGAN could be especially valuable to domains where pre-trained models are not available or when the annotations are very complex. Another direction of future work might be to extend the RenderGAN framework to other fields. For example, in speech synthesis, one could use an existing software as a basic model and improve the realism of the output with a similar approach as in the RenderGAN framework.\nB HANDMADE AUGMENTATIONS\nWe constructed augmentations for blur, lighting, background, noise and spotlights manually. For synthesizing lighting, background and noise, we use image pyramids, i.e. a set of images L0, . . . , L6 of size (2i \u00d7 2i) for 0 \u2264 i \u2264 6. Each level Li in the pyramid is weighted by a scalar \u03c9i. Each pixel of the different level Li is drawn from N (0, 1). The generated image I6 is given by:\nI0 = \u03c90L0 (10) Ii = \u03c9iLi + upscale(Ii\u22121) (11)\n, where upscale doubles the image dimensions. The pyramid enables us to generate random images while controlling their frequency domain by weighting the pyramid levels appropriately.\n\u2022 Blur: Gaussian blur with randomly sampled scale. \u2022 Lighting: Similar as in the RenderGAN. Here, the scaling of the white and black parts\nand shifting is constructed with image pyramids. \u2022 Background: image pyramids with the lower levels weight more. \u2022 Noise: image pyramids with only the last two layer. \u2022 Spotlights: overlay with possible multiple 2D Gauss function with a random position on\nthe tag and random covariance.\nWe selected all parameters manually by comparing the generated to real images. However, using slightly more unrealistic images resulted in better performance of the DCNN trained with the HM 3D data. The parameters of the handmade augmentations can be found online in our source code repository.\nC AUGMENTATIONS OF THE REAL DATA\nWe scale and shift the pixel intensities randomly, i.e. sI+t, where I is the image and s, t are scalars. The noise is sampled for each pixel fromN (0, ), where \u223c max(0, N (\u00b5n, \u03c3n)) is drawn for each image separately. Different affine transformations (rotation, scale, translation, and shear) are used.\nD TRAINING SAMPLES\n", "cats": {"READ": 1.0, "DONTREAD": 0.0}, "reviews": [{"IS_META_REVIEW": true, "comments": "This paper addresses the problem of decoding barcode-like markers depicted in an image.  The main insight is to train a CNN from generated data produced from a GAN.  The GAN is trained using unlabeled images, and leverages a \"3D model\" that undergoes learnt image transformations (e.g., blur, lighting, background).  The parameters for the image transformations are trained such that it confuses a GAN discriminator.  A CNN is trained using images generated from the GAN and compared with hand-crafted features and from training with real images.  The proposed method out-performs both baselines on decoding the barcode markers.\n\nThe proposed GAN architecture could potentially be interesting.  However, I won\u2019t champion the paper as the evaluation could be improved.\n\nA critical missing baseline is a comparison against a generic GAN.  Without this it\u2019s hard to judge the benefit of the more structured GAN.  Also, it would be worth seeing the result when one combines generated and real images for the final task. \n\nA couple of references that are relevant to this work (for object detection using rendered views of 3D shapes):\n\n[A] Xingchao Peng, Baochen Sun, Karim Ali, Kate Saenko, Learning Deep Object Detectors from 3D Models; ICCV, 2015.\n\n[B] Deep Exemplar 2D-3D Detection by Adapting from Real to Rendered Views. Francisco Massa, Bryan C. Russell, Mathieu Aubry. CVPR 2016.\n\nThe problem domain (decoding barcode markers on bees) is limited.  It would be great to see this applied to another problem domain, e.g., object detection from 3D models as shown in paper reference [A], where direct comparison against prior work could be performed.  \n\nI found the writing to be somewhat vague throughout.  For instance, on first reading of the introduction it is not clear what exactly is the contribution of the paper.  \n\nMinor comments:\n\nFig 3 - Are these really renders from a 3D model?  The images look like 2D images, perhaps spatially warped via a homography.  \n\nPage 3: \"chapter\" => \"section\".\n\nIn Table 2, what is the loss used for the DCNN?\n\nFig 9 (a) - The last four images look like they have strange artifacts. Can you explain these?"}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "This paper was fairly well received by the reviewers in terms of the underling idea but the fact that a very specialized problem was the focus of the paper held back reviewers from giving stronger ratings. The question of what sorts of baselines would be reasonable was discussed extensively as the reviewers felt that other credible baselines should be included. The authors argue certain baselines are not appropriate but they were not able to clearly sway the reviewers to a more positive rating based on their response to this issue. We recommend a workshop invitation for this paper.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"DATE": "12 Jan 2017", "TITLE": "Updated paper", "IS_META_REVIEW": false, "comments": "We uploaded a new version of the paper based on the peer review feedback.\n\n* Clarified that an unconstrained GAN is not a suitable baseline.\n* Added additional references pointed out by reviewer 2.\n* Extracted a related work section to improve the overall clarity and also stated our contribution explicitly.\n* Various modifications to improve clarity.", "OTHER_KEYS": "Leon Sixt"}, {"DATE": "05 Jan 2017", "TITLE": "General Rebuttal", "IS_META_REVIEW": false, "comments": "\nThank you very much for your reviews. Your feedback helped to improve the\nmanuscript significantly, and we are preparing a revised version of the\nmanuscript with changes outlined either below or in our responses to each\nreviewer. Multiple valid points of criticism were raised during the review\nprocess and have already been worked into the current version of the document.\nFor example, we included hand-designed augmentations for comparison with the\nlearned ones.\n\nHowever, in two of the three reviews there seems to be a major misunderstanding\nthat we would like to clarify here. Since this relates to the central finding of\nour paper, we would like to provide a detailed response to this point. We hope\nthat, in the light of this fact, the reviewer\u2019s rating of our contribution\u2019s\nimportance and novelty will be reconsidered.\n\n> Reviewer 2: \u201cA critical missing baseline is a comparison against a generic GAN.\n> Without this it\u2019s hard to judge the benefit of the more structured GAN.  Also,\n> it would be worth seeing the result when one combines generated and real images\n> for the final task.\u201d\n\n> Reviewer 3: \u201c [...] the proposed method is more model driven that previous GAN\n> models. But does it pay off? how would a traditional GAN approach perform? [...]\n> The answers of the authors only partially addresses the point. The key proposal\n> of the submission seems parameterised modules that can be trained to match the\n> real data distribution. but it remains unclear why not a more generic\n> parameterisation can also do the job. E.g. a neural network - as done in regular\n> GANs. The benefit of introducing a stronger model is unclear.\u201d\n\nThe main point of critique here is that a comparison with a generic GAN\n(Goodfellow et al. 2014) is missing. This comment implies that both methods (GAN\nand RenderGAN) share the same task domain, which is incorrect. The task we\naddress is generating _labeled_ data. We emphasize that we do not refer to the\nbinary class label but rather to higher dimensional labels. In our example\nscenario, this corresponds to images of bee markers and their respective bit\nconfiguration (its ID) and rotations in 3D space. A generic GAN cannot generate\nlabels, it learns to generate realistic images _without_ labels. Ultimately, we\nwant to train a convnet (\u2018decoder network\u2019) in a supervised setting to map an\nimage to its respective labels. Thus, we need labeled samples and hence,\na conventional GAN cannot be used as a baseline! Stated formally: the RenderGAN\nsamples from the joint distribution p(l, x) of labels l and data x whereas\na conventional GAN can only sample from the data distribution p(x).\n\nThere are two alternative approaches to our RenderGAN, one being a conventional\n3-dimensional rendering pipeline that can be used to generate images of bee\nmarkers with known ID and spatial orientation. Secondly, one could train the\ndecoder network with manually labeled data. Both approaches have been\nimplemented and tested against the RenderGAN and do not perform satisfyingly. To\nimprove both alternatives\u2019 performance one would need to either tune the\nrendering pipeline to match the details of the real world imaging process, or\nlabel more data manually. Both measures are time-consuming and do not generalize\nwell when changing parts of the imaging process (lighting, cameras, compression,\netc.) or the marker design.\n\nOur approach is to extend a generic GAN by adding several network modules, the\nfirst being the network equivalent of a simple 3D model. Secondly, we learn\na number of parameterised augmentation functions. We would like to point out\nthat this approach was _not_ chosen to improve the generative capabilities of\nthe network but to constrain it in such a way that the image produced by the GAN\nis correct with respect to the labels fed into the network. In our use case,\neach image produced by the GAN has to preserve the given bit pattern and\nrotation in space provided by the 3D model for the labels to remain valid. This\npoint was already addressed in our paper and the pre-review questions:\n\n> Paper Introduction: \u201c[...] We constrain the augmentation of the images such that\n> the high-level information represented by the 3D model is preserved. The\n> RenderGAN framework allows us to generate images of which the labels are known\n> from the 3D model, and that also look strikingly real due to the GAN framework.\n> The training procedure of the RenderGAN framework does not require any labels.\n> We can generate high-quality, labeled data with a simple 3D model and a large\n> amount of unlabeled data.\u201d\n\n> Our reply to Reviewer 3: \u201c[...] The payoff is that we can generate labeled data\n> with only a simple 3D model and unlabeled data. You are right. A DCGAN\n> architecture can model all mentioned effects, even affine transformations. We\n> trained a DCGAN on the data, and the quality of the synthesized images is\n> similar. However, no labels can be collected in the conventional GAN framework.\n> [...]\u201d\n\nAll reviewers question the necessity of the constraints we introduced. One of\nour early approaches was to add an offset to the 3D model, i.e. x = t + g(t)\nwhere x is the synthesized image, t is an image from the 3D model, and g an\nunconstrained generator. However, in our experiments, the generator learned to\nsynthesize realistic images but ignored the given template t completely. Thus,\nno valid labels of the synthetic images could be collected. Since a decoder\nnetwork cannot be trained without labels, this approach cannot be used as\na baseline. We will revise our paper to clarify that an unconstrained GAN is not\na suitable baseline for our task.\n", "OTHER_KEYS": "Leon Sixt"}, {"TITLE": "The proposed model has potential merits, but the paper is missing a critical baseline in the evaluation.", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "This paper addresses the problem of decoding barcode-like markers depicted in an image.  The main insight is to train a CNN from generated data produced from a GAN.  The GAN is trained using unlabeled images, and leverages a \"3D model\" that undergoes learnt image transformations (e.g., blur, lighting, background).  The parameters for the image transformations are trained such that it confuses a GAN discriminator.  A CNN is trained using images generated from the GAN and compared with hand-crafted features and from training with real images.  The proposed method out-performs both baselines on decoding the barcode markers.\n\nThe proposed GAN architecture could potentially be interesting.  However, I won\u2019t champion the paper as the evaluation could be improved.\n\nA critical missing baseline is a comparison against a generic GAN.  Without this it\u2019s hard to judge the benefit of the more structured GAN.  Also, it would be worth seeing the result when one combines generated and real images for the final task. \n\nA couple of references that are relevant to this work (for object detection using rendered views of 3D shapes):\n\n[A] Xingchao Peng, Baochen Sun, Karim Ali, Kate Saenko, Learning Deep Object Detectors from 3D Models; ICCV, 2015.\n\n[B] Deep Exemplar 2D-3D Detection by Adapting from Real to Rendered Views. Francisco Massa, Bryan C. Russell, Mathieu Aubry. CVPR 2016.\n\nThe problem domain (decoding barcode markers on bees) is limited.  It would be great to see this applied to another problem domain, e.g., object detection from 3D models as shown in paper reference [A], where direct comparison against prior work could be performed.  \n\nI found the writing to be somewhat vague throughout.  For instance, on first reading of the introduction it is not clear what exactly is the contribution of the paper.  \n\nMinor comments:\n\nFig 3 - Are these really renders from a 3D model?  The images look like 2D images, perhaps spatially warped via a homography.  \n\nPage 3: \"chapter\" => \"section\".\n\nIn Table 2, what is the loss used for the DCNN?\n\nFig 9 (a) - The last four images look like they have strange artifacts. Can you explain these?\n", "IS_META_REVIEW": false, "RECOMMENDATION": 5, "DATE": "17 Dec 2016", "REVIEWER_CONFIDENCE": 3}, {"TITLE": "Review", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "The paper proposes an approach to generating synthetic training data for deep networks, based on rendering 3D models and learning additional transformations with adversarial training. The approach is applied to generating barcode-like markers used for honeybee identification. The authors demonstrate that a classifier trained on synthetic data generated with the proposed approach outperforms both training on (limited) real data and training on data with hand-designed augmentations. \n\nThe topic of the paper \u2014 using machine learning (in particular, adversarial training) for generating realistic synthetic training data \u2014 is very interesting and important. The proposed method looks reasonable, and the paper is written well. The downside is that experiments are limited to a fairly simple and not-widely-known domain of honeybee marker classification. While I am sure this is an important task by itself, in order to demonstrate general applicability of the method and to allow comparison with existing techniques, experiments on some standard and/or realistic datasets would be very helpful. Overall, I recommend acceptance, but encourage the authors to perform experiments on more datasets.\n\nI appreciate that the authors added a baseline with manually designed transformations. This strengthens the paper.\n\nAs Reviewer3 points out, it would be interesting to analyze if restricting GAN to a fixed set of transformations is necessary here, and which transformations are most important. Perhaps this would provide some guidelines for designing sets of transformations for more complicated scenarios.\n\nThe authors should tone down their claims such as \u201cOur method is an improvement over previous work  <...> Whereas previous work relied on real data for training using pre-trained models or mixing real and generated data, we were able to train a DCNN from scratch with generated data that performed well when tested on real data. \u201c. This is not a fair comparison: the domain studied by authors in this work is much simpler than what was studied in these previous works, so this comparison is not appropriate.", "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "RenderGAN: Generating Realistic Labeled Data", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "The submission proposes an interesting way to match synthetic data to real data in a GAN type architecture.\nThe main novelty are parametric modules that emulate different transformations and artefact that allow to match the natural appearance.\n\nseveral points were raised during the discussion:\n\n1. the proposed method is more model driven that previous GAN models. But does it pay off? how would a traditional GAN approach perform? The mentioned effects like blur, lighting and background could also potentially be modelled by upsamling network that directly predicts the image. I would assume that blur and lighting can be modelled by convolutions. transformations to some extend by convolutions - or spatial transformer networks.\nThe answers of the authors only partially addresses the point. The key proposal of the submission seems parameterised modules that can be trained to match the real data distribution. but it remains unclear why not a more generic parameterisation can also do the job. E.g. a neural network - as done in regular GANs. The benefit of introducing a stronger model is unclear. Using a render engine to generate the initial sample appearance if of limited novelty.\n\n\n2. how does it compare to traditional data augmentation techniques, e.g. noise, dropout, transformations. you are linking to keras code - where data augmentation is readily available and could be tested (ImageDataGenerator)\nThe authors reply that plenty of such augmentation was used and more details are going to be provided in the appendix. it would have been appreciated if such information was directly included in the revision - so that the procedure could be directly checked. right now - this remains a point of uncertainty.\n\n3. How do the different stages (\\phis) effect performance? which are the most important ones?\nThe authors do evaluate the effect of hand tuning the transformation stages vs. learning them. it would be great to also include results of including/excluding stages completely - and also reporting how much the initial jittering of the data helps.\n\nWhile there is an interesting idea of (limited) novelty to the paper, there are some concerns about evalations and comparisons as outlined above. In addition, only success on a single dataset/task is shown. Yet the task is interesting and seems challenging. Overall, this remains makes only a weak recommendation for acceptance.", "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"DATE": "09 Dec 2016", "TITLE": "Update paper", "IS_META_REVIEW": false, "comments": "We updated our paper based on the feedback from the pre-review questions.  We\nincluded handmade augmentation in the evaluation.  We also retrained the DCNN on\nthe real data. Thanks for the feedback.", "OTHER_KEYS": "Leon Sixt"}, {"DATE": "02 Dec 2016", "TITLE": "pre-review questions", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3"}, {"DATE": "02 Dec 2016", "TITLE": "pre-review questions", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1"}, {"IS_META_REVIEW": true, "comments": "This paper addresses the problem of decoding barcode-like markers depicted in an image.  The main insight is to train a CNN from generated data produced from a GAN.  The GAN is trained using unlabeled images, and leverages a \"3D model\" that undergoes learnt image transformations (e.g., blur, lighting, background).  The parameters for the image transformations are trained such that it confuses a GAN discriminator.  A CNN is trained using images generated from the GAN and compared with hand-crafted features and from training with real images.  The proposed method out-performs both baselines on decoding the barcode markers.\n\nThe proposed GAN architecture could potentially be interesting.  However, I won\u2019t champion the paper as the evaluation could be improved.\n\nA critical missing baseline is a comparison against a generic GAN.  Without this it\u2019s hard to judge the benefit of the more structured GAN.  Also, it would be worth seeing the result when one combines generated and real images for the final task. \n\nA couple of references that are relevant to this work (for object detection using rendered views of 3D shapes):\n\n[A] Xingchao Peng, Baochen Sun, Karim Ali, Kate Saenko, Learning Deep Object Detectors from 3D Models; ICCV, 2015.\n\n[B] Deep Exemplar 2D-3D Detection by Adapting from Real to Rendered Views. Francisco Massa, Bryan C. Russell, Mathieu Aubry. CVPR 2016.\n\nThe problem domain (decoding barcode markers on bees) is limited.  It would be great to see this applied to another problem domain, e.g., object detection from 3D models as shown in paper reference [A], where direct comparison against prior work could be performed.  \n\nI found the writing to be somewhat vague throughout.  For instance, on first reading of the introduction it is not clear what exactly is the contribution of the paper.  \n\nMinor comments:\n\nFig 3 - Are these really renders from a 3D model?  The images look like 2D images, perhaps spatially warped via a homography.  \n\nPage 3: \"chapter\" => \"section\".\n\nIn Table 2, what is the loss used for the DCNN?\n\nFig 9 (a) - The last four images look like they have strange artifacts. Can you explain these?"}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "This paper was fairly well received by the reviewers in terms of the underling idea but the fact that a very specialized problem was the focus of the paper held back reviewers from giving stronger ratings. The question of what sorts of baselines would be reasonable was discussed extensively as the reviewers felt that other credible baselines should be included. The authors argue certain baselines are not appropriate but they were not able to clearly sway the reviewers to a more positive rating based on their response to this issue. We recommend a workshop invitation for this paper.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"DATE": "12 Jan 2017", "TITLE": "Updated paper", "IS_META_REVIEW": false, "comments": "We uploaded a new version of the paper based on the peer review feedback.\n\n* Clarified that an unconstrained GAN is not a suitable baseline.\n* Added additional references pointed out by reviewer 2.\n* Extracted a related work section to improve the overall clarity and also stated our contribution explicitly.\n* Various modifications to improve clarity.", "OTHER_KEYS": "Leon Sixt"}, {"DATE": "05 Jan 2017", "TITLE": "General Rebuttal", "IS_META_REVIEW": false, "comments": "\nThank you very much for your reviews. Your feedback helped to improve the\nmanuscript significantly, and we are preparing a revised version of the\nmanuscript with changes outlined either below or in our responses to each\nreviewer. Multiple valid points of criticism were raised during the review\nprocess and have already been worked into the current version of the document.\nFor example, we included hand-designed augmentations for comparison with the\nlearned ones.\n\nHowever, in two of the three reviews there seems to be a major misunderstanding\nthat we would like to clarify here. Since this relates to the central finding of\nour paper, we would like to provide a detailed response to this point. We hope\nthat, in the light of this fact, the reviewer\u2019s rating of our contribution\u2019s\nimportance and novelty will be reconsidered.\n\n> Reviewer 2: \u201cA critical missing baseline is a comparison against a generic GAN.\n> Without this it\u2019s hard to judge the benefit of the more structured GAN.  Also,\n> it would be worth seeing the result when one combines generated and real images\n> for the final task.\u201d\n\n> Reviewer 3: \u201c [...] the proposed method is more model driven that previous GAN\n> models. But does it pay off? how would a traditional GAN approach perform? [...]\n> The answers of the authors only partially addresses the point. The key proposal\n> of the submission seems parameterised modules that can be trained to match the\n> real data distribution. but it remains unclear why not a more generic\n> parameterisation can also do the job. E.g. a neural network - as done in regular\n> GANs. The benefit of introducing a stronger model is unclear.\u201d\n\nThe main point of critique here is that a comparison with a generic GAN\n(Goodfellow et al. 2014) is missing. This comment implies that both methods (GAN\nand RenderGAN) share the same task domain, which is incorrect. The task we\naddress is generating _labeled_ data. We emphasize that we do not refer to the\nbinary class label but rather to higher dimensional labels. In our example\nscenario, this corresponds to images of bee markers and their respective bit\nconfiguration (its ID) and rotations in 3D space. A generic GAN cannot generate\nlabels, it learns to generate realistic images _without_ labels. Ultimately, we\nwant to train a convnet (\u2018decoder network\u2019) in a supervised setting to map an\nimage to its respective labels. Thus, we need labeled samples and hence,\na conventional GAN cannot be used as a baseline! Stated formally: the RenderGAN\nsamples from the joint distribution p(l, x) of labels l and data x whereas\na conventional GAN can only sample from the data distribution p(x).\n\nThere are two alternative approaches to our RenderGAN, one being a conventional\n3-dimensional rendering pipeline that can be used to generate images of bee\nmarkers with known ID and spatial orientation. Secondly, one could train the\ndecoder network with manually labeled data. Both approaches have been\nimplemented and tested against the RenderGAN and do not perform satisfyingly. To\nimprove both alternatives\u2019 performance one would need to either tune the\nrendering pipeline to match the details of the real world imaging process, or\nlabel more data manually. Both measures are time-consuming and do not generalize\nwell when changing parts of the imaging process (lighting, cameras, compression,\netc.) or the marker design.\n\nOur approach is to extend a generic GAN by adding several network modules, the\nfirst being the network equivalent of a simple 3D model. Secondly, we learn\na number of parameterised augmentation functions. We would like to point out\nthat this approach was _not_ chosen to improve the generative capabilities of\nthe network but to constrain it in such a way that the image produced by the GAN\nis correct with respect to the labels fed into the network. In our use case,\neach image produced by the GAN has to preserve the given bit pattern and\nrotation in space provided by the 3D model for the labels to remain valid. This\npoint was already addressed in our paper and the pre-review questions:\n\n> Paper Introduction: \u201c[...] We constrain the augmentation of the images such that\n> the high-level information represented by the 3D model is preserved. The\n> RenderGAN framework allows us to generate images of which the labels are known\n> from the 3D model, and that also look strikingly real due to the GAN framework.\n> The training procedure of the RenderGAN framework does not require any labels.\n> We can generate high-quality, labeled data with a simple 3D model and a large\n> amount of unlabeled data.\u201d\n\n> Our reply to Reviewer 3: \u201c[...] The payoff is that we can generate labeled data\n> with only a simple 3D model and unlabeled data. You are right. A DCGAN\n> architecture can model all mentioned effects, even affine transformations. We\n> trained a DCGAN on the data, and the quality of the synthesized images is\n> similar. However, no labels can be collected in the conventional GAN framework.\n> [...]\u201d\n\nAll reviewers question the necessity of the constraints we introduced. One of\nour early approaches was to add an offset to the 3D model, i.e. x = t + g(t)\nwhere x is the synthesized image, t is an image from the 3D model, and g an\nunconstrained generator. However, in our experiments, the generator learned to\nsynthesize realistic images but ignored the given template t completely. Thus,\nno valid labels of the synthetic images could be collected. Since a decoder\nnetwork cannot be trained without labels, this approach cannot be used as\na baseline. We will revise our paper to clarify that an unconstrained GAN is not\na suitable baseline for our task.\n", "OTHER_KEYS": "Leon Sixt"}, {"TITLE": "The proposed model has potential merits, but the paper is missing a critical baseline in the evaluation.", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "This paper addresses the problem of decoding barcode-like markers depicted in an image.  The main insight is to train a CNN from generated data produced from a GAN.  The GAN is trained using unlabeled images, and leverages a \"3D model\" that undergoes learnt image transformations (e.g., blur, lighting, background).  The parameters for the image transformations are trained such that it confuses a GAN discriminator.  A CNN is trained using images generated from the GAN and compared with hand-crafted features and from training with real images.  The proposed method out-performs both baselines on decoding the barcode markers.\n\nThe proposed GAN architecture could potentially be interesting.  However, I won\u2019t champion the paper as the evaluation could be improved.\n\nA critical missing baseline is a comparison against a generic GAN.  Without this it\u2019s hard to judge the benefit of the more structured GAN.  Also, it would be worth seeing the result when one combines generated and real images for the final task. \n\nA couple of references that are relevant to this work (for object detection using rendered views of 3D shapes):\n\n[A] Xingchao Peng, Baochen Sun, Karim Ali, Kate Saenko, Learning Deep Object Detectors from 3D Models; ICCV, 2015.\n\n[B] Deep Exemplar 2D-3D Detection by Adapting from Real to Rendered Views. Francisco Massa, Bryan C. Russell, Mathieu Aubry. CVPR 2016.\n\nThe problem domain (decoding barcode markers on bees) is limited.  It would be great to see this applied to another problem domain, e.g., object detection from 3D models as shown in paper reference [A], where direct comparison against prior work could be performed.  \n\nI found the writing to be somewhat vague throughout.  For instance, on first reading of the introduction it is not clear what exactly is the contribution of the paper.  \n\nMinor comments:\n\nFig 3 - Are these really renders from a 3D model?  The images look like 2D images, perhaps spatially warped via a homography.  \n\nPage 3: \"chapter\" => \"section\".\n\nIn Table 2, what is the loss used for the DCNN?\n\nFig 9 (a) - The last four images look like they have strange artifacts. Can you explain these?\n", "IS_META_REVIEW": false, "RECOMMENDATION": 5, "DATE": "17 Dec 2016", "REVIEWER_CONFIDENCE": 3}, {"TITLE": "Review", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "The paper proposes an approach to generating synthetic training data for deep networks, based on rendering 3D models and learning additional transformations with adversarial training. The approach is applied to generating barcode-like markers used for honeybee identification. The authors demonstrate that a classifier trained on synthetic data generated with the proposed approach outperforms both training on (limited) real data and training on data with hand-designed augmentations. \n\nThe topic of the paper \u2014 using machine learning (in particular, adversarial training) for generating realistic synthetic training data \u2014 is very interesting and important. The proposed method looks reasonable, and the paper is written well. The downside is that experiments are limited to a fairly simple and not-widely-known domain of honeybee marker classification. While I am sure this is an important task by itself, in order to demonstrate general applicability of the method and to allow comparison with existing techniques, experiments on some standard and/or realistic datasets would be very helpful. Overall, I recommend acceptance, but encourage the authors to perform experiments on more datasets.\n\nI appreciate that the authors added a baseline with manually designed transformations. This strengthens the paper.\n\nAs Reviewer3 points out, it would be interesting to analyze if restricting GAN to a fixed set of transformations is necessary here, and which transformations are most important. Perhaps this would provide some guidelines for designing sets of transformations for more complicated scenarios.\n\nThe authors should tone down their claims such as \u201cOur method is an improvement over previous work  <...> Whereas previous work relied on real data for training using pre-trained models or mixing real and generated data, we were able to train a DCNN from scratch with generated data that performed well when tested on real data. \u201c. This is not a fair comparison: the domain studied by authors in this work is much simpler than what was studied in these previous works, so this comparison is not appropriate.", "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "RenderGAN: Generating Realistic Labeled Data", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "The submission proposes an interesting way to match synthetic data to real data in a GAN type architecture.\nThe main novelty are parametric modules that emulate different transformations and artefact that allow to match the natural appearance.\n\nseveral points were raised during the discussion:\n\n1. the proposed method is more model driven that previous GAN models. But does it pay off? how would a traditional GAN approach perform? The mentioned effects like blur, lighting and background could also potentially be modelled by upsamling network that directly predicts the image. I would assume that blur and lighting can be modelled by convolutions. transformations to some extend by convolutions - or spatial transformer networks.\nThe answers of the authors only partially addresses the point. The key proposal of the submission seems parameterised modules that can be trained to match the real data distribution. but it remains unclear why not a more generic parameterisation can also do the job. E.g. a neural network - as done in regular GANs. The benefit of introducing a stronger model is unclear. Using a render engine to generate the initial sample appearance if of limited novelty.\n\n\n2. how does it compare to traditional data augmentation techniques, e.g. noise, dropout, transformations. you are linking to keras code - where data augmentation is readily available and could be tested (ImageDataGenerator)\nThe authors reply that plenty of such augmentation was used and more details are going to be provided in the appendix. it would have been appreciated if such information was directly included in the revision - so that the procedure could be directly checked. right now - this remains a point of uncertainty.\n\n3. How do the different stages (\\phis) effect performance? which are the most important ones?\nThe authors do evaluate the effect of hand tuning the transformation stages vs. learning them. it would be great to also include results of including/excluding stages completely - and also reporting how much the initial jittering of the data helps.\n\nWhile there is an interesting idea of (limited) novelty to the paper, there are some concerns about evalations and comparisons as outlined above. In addition, only success on a single dataset/task is shown. Yet the task is interesting and seems challenging. Overall, this remains makes only a weak recommendation for acceptance.", "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"DATE": "09 Dec 2016", "TITLE": "Update paper", "IS_META_REVIEW": false, "comments": "We updated our paper based on the feedback from the pre-review questions.  We\nincluded handmade augmentation in the evaluation.  We also retrained the DCNN on\nthe real data. Thanks for the feedback.", "OTHER_KEYS": "Leon Sixt"}, {"DATE": "02 Dec 2016", "TITLE": "pre-review questions", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3"}, {"DATE": "02 Dec 2016", "TITLE": "pre-review questions", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1"}]}
{"text": "Prior work has demonstrated that exploiting the sparsity can dramatically improve the energy efficiency and reduce the memory footprint of Convolutional Neural Networks (CNNs). However, these sparsity-centric optimization techniques might be less effective for Long Short-Term Memory (LSTM) based Recurrent Neural Networks (RNNs), especially for the training phase, because of the significant structural difference between the neurons. To investigate if there is possible sparsity-centric optimization for training LSTM-based RNNs, we studied several applications and observed that there is potential sparsity in the gradients generated in the backward propagation. In this paper, we investigate why the sparsity exists and propose a simple yet effective thresholding technique to induce further more sparsity during the LSTM-based RNN training. The experimental results show that the proposed technique can increase the sparsity of linear gate gradients to more than 80% without loss of performance, which makes more than 50% multiply-accumulate (MAC) operations redundant for the entire LSTM training process. These redundant MAC operations can be eliminated by hardware techniques to improve the energy efficiency and the training speed of LSTM-based RNNs.\n1 INTRODUCTION\nDeep neural networks have achieved state-of-the-art performance in many different tasks, such as computer vision (Krizhevsky et al., 2012) (Simonyan & Zisserman, 2015), speech recognition, and natural language processing (Karpathy et al., 2016). The underlying representational power of these neural networks comes from the huge parameter space, which results in an extremely large amount of computation operations and memory footprint. To reduce the memory usage and accelerate the training process, the research community has strived to eliminate the redundancy in the deep neural networks (Han et al., 2016b). Exploiting the sparsity in both weights and activations of Convolutional Neural Networks (CNNs), sparsity-centric optimization techniques (Han et al., 2016a) (Albericio et al., 2016) have been proposed to improve the speed and energy efficiency of CNN accelerators.\nThese sparsity-centric approaches can be classified into two categories: (1) pruning unimportant weight parameters and (2) skipping zero values in activations to eliminate multiply-accumulate (MAC) operations with zero operands. Although both categories have achieved promising results for CNNs, it remains unclear if they are applicable to training other neural networks, such as LSTMbased RNNs. The network pruning approach is not suitable for training because it only benefits the inference phase of neural networks by iteratively pruning and re-training. The approach that exploits the sparsity in the activations can be used for training because the activations are involved in both\nthe forward propagation and the backward propagation. But there are still some issues if we directly apply it to LSTM-based RNNs.\nThe sparsity in CNN activations mostly comes from the Rectified Linear Unit (ReLU) activation function, which sets all negative values to zero. However, Long Short-Term Memory, one of the most popular RNN cells, does not adopt the ReLU function. Therefore, LSTM should exhibit much less sparsity in activations than CNNs, intuitively. Furthermore, the structure of an LSTM cell is much more complicated than neurons in convolutional layers or fully connected layers of a CNN.\nTo explore additional opportunities to apply sparsity-centric optimization to LSTM-based RNNs, we conducted an application characterization on several LSTM-based RNN applications, including character-based language model, image captioning, and machine translation. Although the experimental results of the application characterization show that there is little sparsity in the activations, we observed potential sparsity in backward propagation of the LSTM training process. The activation values of the gates (input gate, forget gate, and output gate) and the new cell state exhibit a skewed distribution due to their functionality. That is, a large fraction of the activation values of these Sigmoid-based gates are either close to 1 or close to 0 (for the Tanh-based new cell activations, values are close to -1 or 1). This skewed distribution will lead to a considerable amount of very small values in the LSTM backward propagation since there is a term \u03c3(x)(1\u2212 \u03c3(x)) in the gradients of the Sigmoid-based gates (tanh(x)(1\u2212 tanh(x)) for the gradients of the new cell gradients), which will be zero given \u03c3(x) = 0 or \u03c3(x) = 1 (tanh(x) = \u22121 or tanh(x) = 1 for the new cell gradients). In real-world implementations, these very small values might be clamped to zero as they are in the form of floating-point numbers, of which the precision is limited. Therefore, there is potential sparsity in the gradients of the backward propagation of LSTM training.\nTo ensure that there is non-trivial amount of sparsity for hardware designers to exploit, we propose \u201csparsified\u201d SGD, a rounding to zero technique to induce more sparsity in the gradients. This approach can be seen as a stochastic gradient descent (SGD) learning algorithm with sparsifying, which strips the precision of floating point numbers for unimportant small gradients. Experiment results show that with proper thresholds, we can make 80% of the gradients of the gate inputs to zero without performance loss for all applications and datasets we tested so far. As the sparse gradients of the gate inputs are involved in 67% matrix multiplications, more than 50% MAC operations are redundant in the entire LSTM training process. Eliminating these ineffectual MAC operations with hardware techniques, the energy efficiency and training speed of LSTM-based RNNs will be improved significantly.\n2 BACKGROUND AND MOTIVATION\nIn this section, we first review some of the prior work on sparsity-centric optimization techniques for neural networks, and then illustrate the application characterization example as the motivation for our research.\n2.1 SPARSITY-CENTRIC OPTIMIZATION FOR NEURAL NETWORKS\nIt has been demonstrated that there is significant redundancy in the parameterization of deep neural networks (Denil et al., 2013). Consequently, the over-sized parameter space results in sparsity in the weight parameters of a neural network. Besides the parameters, there is also sparsity in the activations of each layer in a network, which comes from two sources: (1) the sparsity in weight parameters and (2) the activation function of neurons, such as ReLU.\nAs the sparsity in weight parameters do not depend on the input data, it is often referred to as static sparsity. On the other hand, the sparsity in the activations depend on not only the weight values but also the input data. Therefore, we refer to the sparsity in the activations as dynamic sparsity.\nExploiting sparsity can dramatically reduce the network size and thus improve the computing performance and energy efficiency. For example, Deep Compression (Han et al., 2016b) applied network pruning to CNNs to significantly reduce the footprint of the weights, which enables us to store all the weights on SRAM. However, the static sparsity can only help the inference phase but not training because weight parameters are adjusted during training. Fortunately, leveraging the dynamic sparsity can benefit both inference and training of neural networks. Recent publications (Han et al.,\n2016a) (Albericio et al., 2016) have proposed various approaches to eliminate ineffectual MAC operations with zero operands. Although these sparsity-centric optimization approaches have achieved promising results on CNNs, much less attention has been paid to LSTM-based RNNs, because there is a common belief that the major source of sparsity is the ReLU function, which is widely used in the convolutional layers but not in LSTM-based RNNs. To accelerate LSTM-based RNNs and improve the energy efficiency, we investigate opportunities to exploit sparsity in the LSTM-based RNN training process. As an initial step, in this paper we focus on the basic LSTM cell without peephole or other advanced features, as shown in Figure 1.\n2.2 APPLICATION CHARACTERIZATION\nTo reveal if there is sparsity in LSTM training, we conduct an application characterization study. We start with a character-based language model as described in (Karpathy et al., 2016). This characterbased language model takes a sequence of characters as input and predicts the next character of this sequence. The characters are represented in one-hot vectors, which are transformed into distributed vectors by a word2vec layer. Then the distributed vectors feed into an RNN model based on LSTM cells, followed by a linear classifier.\nThe LSTM cells used in this character-based language model are all basic LSTM cells. For each cell, the forward propagation flow is as below:\nit = \u03c3(W ixt + U iht\u22121 + b i)\nft = \u03c3(W fxt + U fht\u22121 + b f )\not = \u03c3(W oxt + U oht\u22121 + b o)\ngt = tanh(W gxt + U ght\u22121 + b g)\nct = ft \u25e6 ct\u22121 + it \u25e6 gt ht = ot \u25e6 tanh(ct)\nAs shown in Figure 1, it, ft, and ot stand for input gate, forget gate, and output gate, respectively. These sigmoid-based gates (\u03c3 stands for sigmoid) are used to prevent irrelevant input from affecting the memory cell (ct). The new cell state (gt) is a preliminary summary of the current input from the previous layer and the previous status of current layer. The final hidden status ht is the output of the LSTM cell if it is seen as a black box.\nSince the gates are introduced to prevent irrelevant inputs from affecting the memory cell ct, we have a hypothesis that a large fraction of the activations of these gates should be either close to 1 or close to 0, representing the control signal on or off, respectively. Similarly, the tanh-based new cell status is active if its activation is 1 or inactive if it is -1. There should also be a considerable portion of the activations close to 1 or -1.\nTo validate our hypothesis, we extracted the activations of the sigmoid-based gates and tanh-based new cell state from several model snapshots during training the character-based language model. Figure 2 shows the histogram of the activation values of the gates and the new cell. The red curves represent the activation values generated by a snapshot model which is 0.5% trained (in terms of total number of iterations) while the bars represent the activation values generated by a fully trained\nmodel. We can observe skewed distributions from each gate (and new cell) for both the 0.5% trained snapshot model and the fully trained model. Furthermore, the fully trained model shows a distribution that is more skewed to the leftmost and the rightmost. Additionally, other un-shown snapshots demonstrate that the distribution becomes consistently more skewed as the training process goes on. We also observed that after 10% of the training process, the distribution becomes steady, almost the same as the fully trained model.\nBesides the character-based language model, we also conducted the same characterization to the image captioning task described in (Karpathy & Li, 2015). The activation values of the RNN layer in the image captioning task exhibit the skewed distribution too. Even though we did not observe sparsity in the gate activations, the skewed distribution indicates potential sparsity in the LSTMbased RNN backward propagation, which will be shown in the next section.\n3 SPARSIFIED STOCHASTIC GRADIENT DESCENT FOR LSTM\nIn this section, we first show how the skewed distribution of gate values leads to potential sparsity in the LSTM backward propagation, and then we propose the \u201csparsified\u201d SGD to induce more sparsity in LSTM training.\n3.1 POTENTIAL SPARSITY IN LSTM BACKWARD PROPAGATION\nTo show how the skewed distribution in the gate activations results in potential sparsity in the LSTMbased RNN backward propagation, we need to review the forward and backward propagation at first. We can re-write the forward propagation equations as\nnet(i)t =W ixt + U iht\u22121 + b i\nnet(f)t =W fxt + U fht\u22121 + b f\nnet(o)t =W oxt + U oht\u22121 + b o\nnet(g)t =W gxt + U ght\u22121 + b g\nit = \u03c3(net(i)t)\nft = \u03c3(net(f)t)\not = \u03c3(net(o)t)\ngt = tanh(net(g)t)\nct = ft \u25e6 ct\u22121 + it \u25e6 gt ht = ot \u25e6 tanh(ct)\nHere we introduce variables net(i), net(f), net(o) and net(g) to represent the linear part of the gates and the new cell state. In GPU implementations such as cuDNN v5 (Appleyard et al., 2016), these linear gates (including new cell state from now on) are usually calculated in one step since they share the same input vectors xt and ht\u22121. Therefore we can use a uniform representation for the four linear gates, that is\nnett =Wxt + Uht\u22121 + b\nThe matrix W here stands for the combination of the matrices W i, W f , W o and W g and the matrix U stands for the combination of the matrices U i, Uf , Uo and Ug .\nWith these denotations, we can express the backward propagation as\ndot = dht \u25e6 tanh(ct)\ndct = dht \u25e6 (1\u2212 tanh2(ct)) \u25e6 ot + ft \u25e6 ct+1 dnet(g)t = dct \u25e6 it \u25e6 (1\u2212 g2t )\ndnet(o)t = dht \u25e6 tanh(ct) \u25e6 (1\u2212 ot) \u25e6 ot dnet(f)t = dct \u25e6 ct\u22121 \u25e6 (1\u2212 ft) \u25e6 ft dnet(i)t = dct \u25e6 gt \u25e6 (1\u2212 it) \u25e6 it\ndxt = dnettW T\ndht\u22121 = dnettU T\ndW+ = xtdnett\ndU+ = ht\u22121dnett\nIn the equations of the backward propagation, we use dnet to denote the gradient of the linear gates.\nFrom these equations we can see that for each linear gate gradient there is one term introduced by the sigmoid function or the tanh function, e.g. (1\u2212g2t ) in dnet(g)t and (1\u2212ot)\u25e6ot in dnet(o)t. As we observed in the application characterization results, the activation values of these gates exhibit skewed distribution, which means a large fraction of ot, ft and it are close to 0 or 1 (gt close to -1 or 1). The skewed distribution makes a large fraction of the linear gate gradients close to zero because (1 \u2212 g2t ), (1 \u2212 ot) \u25e6 ot, (1 \u2212 ft) \u25e6 ft and (1 \u2212 it) \u25e6 it are mostly close to zero given the skewed distribution of the gate activations.\nWhen implementing the LSTM-based RNNs, we usually use 32-bit floating point numbers to represent the gradients. Due to the precision limit, floating point numbers will round extremely small values to zero. Therefore, there is potential sparsity in dnet since a large fraction of the linear gate gradients are close to zero.\n3.2 INDUCING MORE SPARSITY\nIn the previous section we showed how the skewed distribution in gate activations results in potential sparsity in linear gate gradients theoretically. However, from mathematical perspective, there will be no sparsity in linear gate gradients if the floating point numbers in computers have infinite precision since they are only close to zero rather than be zero. Even the precision of 32-bit floating point numbers is not infinite, the 8-bit exponential part can still accommodate an extremely large dynamic range, which makes the sparsity less interesting to hardware accelerator designers. Fortunately, recent attempts to train neural networks with 16-bit floating points (Gupta et al., 2015) and fixed points (Lin et al., 2015) have shown acceptable performance with smaller dynamic range. This inspires us to induce more sparsity by rounding very small linear gate gradients to zero, which is similar to replace 32-bit floating points with 16-bit floating points or fixed points.\nThe intuition behind this \u201crounding to zero\u201d approach is that pruning CNNs will not affect the overall training performance. Similarly, thresholding very small gradient (dnet) values to zero is likely not to affect the overall training accuracy. Therefore, we propose a simple static thresholding approach which sets small dnet values below a threshold t to zero. By doing this, we can increase the sparsity in dnet even further than the original sparsity caused by limited dynamic range of floating\npoint numbers. With our static thresholding technique, the backward propagation of LSTM training becomes as below:\ndot = dht \u25e6 tanh(ct) dct = dht \u25e6 (1\u2212 tanh2(ct)) \u25e6 ot + ft \u25e6 ct+1\ndnet(g)t = dct \u25e6 it \u25e6 (1\u2212 g2t ) dnet(o)t = dht \u25e6 tanh(ct) \u25e6 (1\u2212 ot) \u25e6 ot dnet(f)t = dct \u25e6 ct\u22121 \u25e6 (1\u2212 ft) \u25e6 ft dnet(i)t = dct \u25e6 gt \u25e6 (1\u2212 it) \u25e6 it dnett = (dnett > t)?dnett : 0\ndxt = dnettW T\ndht\u22121 = dnettU T\ndW+ = xtdnett\ndU+ = ht\u22121dnett\nIn this \u201csparsified\u201d SGD backward propagation, a new hyper-parameter t is introduced to control the sparsity we would like to induce in dnet. Clearly, the optimal threshold t is the highest one that has no impact on the training performance since it can induce the highest sparsity in dnet. Therefore, to select the threshold, we need to monitor the impact on the gradients. As the SGD only uses the gradients of the weights (dW ) to update the weights, dW is the only gradients we need to care about. From the equations of the backward propagation we can see that dW is computed based on dnet, which is sparsified by our approach. Although sparsifying dnet affects dW , we can control the change of dW by setting the threshold. To determine the largest acceptable threshold, we conducted an evaluation of the impact caused by different thresholds on one single step in LSTM training. The application here is the same as the one in the application characterization.\nFigure 3 shows the evaluation result. We measure the change of dW by the normalized inner product of sparsified dW and the original dW without sparisifying (the baseline shown in Figure 3). If we denote the original weight gradient as dW0, the correlation between sparsified dW and dW0 can be measured by normalized inner product\ncorrelation = dW \u00b7 dW0\n||dW || \u00b7 ||dW0||\nIf the correlation is 1, it means dW is exactly the same to dW0. If the correlation is 0, it means dW is orthogonal to dW0. The higher the correlation is, the less impact the sparsification has on this single step backward propagation. From Figure 3 we can see that even without our thresholding technique, the dnet still exhibits approximately 10% sparsity. These zero values are resulted from the limited dynamic range of floating point numbers, in which extremely small values are rounded to zero. By applying the thresholds to dnet, we can induce more sparsity shown by the bars. Even with a low threshold (10\u22128), the sparsity in dnet is increased to about 45%. With a relatively high threshold (10\u22126), the sparsity can be increased to around 80%. Although the sparsity is high, the correlation between the sparsified dW and dW0 is close to 1 even with the high threshold. Therefore, we can hypothesize that we can safely induce a considerable amount of sparsity with an appropriate threshold. It is straightforward to understand that the threshold cannot be arbitrarily large since we need to contain the information of the gradients. For example, if we increase the threshold even further to 10\u22125, the correlation will drop to 0.26, which is far from the original dW0 and not acceptable.\nWe have demonstrated that we can induce more sparsity by rounding small dnet to zero while maintaining the information in dW . However, this is only an evaluation on one single iteration of training. To show the generality of our static thresholding approach, we applied the thresholds to the entire training process.\n4 AN ENTIRE LSTM TRAINING WITH SPARSIFIED SGD\nIn this section, we first present the sparsity induced by applying our sprsified SGD to an entire training process, and then discuss the generality of our approach.\n4.1 CHARACTER-BASED LANGUAGE MODEL\nTo validate our proposed static thresholding approach, we apply it to the entire LSTM-based RNN training process. We first conducted an experiment on training a character-based language model. The language model consists of one word2vec layer, three LSTM-based RNN layers, and one linear classifier layer. The number of LSTM cells per RNN layer is 256. We feed the network with sequences of 100 characters each. The training dataset is a truncated Wikipedia dataset. We apply a fixed threshold to all dnet gradients for every iteration during the whole training process.\nFigure 4 shows the sparsity of the linear gate gradients (dnet) of each layer during the whole training process. In the baseline configuration, the training method is standard SGD without sparsifying (zero\nthreshold). The baseline configuration exhibits about 20% sparsity in dnet. By applying only a low threshold (10\u22127), the sparsity is increased to around 70%. And we can consistently increase the sparsity further by raising the threshold. However, we have to monitor the impact of the threshold on the overall training performance to check if the threshold is too large to use.\nFigure 5 shows the validation loss of each iteration. We observe that up to the medium threshold (10\u22126), the validation loss of the model trained with sparsified SGD keeps close to the baseline. However, if we continues raising the threshold to 10\u22125, the validation loss becomes unacceptably higher than the baseline. Although the validation loss with the 10\u22125 threshold is consistently decreasing as the training goes on, we conservatively do not pick this configuration to train the LSTM network. So combining Figure 4 with Figure 5, we can choose the threshold 10\u22126 to train the character-based language model to achieve about 80% sparsity in dnet.\nSince the linear gate gradients dnet are involved in all the four matrix multiplications in the backward propagation, there are 80% MAC operations in these matrix multiplications have zero operands. Furthermore, there are six matrix multiplications (all of them are of the same amount of computation) in one LSTM training iteration and four out of them (67%) are sparse. So there are more than 50% MAC operations will have zero operands introduced by our sparsified SGD in one LSTM training iteration. The MAC operations with zero operands produce zero output and thus make no contribution to the final results. These redundant MAC operations can be eliminated by hardware techniques similar to (Han et al., 2016a) (Albericio et al., 2016) to improve the energy efficiency of LSTM training.\n4.2 SENSITIVITY TEST\nOur static thresholding approach can induce more than 80% sparsity in linear gate gradients of the character-based language model training. To demonstrate the generality of our approach, we then changed the topology of the RNN layers in the character-based language model with several different LSTM-based RNNs for a sensitivity test. The network topologies used in the sensitivity test are shown below.\n\u2022 Number of layers: 2, 3, 6, 9; \u2022 Number of LSTM cells per layer: 128, 256, 512; \u2022 Sequence length: 25, 50, 100.\nWe also trained the network with other datasets, such as the tiny-Shakespear dataset and the novel War and Peace. For all the data points we collected from the sensitivity test, we can always achieve\nmore than 80% sparsity in dnet with less than 1% loss of performance in terms of validation loss with respect to the baseline.\nMoreover, we also validated our approach by training an image captioning application (Karpathy & Li, 2015) with MSCOCO dataset (Lin et al., 2014) and a machine translation application known as Seq2Seq (Sutskever et al., 2014) with WMT15 dataset. As both the two applications are implemented based on graph model (Torch and TensorFlow, respectively), we plugged a custom operation in the automatically generated backward propagation subgraph to implement our proposed sparsified SGD. The experiment results show that the conclusion for the character-based language model still holds for the two applications.\n4.3 DISCUSSION\nSo far all our experiment results show promising results and we believe our sparsified SGD is a general approach to induce sparsity for LSTM-based RNN training. From the computer hardware perspective, the sparsified SGD is similar to reduced precision implementation while the impact of sparsified SGD is much less since we still use full 32-bit floating point numbers. From the theory perspective, SGD itself is a gradient descent with noise and thresholding very small gradients to zero is nothing more than an additional noise source. Since training with SGD is robust to noise, the thresholding approach will likely not affect the overall training performance. Additionally, the weight gradients dW are aggregated through many time steps, which makes the LSTM more robust to the noise introduce by sparsifying the linear gate gradients.\n5 CONCLUSION AND FUTURE WORK\nIn this paper, we conducted an application characterization to an LSTM-based RNN application and observe skewed distribution in the sigmoid-based gates and the tanh-based new cell state, which indicates potential sparsity in the linear gate gradients during backward propagation with SGD. The linear gate gradients are involved with 67% MAC operations in an entire LSTM training process so that we can improve the energy efficiency of hardware implementations if the linear gate gradients are sparse. We propose a simple yet effective rounding to zero technique, which can make the sparsity of the linear gate gradients higher than 80% without loss of performance. Therefore, more than 50% MAC operations are redundant in an entire sparsified LSTM training.\nObviously, the static-threshold approach is not optimal. In future, we will design a dynamicthreshold approach based on the learning rate, L2-norm of the gradients and the network topology. Hardware techniques will also be introduced to exploit the sparsity to improve the energy efficiency and training speed of LSTM-based RNNs for GPU and other hardware accelerators.\n", "cats": {"READ": 1.0, "DONTREAD": 0.0}, "reviews": [{"IS_META_REVIEW": true, "comments": "CONTRIBUTIONS\nWhen training LSTMs, many of the intermediate gradients are close to zero due to the flat shape of the tanh and sigmoid nonlinearities far from the origin. This paper shows that rounding these small gradients to zero results in matrices with up to 80% sparsity during training, and that training character-level LSTM language models with this sparsification does not significantly change the final performance of the model. The authors argue that this sparsity could be exploited with specialized hardware to improve the energy efficiency and speed of recurrent network training.\n\nNOVELTY\nThresholding gradients to induce sparsity and improve efficiency in RNN training is a novel result to my knowledge.\n\nMISSING CITATIONS\nPrior work has explored low-precision arithmetic for recurrent neural network language models:\n\nHubara et al, \u201cQuantized Neural Networks: Training Neural Networks with\nLow Precision Weights and Activations\u201d,"}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "The main point of the paper was that sparsifying gradients does not hurt performance; however, this in itself is not enough for a publication in this venue. As noted by R1 and R2, showing how this can help in more energy efficient training would make for a good paper; without that aspect the paper only presents an observation that is not too surprising to the practitioners in this area.\n \n Further, while the main point of the paper was relatively clear, the scientific presentation was not rigorous enough. All the reviewers point out that several details were missing (including test set performance, reporting of results on the different sets, etc). \n \n Paper would be strengthened by a better exploration of the problem.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"TITLE": "No Title", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "The findings of applying sparsity in the backward gradients for training LSTMs is interesting. \n\nBut the paper seems incomplete without the proper experimental justification. Only the validation loss is reported which is definitely insufficient. Proper testing results and commonly reported evaluation criterion needs to be included to support the claim of no degradation when applying the proposed sparsity technique. \n\nAlso actual justification of the gains in terms of speed and efficiency would make the paper much stronger.\n", "IS_META_REVIEW": false, "RECOMMENDATION": 4, "DATE": "17 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "Detailed analysis/implementation needed", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "This paper presents the observation that it is possible to utilize sparse operations in the training of LSTM networks without loss of accuracy. This observation is novel (although not too surprising) to my knowledge, but I must state that I am not very familiar with research on fast RNN implmentations.\n\nMinor note:\nThe LSTM language model does not use a 'word2vec' layer. It is simply a linear embedding layer. Word2vec is the name of a specific model which is not directly to character level language models.\n\nThe paper presents the central observation clearly. However, it will be much more convincing if a well known dataset and experiment set up are used, such as Graves (2013) or Sutskever et al (2014), and actual training, validation and test performances are reported.\n\nWhile the main observation is certainly interesting, I think it is not sufficient to be the subject of a full conference paper without implementation (or simulation) and benchmarking of the promised speedups on multiple tasks. For example, how would the gains be affected by various architecture choices?\n\nAt present this is an interesting technical report and I would like to see more detailed results in the future.", "IS_META_REVIEW": false, "RECOMMENDATION": 5, "DATE": "17 Dec 2016", "REVIEWER_CONFIDENCE": 3}, {"TITLE": "Review: Training Long Short-Term Memory with Sparsified Stochastic Gradient Descent", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "CONTRIBUTIONS\nWhen training LSTMs, many of the intermediate gradients are close to zero due to the flat shape of the tanh and sigmoid nonlinearities far from the origin. This paper shows that rounding these small gradients to zero results in matrices with up to 80% sparsity during training, and that training character-level LSTM language models with this sparsification does not significantly change the final performance of the model. The authors argue that this sparsity could be exploited with specialized hardware to improve the energy efficiency and speed of recurrent network training.\n\nNOVELTY\nThresholding gradients to induce sparsity and improve efficiency in RNN training is a novel result to my knowledge.\n\nMISSING CITATIONS\nPrior work has explored low-precision arithmetic for recurrent neural network language models:\n\nHubara et al, \u201cQuantized Neural Networks: Training Neural Networks with\nLow Precision Weights and Activations\u201d, ", "IS_META_REVIEW": false, "RECOMMENDATION": 4, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"DATE": "02 Dec 2016", "TITLE": "training speed comparisons and final model performance", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1"}, {"DATE": "02 Dec 2016", "TITLE": "Results of Section 4.2", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3"}, {"IS_META_REVIEW": true, "comments": "CONTRIBUTIONS\nWhen training LSTMs, many of the intermediate gradients are close to zero due to the flat shape of the tanh and sigmoid nonlinearities far from the origin. This paper shows that rounding these small gradients to zero results in matrices with up to 80% sparsity during training, and that training character-level LSTM language models with this sparsification does not significantly change the final performance of the model. The authors argue that this sparsity could be exploited with specialized hardware to improve the energy efficiency and speed of recurrent network training.\n\nNOVELTY\nThresholding gradients to induce sparsity and improve efficiency in RNN training is a novel result to my knowledge.\n\nMISSING CITATIONS\nPrior work has explored low-precision arithmetic for recurrent neural network language models:\n\nHubara et al, \u201cQuantized Neural Networks: Training Neural Networks with\nLow Precision Weights and Activations\u201d,"}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "The main point of the paper was that sparsifying gradients does not hurt performance; however, this in itself is not enough for a publication in this venue. As noted by R1 and R2, showing how this can help in more energy efficient training would make for a good paper; without that aspect the paper only presents an observation that is not too surprising to the practitioners in this area.\n \n Further, while the main point of the paper was relatively clear, the scientific presentation was not rigorous enough. All the reviewers point out that several details were missing (including test set performance, reporting of results on the different sets, etc). \n \n Paper would be strengthened by a better exploration of the problem.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"TITLE": "No Title", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "The findings of applying sparsity in the backward gradients for training LSTMs is interesting. \n\nBut the paper seems incomplete without the proper experimental justification. Only the validation loss is reported which is definitely insufficient. Proper testing results and commonly reported evaluation criterion needs to be included to support the claim of no degradation when applying the proposed sparsity technique. \n\nAlso actual justification of the gains in terms of speed and efficiency would make the paper much stronger.\n", "IS_META_REVIEW": false, "RECOMMENDATION": 4, "DATE": "17 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "Detailed analysis/implementation needed", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "This paper presents the observation that it is possible to utilize sparse operations in the training of LSTM networks without loss of accuracy. This observation is novel (although not too surprising) to my knowledge, but I must state that I am not very familiar with research on fast RNN implmentations.\n\nMinor note:\nThe LSTM language model does not use a 'word2vec' layer. It is simply a linear embedding layer. Word2vec is the name of a specific model which is not directly to character level language models.\n\nThe paper presents the central observation clearly. However, it will be much more convincing if a well known dataset and experiment set up are used, such as Graves (2013) or Sutskever et al (2014), and actual training, validation and test performances are reported.\n\nWhile the main observation is certainly interesting, I think it is not sufficient to be the subject of a full conference paper without implementation (or simulation) and benchmarking of the promised speedups on multiple tasks. For example, how would the gains be affected by various architecture choices?\n\nAt present this is an interesting technical report and I would like to see more detailed results in the future.", "IS_META_REVIEW": false, "RECOMMENDATION": 5, "DATE": "17 Dec 2016", "REVIEWER_CONFIDENCE": 3}, {"TITLE": "Review: Training Long Short-Term Memory with Sparsified Stochastic Gradient Descent", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "CONTRIBUTIONS\nWhen training LSTMs, many of the intermediate gradients are close to zero due to the flat shape of the tanh and sigmoid nonlinearities far from the origin. This paper shows that rounding these small gradients to zero results in matrices with up to 80% sparsity during training, and that training character-level LSTM language models with this sparsification does not significantly change the final performance of the model. The authors argue that this sparsity could be exploited with specialized hardware to improve the energy efficiency and speed of recurrent network training.\n\nNOVELTY\nThresholding gradients to induce sparsity and improve efficiency in RNN training is a novel result to my knowledge.\n\nMISSING CITATIONS\nPrior work has explored low-precision arithmetic for recurrent neural network language models:\n\nHubara et al, \u201cQuantized Neural Networks: Training Neural Networks with\nLow Precision Weights and Activations\u201d, ", "IS_META_REVIEW": false, "RECOMMENDATION": 4, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"DATE": "02 Dec 2016", "TITLE": "training speed comparisons and final model performance", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1"}, {"DATE": "02 Dec 2016", "TITLE": "Results of Section 4.2", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3"}]}
{"text": "1 INTRODUCTION\nMany of the most successful current deep learning architectures for vision rely on supervised learning from large sets of labeled training images. While the performance of these networks is undoubtedly impressive, reliance on such large numbers of training examples limits the utility of deep learning in many domains where such datasets are not available. Furthermore, the need for large numbers of labeled examples stands at odds with human visual learning, where one or a few views of an object is often all that is needed to enable robust recognition of that object across a wide range of different views, lightings and contexts. The development of a representation that facilitates such abilities, especially in an unsupervised way, is a largely unsolved problem.\nIn addition, while computer vision models are typically trained using static images, in the real world, visual objects are rarely experienced as disjoint snapshots. Instead, the visual world is alive with movement, driven both by self-motion of the viewer and the movement of objects within the scene. Many have suggested that temporal experience with objects as they move and undergo transformations can serve as an important signal for learning about the structure of objects (Fo\u0308ldia\u0301k, 1991; Softky, 1996; Wiskott & Sejnowski, 2002; George & Hawkins, 2005; Palm, 2012; O\u2019Reilly et al., 2014; Agrawal et al., 2015; Goroshin et al., 2015a; Lotter et al., 2015; Mathieu et al., 2016; Srivastava et al., 2015; Wang & Gupta, 2015; Whitney et al., 2016). For instance, Wiskott and Sejnowski proposed \u201cslow feature analysis\u201d as a framework for exploiting temporal structure in video streams (Wiskott & Sejnowski, 2002). Their approach attempts to build feature representations that extract\nCode and video examples can be found at: https://coxlab.github.io/prednet/\nslowly-varying parameters, such as object identity, from parameters that produce fast changes in the image, such as movement of the object. While approaches that rely on temporal coherence have arguably not yet yielded representations as powerful as those learned by supervised methods, they nonetheless point to the potential of learning useful representations from video (Mohabi et al., 2009; Sun et al., 2014; Goroshin et al., 2015a; Maltoni & Lomonaco, 2015; Wang & Gupta, 2015).\nHere, we explore another potential principle for exploiting video for unsupervised learning: prediction of future image frames (Softky, 1996; Palm, 2012; O\u2019Reilly et al., 2014; Goroshin et al., 2015b; Srivastava et al., 2015; Mathieu et al., 2016; Patraucean et al., 2015; Finn et al., 2016; Vondrick et al., 2016). A key insight here is that in order to be able to predict how the visual world will change over time, an agent must have at least some implicit model of object structure and the possible transformations objects can undergo. To this end, we have designed a neural network architecture, which we informally call a \u201cPredNet,\u201d that attempts to continually predict the appearance of future video frames, using a deep, recurrent convolutional network with both bottom-up and topdown connections. Our work here builds on previous work in next-frame video prediction (Ranzato et al., 2014; Michalski et al., 2014; Srivastava et al., 2015; Mathieu et al., 2016; Lotter et al., 2015; Patraucean et al., 2015; Oh et al., 2015; Finn et al., 2016; Xue et al., 2016; Vondrick et al., 2016; Brabandere et al., 2016), but we take particular inspiration from the concept of \u201cpredictive coding\u201d from the neuroscience literature (Rao & Ballard, 1999; Rao & Sejnowski, 2000; Lee & Mumford, 2003; Friston, 2005; Summerfield et al., 2006; Egner et al., 2010; Bastos et al., 2012; Spratling, 2012; Chalasani & Principe, 2013; Clark, 2013; O\u2019Reilly et al., 2014; Kanai et al., 2015). Predictive coding posits that the brain is continually making predictions of incoming sensory stimuli (Rao & Ballard, 1999; Friston, 2005). Top-down (and perhaps lateral) connections convey these predictions, which are compared against actual observations to generate an error signal. The error signal is then propagated back up the hierarchy, eventually leading to an update of the predictions.\nWe demonstrate the effectiveness of our model for both synthetic sequences, where we have access to the underlying generative model and can investigate what the model learns, as well as natural videos. Consistent with the idea that prediction requires knowledge of object structure, we find that these networks successfully learn internal representations that are well-suited to subsequent recognition and decoding of latent object parameters (e.g. identity, view, rotation speed, etc.). We also find that our architecture can scale effectively to natural image sequences, by training using car-mounted camera videos. The network is able to successfully learn to predict both the movement of the camera and the movement of objects in the camera\u2019s view. Again supporting the notion of prediction as an unsupervised learning rule, the model\u2019s learned representation in this setting supports decoding of the current steering angle.\n2 THE PREDNET MODEL\nThe PredNet architecture is diagrammed in Figure 1. The network consists of a series of repeating stacked modules that attempt to make local predictions of the input to the module, which is then subtracted from the actual input and passed along to the next layer. Briefly, each module of the network consists of four basic parts: an input convolutional layer (Al), a recurrent representation layer (Rl), a prediction layer (A\u0302l), and an error representation (El). The representation layer, Rl, is a recurrent convolutional network that generates a prediction, A\u0302l, of what the layer input, Al, will be on the next frame. The network takes the difference between Al and A\u0302l and outputs an error representation, El, which is split into separate rectified positive and negative error populations. The error, El, is then passed forward through a convolutional layer to become the input to the next layer (Al+1). The recurrent prediction layerRl receives a copy of the error signalEl, along with top-down input from the representation layer of the next level of the network (Rl+1). The organization of the network is such that on the first time step of operation, the \u201cright\u201d side of the network (Al\u2019s andEl\u2019s) is equivalent to a standard deep convolutional network. Meanwhile, the \u201cleft\u201d side of the network (the Rl\u2019s) is equivalent to a generative deconvolutional network with local recurrence at each stage. The architecture described here is inspired by that originally proposed by (Rao & Ballard, 1999), but is formulated in a modern deep learning framework and trained end-to-end using gradient descent, with a loss function implicitly embedded in the network as the firing rates of the error neurons. Our work also shares motivation with the Deep Predictive Coding Networks of Chalasani & Principe (2013); however, their framework is based upon sparse coding and a linear dynamical system with greedy layer-wise training, whereas ours is rooted in convolutional and recurrent neural networks trained with backprop.\nWhile the architecture is general with respect to the kinds of data it models, here we focus on image sequence (video) data. Consider a sequence of images, xt. The target for the lowest layer is set to the the actual sequence itself, i.e. At0 = xt \u2200t. The targets for higher layers, Atl for l > 0, are computed by a convolution over the error units from the layer below, Etl\u22121, followed by rectified linear unit (ReLU) activation and max-pooling. For the representation neurons, we specifically use convolutional LSTM units (Hochreiter & Schmidhuber, 1997; Shi et al., 2015). In our setting, the Rtl hidden state is updated according to R t\u22121 l , E t\u22121 l , as well as R t l+1, which is first spatially upsampled (nearest-neighbor), due to the pooling present in the feedforward path. The predictions, A\u0302tl are made through a convolution of the R t l stack followed by a ReLU non-linearity. For the lowest layer, A\u0302tl is also passed through a saturating non-linearity set at the maximum pixel value: SatLU(x; pmax) := min(pmax, x). Finally, the error response, Etl , is calculated from the difference between A\u0302tl and A t l and is split into ReLU-activated positive and negative prediction errors, which are concatenated along the feature dimension. As discussed in (Rao & Ballard, 1999), although not explicit in their model, the separate error populations are analogous to the existence of on-center, off-surround and off-center, on-surround neurons early in the visual system.\nThe full set of update rules are listed in Equations (1) to (4). The model is trained to minimize the weighted sum of the activity of the error units. Explicitly, the training loss is formalized in Equation 5 with weighting factors by time, \u03bbt, and layer, \u03bbl, and where nl is the number of units in the lth layer. With error units consisting of subtraction followed by ReLU activation, the loss at each layer is equivalent to an L1 error. Although not explored here, other error unit implementations, potentially even probabilistic or adversarial (Goodfellow et al., 2014), could also be used.\nAtl = { xt if l = 0 MAXPOOL(RELU(CONV(Etl\u22121))) l > 0\n(1)\nA\u0302tl = RELU(CONV(R t l)) (2)\nEtl = [RELU(A t l \u2212 A\u0302tl); RELU(A\u0302tl \u2212Atl)] (3) Rtl = CONVLSTM(E t\u22121 l , R t\u22121 l ,UPSAMPLE(R t l+1)) (4)\nLtrain = \u2211 t \u03bbt \u2211 l \u03bbl nl \u2211 nl Etl (5)\nAlgorithm 1 Calculation of PredNet states Require: xt\n1: At0 \u2190 xt 2: E0l , R 0 l \u2190 0 3: for t = 1 to T do 4: for l = L to 0 do . Update Rtl states 5: if l = L then 6: RtL = CONVLSTM(E t\u22121 L , R t\u22121 L ) 7: else 8: Rtl = CONVLSTM(E t\u22121 l , R t\u22121 l ,UPSAMPLE(R t l+1))\n9: for l = 0 to L do . Update A\u0302tl , Atl , Etl states 10: if l = 0 then 11: A\u0302t0 = SATLU(RELU(CONV(R t 0))) 12: else 13: A\u0302tl = RELU(CONV(R t l )) 14: Etl = [RELU(A t l \u2212 A\u0302tl); RELU(A\u0302tl \u2212Alt)] 15: if l < L then 16: Atl+1 = MAXPOOL(CONV(E l t))\nThe order in which each unit in the model is updated must also be specified, and our implementation is described in Algorithm 1. Updating of states occurs through two passes: a top-down pass where the Rtl states are computed, and then a forward pass to calculate the predictions, errors, and higher level targets. A last detail of note is that Rl and El are initialized to zero, which, due to the convolutional nature of the network, means that the initial prediction is spatially uniform.\n3 EXPERIMENTS\n3.1 RENDERED IMAGE SEQUENCES\nTo gain an understanding of the representations learned in the proposed framework, we first trained PredNet models using synthetic images, for which we have access to the underlying generative stimulus model and all latent parameters. We created sequences of rendered faces rotating with two degrees of freedom, along the \u201cpan\u201d (out-of-plane) and \u201croll\u201d (in-plane) axes. The faces start at a random orientation and rotate at a random constant velocity for a total of 10 frames. A different face was sampled for each sequence. The images were processed to be grayscale, with values normalized between 0 and 1, and 64x64 pixels in size. We used 16K sequences for training and 800 for both validation and testing.\nPredictions generated by a PredNet model are shown in Figure 2. The model is able to accumulate information over time to make accurate predictions of future frames. Since the representation neurons are initialized to zero, the prediction at the first time step is uniform. On the second time step, with no motion information yet, the prediction is a blurry reconstruction of the first time step. After further iterations, the model adapts to the underlying dynamics to generate predictions that closely match the incoming frame.\nFor choosing the hyperparameters of the model, we performed a random search and chose the model that had the lowest L1 error in frame prediction averaged over time steps 2-10 on a validation set. Given this selection criteria, the best performing models tended to have a loss solely concentrated at the lowest layer (i.e. \u03bb0 = 1, \u03bbl>0 = 0), which is the case for the model shown. Using an equal loss at each layer considerably degraded predictions, but enforcing a moderate loss on upper layers that was one magnitude smaller than the lowest layer (i.e. \u03bb0 = 1, \u03bbl>0 = 0.1) led to only slightly worse predictions, as illustrated in Figure 9 in the Appendix. In all cases, the time loss weight, \u03bbt, was set to zero for the first time step and then one for all time steps after. As for the remaining hyperparameters, the model shown has 5 layers with 3x3 filter sizes for all convolutions, max-pooling of stride 2, and number of channels per layer, for bothAl andRl units, of (1, 32, 64, 128, 256). Model weights were optimized using the Adam algorithm (Kingma & Ba, 2014).\nActual\nPredicted\nActual\nPredicted\nActual\nPredicted\nQuantitative evaluation of generative models is a difficult, unsolved problem (Theis et al., 2016), but here we report prediction error in terms of meansquared error (MSE) and the Structural Similarity Index Measure (SSIM) (Wang et al., 2004). SSIM is designed to be more correlated with perceptual judgments, and ranges from\u22121 and 1, with a larger score indicating greater similarity. We compare the PredNet to the trivial solution of copying the last\nframe, as well as a control model that shares the overall architecture and training scheme of the PredNet, but that sends forward the layer-wise activations (Al) rather than the errors (El). This model thus takes the form of a more traditional encoder-decoder pair, with a CNN encoder that has lateral skip connections to a convolutional LSTM decoder. The performance of all models on the rotating faces dataset is summarized in Table 1, where the scores were calculated as an average over all predictions after the first frame. We report results for the PredNet model trained with loss only on the lowest layer, denoted as PredNet L0, as well as the model trained with an 0.1 weight on upper layers, denoted as PredNet Lall. Both PredNet models outperformed the baselines on both measures, with the L0 model slightly outperforming Lall, as expected for evaluating the pixel-level predictions.\nSynthetic sequences were chosen as the initial training set in order to better understand what is learned in different layers of the model, specifically with respect to the underlying generative model (Kulkarni et al., 2015). The rotating faces were generated using the FaceGen software package (Singular Inversions, Inc.), which internally generates 3D face meshes by a principal component analysis in \u201cface space\u201d, derived from a corpus of 3D face scans. Thus, the latent parameters of the image sequences used here consist of the initial pan and roll angles, the pan and roll velocities, and the principal component (PC) values, which control the \u201cidentity\u201d of the face. To understand the information contained in the trained models, we decoded the latent parameters from the representation neurons (Rl) in different layers, using a ridge regression. The Rl states were taken at the earliest possible informative time steps, which, in the our notation, are the second and third steps, respectively, for the static and dynamic parameters. The regression was trained using 4K sequences with 500 for validation and 1K for testing. For a baseline comparison of the information implicitly embedded in the network architecture, we compare to the decoding accuracies of an untrained network with random initial weights. Note that in this randomly initialized case, we still expect above-chance decoding performance, given past theoretical and empirical work with random networks (Pinto et al., 2009; Jarrett et al., 2009; Saxe et al., 2010).\nLatent variable decoding accuracies of the pan and roll velocities, pan initial angle, and first PC are shown in the left panel of Figure 3. There are several interesting patterns. First, the trained models learn a representation that generally permits a better linear decoding of the underlying latent factors than the randomly initialized model, with the most striking difference in terms of the the pan rotation speed (\u03b1pan). Second, the most notable difference between the Lall and L0 versions occurs with the first principle component, where the model trained with loss on all layers has a higher decoding accuracy than the model trained with loss only on the lowest layer.\nThe latent variable decoding analysis suggests that the model learns a representation that may generalize well to other tasks for which it was not explicitly trained. To investigate this further, we assessed the models in a classification task from single, static images. We created a dataset of 25 previously unseen FaceGen faces at 7 pan angles, equally spaced between [\u2212\u03c02 , \u03c0 2 ], and 8 roll angles, equally spaced between [0, 2\u03c0). There were therefore 7 \u00b7 8 = 56 orientations per identity, which were tested in a cross-validated fashion. A linear SVM to decode face identity was fit on a model\u2019s representation of a random subset of orientations and then tested on the remaining angles. For each size of the SVM training set, ranging from 1-40 orientations per face, 50 different random splits were generated, with results averaged over the splits.\nFor the static face classification task, we compare the PredNets to a standard autoencoder and a variant of the Ladder Network (Valpola, 2015; Rasmus et al., 2015). Both models were constructed to have the same number of layers and channel sizes as the PredNets, as well as a similar alternating convolution/max-pooling, then upsampling/convolution scheme. As both networks are autoencoders, they were trained with a reconstruction loss, with a dataset consisting of all of the individual frames from the sequences used to train the PredNets. For the Ladder Network, which is a denoising autoencoder with lateral skip connections, one must also choose a noise parameter, as well as the relative weights of each layer in the total cost. We tested noise levels ranging from 0 to 0.5 in increments of 0.1, with loss weights either evenly distributed across layers, solely concentrated at the pixel layer, or 1 at the bottom layer and 0.1 at upper layers (analogous to the PredNet Lall model). Shown is the model that performed best for classification, which consisted of 0.4 noise and only pixel weighting. Lastly, as in our architecture, the Ladder Network has lateral and top-down streams that are combined by a combinator function. Inspired by (Pezeshki et al., 2015), where a learnable MLP improved results, and to be consistent in comparing to the PredNet, we used a purely convolutional combinator. Given the distributed representation in both networks, we decoded from a concatenation of the feature representations at all layers, except the pixel layer. For the PredNets, the representation units were used and features were extracted after processing one input frame.\nFace classification accuracies using the representations learned by the L0 and Lall PredNets, a standard autoencoder, and a Ladder Network variant are shown in the right panel of Figure 3. Both PredNets compare favorably to the other models at all sizes of the training set, suggesting they learn a representation that is relatively tolerant to object transformations. Similar to the decoding accuracy of the first principle component, the PredNet Lall model actually outperformed the L0 variant. Altogether, these results suggest that predictive training with the PredNet can be a viable alternative to other models trained with a more traditional reconstructive or denoising loss, and that the relative layer loss weightings (\u03bbl\u2019s) may be important for the particular task at hand.\n3.2 NATURAL IMAGE SEQUENCES\nWe next sought to test the PredNet architecture on complex, real-world sequences. As a testbed, we chose car-mounted camera videos, since these videos span across a wide range of settings and are characterized by rich temporal dynamics, including both self-motion of the vehicle and the motion of other objects in the scene (Agrawal et al., 2015). Models were trained using the raw videos from the KITTI dataset (Geiger et al., 2013), which were captured by a roof-mounted camera on a car driving around an urban environment in Germany. Sequences of 10 frames were sampled from the \u201cCity\u201d, \u201cResidential\u201d, and \u201cRoad\u201d categories, with 57 recording sessions used for training and 4 used for validation. Frames were center-cropped and downsampled to 128x160 pixels. In total, the training set consisted of roughly 41K frames.\nA random hyperparameter search, with model selection based on the validation set, resulted in a 4 layer model with 3x3 convolutions and layer channel sizes of (3, 48, 96, 192). Models were again trained with Adam (Kingma & Ba, 2014) using a loss either solely computed on the lowest layer (L0) or with a weight of 1 on the lowest layer and 0.1 on the upper layers (Lall). Adam parameters were initially set to their default values (\u03b1 = 0.001, \u03b21 = 0.9, \u03b22 = 0.999) with the learning rate, \u03b1, decreasing by a factor of 10 halfway through training. To assess that the network had indeed learned a robust representation, we tested on the CalTech Pedestrian dataset (Dolla\u0301r et al., 2009), which consists of videos from a dashboard-mounted camera on a vehicle driving around Los Angeles. Testing sequences were made to match the frame rate of the KITTI dataset and again cropped to 128x160 pixels. Quantitative evaluation was performed on the entire CalTech test partition, split into sequences of 10 frames.\nSample PredNet predictions (for the L0 model) on the CalTech Pedestrian dataset are shown in Figure 4, and example videos can be found at https://coxlab.github.io/prednet/. The model is able to make fairly accurate predictions in a wide range of scenarios. In the top sequence of Fig. 4, a car is passing in the opposite direction, and the model, while not perfect, is able to predict its trajectory, as well as fill in the ground it leaves behind. Similarly in Sequence 3, the model is able to predict the motion of a vehicle completing a left turn. Sequences 2 and 5 illustrate that the PredNet can judge its own movement, as it predicts the appearance of shadows and a stationary vehicle as they approach. The model makes reasonable predictions even in difficult scenarios, such as when the camera-mounted vehicle is turning. In Sequence 4, the model predicts the position of a tree, as the vehicle turns onto a road. The turning sequences also further illustrate the model\u2019s ability to \u201cfill-in\u201d, as it is able to extrapolate sky and tree textures as unseen regions come into view. As an additional control, we show a sequence at the bottom of Fig. 4, where the input has been temporally scrambled. In this case, the model generates blurry frames, which mostly just resemble the previous frame. Finally, although the PredNet shown here was trained to predict one frame ahead, it is also possible to predict multiple frames into the future, by feeding back predictions as the inputs and recursively iterating. We explore this in Appendix 5.3.\nQuantitatively, the PredNet models again outperformed the CNN-LSTM EncoderDecoder. To ensure that the difference in performance was not simply because of the choice of hyperparameters, we trained models with four other sets of hyperparameters, which were sampled from the initial random search over the number of layers, fil-\nter sizes, and number of filters per layer. For each of the four additional sets, the PredNet L0 had the best performance, with an average error reduction of 14.7% and 14.9% for MSE and SSIM,\nrespectively, compared to the CNN-LSTM Encoder-Decoder. More details, as well as a thorough investigation of systematically simplified models on the continuum between the PredNet and the CNN-LSTM Encoder-Decoder can be found in Appendix 5.1. Briefly, the elementwise subtraction operation in the PredNet seems to be beneficial, and the nonlinearity of positive/negative splitting also adds modest improvements. Finally, while these experiments measure the benefits of each component of our model, we also directly compare against recent work in a similar car-cam setting, by reporting results on a 64x64 pixel, grayscale car-cam dataset released by Brabandere et al. (2016). Our PredNet model outperforms the model by Brabandere et al. (2016) by 29%. Details can be found in Appendix 5.2. Also in Appendix 5.2, we present results for the Human3.6M (Ionescu et al., 2014) dataset, as reported by Finn et al. (2016). Without re-optimizing hyperparameters, our\nmodel underperforms the concurrently developed DNA model by Finn et al. (2016), but outperforms the model by Mathieu et al. (2016).\nTo test the implicit encoding of latent parameters in the car-cam setting, we used the internal representation in the PredNet to estimate the car\u2019s steering angle (Bojarski et al., 2016; Biasini et al., 2016). We used a dataset released by Comma.ai (Biasini et al., 2016) consisting of 11 videos totaling about 7 hours of mostly highway driving. We first trained networks for next-frame prediction and then fit a linear fully-connected layer on the learned representation to estimate the steering angle, using a MSE loss. We again concatenate the Rl representation at all layers, but first spatially average pool lower layers to match the spatial size of the upper layer, in order to reduce dimensionality. Steering angle estimation results, using the representation on the 10th time step, are shown in Figure 5. Given just 1K labeled training examples, a simple linear readout on the PredNet L0 representation explains 74% of the variance in the steering angle and outperforms the CNN-LSTM Enc.-Dec. by 35%. With 25K labeled training examples, the PredNet L0 has a MSE (in degrees2) of 2.14. As a point of reference, a CNN model designed to predict the steering angle (Biasini et al., 2016), albeit from a single frame instead of multiple frames, achieve a MSE of ~4 when trained end-to-end using 396K labeled training examples. Details of this analysis can be found in Appendix 8. Interestingly, in this task, the PredNet Lall model actually underperformed the L0 model and slightly underperformed the CNN-LSTM Enc.-Dec, again suggesting that the \u03bbl parameter can affect the representation learned, and different values may be preferable in different end tasks. Nonetheless, the readout from the Lall model still explained a substantial proportion of the steering angle variance and strongly outperformed the random initial weights. Overall, this analysis again demonstrates that a representation learned through prediction, and particularly with the PredNet model with appropriate hyperparameters, can contain useful information about underlying latent parameters.\n4 DISCUSSION\nAbove, we have demonstrated a predictive coding inspired architecture that is able to predict future frames in both synthetic and natural image sequences. Importantly, we have shown that learning to predict how an object or scene will move in a future frame confers advantages in decoding latent parameters (such as viewing angle) that give rise to an object\u2019s appearance, and can improve recognition performance. More generally, we argue that prediction can serve as a powerful unsupervised learning signal, since accurately predicting future frames requires at least an implicit model of the objects that make up the scene and how they are allowed to move. Developing a deeper understanding of the nature of the representations learned by the networks, and extending the architecture, by, for instance, allowing sampling, are important future directions.\nACKNOWLEDGMENTS\nWe would like to thank Rasmus Berg Palm for fruitful discussions and early brainstorming. We would also like to thank the developers of Keras (Chollet, 2016). This work was supported by IARPA (contract D16PC00002), the National Science Foundation (NSF IIS 1409097), and the Center for Brains, Minds and Machines (CBMM, NSF STC award CCF-1231216).\n5 APPENDIX\n5.1 ADDITIONAL CONTROL MODELS\nTable 3 contains results for additional variations of the PredNet and CNN-LSTM Encoder-Decoder evaluated on the CalTech Pedestrian Dataset after being trained on KITTI. We evaluate the models in terms of pixel prediction, thus using the PredNet model trained with loss only on the lowest layer (PredNet L0) as the base model. In addition to mean-squared error (MSE) and the Structural Similarity Index Measure (SSIM), we include calculations of the Peak Signal-To-Noise Ratio (PSNR). For each model, we evaluate it with the original set of hyperparameters (controlling the number of layers, filter sizes, and number of filters per layer), as well as with the four additional sets of hyperparameters that were randomly sampled from the initial random search (see main text for more details). Below is an explanation of the additional control models:\nMSE (x 10\u22123) PSNR SSIM PredNet 3.13 (3.33) 25.8 (25.5) 0.884 (0.878) PredNet (no El split) 3.20 (3.37) 25.6 (25.4) 0.883 (0.878) CNN-LSTM Enc.-Dec. 3.67 (3.91) 25.0 (24.6) 0.865 (0.856) CNN-LSTM Enc.-Dec. (2x Al filts) 3.82 (3.97) 24.8 (24.6) 0.857 (0.853) CNN-LSTM Enc.-Dec. (except pass E0) 3.41 (3.61) 25.4 (25.1) 0.873 (0.866) CNN-LSTM Enc.-Dec. (+/- split) 3.71 (3.84) 24.9 (24.7) 0.861 (0.857) Copy Last Frame 7.95 20.0 0.762\nEqualizing the number of filters in the CNN-LSTM Encoder-Decoder (2x Al filts) cannot account for its performance difference with the PredNet, and actually leads to overfitting and a decrease in performance. Passing the error at the lowest layer (E0) in the CNN-LSTM Enc.-Dec. improves performance, but still does not match the PredNet, where errors are passed at all layers. Finally, splitting the activationsAl into positive and negative populations in the CNN-LSTM Enc.-Dec. does not help, but the PredNet with linear error activation (\u201cno El split\u201d) performs slightly worse than the original split version. Together, these results suggest that the PredNet\u2019s error passing operation can lead to improvements in next-frame prediction performance.\n5.2 COMPARING AGAINST OTHER MODELS\nWhile our main comparison in the text was a control model that isolates the effects of the more unique components in the PredNet, here we directly compare against other published models. We report results on a 64x64 pixel, grayscale car-cam dataset and the Human3.6M dataset (Ionescu et al., 2014) to compare against the two concurrently developed models by Brabandere et al. (2016)\nand Finn et al. (2016), respectively. For both comparisons, we use a model with the same hyperparameters (# of layers, # of filters, etc.) of the PredNet L0 model trained on KITTI, but train from scratch on the new datasets. The only modification we make is to train using an L2 loss instead of the effective L1 loss, since both models train with an L2 loss and report results using L2-based metrics (MSE for Brabandere et al. (2016) and PSNR for Finn et al. (2016)). That is, we keep the original PredNet model intact but directly optimize using MSE between actual and predicted frames. We measure next-frame prediction performance after inputting 3 frames and 10 frames, respectively, for the 64x64 car-cam and Human3.6M datasets, to be consistent with the published works. We also include the results using a feedforward multi-scale network, similar to the model of Mathieu et al. (2016), on Human3.6M, as reported by Finn et al. (2016).\nOn a dataset similar to KITTI, our model outperforms the model proposed by Brabandere et al. (2016). On Human3.6M, our model outperforms a model similar to (Mathieu et al., 2016), but underperforms Finn et al. (2016), although we note we did not perform any hyperparameter optimization.\n5.3 MULTIPLE TIME STEP PREDICTION\nWhile the models presented here were originally trained to predict one frame ahead, they can be made to predict multiple frames by treating predictions as actual input and recursively iterating. Examples of this process are shown in Figure 6 for the PredNet L0 model. Although the next frame predictions are reasonably accurate, the model naturally breaks down when extrapolating further into the future. This is not surprising since the predictions will unavoidably have different statistics than the natural images for which the model was trained to handle (Bengio et al., 2015). If we additionally train the model to process its own predictions, the model is better able to extrapolate. The third row for every sequence shows the output of the original PredNet fine-tuned for extrapolation. Starting from the trained weights, the model was trained with a loss over 15 time steps, where the actual frame was inputted for the first 10 and then the model\u2019s predictions were used as input to the network for the last 5. For the first 10 time steps, the training loss was calculated on the El activations as usual, and for the last 5, it was calculated directly as the mean absolute error with respect to the ground truth frames. Despite eventual blurriness (which might be expected to some extent due to uncertainty), the fine-tuned model captures some key structure in its extrapolations after the tenth time step. For instance, in the first sequence, the model estimates the general shape of an upcoming shadow, despite minimal information in the last seen frame. In the second sequence, the model is able to extrapolate the motion of a car moving to the right. The reader is again encouraged to visit https://coxlab.github.io/prednet/ to view the predictions in video form. Quantitatively, the MSE of the model\u2019s predictions stay well below the trivial solution of copying the last seen frame, as illustrated in Fig 7. The MSE increases fairly linearly from time steps 2-10, even though the model was only trained for up to t+ 5 prediction.\n5.4 ADDITIONAL STEERING ANGLE ANALYSIS\nIn Figure 8, we show the steering angle estimation accuracy on the Comma.ai (Biasini et al., 2016) dataset using the representation learned by the PredNet L0 model, as a function of the number of frames inputted into the model. The PredNet\u2019s representation at all layers was concatenated (after spatially pooling lower layers to a common spatial resolution) and a fully-connected readout was fit using MSE. For each level of the number of training examples, we average over 10 cross-validation splits. To serve as points of reference, we include results for two static models. The first model is an autoencoder trained on single frame reconstruction with appropriately matching hyperparameters. A fully-connected layer was fit on the autoencoder\u2019s representation to estimate the steering angle in the same fashion as the PredNet. The second model is the default model in the posted Comma.ai code (Biasini et al., 2016), which is a five layer CNN. This model is trained end-to-end to estimate\nthe steering angle given the current frame as input, with a MSE loss. In addition to 25K examples, we trained a version using all of the frames in the Comma dataset (~396K). For all models, the final weights were chosen at the minimum validation error during training. Given the relatively small number of videos in the dataset compared to the average duration of each video, we used 5% of each video for validation and testing, chosen as a random continuous chunk, and discarded the 10 frames before and after the chosen segments from the training set.\nAs illustrated in Figure 8, the PredNet\u2019s performance gets better over time, as one might expect, as the model is able to accumulate more information. Interestingly, it performs reasonably well after just one time step, in a regime that is orthogonal to the training procedure of the PredNet where there are no dynamics. Altogether, these results again point to the usefulness of the model in learning underlying latent parameters.\n5.5 PREDNET Lall NEXT-FRAME PREDICTIONS\nFigures 9 and 10 compare next-frame predictions by the PredNet Lall model, trained with a prediction loss on all layers (\u03bb0 = 1, \u03bbl>0 = 0.1), and the PredNet L0 model, trained with a loss only on the lowest layer. At first glance, the difference in predictions seem fairly minor, and indeed, in terms of MSE, the Lall model only underperformed the L0 version by 3% and 6%, respectively, for the rotating faces and CalTech Pedestrian datasets. Upon careful inspection, however, it is apparent that the Lall predictions lack some of the finer details of the L0 predictions and are more blurry in regions of high variance. For instance, with the rotating faces, the facial features are less defined and with CalTech, details of approaching shadows and cars are less precise.\nActual\nPredNet \ud835\udc3f0\nPredNet \ud835\udc3f\ud835\udc4e\ud835\udc59\ud835\udc59\nError \ud835\udc3f\ud835\udc4e\ud835\udc59\ud835\udc59 - \ud835\udc3f0\nActual\nPredNet \ud835\udc3f0\nPredNet \ud835\udc3f\ud835\udc4e\ud835\udc59\ud835\udc59\nError \ud835\udc3f\ud835\udc4e\ud835\udc59\ud835\udc59 - \ud835\udc3f0\nActual\nPredNet \ud835\udc3f0\nPredNet \ud835\udc3f\ud835\udc4e\ud835\udc59\ud835\udc59\nError \ud835\udc3f\ud835\udc4e\ud835\udc59\ud835\udc59 - \ud835\udc3f0\nActual\nPredNet \ud835\udc3f0\nPredNet \ud835\udc3f\ud835\udc4e\ud835\udc59\ud835\udc59\nError \ud835\udc3f\ud835\udc4e\ud835\udc59\ud835\udc59 - \ud835\udc3f0\n", "cats": {"READ": 1.0, "DONTREAD": 0.0}, "reviews": [{"IS_META_REVIEW": true, "comments": "Paper Summary\nThis paper proposes an unsupervised learning model in which the network\npredicts what its state would look like at the next time step (at input layer\nand potentially other layers).  When these states are observed, an error signal\nis computed by comparing the predictions and the observations. This error\nsignal is fed back into the model. The authors show that this model is able to\nmake good predictions on a toy dataset of rotating 3D faces as well as on\nnatural videos. They also show that these features help perform supervised\ntasks.\n\nStrengths\n- The model is an interesting embodiment of the idea of predictive coding\n  implemented using a end-to-end backpropable recurrent neural network architecture.\n- The idea of feeding forward an error signal is perhaps not used as widely as it could\n  be, and this work shows a compelling example of using it. \n- Strong empirical results and relevant comparisons show that the model works well.\n- The authors present a detailed ablative analysis of the proposed model.\n\nWeaknesses\n- The model (esp. in Fig 1) is presented as a generalized predictive model\n  where next step predictions are made at each layer. However, as discovered by\nrunning the experiments, only the predictions at the input layer are the ones\nthat actually matter and the optimal choice seems to be to turn off the error\nsignal from the higher layers. While the authors intend to address this in future\nwork, I think this point merits some more discussion in the current work, given\nthe way this model is presented.\n- The network currently lacks stochasticity and does not model the future as a\n  multimodal distribution (However, this is mentioned as potential future work).\n\nQuality\nThe experiments are well-designed and a detailed analysis is provided\nin the appendix.\n\nClarity\nThe paper is well-written and easy to follow.\n\nOriginality\nSome deep models have previously been proposed that use predictive coding.\nHowever, the proposed model is most probably novel in the way it feds back the\nerror signal and implements the entire model as a single differentiable\nnetwork.\n\nSignificance\nThis paper will be of wide interest to the growing set of researchers working\nin unsupervised learning of time series. This helps draw attention to\npredictive coding as an important learning paradigm.\n\nOverall\nGood paper with detailed and well-designed experiments. The idea of feeding\nforward the error signal is not being used as much as it could be in our\ncommunity. This work helps to draw the community's attention to this idea."}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "This paper proposes an interesting architecture for predicting future frames of videos using end-to-end trained deep predictive coding.\n  The architecture is well presented and the paper is clearly written. The experiments are extensive and convincing, include ablation analyses, and show that this architecture performs well compared to other current methods.\n Overall, this is an interesting, solid contribution.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"TITLE": "an interesting architecture for future prediction inspired by deep predictive coding", "MEANINGFUL_COMPARISON": 5, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "An interesting architecture that accumulates and continuously corrects mistakes as you see more and more of a video sequence.\n\nClarity: The video you generated seems very helpful towards understanding the information flow in your network, it would be nice to link to it from the paper.\n\n \"Our model with hyperparameters optimized for KITTI underperforms the model of Finn et al. (2016), but outperforms the previous state-of-the-art model by Mathieu et al. (2016).\"\n\n It is not clear how different are the train and test sequences at the moment, since standard benchmarks do not really exist for video prediction and each author picks his/her favorite. Underperforming Finn et al 206 at the H3.6m Walking videos is a bit disappointing.\n\n", "ORIGINALITY": 4, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "19 Dec 2016", "REVIEWER_CONFIDENCE": 3}, {"OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "TITLE": "Review", "comments": "Learning about the physical structure and semantics of the world from video (without supervision) is a very hot area in computer vision and machine learning.\nIn this paper, the authors investigate how the prediction of future image frames (inherently unsupervised) can help to deduce object/s structure and it's properties (in this case single object pose, category, and steering angle, (after a supervised linear readout step))\n\nI enjoyed reading this paper, it is clear, interesting and proposes an original network architecture (PredNet) for video frame prediction that has produced promising results on both synthetic and natural images.\nMoreover, the extensive experimental evaluation and analysis the authors provide puts it on solid ground to which others can compare.\n\nThe weaknesses:\n- the link to predictive coding should be better explained in the paper if it is to be used as a motivation for the prednet model.\n- any idea that the proposed method is learning an implicit `model' of the `objects' that make up the `scene' is vague and far fetched, but it sounds great.\n\nMinor comment:\nNext to the number of labeled training examples (Fig.5), it would be interesting to see how much unsupervised training data was used to train your representations.", "ORIGINALITY": 4, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "RECOMMENDATION": 8, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "Good paper, nice example of using the idea of feeding forward error signals.", "MEANINGFUL_COMPARISON": 2, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "Paper Summary\nThis paper proposes an unsupervised learning model in which the network\npredicts what its state would look like at the next time step (at input layer\nand potentially other layers).  When these states are observed, an error signal\nis computed by comparing the predictions and the observations. This error\nsignal is fed back into the model. The authors show that this model is able to\nmake good predictions on a toy dataset of rotating 3D faces as well as on\nnatural videos. They also show that these features help perform supervised\ntasks.\n\nStrengths\n- The model is an interesting embodiment of the idea of predictive coding\n  implemented using a end-to-end backpropable recurrent neural network architecture.\n- The idea of feeding forward an error signal is perhaps not used as widely as it could\n  be, and this work shows a compelling example of using it. \n- Strong empirical results and relevant comparisons show that the model works well.\n- The authors present a detailed ablative analysis of the proposed model.\n\nWeaknesses\n- The model (esp. in Fig 1) is presented as a generalized predictive model\n  where next step predictions are made at each layer. However, as discovered by\nrunning the experiments, only the predictions at the input layer are the ones\nthat actually matter and the optimal choice seems to be to turn off the error\nsignal from the higher layers. While the authors intend to address this in future\nwork, I think this point merits some more discussion in the current work, given\nthe way this model is presented.\n- The network currently lacks stochasticity and does not model the future as a\n  multimodal distribution (However, this is mentioned as potential future work).\n\nQuality\nThe experiments are well-designed and a detailed analysis is provided\nin the appendix.\n\nClarity\nThe paper is well-written and easy to follow.\n\nOriginality\nSome deep models have previously been proposed that use predictive coding.\nHowever, the proposed model is most probably novel in the way it feds back the\nerror signal and implements the entire model as a single differentiable\nnetwork.\n\nSignificance\nThis paper will be of wide interest to the growing set of researchers working\nin unsupervised learning of time series. This helps draw attention to\npredictive coding as an important learning paradigm.\n\nOverall\nGood paper with detailed and well-designed experiments. The idea of feeding\nforward the error signal is not being used as much as it could be in our\ncommunity. This work helps to draw the community's attention to this idea.", "ORIGINALITY": 4, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "RECOMMENDATION": 8, "DATE": "16 Dec 2016", "CLARITY": 5, "REVIEWER_CONFIDENCE": 5}, {"DATE": "13 Dec 2016", "TITLE": "Update for all reviewers and commenters", "IS_META_REVIEW": false, "comments": "In response to the helpful comments and questions, we have made several changes to the manuscript:\n\n1.  In our original manuscript, we primarily compared the PredNet to a CNN-LSTM Encoder-Decoder, which we chose because it serves as a tight control for the more novel elements of our architecture. However, we agree that it is useful to compare against other published architectures.  One reason that this isn\u2019t a trivial task is because a standard benchmark for next frame prediction arguably has yet to be established.  Another issue is that published models are often optimized for performance on particular datasets, so evaluating competing models on KITTI/CalTech isn\u2019t necessarily fair to those models.  Searching the very recent literature, we found that the most relevant comparison to make is probably against the DFN model by Brabandere et al. (2016), which was recently presented at NIPS and was developed concurrently with our work.  One of their experiments was on a 64x64 pixel, grayscale car-cam dataset.  Training our KITTI model on this dataset, we outperform their results by 29%.  To compare against another concurrently developed model, also published at NIPS 2016, we have additionally evaluated on the Human3.6M dataset (Ionescu et al., 2014).  Our model with hyperparameters optimized for KITTI underperforms the model of Finn et al. (2016), but outperforms the previous state-of-the-art model by Mathieu et al. (2016).  We have added all of these comparisons to the appendix.\n\n2.  To make the main text more clear and concise, and to properly explain all of the necessary details, we have moved portions of the steering angle analysis to the appendix.  Our main point has been to demonstrate that our model learns a representation of important underlying factors, using other models as points of reference, so we have emphasized this.\n\nAt the reviewer\u2019s suggestion, we have added a video clip to help illustrate the flow of information in the network: ", "OTHER_KEYS": "William Lotter"}, {"TITLE": "Model architecture - finetuning", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "MEANINGFUL_COMPARISON": 5, "comments": "", "ORIGINALITY": 4, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "DATE": "08 Dec 2016"}, {"DATE": "02 Dec 2016", "TITLE": "Questions", "IS_META_REVIEW": false, "comments": "\nWhat's the difference between this work and \"Deep Predictive Coding Networks\" (ref [4]).\nPlease explain it clearly in the text.\n\nWhy not comparing the performance of prednet with the previous work of authors(ref[25])?\n\nThe improvement in MSE of prednet over previous frame prediction is 0.005. How significant is this?\nMaybe you could provide images showing the difference between images in t+1 and t, and between t+1 and your prediction.\nThis could reveal in which regions prednet does better than previous frame prediction. ", "OTHER_KEYS": "(anonymous)"}, {"TITLE": "Why loss at input layer only and other questions", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "MEANINGFUL_COMPARISON": 2, "comments": "", "ORIGINALITY": 4, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "DATE": "02 Dec 2016", "CLARITY": 5}, {"TITLE": "Several questions!", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "", "ORIGINALITY": 4, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "DATE": "24 Nov 2016"}, {"IS_META_REVIEW": true, "comments": "Paper Summary\nThis paper proposes an unsupervised learning model in which the network\npredicts what its state would look like at the next time step (at input layer\nand potentially other layers).  When these states are observed, an error signal\nis computed by comparing the predictions and the observations. This error\nsignal is fed back into the model. The authors show that this model is able to\nmake good predictions on a toy dataset of rotating 3D faces as well as on\nnatural videos. They also show that these features help perform supervised\ntasks.\n\nStrengths\n- The model is an interesting embodiment of the idea of predictive coding\n  implemented using a end-to-end backpropable recurrent neural network architecture.\n- The idea of feeding forward an error signal is perhaps not used as widely as it could\n  be, and this work shows a compelling example of using it. \n- Strong empirical results and relevant comparisons show that the model works well.\n- The authors present a detailed ablative analysis of the proposed model.\n\nWeaknesses\n- The model (esp. in Fig 1) is presented as a generalized predictive model\n  where next step predictions are made at each layer. However, as discovered by\nrunning the experiments, only the predictions at the input layer are the ones\nthat actually matter and the optimal choice seems to be to turn off the error\nsignal from the higher layers. While the authors intend to address this in future\nwork, I think this point merits some more discussion in the current work, given\nthe way this model is presented.\n- The network currently lacks stochasticity and does not model the future as a\n  multimodal distribution (However, this is mentioned as potential future work).\n\nQuality\nThe experiments are well-designed and a detailed analysis is provided\nin the appendix.\n\nClarity\nThe paper is well-written and easy to follow.\n\nOriginality\nSome deep models have previously been proposed that use predictive coding.\nHowever, the proposed model is most probably novel in the way it feds back the\nerror signal and implements the entire model as a single differentiable\nnetwork.\n\nSignificance\nThis paper will be of wide interest to the growing set of researchers working\nin unsupervised learning of time series. This helps draw attention to\npredictive coding as an important learning paradigm.\n\nOverall\nGood paper with detailed and well-designed experiments. The idea of feeding\nforward the error signal is not being used as much as it could be in our\ncommunity. This work helps to draw the community's attention to this idea."}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "This paper proposes an interesting architecture for predicting future frames of videos using end-to-end trained deep predictive coding.\n  The architecture is well presented and the paper is clearly written. The experiments are extensive and convincing, include ablation analyses, and show that this architecture performs well compared to other current methods.\n Overall, this is an interesting, solid contribution.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"TITLE": "an interesting architecture for future prediction inspired by deep predictive coding", "MEANINGFUL_COMPARISON": 5, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "An interesting architecture that accumulates and continuously corrects mistakes as you see more and more of a video sequence.\n\nClarity: The video you generated seems very helpful towards understanding the information flow in your network, it would be nice to link to it from the paper.\n\n \"Our model with hyperparameters optimized for KITTI underperforms the model of Finn et al. (2016), but outperforms the previous state-of-the-art model by Mathieu et al. (2016).\"\n\n It is not clear how different are the train and test sequences at the moment, since standard benchmarks do not really exist for video prediction and each author picks his/her favorite. Underperforming Finn et al 206 at the H3.6m Walking videos is a bit disappointing.\n\n", "ORIGINALITY": 4, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "19 Dec 2016", "REVIEWER_CONFIDENCE": 3}, {"OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "TITLE": "Review", "comments": "Learning about the physical structure and semantics of the world from video (without supervision) is a very hot area in computer vision and machine learning.\nIn this paper, the authors investigate how the prediction of future image frames (inherently unsupervised) can help to deduce object/s structure and it's properties (in this case single object pose, category, and steering angle, (after a supervised linear readout step))\n\nI enjoyed reading this paper, it is clear, interesting and proposes an original network architecture (PredNet) for video frame prediction that has produced promising results on both synthetic and natural images.\nMoreover, the extensive experimental evaluation and analysis the authors provide puts it on solid ground to which others can compare.\n\nThe weaknesses:\n- the link to predictive coding should be better explained in the paper if it is to be used as a motivation for the prednet model.\n- any idea that the proposed method is learning an implicit `model' of the `objects' that make up the `scene' is vague and far fetched, but it sounds great.\n\nMinor comment:\nNext to the number of labeled training examples (Fig.5), it would be interesting to see how much unsupervised training data was used to train your representations.", "ORIGINALITY": 4, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "RECOMMENDATION": 8, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "Good paper, nice example of using the idea of feeding forward error signals.", "MEANINGFUL_COMPARISON": 2, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "Paper Summary\nThis paper proposes an unsupervised learning model in which the network\npredicts what its state would look like at the next time step (at input layer\nand potentially other layers).  When these states are observed, an error signal\nis computed by comparing the predictions and the observations. This error\nsignal is fed back into the model. The authors show that this model is able to\nmake good predictions on a toy dataset of rotating 3D faces as well as on\nnatural videos. They also show that these features help perform supervised\ntasks.\n\nStrengths\n- The model is an interesting embodiment of the idea of predictive coding\n  implemented using a end-to-end backpropable recurrent neural network architecture.\n- The idea of feeding forward an error signal is perhaps not used as widely as it could\n  be, and this work shows a compelling example of using it. \n- Strong empirical results and relevant comparisons show that the model works well.\n- The authors present a detailed ablative analysis of the proposed model.\n\nWeaknesses\n- The model (esp. in Fig 1) is presented as a generalized predictive model\n  where next step predictions are made at each layer. However, as discovered by\nrunning the experiments, only the predictions at the input layer are the ones\nthat actually matter and the optimal choice seems to be to turn off the error\nsignal from the higher layers. While the authors intend to address this in future\nwork, I think this point merits some more discussion in the current work, given\nthe way this model is presented.\n- The network currently lacks stochasticity and does not model the future as a\n  multimodal distribution (However, this is mentioned as potential future work).\n\nQuality\nThe experiments are well-designed and a detailed analysis is provided\nin the appendix.\n\nClarity\nThe paper is well-written and easy to follow.\n\nOriginality\nSome deep models have previously been proposed that use predictive coding.\nHowever, the proposed model is most probably novel in the way it feds back the\nerror signal and implements the entire model as a single differentiable\nnetwork.\n\nSignificance\nThis paper will be of wide interest to the growing set of researchers working\nin unsupervised learning of time series. This helps draw attention to\npredictive coding as an important learning paradigm.\n\nOverall\nGood paper with detailed and well-designed experiments. The idea of feeding\nforward the error signal is not being used as much as it could be in our\ncommunity. This work helps to draw the community's attention to this idea.", "ORIGINALITY": 4, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "RECOMMENDATION": 8, "DATE": "16 Dec 2016", "CLARITY": 5, "REVIEWER_CONFIDENCE": 5}, {"DATE": "13 Dec 2016", "TITLE": "Update for all reviewers and commenters", "IS_META_REVIEW": false, "comments": "In response to the helpful comments and questions, we have made several changes to the manuscript:\n\n1.  In our original manuscript, we primarily compared the PredNet to a CNN-LSTM Encoder-Decoder, which we chose because it serves as a tight control for the more novel elements of our architecture. However, we agree that it is useful to compare against other published architectures.  One reason that this isn\u2019t a trivial task is because a standard benchmark for next frame prediction arguably has yet to be established.  Another issue is that published models are often optimized for performance on particular datasets, so evaluating competing models on KITTI/CalTech isn\u2019t necessarily fair to those models.  Searching the very recent literature, we found that the most relevant comparison to make is probably against the DFN model by Brabandere et al. (2016), which was recently presented at NIPS and was developed concurrently with our work.  One of their experiments was on a 64x64 pixel, grayscale car-cam dataset.  Training our KITTI model on this dataset, we outperform their results by 29%.  To compare against another concurrently developed model, also published at NIPS 2016, we have additionally evaluated on the Human3.6M dataset (Ionescu et al., 2014).  Our model with hyperparameters optimized for KITTI underperforms the model of Finn et al. (2016), but outperforms the previous state-of-the-art model by Mathieu et al. (2016).  We have added all of these comparisons to the appendix.\n\n2.  To make the main text more clear and concise, and to properly explain all of the necessary details, we have moved portions of the steering angle analysis to the appendix.  Our main point has been to demonstrate that our model learns a representation of important underlying factors, using other models as points of reference, so we have emphasized this.\n\nAt the reviewer\u2019s suggestion, we have added a video clip to help illustrate the flow of information in the network: ", "OTHER_KEYS": "William Lotter"}, {"TITLE": "Model architecture - finetuning", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "MEANINGFUL_COMPARISON": 5, "comments": "", "ORIGINALITY": 4, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "DATE": "08 Dec 2016"}, {"DATE": "02 Dec 2016", "TITLE": "Questions", "IS_META_REVIEW": false, "comments": "\nWhat's the difference between this work and \"Deep Predictive Coding Networks\" (ref [4]).\nPlease explain it clearly in the text.\n\nWhy not comparing the performance of prednet with the previous work of authors(ref[25])?\n\nThe improvement in MSE of prednet over previous frame prediction is 0.005. How significant is this?\nMaybe you could provide images showing the difference between images in t+1 and t, and between t+1 and your prediction.\nThis could reveal in which regions prednet does better than previous frame prediction. ", "OTHER_KEYS": "(anonymous)"}, {"TITLE": "Why loss at input layer only and other questions", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "MEANINGFUL_COMPARISON": 2, "comments": "", "ORIGINALITY": 4, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "DATE": "02 Dec 2016", "CLARITY": 5}, {"TITLE": "Several questions!", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "", "ORIGINALITY": 4, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "DATE": "24 Nov 2016"}]}
{"text": "HIERARCHICAL MULTISCALE RECURRENT NEURAL NETWORKS\n1 INTRODUCTION\nOne of the key principles of learning in deep neural networks as well as in the human brain is to obtain a hierarchical representation with increasing levels of abstraction (Bengio, 2009; LeCun et al., 2015; Schmidhuber, 2015). A stack of representation layers, learned from the data in a way to optimize the target task, make deep neural networks entertain advantages such as generalization to unseen examples (Hoffman et al., 2013), sharing learned knowledge among multiple tasks, and discovering disentangling factors of variation (Kingma & Welling, 2013). The remarkable recent successes of the deep convolutional neural networks are particularly based on this ability to learn hierarchical representation for spatial data (Krizhevsky et al., 2012). For modelling temporal data, the recent resurgence of recurrent neural networks (RNN) has led to remarkable advances (Mikolov et al., 2010; Graves, 2013; Cho et al., 2014; Sutskever et al., 2014; Vinyals et al., 2015). However, unlike the spatial data, learning both hierarchical and temporal representation has been among the long-standing challenges of RNNs in spite of the fact that hierarchical multiscale structures naturally exist in many temporal data (Schmidhuber, 1991; Mozer, 1993; El Hihi & Bengio, 1995; Lin et al., 1996; Koutn\u00edk et al., 2014).\nA promising approach to model such hierarchical and temporal representation is the multiscale RNNs (Schmidhuber, 1992; El Hihi & Bengio, 1995; Koutn\u00edk et al., 2014). Based on the observation that high-level abstraction changes slowly with temporal coherency while low-level abstraction has quickly changing features sensitive to the precise local timing (El Hihi & Bengio, 1995), the multiscale RNNs group hidden units into multiple modules of different timescales. In addition to the fact that the architecture fits naturally to the latent hierarchical structures in many temporal data, the multiscale approach provides the following advantages that resolve some inherent problems of standard RNNs: (a) computational efficiency obtained by updating the high-level layers less frequently, (b) efficiently delivering long-term dependencies with fewer updates at the high-level layers, which mitigates the vanishing gradient problem, (c) flexible resource allocation (e.g., more hidden units to the higher layers that focus on modelling long-term dependencies and less hidden units to the lower layers which are in charge of learning short-term dependencies). In addition, the learned latent hierarchical structures can provide useful information to other downstream tasks such \u2217Yoshua Bengio is CIFAR Senior Fellow.\nas module structures in computer program learning, sub-task structures in hierarchical reinforcement learning, and story segments in video understanding.\nThere have been various approaches to implementing the multiscale RNNs. The most popular approach is to set the timescales as hyperparameters (El Hihi & Bengio, 1995; Koutn\u00edk et al., 2014; Bahdanau et al., 2016) instead of treating them as dynamic variables that can be learned from the data (Schmidhuber, 1991; 1992; Chung et al., 2015; 2016). However, considering the fact that non-stationarity is prevalent in temporal data, and that many entities of abstraction such as words and sentences are in variable length, we claim that it is important for an RNN to dynamically adapt its timescales to the particulars of the input entities of various length. While this is trivial if the hierarchical boundary structure is provided (Sordoni et al., 2015), it has been a challenge for an RNN to discover the latent hierarchical structure in temporal data without explicit boundary information.\nIn this paper, we propose a novel multiscale RNN model, which can learn the hierarchical multiscale structure from temporal data without explicit boundary information. This model, called a hierarchical multiscale recurrent neural network (HM-RNN), does not assign fixed update rates, but adaptively determines proper update times corresponding to different abstraction levels of the layers. We find that this model tends to learn fine timescales for low-level layers and coarse timescales for high-level layers. To do this, we introduce a binary boundary detector at each layer. The boundary detector is turned on only at the time steps where a segment of the corresponding abstraction level is completely processed. Otherwise, i.e., during the within segment processing, it stays turned off. Using the hierarchical boundary states, we implement three operations, UPDATE, COPY and FLUSH, and choose one of them at each time step. The UPDATE operation is similar to the usual update rule of the long short-term memory (LSTM) (Hochreiter & Schmidhuber, 1997), except that it is executed sparsely according to the detected boundaries. The COPY operation simply copies the cell and hidden states of the previous time step. Unlike the leaky integration of the LSTM or the Gated Recurrent Unit (GRU) (Cho et al., 2014), the COPY operation retains the whole states without any loss of information. The FLUSH operation is executed when a boundary is detected, where it first ejects the summarized representation of the current segment to the upper layer and then reinitializes the states to start processing the next segment. Learning to select a proper operation at each time step and to detect the boundaries, the HM-RNN discovers the latent hierarchical structure of the sequences. We find that the straight-through estimator (Hinton, 2012; Bengio et al., 2013; Courbariaux et al., 2016) is efficient for training this model containing discrete variables.\nWe evaluate our model on two tasks: character-level language modelling and handwriting sequence generation. For the character-level language modelling, the HM-RNN achieves the state-of-the-art results on the Text8 dataset, and comparable results to the state-of-the-art on the Penn Treebank and Hutter Prize Wikipedia datasets. The HM-RNN also outperforms the standard RNN on the handwriting sequence generation using the IAM-OnDB dataset. In addition, we demonstrate that the hierarchical structure found by the HM-RNN is indeed very similar to the intrinsic structure observed in the data. The contributions of this paper are:\n\u2022 We propose for the first time an RNN model that can learn a latent hierarchical structure of a sequence without using explicit boundary information. \u2022 We show that it is beneficial to utilize the above structure through empirical evaluation. \u2022 We show that the straight-through estimator is an efficient way of training a model containing\ndiscrete variables. \u2022 We propose the slope annealing trick to improve the training procedure based on the\nstraight-through estimator.\n2 RELATED WORK\nTwo notable early attempts inspiring our model are Schmidhuber (1992) and El Hihi & Bengio (1995). In these works, it is advocated to stack multiple layers of RNNs in a decreasing order of update frequency for computational and learning efficiency. In Schmidhuber (1992), the author shows a model that can self-organize a hierarchical multiscale structure. Particularly in El Hihi & Bengio (1995), the advantages of incorporating a priori knowledge, \u201ctemporal dependencies are structured hierarchically\", into the RNN architecture is studied. The authors propose an RNN architecture that updates each layer with a fixed but different rate, called a hierarchical RNN.\nLSTMs (Hochreiter & Schmidhuber, 1997) employ the multiscale update concept, where the hidden units have different forget and update rates and thus can operate with different timescales. However, unlike our model, these timescales are not organized hierarchically. Although the LSTM has a selfloop for the gradients that helps to capture the long-term dependencies by mitigating the vanishing gradient problem, in practice, it is still limited to a few hundred time steps due to the leaky integration by which the contents to memorize for a long-term is gradually diluted at every time step. Also, the model remains computationally expensive because it has to perform the update at every time step for each unit. However, our model is less prone to these problems because it learns a hierarchical structure such that, by design, high-level layers learn to perform less frequent updates than low-level layers. We hypothesize that this property mitigates the vanishing gradient problem more efficiently while also being computationally more efficient.\nA more recent model, the clockwork RNN (CW-RNN) (Koutn\u00edk et al., 2014) extends the hierarchical RNN (El Hihi & Bengio, 1995) and the NARX RNN (Lin et al., 1996)1. The CW-RNN tries to solve the issue of using soft timescales in the LSTM, by explicitly assigning hard timescales. In the CW-RNN, hidden units are partitioned into several modules, and different timescales are assigned to the modules such that a module i updates its hidden units at every 2(i\u22121)-th time step. The CW-RNN is computationally more efficient than the standard RNN including the LSTM since hidden units are updated only at the assigned clock rates. However, finding proper timescales in the CW-RNN remains as a challenge whereas our model learns the intrinsic timescales from the data. In the biscale RNNs (Chung et al., 2016), the authors proposed to model layer-wise timescales adaptively by having additional gating units, however this approach still relies on the soft gating mechanism like LSTMs.\nOther forms of Hierarchical RNN (HRNN) architectures have been proposed in the cases where the explicit hierarchical boundary structure is provided. In Ling et al. (2015), after obtaining the word boundary via tokenization, the HRNN architecture is used for neural machine translation by modelling the characters and words using the first and second RNN layers, respectively. A similar HRNN architecture is also adopted in Sordoni et al. (2015) to model dialogue utterances. However, in many cases, hierarchical boundary information is not explicitly observed or expensive to obtain. Also, it is unclear how to deploy more layers than the number of boundary levels that is explicitly observed in the data.\nWhile the above models focus on online prediction problems, where a prediction needs to be made by using only the past data, in some cases, predictions are made after observing the whole sequence. In this setting, the input sequence can be regarded as 1-D spatial data, convolutional neural networks with 1-D kernels are proposed in Kim (2014) and Kim et al. (2015) for language modelling and sentence classification. Also, in Chan et al. (2016) and Bahdanau et al. (2016), the authors proposed to obtain high-level representation of the sequences of reduced length by repeatedly merging or pooling the lower-level representation of the sequences.\nHierarchical RNN architectures have also been used to discover the segmentation structure in sequences (Fern\u00e1ndez et al., 2007; Kong et al., 2015). It is however different to our model in the sense that they optimize the objective with explicit labels on the hierarchical segments while our model discovers the intrinsic structure only from the sequences without segment label information.\nThe COPY operation used in our model can be related to Zoneout (Krueger et al., 2016) which is a recurrent generalization of stochastic depth (Huang et al., 2016). In Zoneout, an identity transformation is randomly applied to each hidden unit at each time step according to a Bernoulli distribution. This results in occasional copy operations of the previous hidden states. While the focus of Zoneout is to propose a regularization technique similar to dropout (Srivastava et al., 2014) (where the regularization strength is controlled by a hyperparameter), our model learns (a) to dynamically determine when to copy from the context inputs and (b) to discover the hierarchical multiscale structure and representation. Although the main goal of our proposed model is not regularization, we found that our model also shows very good generalization performance.\n3 HIERARCHICAL MULTISCALE RECURRENT NEURAL NETWORKS\n3.1 MOTIVATION\nTo begin with, we provide an example of how a stacked RNN can model temporal data in an ideal setting, i.e., when the hierarchy of segments is provided (Sordoni et al., 2015; Ling et al., 2015). In Figure 1 (a), we depict a hierarchical RNN (HRNN) for language modelling with two layers: the first layer receives characters as inputs and generates word-level representations (C2W-RNN), and the second layer takes the word-level representations as inputs and yields phrase-level representations (W2P-RNN).\nAs shown, by means of the provided end-of-word labels, the C2W-RNN obtains word-level representation after processing the last character of each word and passes the word-level representation to the W2P-RNN. Then, the W2P-RNN performs an update of the phrase-level representation. Note that the hidden states of the W2P-RNN remains unchanged while all the characters of a word are processed by the C2W-RNN. When the C2W-RNN starts to process the next word, its hidden states are reinitialized using the latest hidden states of the W2P-RNN, which contain summarized representation of all the words that have been processed by that time step, in that phrase.\nFrom this simple example, we can see the advantages of having a hierarchical multiscale structure: (1) as the W2P-RNN is updated at a much slower update rate than the C2W-RNN, a considerable amount of computation can be saved, (2) gradients are backpropagated through a much smaller number of time steps, and (3) layer-wise capacity control becomes possible (e.g., use a smaller number of hidden units in the first layer which models short-term dependencies but whose updates are invoked much more often).\nCan an RNN discover such hierarchical multiscale structure without explicit hierarchical boundary information? Considering the fact that the boundary information is difficult to obtain (for example, consider languages where words are not always cleanly separated by spaces or punctuation symbols, and imperfect rules are used to separately perform segmentation) or usually not provided at all, this is a legitimate problem. It gets worse when we consider higher-level concepts which we would like the RNN to discover autonomously. In Section 2, we discussed the limitations of the existing RNN models under this setting, which either have to update all units at every time step or use fixed update frequencies (El Hihi & Bengio, 1995; Koutn\u00edk et al., 2014). Unfortunately, this kind of approach is not well suited to the case where different segments in the hierarchical decomposition have different lengths: for example, different words have different lengths, so a fixed hierarchy would not update its upper-level units in synchrony with the natural boundaries in the data.\n3.2 THE PROPOSED MODEL\nA key element of our model is the introduction of a parametrized boundary detector, which outputs a binary value, in each layer of a stacked RNN, and learns when a segment should end in such a way to optimize the overall target objective. Whenever the boundary detector is turned on at a time step of layer ` (i.e., when the boundary state is 1), the model considers this to be the end of a\n1The acronym NARX stands for Non-linear Auto-Regressive model with eXogenous inputs.\nsegment corresponding to the latent abstraction level of that layer (e.g., word or phrase) and feeds the summarized representation of the detected segment into the upper layer (`+ 1). Using the boundary states, at each time step, each layer selects one of the following operations: UPDATE, COPY or FLUSH. The selection is determined by (1) the boundary state of the current time step in the layer below z`\u22121t and (2) the boundary state of the previous time step in the same layer z ` t\u22121.\nIn the following, we describe an HM-RNN based on the LSTM update rule. We call this model a hierarchical multiscale LSTM (HM-LSTM). Consider an HM-LSTM model of L layers (` = 1, . . . , L) which, at each layer `, performs the following update at time step t:\nh`t, c ` t, z ` t = f ` HM-LSTM(c ` t\u22121,h ` t\u22121,h `\u22121 t ,h `+1 t\u22121, z ` t\u22121, z `\u22121 t ). (1)\nHere, h and c denote the hidden and cell states, respectively. The function f `HM-LSTM is implemented as follows. First, using the two boundary states z`t\u22121 and z `\u22121 t , the cell state is updated by:\nc`t =  f `t c`t\u22121 + i`t g`t if z`t\u22121 = 0 and z`\u22121t = 1 (UPDATE) c`t\u22121 if z ` t\u22121 = 0 and z `\u22121 t = 0 (COPY)\ni`t g`t if z`t\u22121 = 1 (FLUSH), (2)\nand then the hidden state is obtained by:\nh`t = { h`t\u22121 if COPY, o`t tanh(c`t) otherwise.\n(3)\nHere, (f , i,o) are forget, input, output gates, and g is a cell proposal vector. Note that unlike the LSTM, it is not necessary to compute these gates and cell proposal values at every time step. For example, in the case of the COPY operation, we do not need to compute any of these values and thus can save computations.\nThe COPY operation, which simply performs (c`t,h ` t)\u2190 (c`t\u22121,h`t\u22121), implements the observation that an upper layer should keep its state unchanged until it receives the summarized input from the lower layer. The UPDATE operation is performed to update the summary representation of the layer ` if the boundary z`\u22121t is detected from the layer below but the boundary z ` t\u22121 was not found at the previous time step. Hence, the UPDATE operation is executed sparsely unlike the standard RNNs where it is executed at every time step, making it computationally inefficient. If a boundary is detected, the FLUSH operation is executed. The FLUSH operation consists of two sub-operations: (a) EJECT to pass the current state to the upper layer and then (b) RESET to reinitialize the state before starting to read a new segment. This operation implicitly forces the upper layer to absorb the summary information of the lower layer segment, because otherwise it will be lost. Note that the FLUSH operation is a hard reset in the sense that it completely erases all the previous states of the same layer, which is different from the soft reset or soft forget operation in the GRU or LSTM.\nWhenever needed (depending on the chosen operation), the gate values (f `t , i ` t,o ` t), the cell proposal g`t , and the pre-activation of the boundary detector z\u0303 ` t\n2 are then obtained by: f `t i`t o`t g`t z\u0303`t  =  sigm sigm sigm tanh hard sigm  fslice (srecurrent(`)t + stop-down(`)t + sbottom-up(`)t + b(`)) , (4) where\ns recurrent(`) t = U ` `h ` t\u22121, (5)\ns top-down(`) t = z ` t\u22121U ` `+1h `+1 t\u22121, (6)\ns bottom-up(`) t = z `\u22121 t W ` `\u22121h `\u22121 t . (7)\nHere, we useW ji \u2208 R(4dim(h `)+1)\u00d7dim(h`\u22121), U ji \u2208 R(4dim(h `)+1)\u00d7dim(h`) to denote state transition parameters from layer i to layer j, and b \u2208 R4dim(h`)+1 is a bias term. In the last layer L, the\n2z\u0303`t can also be implemented as a function of h`t , e.g., z\u0303`t = hard sigm(Uh`t).\ntop-down connection is ignored, and we use h0t = xt. Since the input should not be omitted, we set z0t = 1 for all t. Also, we do not use the boundary detector for the last layer. The hard sigm is defined by hard sigm(x) = max ( 0,min ( 1, ax+1\n2\n)) with a being the slope variable.\nUnlike the standard LSTM, the HM-LSTM has a top-down connection from (`+ 1) to `, which is allowed to be activated only if a boundary is detected at the previous time step of the layer ` (see Eq. 6). This makes the layer ` to be initialized with more long-term information after the boundary is detected and execute the FLUSH operation. In addition, the input from the lower layer (` \u2212 1) becomes effective only when a boundary is detected at the current time step in the layer (`\u2212 1) due to the binary gate z`\u22121t . Figure 2 (left) shows the gating mechanism of the HM-LSTM at time step t.\nFinally, the binary boundary state z`t is obtained by:\nz`t = fbound(z\u0303 ` t ). (8)\nFor the binarization function fbound : R\u2192 {0, 1}, we can either use a deterministic step function:\nz`t = { 1 if z\u0303`t > 0.5 0 otherwise,\n(9)\nor sample from a Bernoulli distribution z`t \u223c Bernoulli(z\u0303`t ). Although this binary decision is a key to our model, it is usually difficult to use stochastic gradient descent to train such model with discrete decisions as it is not differentiable.\n3.3 COMPUTING GRADIENT OF BOUNDARY DETECTOR\nTraining neural networks with discrete variables requires more efforts since the standard backpropagation is no longer applicable due to the non-differentiability. Among a few methods for training a neural network with discrete variables such as the REINFORCE (Williams, 1992; Mnih & Gregor, 2014) and the straight-through estimator (Hinton, 2012; Bengio et al., 2013), we use the straightthrough estimator to train our model. The straight-through estimator is a biased estimator because the non-differentiable function used in the forward pass (i.e., the step function in our case) is replaced by a differentiable function during the backward pass (i.e., the hard sigmoid function in our case). The straight-through estimator, however, is much simpler and often works more efficiently in practice than other unbiased but high-variance estimators such as the REINFORCE. The straight-through estimator has also been used in Courbariaux et al. (2016) and Vezhnevets et al. (2016).\nThe Slope Annealing Trick. In our experiment, we use the slope annealing trick to reduce the bias of the straight-through estimator. The idea is to reduce the discrepancy between the two functions used during the forward pass and the backward pass. That is, by gradually increasing the slope a of the hard sigmoid function, we make the hard sigmoid be close to the step function. Note that starting with a high slope value from the beginning can make the training difficult while it is more applicable later when the model parameters become more stable. In our experiments, starting from slope a = 1, we slowly increase the slope until it reaches a threshold with an appropriate scheduling.\n4 EXPERIMENTS\nWe evaluate the proposed model on two tasks, character-level language modelling and handwriting sequence generation. Character-level language modelling is a representative example of discrete\nsequence modelling, where the discrete symbols form a distinct hierarchical multiscale structure. The performance on real-valued sequences is tested on the handwriting sequence generation in which a relatively clear hierarchical multiscale structure exists compared to other data such as speech signals.\n4.1 CHARACTER-LEVEL LANGUAGE MODELLING\nA sequence modelling task aims at learning the probability distribution over sequences by minimizing the negative log-likelihood of the training sequences:\nmin \u03b8 \u2212 1 N N\u2211 n=1 Tn\u2211 t=1 log p (xnt | xn<t; \u03b8) , (10)\nwhere \u03b8 is the model parameter, N is the number of training sequences, and Tn is the length of the n-th sequence. A symbol at time t of sequence n is denoted by xnt , and x n <t denotes all previous symbols at time t. We evaluate our model on three benchmark text corpora: (1) Penn Treebank, (2) Text8 and (3) Hutter Prize Wikipedia. We use the bits-per-character (BPC), E[\u2212 log2 p(xt+1 | x\u2264t)], as the evaluation metric.\nModel We use a model consisting of an input embedding layer, an RNN module and an output module. The input embedding layer maps each input symbol into 128-dimensional continuous vector without using any non-linearity. The RNN module is the HM-LSTM, described in Section 3, with three layers. The output module is a feedforward neural network with two layers, an output embedding layer and a softmax layer. Figure 2 (right) shows a diagram of the output module. At each time step, the output embedding layer receives the hidden states of the three RNN layers as input. In order to adaptively control the importance of each layer at each time step, we also introduce three scalar gating units g`t \u2208 R to each of the layer outputs:\ng`t = sigm(w `[h1t ; \u00b7 \u00b7 \u00b7 ;hLt ]), (11)\nwhere w` \u2208 R \u2211L\n`=1 dim(h `) is the weight parameter. The output embedding het is computed by:\nhet = ReLU ( L\u2211 `=1 g`tW e `h ` t ) , (12)\nwhere L = 3 and ReLU(x) = max(0, x) (Nair & Hinton, 2010). Finally, the probability distribution for the next target character is computed by the softmax function, softmax(xj) = e\nxj\u2211K k=1 exk , where\neach output class is a character.\nPenn Treebank We process the Penn Treebank dataset (Marcus et al., 1993) by following the procedure introduced in Mikolov et al. (2012). Each update is done by using a mini-batch of 64 examples of length 100 to prevent the memory overflow problem when unfolding the RNN in time for backpropagation. The last hidden state of a sequence is used to initialize the hidden state of the next sequence to approximate the full backpropagation. We train the model using Adam (Kingma & Ba, 2014) with an initial learning rate of 0.002. We divide the learning rate by a factor of 50 when the validation negative log-likelihood stopped decreasing. The norm of the gradient is clipped with a threshold of 1 (Mikolov et al., 2010; Pascanu et al., 2012). We also apply layer normalization (Ba et al., 2016) to our models. For all of the character-level language modelling experiments, we apply the same procedure, but only change the number of hidden units, mini-batch size and the initial learning rate.\nFor the Penn Treebank dataset, we use 512 units in each layer of the HM-LSTM and for the output embedding layer. In Table 1 (left), we compare the test BPCs of four variants of our model to other baseline models. Note that the HM-LSTM using the step function for the hard boundary decision outperforms the others using either sampling or soft boundary decision (i.e., hard sigmoid). The test BPC is further improved with the slope annealing trick, which reduces the bias of the straight-through estimator. We increased the slope a with the following schedule a = min (5, 1 + 0.04 \u00b7Nepoch), where Nepoch is the maximum number of epochs. The HM-LSTM achieves test BPC score of 1.24. For the remaining tasks, we fixed the hard boundary decision using the step function without slope annealing due to the difficulty of finding a good annealing schedule on large-scale datasets.\nText8 The Text8 dataset (Mahoney, 2009) consists of 100M characters extracted from the Wikipedia corpus. Text8 contains only alphabets and spaces, and thus we have total 27 symbols. In order to compare with other previous works, we follow the data splits used in Mikolov et al. (2012). We use 1024 units for each HM-LSTM layer and 2048 units for the output embedding layer. The mini-batch size and the initial learning rate are set to 128 and 0.001, respectively. The results are shown in Table 2. The HM-LSTM obtains the state-of-the-art test BPC 1.29.\nHutter Prize Wikipedia The Hutter Prize Wikipedia (enwik8) dataset (Hutter, 2012) contains 205 symbols including XML markups and special characters. We follow the data splits used in Graves (2013) where the first 90M characters are used to train the model, the next 5M characters for validation, and the remainders for the test set. We use the same model size, mini-batch size and the initial learning rate as in the Text8. In Table 1 (right), we show the HM-LSTM achieving the test BPC 1.32, which is a tie with the state-of-the-art result among the neural models. Although the neural models, show remarkable performances, their compression performance is still behind the best models such as PAQ8hp12 (Mahoney, 2005) and decomp8 (Mahoney, 2009).\nVisualizing Learned Hierarchical Multiscale Structure In Figure 3 and 4, we visualize the boundaries detected by the boundary detectors of the HM-LSTM while reading a character sequence of total length 270 taken from the validation set of either the Penn Treebank or Hutter Prize Wikipedia dataset. Due to the page width limit, the figure contains the sequence partitioned into three segments of length 90. The white blocks indicate boundaries z`t = 1 while the black blocks indicate the non-boundaries z`t = 0.\nInterestingly in both figures, we can observe that the boundary detector of the first layer, z1, tends to be turned on when it sees a space or after it sees a space, which is a reasonable breakpoint to separate between words. This is somewhat surprising because the model self-organizes this structure\nwithout any explicit boundary information. In Figure 3, we observe that the z1 tends to detect the boundaries of the words but also fires within the words, where the z2 tends to fire when it sees either an end of a word or 2, 3-grams. In Figure 4, we also see flushing in the middle of a word, e.g., \u201ctele-FLUSH-phone\u201d. Note that \u201ctele\u201d is a prefix after which a various number of postfixes can follow. From these, it seems that the model uses to some extent the concept of surprise to learn the boundary. Although interpretation of the second layer boundaries is not as apparent as the first layer boundaries, it seems to segment at reasonable semantic / syntactic boundaries, e.g., \u201cconsumers may\u201d - \u201cwant to move their telephones a\u201d - \u201clittle closer to the tv set <unk>\u201d, and so on.\nAnother remarkable point is the fact that we do not pose any constraint on the number of boundaries that the model can fire up. The model, however, learns that it is more beneficial to delay the information ejection to some extent. This is somewhat counterintuitive because it might look more beneficial to feed the fresh update to the upper layers at every time step without any delay. We conjecture the reason that the model works in this way is due to the FLUSH operation that poses an implicit constraint on the frequency of boundary detection, because it contains both a reward (feeding fresh information to upper layers) and a penalty (erasing accumulated information). The model finds an optimal balance between the reward and the penalty.\nTo understand the update mechanism more intuitively, in Figure 4, we also depict the heatmap of the `2-norm of the hidden states along with the states of the boundary detectors. As we expect, we can see that there is no change in the norm value within segments due to the COPY operation. Also, the color of \u2016h1\u2016 changes quickly (at every time step) because there is no COPY operation in the first layer. The color of \u2016h2\u2016 changes less frequently based on the states of z1t and z2t\u22121. The color of \u2016h3\u2016 changes even slowly, i.e., only when z2t = 1. A notable advantage of the proposed architecture is that the internal process of the RNN becomes more interpretable. For example, we can substitute the states of z1t and z 2 t\u22121 into Eq. 2 and infer which operation among the UPDATE, COPY and FLUSH was applied to the second layer at time step t. We can also inspect the update frequencies of the layers simply by counting how many UPDATE and FLUSH operations were made in each layer. For example in Figure 4, we see that the first layer updates at every time step (which is 270 UPDATE operations), the second layer updates 56 times,\nand only 9 updates has made in the third layer. Note that, by design, the first layer performs UPDATE operation at every time step and then the number of UPDATE operations decreases as the layer level increases. In this example, the total number of updates is 335 for the HM-LSTM which is 60% of reduction from the 810 updates of the standard RNN architecture.\n4.2 HANDWRITING SEQUENCE GENERATION\nWe extend the evaluation of the HM-LSTM to a real-valued sequence modelling task using IAMOnDB (Liwicki & Bunke, 2005) dataset. The IAM-OnDB dataset consists of 12, 179 handwriting examples, each of which is a sequence of (x, y) coordinate and a binary indicator p for pen-tip location, giving us (x1:Tn , y1:Tn , p1:Tn), where n is an index of a sequence. At each time step, the model receives (xt, yt, pt), and the goal is to predict (xt+1, yt+1, pt+1). The pen-up (pt = 1) indicates an end of a stroke, and the pen-down (pt = 0) indicates that a stroke is in progress. There is usually a large shift in the (x, y) coordinate to start a new stroke after the pen-up happens. We remove all sequences whose length is shorter than 300. This leaves us 10, 465 sequences for training, 581 for validation, 582 for test. The average length of the sequences is 648. We normalize the range of the (x, y) coordinates separately with the mean and standard deviation obtained from the training set. We use the mini-batch size of 32, and the initial learning rate is set to 0.0003.\nWe use the same model architecture as used in the character-level language model, except that the output layer is modified to predict real-valued outputs. We use the mixture density network as the output layer following Graves (2013), and use 400 units for each HM-LSTM layer and for the output embedding layer. In Table 3, we compare the log-likelihood averaged over the test sequences of the IAM-OnDB dataset. We observe that the HM-LSTM outperforms the standard LSTM. The slope annealing trick further improves the test log-likelihood of the HM-LSTM into 1167 in our setting. In this experiment, we increased the slope a with the following schedule a = min (3, 1 + 0.004 \u00b7Nepoch). In Figure 5, we let the HM-LSTM to read a randomly picked validation sequence and present the visualization of handwriting examples by segments based on either the states of z2 or the states of pen-tip location3.\n5 CONCLUSION\nIn this paper, we proposed the HM-RNN that can capture the latent hierarchical structure of the sequences. We introduced three types of operations to the RNN, which are the COPY, UPDATE and FLUSH operations. In order to implement these operations, we introduced a set of binary variables and a novel update rule that is dependent on the states of these binary variables. Each binary variable is learned to find segments at its level, therefore, we call this binary variable, a boundary detector. On the character-level language modelling, the HM-LSTM achieved state-of-the-art result on the Text8 dataset and comparable results to the state-of-the-art results on the Penn Treebank and Hutter Prize Wikipedia datasets. Also, the HM-LSTM outperformed the standard LSTM on the handwriting sequence generation. Our results and analysis suggest that the proposed HM-RNN can discover the latent hierarchical structure of the sequences and can learn efficient hierarchical multiscale representation that leads to better generalization performance.\n3The plot function could be found at blog.otoro.net/2015/12/12/handwriting-generation-demo-in-tensorflow/.\nACKNOWLEDGMENTS\nThe authors would like to thank Alex Graves, Tom Schaul and Hado van Hasselt for their fruitful comments and discussion. We acknowledge the support of the following agencies for research funding and computing support: Ubisoft, Samsung, IBM, Facebook, Google, Microsoft, NSERC, Calcul Qu\u00e9bec, Compute Canada, the Canada Research Chairs and CIFAR. The authors thank the developers of Theano (Team et al., 2016). JC would like to thank Arnaud Bergenon and Fr\u00e9d\u00e9ric Bastien for their technical support. JC would also like to thank Guillaume Alain, Kyle Kastner and David Ha for providing us useful pieces of code.\n", "cats": {"READ": 1.0, "DONTREAD": 0.0}, "reviews": [{"IS_META_REVIEW": true, "comments": "This paper proposes a novel variant of recurrent networks that is able to learn the hierarchy of information in sequential data (e.g., character->word). Their approach does not require boundary information to segment the sequence in meaningful groups (like in Chung et al., 2016).\n\nTheir model is organized as a set of layers that aim at capturing the information form different \u201clevel of abstraction\u201d. The lowest level activate the upper one and decide when to update it based on a controller (or state cell, called c). A key feature of their model is that c is a discrete variable, allowing potentially fast inference time. However, this makes their model more challenging to learn, leading to the use of the straight-through estimator by Hinton, 2012. \n\nThe experiment section is thorough and their model obtain competitive performance on several challenging tasks. The qualitative results show also that their model can capture natural boundaries.\n\nOverall this paper presents a strong and novel model with promising experimental results.\n\n\n\nOn a minor note, I have few remarks/complaints about the writing and the related work:\n\n- In the introduction:\n\u201cOne of the key principles of learning in deep neural networks as well as in the human brain\u201d : please provide evidence for the \u201chuman brain\u201d part of this claim.\n\u201cFor modelling temporal data, the recent resurgence of recurrent neural networks (RNN) has led to remarkable advances\u201d I believe you re missing Mikolov et al. 2010 in the references.\n\u201cin spite of the fact that hierarchical multiscale structures naturally exist in many temporal data\u201d: missing reference to Lin et al., 1996\n\n- in the related work:\n\u201cA more recent model, the clockwork RNN (CW-RNN) (Koutn\u00edk et al., 2014) extends the hierarchicalRNN (El Hihi & Bengio, 1995)\u201d : It extends the NARX model of Lin et al. 1996, not the El Hihi & Bengio, 1995.\nWhile the above models focus on online prediction problems, where a prediction needs to be made\u2026\u201d: I believe there is a lot of missing references, in particular to Socher\u2019s work or older recursive networks.\n\u201cThe norm of the gradient is clipped with a threshold of 1 (Pascanu et al., 2012)\u201d: this is not the first work using gradient clipping. I believe it was introduced in Mikolov et al., 2010.\n\nMissing references:\n\u201cRecurrent neural network based language model.\u201d, Mikolov et al. 2010\n\u201cLearning long-term dependencies in NARX recurrent neural networks\u201d, Lin et al. 1996\n\u201cSequence labelling in structured domains with hierarchical recurrent neural networks\u201c, Fernandez et al. 2007\n\u201cLearning sequential tasks by incrementally adding  higher  orders\u201d, Ring, 1993"}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "This extension to RNNs is clearly motivated, and the details of the proposed method are sensible. The paper would have benefitted from more experiments such as those in Figure 5 teasing out the representations learned by this model.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"TITLE": "No Title", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "This paper proposes a new multiscale recurrent neural network, where each layer has different time scale, and the scale is not fixed but variable and determined by a neural network. The method is elegantly formulated within a recurrent neural network framework, and shows the state-of-the-art performance on several benchmarks. The paper is well written.\n\nQuestion) Can you extend it to bidirectional RNN? \n", "IS_META_REVIEW": false, "RECOMMENDATION": 8, "DATE": "23 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "well evaluated and written paper, novel flush operation", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "The paper proposes a modified RNN architecture with multiple layers, where higher layers are only passed lower layer states if a FLUSH operation is predicted, consisting of passing up the state and reseting the lower layer's state. In order to select one of three operations at each time step, the authors propose using the straight-through estimator with a slope-annealing trick during training. Empirical results and visualizations illustrate that the modified architecture performs well at boundary detection.\n\nPros:\n- Paper is well-motivated, exceptionally well-composed\n- Provides promising initial results on learning hierarchical representations through visualizations and thorough experiments on language modeling and handwriting generation\n- The annealing trick with the straight-through estimator also seems potentially useful for other tasks containing discrete variables, and the trade-off in the flush operation is innovative.\nCons:\n- In a couple cases the paper does not fully deliver. Empirical results on computational savings are not given, and hierarchy beyond a single level (where the data contains separators such as spaces and pen up/down) does not seem to be demonstrated.\n- It's unclear whether better downstream performance is due to use of hierarchical information or due to the architecture changes acting as regularization, something which could hopefully be addressed.\n\n\n", "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "17 Dec 2016", "REVIEWER_CONFIDENCE": 3}, {"TITLE": "Good paper", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer4", "comments": "This paper proposes a novel variant of recurrent networks that is able to learn the hierarchy of information in sequential data (e.g., character->word). Their approach does not require boundary information to segment the sequence in meaningful groups (like in Chung et al., 2016).\n\nTheir model is organized as a set of layers that aim at capturing the information form different \u201clevel of abstraction\u201d. The lowest level activate the upper one and decide when to update it based on a controller (or state cell, called c). A key feature of their model is that c is a discrete variable, allowing potentially fast inference time. However, this makes their model more challenging to learn, leading to the use of the straight-through estimator by Hinton, 2012. \n\nThe experiment section is thorough and their model obtain competitive performance on several challenging tasks. The qualitative results show also that their model can capture natural boundaries.\n\nOverall this paper presents a strong and novel model with promising experimental results.\n\n\n\nOn a minor note, I have few remarks/complaints about the writing and the related work:\n\n- In the introduction:\n\u201cOne of the key principles of learning in deep neural networks as well as in the human brain\u201d : please provide evidence for the \u201chuman brain\u201d part of this claim.\n\u201cFor modelling temporal data, the recent resurgence of recurrent neural networks (RNN) has led to remarkable advances\u201d I believe you re missing Mikolov et al. 2010 in the references.\n\u201cin spite of the fact that hierarchical multiscale structures naturally exist in many temporal data\u201d: missing reference to Lin et al., 1996\n\n- in the related work:\n\u201cA more recent model, the clockwork RNN (CW-RNN) (Koutn\u00edk et al., 2014) extends the hierarchicalRNN (El Hihi & Bengio, 1995)\u201d : It extends the NARX model of Lin et al. 1996, not the El Hihi & Bengio, 1995.\nWhile the above models focus on online prediction problems, where a prediction needs to be made\u2026\u201d: I believe there is a lot of missing references, in particular to Socher\u2019s work or older recursive networks.\n\u201cThe norm of the gradient is clipped with a threshold of 1 (Pascanu et al., 2012)\u201d: this is not the first work using gradient clipping. I believe it was introduced in Mikolov et al., 2010.\n\nMissing references:\n\u201cRecurrent neural network based language model.\u201d, Mikolov et al. 2010\n\u201cLearning long-term dependencies in NARX recurrent neural networks\u201d, Lin et al. 1996\n\u201cSequence labelling in structured domains with hierarchical recurrent neural networks\u201c, Fernandez et al. 2007\n\u201cLearning sequential tasks by incrementally adding  higher  orders\u201d, Ring, 1993\n", "IS_META_REVIEW": false, "RECOMMENDATION": 8, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"DATE": "08 Dec 2016", "TITLE": "data size of Penn Treebank and toolkit", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1"}, {"DATE": "05 Dec 2016", "TITLE": "HM-RNN", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3"}, {"IS_META_REVIEW": true, "comments": "This paper proposes a novel variant of recurrent networks that is able to learn the hierarchy of information in sequential data (e.g., character->word). Their approach does not require boundary information to segment the sequence in meaningful groups (like in Chung et al., 2016).\n\nTheir model is organized as a set of layers that aim at capturing the information form different \u201clevel of abstraction\u201d. The lowest level activate the upper one and decide when to update it based on a controller (or state cell, called c). A key feature of their model is that c is a discrete variable, allowing potentially fast inference time. However, this makes their model more challenging to learn, leading to the use of the straight-through estimator by Hinton, 2012. \n\nThe experiment section is thorough and their model obtain competitive performance on several challenging tasks. The qualitative results show also that their model can capture natural boundaries.\n\nOverall this paper presents a strong and novel model with promising experimental results.\n\n\n\nOn a minor note, I have few remarks/complaints about the writing and the related work:\n\n- In the introduction:\n\u201cOne of the key principles of learning in deep neural networks as well as in the human brain\u201d : please provide evidence for the \u201chuman brain\u201d part of this claim.\n\u201cFor modelling temporal data, the recent resurgence of recurrent neural networks (RNN) has led to remarkable advances\u201d I believe you re missing Mikolov et al. 2010 in the references.\n\u201cin spite of the fact that hierarchical multiscale structures naturally exist in many temporal data\u201d: missing reference to Lin et al., 1996\n\n- in the related work:\n\u201cA more recent model, the clockwork RNN (CW-RNN) (Koutn\u00edk et al., 2014) extends the hierarchicalRNN (El Hihi & Bengio, 1995)\u201d : It extends the NARX model of Lin et al. 1996, not the El Hihi & Bengio, 1995.\nWhile the above models focus on online prediction problems, where a prediction needs to be made\u2026\u201d: I believe there is a lot of missing references, in particular to Socher\u2019s work or older recursive networks.\n\u201cThe norm of the gradient is clipped with a threshold of 1 (Pascanu et al., 2012)\u201d: this is not the first work using gradient clipping. I believe it was introduced in Mikolov et al., 2010.\n\nMissing references:\n\u201cRecurrent neural network based language model.\u201d, Mikolov et al. 2010\n\u201cLearning long-term dependencies in NARX recurrent neural networks\u201d, Lin et al. 1996\n\u201cSequence labelling in structured domains with hierarchical recurrent neural networks\u201c, Fernandez et al. 2007\n\u201cLearning sequential tasks by incrementally adding  higher  orders\u201d, Ring, 1993"}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "This extension to RNNs is clearly motivated, and the details of the proposed method are sensible. The paper would have benefitted from more experiments such as those in Figure 5 teasing out the representations learned by this model.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"TITLE": "No Title", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "This paper proposes a new multiscale recurrent neural network, where each layer has different time scale, and the scale is not fixed but variable and determined by a neural network. The method is elegantly formulated within a recurrent neural network framework, and shows the state-of-the-art performance on several benchmarks. The paper is well written.\n\nQuestion) Can you extend it to bidirectional RNN? \n", "IS_META_REVIEW": false, "RECOMMENDATION": 8, "DATE": "23 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "well evaluated and written paper, novel flush operation", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "The paper proposes a modified RNN architecture with multiple layers, where higher layers are only passed lower layer states if a FLUSH operation is predicted, consisting of passing up the state and reseting the lower layer's state. In order to select one of three operations at each time step, the authors propose using the straight-through estimator with a slope-annealing trick during training. Empirical results and visualizations illustrate that the modified architecture performs well at boundary detection.\n\nPros:\n- Paper is well-motivated, exceptionally well-composed\n- Provides promising initial results on learning hierarchical representations through visualizations and thorough experiments on language modeling and handwriting generation\n- The annealing trick with the straight-through estimator also seems potentially useful for other tasks containing discrete variables, and the trade-off in the flush operation is innovative.\nCons:\n- In a couple cases the paper does not fully deliver. Empirical results on computational savings are not given, and hierarchy beyond a single level (where the data contains separators such as spaces and pen up/down) does not seem to be demonstrated.\n- It's unclear whether better downstream performance is due to use of hierarchical information or due to the architecture changes acting as regularization, something which could hopefully be addressed.\n\n\n", "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "17 Dec 2016", "REVIEWER_CONFIDENCE": 3}, {"TITLE": "Good paper", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer4", "comments": "This paper proposes a novel variant of recurrent networks that is able to learn the hierarchy of information in sequential data (e.g., character->word). Their approach does not require boundary information to segment the sequence in meaningful groups (like in Chung et al., 2016).\n\nTheir model is organized as a set of layers that aim at capturing the information form different \u201clevel of abstraction\u201d. The lowest level activate the upper one and decide when to update it based on a controller (or state cell, called c). A key feature of their model is that c is a discrete variable, allowing potentially fast inference time. However, this makes their model more challenging to learn, leading to the use of the straight-through estimator by Hinton, 2012. \n\nThe experiment section is thorough and their model obtain competitive performance on several challenging tasks. The qualitative results show also that their model can capture natural boundaries.\n\nOverall this paper presents a strong and novel model with promising experimental results.\n\n\n\nOn a minor note, I have few remarks/complaints about the writing and the related work:\n\n- In the introduction:\n\u201cOne of the key principles of learning in deep neural networks as well as in the human brain\u201d : please provide evidence for the \u201chuman brain\u201d part of this claim.\n\u201cFor modelling temporal data, the recent resurgence of recurrent neural networks (RNN) has led to remarkable advances\u201d I believe you re missing Mikolov et al. 2010 in the references.\n\u201cin spite of the fact that hierarchical multiscale structures naturally exist in many temporal data\u201d: missing reference to Lin et al., 1996\n\n- in the related work:\n\u201cA more recent model, the clockwork RNN (CW-RNN) (Koutn\u00edk et al., 2014) extends the hierarchicalRNN (El Hihi & Bengio, 1995)\u201d : It extends the NARX model of Lin et al. 1996, not the El Hihi & Bengio, 1995.\nWhile the above models focus on online prediction problems, where a prediction needs to be made\u2026\u201d: I believe there is a lot of missing references, in particular to Socher\u2019s work or older recursive networks.\n\u201cThe norm of the gradient is clipped with a threshold of 1 (Pascanu et al., 2012)\u201d: this is not the first work using gradient clipping. I believe it was introduced in Mikolov et al., 2010.\n\nMissing references:\n\u201cRecurrent neural network based language model.\u201d, Mikolov et al. 2010\n\u201cLearning long-term dependencies in NARX recurrent neural networks\u201d, Lin et al. 1996\n\u201cSequence labelling in structured domains with hierarchical recurrent neural networks\u201c, Fernandez et al. 2007\n\u201cLearning sequential tasks by incrementally adding  higher  orders\u201d, Ring, 1993\n", "IS_META_REVIEW": false, "RECOMMENDATION": 8, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"DATE": "08 Dec 2016", "TITLE": "data size of Penn Treebank and toolkit", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1"}, {"DATE": "05 Dec 2016", "TITLE": "HM-RNN", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3"}]}
{"text": "Identifying Temporal Orientation of Word Senses\n1 000\n011\n012\n013\n014\n015\n016\n017\n018\n019\n020\n021\n022\n023\n024\n025\n026\n027\n028\n029\n030\n031\n032\n033\n034\n035\n036\n037\n038\n039\n040\n041\n042\n043\n044\n045\n046\n047\n048\n049\n061\n062\n063\n064\n065\n066\n067\n068\n069\n070\n071\n072\n073\n074\n075\n076\n077\n078\n079\n080\n081\n082\n083\n084\n085\n086\n087\n088\n089\n090\n091\n092\n093\n094\n095\n096\n097\n098\n099\nThe ability to capture the time information conveyed in natural language is essential to many natural language processing applications such as information retrieval, question answering, automatic summarization, targeted marketing, loan repayment forecasting, and understanding economic patterns. Therefore, a lexical temporal resource associating word senses with their underlying temporal orientation would be crucial for the computational tasks aiming at interpretation of language of time in text.\nIn this paper, we propose a semisupervised minimum cuts paradigm that makes use of WordNet definitions or \u2018glosses\u2019, its conceptual-semantic and lexical relations to supplement WordNet entries with information on the temporality of its word senses. Intrinsic and extrinsic evaluation results show that the proposed approach outperforms prior semisupervised, non-graph classification approaches to the temporality recognition of word senses/concepts, and confirm the soundness of the proposed approach.\n1 Introduction\nThere is considerable academic and commercial interest in processing time information in text, where that information is expressed either explicitly, or implicitly, or connotatively. Recognizing such information and exploiting it for Information Retrieval (IR) and Natural Language Processing (NLP) tasks are important features that can significantly improve the functionality of NLP/IR applications.\nAutomatic identification of temporal expressions in text is usually performed either via time taggers (Str\u00f6tgen and Gertz, 2013), which contain pattern files, such as uni-grams and bi-grams used to express temporal expressions in a given language (e.g. names of months) or various grammatical rules. As a rule-based system, time taggers are limited by the coverage of the rules for the different types of temporal expressions that it recognizes. To exemplify, the word \u2018present\u2019 in the sentence \u201cApple\u2019s iPhone is one of the most popular smartphones at present\u201d when labeled by SUTime1 is tagged as:\n<TIMEX3 tid=\u201ct1\u201d type=\u201cDATE\u201d\nvalue=\u201cPRESENT_REF\u201d>present</TIMEX3> It rightly tags the word \u2018present\u2019 in the above example, and refers to it as the present time when reference date is considered as same as the tagging date. However, such word based indicators can be misleading. For example, below is the tag from SUTime for the word \u2018present\u2019 in the sentence \u201cI was in Oxford Street getting the wife her birthday present\u201d. The tag gives us a false impression by wrongly labeling the word as a temporal one.\n<TIMEX3 tid=\u201ct1\u201d type=\u201cDATE\u201d\nvalue=\u201cPRESENT_REF\u201d>present</TIMEX3> Reasons for this misleading information are i) time taggers usually do not use contextual indicators while deciding on temporality ii) different word senses of a single word can actually be either temporal or atemporal. A typical temporallyambiguous word, i.e. a word that has at least one temporal and at least one atemporal sense, is \u2018present\u2019, as shown by the two examples above.\nWhereas most of the prior computational linguistics and text mining temporal studies have focused on temporal expressions and events, there has been a lack of work looking at the tempo-\n1http://nlp.stanford.edu:8080/sutime/ process\n2\n101\n102\n103\n104\n105\n106\n107\n108\n109\n110\n111\n112\n113\n114\n115\n116\n117\n118\n119\n120\n121\n122\n123\n124\n125\n126\n127\n128\n129\n130\n131\n132\n133\n134\n135\n136\n137\n138\n139\n140\n141\n142\n143\n144\n145\n146\n147\n148\n149\n150\n151\n152\n153\n154\n155\n156\n157\n158\n159\n160\n161\n162\n163\n164\n165\n166\n167\n168\n169\n170\n171\n172\n173\n174\n175\n176\n177\n178\n179\n180\n181\n182\n183\n184\n185\n186\n187\n188\n189\n190\n191\n192\n193\n194\n195\n196\n197\n198\n199\nral orientation of word senses. Therefore, we focus our study on automatically time-tagging word senses into past, present, future, or atemporal using their WordNet (Miller, 1995) definition, instead of tagging temporal words.\nIn this paper, we put forward a semi-supervised graph-based classification paradigm build on an optimization theory namely the max-flow min-cut theorem (Papadimitriou and Steiglitz, 1998). In particular, we propose minimum cut in a connected graph to time-tag each synset of WordNet to one of the four dimensions: atemporal, past, present, and future. Our methodology was evaluated both intrinsically and extrinsically. It outperformed prior approaches to the temporality recognition of word senses/concepts. First, a gold standard is created using a crowdsourced annotation service to test our methodology. Second, temporal classification task is performed. Results show qualitative improvements when compared to previous state-of the-art approaches. Results also evidence that to achieve the performance of a standard supervised approach with our model, we need less than 10% of the training data.\n2 Related Work\nTemporality in NLP and IR: Temporality has recently received increased attention in Natural Language Processing (NLP) and Information Retrieval (IR). Initial works proposed in NLP and IR are exhaustively summarized in (Mani et al., 2005). The introduction of the TempEval task (Verhagen et al., 2009) and subsequent challenges (TempEval-2 and -3) in the Semantic Evaluation workshop series have clearly established the importance of time to deal with different NLP tasks. In IR, the work of (Baeza-Yates, 2005) defines the foundations of Temporal-IR. Since, research have been tackling several topics such as query understanding (Metzler et al., 2009), temporal snippets generation (Alonso et al., 2007), temporal ranking (Kanhabua et al., 2011), temporal clustering (Alonso et al., 2009), or future retrieval (Radinsky and Horvitz, 2013).\nIn order to push forward further research in temporal NLP and IR, (Dias et al., 2014) developed TempoWordNet (TWn), an extension of WordNet (Miller, 1995), where each synset is augmented with its temporal connotation (past, present, future, or atemporal). It mainly relies on the quantitative analysis of the glosses associated to synsets,\nand on the use of the resulting vectorial term representations for semi-supervised synset classification. While (Hasanuzzaman et al., 2014a) show that TWn can be useful to time-tag web snippets, less comprehensive results are shown in (Filannino and Nenadic, 2014), where TWn learning features did not lead to any classification improvements. In order to propose a more reliable resource, (Hasanuzzaman et al., 2014b) recently defined two new propagation strategies: probabilistic and hybrid respectively leading to TWnP and TWnH. Although some improvements was evidenced, no conclusive remarks could be reached.\nThere are several disadvantages of their approaches. First, it relies mostly on WordNet glosses and do not effectively exploit WordNet\u2019s relation structure. Whereas we concentrate on the use of WordNet relations, glosses, and other attributes for the classification process. Second, strategies adopted to build TWnP and TWnH mainly depend on the probability estimates for the classes from Support Vector Machines (SVM) classifiers. However, probabilities are derived without using any post-calibration method. Converting scores to accurate probability estimates for multiclass problems requires a post-calibration procedure. In addition, there is no standard evaluation as to the accuracy of their approach. Graph-based Classification: To the best of our knowledge, we present the first work, which aims to recognize temporal dimension of word senses via graph-based classification algorithm. However, graph-based algorithms have been used to classify sentences and documents into subjective/objective or positive/negative level (Pang and Lee, 2004; Agarwal and Bhattacharyya, 2005), instead of aiming at tagging at word sense level as we do. At the word level, a semi-supervised spin model is used for word polarity determination, where the graph is constructed using a variety of information such as gloss co-occurrences and WordNet links (Takamura et al., 2005). However, their model differs from ours.\n3 Semi-supervised Mincuts\n3.1 Minimum cuts: Main Idea\nUnderlying idea behind classification with minimum cuts (Mincuts) in graph is that similar items should be grouped together in the same cut. Suppose we have n items x1, ......xn to divide into two classes C1 and C2 based on the two types of in-\n3\n201\n202\n203\n204\n205\n206\n207\n208\n209\n210\n211\n212\n213\n214\n215\n216\n217\n218\n219\n220\n221\n222\n223\n224\n225\n226\n227\n228\n229\n230\n231\n232\n233\n234\n235\n236\n237\n238\n239\n240\n241\n242\n243\n244\n245\n246\n247\n248\n249\n267\n268\n269\n270\n271\n272\n273\n274\n275\n276\n277\n278\n279\n280\n281\n282\n283\n284\n285\n286\n287\n288\n289\n290\n291\n292\n293\n294\n295\n296\n297\n298\n299\nformation at hand. The first one, individual score ind j(xi) is the non-negative estimate of each xi\u2019s preference for being in class C j based on the features of xi alone. While the later one, association scores assoc(xi,xk) represent a non-negative estimates of how important it is that xi and xk be in the same class. Overall idea is to maximize each item\u2019s net score i.e. individual score for the class it is assigned to minus its individual score for the other class and penalize putting tightly associated items into different classes. It can be seen as the following optimization problem: assign the xis to C1 and C2 so as to minimize the partition cost.\n\u2211 x\u2208C1 ind2(x)+ \u2211 x\u2208C2 ind1(x)+ \u2211 xi\u2208C1,xk\u2208C2 assoc(xi,xk) (1)\nWe could represent the situation by building an undirected graph G with vertices {v1, .....,vn,s, t}; s and t are source and sink respectively. Add n edges (s,vi), each with weight ind1(xi), n edges (vi, t), each with weight ind2(xi). Finally, add (n 2\n) edges (vi,vk), each with weight assoc(xi,xk). Finally, cuts in G are defined as follows:\nDefinition 1. A cut (S,T ) of G is a partition of its nodes into sets S= {s}\u222a S\u2032 and T = {t}\u222a T \u2032 where s /\u2208 S\u2032, t /\u2208 T \u2032. Its cost cost(S,T ) is the sum of the weights of all edges crossing from S to T . A minimum cut of G is one of minimum cost.\nFigure 1 illustrates an example of the concepts for classifying three items. Brackets enclose example values; here, the individual scores happen to be probabilities. Based on individual scores alone, we would put Y (\u201cPromise.\") in s (Temporal class), N (\u201cChair\") in t (Atemporal class), and be undecided about M (\"Oath\"). But the association scores favour cuts that put Y and M in the same class, as shown in the table. Thus, the minimum cut, indicated by the dashed red line, places M together Y in s.\n3.2 Advantages\nFormulating the task of temporality detection problem on word senses in terms of graphs allows us to model item-specific and pair-wise information independently. Therefore, it is a very flexible paradigm where we have two different views on the data. For example, rule based approaches or machine learning algorithms employing linguistic and other features representing temporal indicators can be used to derive individual scores for a particular item in isolation. The\nedges weighted by the individual scores of a vertex (=word sense) to the source/sink can be interpretative as the probability of a word sense being temporal or atemporal without taking similarity to other senses into account. And we could also simultaneously use conceptual-semantic and lexical relations from WordNet to derive the association scores. The edges between two items weighted by the association scores can indicate how similar/different two senses are. If two senses are connected via a temporality-preserving relation they are likely to be both temporal or in opposite atemporal. An example here is the hyponymy relation, a temporality-preserving relation (Dias et al., 2014), where two hyponymy such as present, nowadays\u2014the period of time that is happening now and now\u2014the momentary present are both temporal. To detect the temporal orientation of word senses, authors in (Dias et al., 2014) adopted a single view instead of two on the data. The ability to combine two views on the data is precisely one of the strengths of our approach.\nSecond, Mincuts can be easily expanded into a semi-supervised framework. This is essential as the existing labeled datasets for our problem are small. In addition, glosses are short, leading to sparse high dimensional vectors in standard feature representations. Also, WordNet connections between different parts of the WordNet hierarchy can also be sparse, leading to relatively isolated senses in a graph in a supervised framework.\n4\n301\n302\n303\n304\n305\n306\n307\n308\n309\n310\n311\n312\n313\n314\n315\n316\n317\n318\n319\n320\n321\n322\n323\n324\n325\n326\n327\n328\n329\n330\n331\n332\n333\n334\n335\n336\n337\n338\n339\n340\n341\n342\n343\n344\n345\n346\n347\n348\n349\n350\n351\n352\n353\n354\n355\n356\n357\n358\n359\n360\n361\n362\n363\n364\n365\n366\n367\n368\n369\n370\n371\n372\n373\n374\n375\n376\n377\n378\n379\n380\n381\n382\n383\n384\n385\n386\n387\n388\n389\n390\n391\n392\n393\n394\n395\n396\n397\n398\n399\nSemi-supervised Mincuts allow us to import unlabeled data that can serve as bridges to isolated components. More importantly, the unlabeled data can be related to the labeled data (by some WordNet relation), it might help pull unlabeled data to the right cuts (categories).\n3.3 Formulation of Semi-Supervised Mincuts\nFormulation of our semi-supervised Mincut for synset temporality classification involves the following steps.\nI We depute two vertices s (source) and t (sink) which corresponds to the \u201ctemporal\u201d and \u201catemporal\u201d category respectively. We call the vertices s and t classification vertices, and all other vertices (labeled, test, and unlabeled data) example vertices. Each example vertex corresponds to one WordNet synset and is connected to both s and t through an undirected weighted edge. This confirms that the graph is connected.\nII The labeled training examples are connected to classification vertices they belong to via edges with high constant non-negative weight. Unlabeled examples are connected to the classification vertices via edges weighted with non-negative scores that indicate the degree of association with temporal/atemporal category. We use a classifier to assign these edges weights.\nIII Conceptual-semantic and lexical relations available in the WordNet are used to construct edges between two example vertices. Such edges can exist between any pair of example vertices, for instance between two unlabeled vertices.\nIV After building graph, we then employ a maximum-flow algorithm to find the minimum s\u2212 t cuts of the graph. The cut in which the source vertex s lies is classified as \u201ctemporal\u201d and the cut in which sink vertex t lies is labeled as \u201catemporal\u201d.\nV In order to fine tune the temporal part, we follow hierarchical strategy by organizing the classes (past, present, future) according to a hierarchy. The hierarchy of classes is decided based on the classes that are easier to discriminate to improve the overall classification accuracy. First, we define two vertices s (source) and t (sink) which correspond to the\n\u201cpast\u201d and \u201cNot_Past\u201d temporal categories respectively. Then we follow the above steps I through IV . This divides the temporal part into two disjoint subsets: past synsets and synsets belong to present and future temporal category. Finally, we repeat steps I through IV where vertices s (source) and t (sink) corresponds to \u201cfuture\u201d and \u201cpresent\u201d category respectively.\nLabeled and unlabeled data selection: We use the same temporal (past, present, future) and atemporal sets of synsets considered as training data at the time of building TempoWordNet (TWnL) (Dias et al., 2014) as training /labeled data for our experiments. For test set, sample of synsets outside the labeled data is selected and annotated using crowdsourcing service. All other synsets outside labeled and test set are considered as unlabeled data. Weighting of edges to the classification vertices: The edge weight (non-negative) to the source s and the sink t denotes how likely it is that an example vertex is put in the cut in which s (temporal) or t (atemporal) lies. For the unlabeled and test examples, a supervised learning strategy (over the labeled data as training set) is used to assign the edge weights. Each synset is represented by its gloss encoded as a vector of word unigrams weighted by their frequency in the gloss. As for classifier, we used SVM from the Weka platform2. In order to ensure that Mincut does not reverse the labels of the labeled training data, we assign a high3 constant non-negative weight of 3 to the edge between a labeled vertex and its corresponding classification vertex, and a low weight of 0.001 to the edge to the other classification vertex. Deriving weights for WordNet relations: While formulating the graph, we connect two vertices by an edge if they are linked by one of the ten (10) WordNet relations in Table 1. Main motivation towards using other relations in addition to the most frequently encoded relations (hypernym, hyponym) among synsets in WordNet is to achieve high graph connectivity. Moreover, we can assign different weights to different relations to reflect the degree to which they are temporality preserving. Therefore, we adopt two strategies to assign\n2http://www.cs.waikato.ac.nz/ml/weka/ [Last access: 12/04/2015].\n3w.r.t. the probability estimates (after calibration) of the classes from SVM.\n5\n401\n402\n403\n404\n405\n406\n407\n408\n409\n410\n411\n412\n413\n414\n415\n416\n417\n418\n419\n420\n421\n422\n423\n424\n425\n426\n427\n428\n429\n430\n431\n432\n433\n434\n435\n436\n437\n438\n439\n440\n441\n442\n443\n444\n445\n446\n447\n448\n449\n450\n451\n452\n453\n454\n455\n456\n457\n458\n459\n460\n461\n462\n463\n464\n465\n466\n467\n468\n469\n470\n471\n472\n473\n474\n475\n476\n477\n478\n479\n480\n481\n482\n483\n484\n485\n486\n487\n488\n489\n490\n491\n492\n493\n494\n495\n496\n497\n498\n499\nweights to different WordNet relations. The first method (ScWt), assigns the same constant weight of 1.0 to all WordNet relations.\nThe second method (DiffWt), considers several degrees of preserving temporality. In order to do this, we adopt a simple strategy to produce large noisy set of temporal 4 and atemporal synsets from WordNet. This method uses a list of 30 handcrafted temporal seeds (equally distributed over past, present, and future temporal categories) proposed in (Dias et al., 2014) along with their direct hyponym5 to classify each WordNet synset with at least one temporal word6 in its gloss as temporal and all other synsets as atemporal. We then simply count how often two synsets connected by a given relation (edge) have the same or different temporal dimension. Finally, weight is calculated by #same/(#same+#different). Results are reported in Table 1.\nWordnet Relation #same #different Weight Direct-Hypernym 61914 9600 0.76 Direct-Hyponym 73268 7246 0.91\nAntonym 1905 3614 0.35 Similar-to 6587 1914 0.77 Derived-from 3630 1947 0.65 Also-see 1037 337 0.75 Attribute 350 109 0.76\nTroponym 6917 2651 0.72 Domain 2380 2895 0.45 Domain-member 2380 2895 0.45\nTable 1: WordNet relation weights (DiffWt Method)\n4 Experiments and Evaluation\n4.1 Datasets\nLabeled Data: We used a list that consists of 632 temporal synsets in WordNet and equal number of atemporal synsets provided by (Dias et al., 2014) as labeled data for our experiments. Temporal synsets are distributed as follows: 210 synsets marked as past, 291 as present, and 131 as future. Building of a Gold Standard: Since to our best knowledge, there is no gold standard resource with temporal association of words (except 30 handcrafted temporal seeds proposed by Dias et al., 2014), we designed our own annotation task using the crowdsourcing service of CrowdFlower platform7. For the annotation task, three hundred\n4To fine tune the temporal part we used the same strategy for tagging past, present, and future synsets following hierarchical strategy\n5Relation which preserves temporality according to (Dias et al., 2014)\n6Most frequent sense of the temporal word from WordNet is selected\n7http://www.crowdflower.com/\nninety eight (398) synsets equally distributed over nouns, verbs, adjectives and adverbs POS categories along with their lemmas and glosses are selected randomly from WordNet 8 as representative of the whole WordNet. Note that this number of synset is statistically significant representative sample of total WordNet synsets (117000 plus synsets) and derived using the formula described in (Israel, 1992). Afterwards, we designed two question that the annotators were expected to answer for a given synset (lemmas and gloss are also provided). While the first question is related to the decision which reflects a synset being temporal or atemporal, the motivation behind the second question is to collect a more fine-grained (past, present, future) gold-standard for synset-temporality association. Details of annotation guideline is out of scope of this paper.\nThe reliability of the annotators was evaluated on the basis of 60 control synsets 9 provided by (Dias et al., 2014) which were clearly associated either with a specific temporal or atemporal dimension and 10 temporally ambiguous synsets associated with more than one temporal dimensions. Similar to (Tekirog\u0306lu et al., ), the raters who scored at least 70% accuracy on average on both sets of control synsets were considered to be reliable. Each unit was annotated by at least 10 reliable raters.\nTable 2 demonstrates the observed agreement. Similar to (Mohammad, 2011; \u00d6zbal et al., 2011), annotations with a majority class greater than 5 is considered as reliable. Indeed for temporal vs atemporal) classification, 84.83 % of the synset annotations the absolute majority agreed on the same decision, while for past, present, and future, 72.36% of the annotations have majority class greater than 5. The high agreement confirms the quality of the resulting gold standard data.\n4.2 Semi-supervised Graph Mincuts\nTemporal Vs Atemporal Classification: Using our formulation in Section 3.3, we construct a connected graph by importing 1264 training set (632 temporal and 632 atemporal synsets), 398 gold standard test set created using a crowdsourced service, and 115996 unlabeled synsets10. We con-\n8WordNet version 3.0 used and selected outside from the labeled data set\n930 temporal synsets (equally distributed over past, present, future) and 30 atemporal synsets\n10All synset of WordNet\u2212(training set+test set)\n6\n501\n502\n503\n504\n505\n506\n507\n508\n509\n510\n511\n512\n513\n514\n515\n516\n517\n518\n519\n520\n521\n522\n523\n524\n525\n526\n527\n528\n529\n530\n531\n532\n533\n534\n535\n536\n537\n538\n539\n540\n541\n542\n543\n544\n545\n546\n547\n548\n549\n550\n551\n552\n553\n554\n555\n556\n557\n558\n559\n560\n561\n562\n563\n564\n565\n566\n567\n568\n569\n570\n571\n572\n573\n574\n575\n576\n577\n578\n579\n580\n581\n582\n583\n584\n585\n586\n587\n588\n589\n590\n591\n592\n593\n594\n595\n596\n597\n598\n599\nMajority Class 3 4 5 6 7 8 9 10 Synset as temporal or atemporal 0 .20 1.21 4.32 10.69 14.56 29.34 19.23 11.01\nTemporal synset into past, present, or future 1.23 3.01 10.45 20.22 16.56 12.34 14.23 9.01\nTable 2: Percentage of synsets in each majority class.\nstruct edge weights to classification vertices, s (temporal) and t (atemporal) by using a SVM classifier discussed above. WordNet relations for links between example vertices are weighted either by non-negative constant value (ScWt) or by the method \u2018DiffWt\u2019 illustrated in Table 1. Temporally tagged synsets into past, present, and future: In order to fine tune the temporal part, we construct another connected graph by importing 632 labeled temporal synsets, temporal part of the gold standard test data (127 synsets out of 398 synsets), temporally tagged synsets as unlabeled data 11 and follow the same strategy presented in Section 3.3 where classification vertices, s and t correspond to \u2018past\u2019 and \u2018Not_past\u2019 temporal categories respectively. Finally, temporal synsets tagged as \u2018Not_past\u2019 classified either as present or future following the same analogy.\n4.3 Evaluation\nThe underlying idea being that a reliable resource must evidence high quality time-tagging as well as improved performance for some external tasks.\n4.4 Intrinsic Evaluation\nBaseline: In order to compare our semisupervised Mincut approach to a reasonable baseline, we use rule based approach to classify goldstandard data into past, present, future, or atemporal based on its lemmas and glosses. First, time expressions in the glosses of synsets are labeled and resolved via Standford\u2019s SUTime tagger, which give accuracy in line with the stateof-the-art systems at identifying time expressions at TempEval. For each synset, Named-entity time tag provided by SUTime (e.g.\u201cfuture_ref\u201d or \u201cpresent_ref\u201d etc.) for the time expression present in its gloss is considered as the temporal class for that particular synset. In case of more than one temporal expression present12 in the gloss of a synset, majority class of the time tags is se-\n11Total number of synsets classified as temporal \u2212 (Total number of temporal synsets in training data+Total number of gold standard temporal synsets tagged as temporal at the time of temporal vs atemporal classification process)\n12Found very rarely < 1%\nlected. Secondly, if no time expression is identified by the time tagger, a list composed of 30 handcrafted temporal seeds proposed in (Dias et al., 2014) along with their direct hyponym and standard temporal adverbials (since), prepositions (before/after), adjectives (former) etc. is used to classify synsets with at least one temporal word13 in its lemma(s) and gloss as temporal (past, present, future) and all other synsets as atemporal. Finally, performance of the rule based approach is measured for the gold standard data set and presented in Table 3. To figure out the contribution of word sense disambiguation, classic Lesk algorithm (Lesk, 1986) is used to choose right sense/synset for a word instead of most frequent sense. We found that contribution is negligible (< 0.4% improvement in overall accuracy).\nTo strengthen comparative evaluation of our semi-supervised Mincut approach, we propose to test our methodology with prior works (TempoWordNet: TWnL, TWnP, and TWnH). Comparative evaluation results are presented in Table 3. Results show the Mincut approach (CFG2) outperforms state-of-the-art approaches. It achieves highest accuracies for both temporal vs. atemporal and past, present, future classification with improvement of 11.3% and 10.3% respectively over the second best TempoWordNet versions (TWnH). Considering the above findings, we select our best Mincut configuration CFG2 for the remaining experiments. Distribution of time-tag synsets produced by this configuration: atemporal=110002, past=1733, present=4193, future=1730. Some examples are given below: \u2022 late\u2013having died recently. (Past) \u2022 present, nowadays\u2013the period of time that is\nhappening now. (Present) \u2022 promise\u2013a verbal commitment by one person to\nanother. (Future) \u2022 field\u2013a piece of land cleared of trees and usually\nenclosed. (Atemporal) Performance with different size training data: We randomly generate subsets of labeled\n13Most frequent sense of the temporal word from WordNet is selected\n7\n601\n602\n603\n604\n605\n606\n607\n608\n609\n610\n611\n612\n613\n614\n615\n616\n617\n618\n619\n620\n621\n622\n623\n624\n625\n626\n627\n628\n629\n630\n631\n632\n633\n634\n635\n636\n637\n638\n639\n640\n641\n642\n643\n644\n645\n646\n647\n648\n649\n650\n651\n652\n653\n654\n655\n656\n657\n658\n659\n660\n661\n662\n663\n664\n665\n666\n667\n668\n669\n670\n671\n672\n673\n674\n675\n676\n677\n678\n679\n680\n681\n682\n683\n684\n685\n686\n687\n688\n689\n690\n691\n692\n693\n694\n695\n696\n697\n698\n699\nTable 3: Accuracy (percentage classified correct) for temporal vs. atemporal and temporal into past, present, future classification using different methods namely TempoWordNet (TWnL, TWnP, TWnH) and Mincuts measured over created gold-standard data. CFG1 corresponds to the Mincut that uses SVM classifier to infer edges weights of unlabeled and test examples to the classification vertices s and t and predefined constant weights for WordNet relations (ScWt). CFG2 corresponds to the Mincut approach that uses the same SVM classifier to infer edges weights but uses a list of temporal synsets to infer weights of Wordnet relations (DiffWt). Both of our configurations perform significantly better than previous approaches. Results are also broken down by precision (p), recall (r), and f1 score for past, present, future, and atemporal categories.\nMethod Baseline TWnL TWnP TWnH CFG1 CFG2 Accuracy 48.8 65.6 62.0 68.4 74.4 79.7\ntemporal (p, r, f1) (52.0, 56.3, 54.0) (63.5, 82.1, 71.6) (55.8, 84.2, 67.1) (67.4, 81.9, 73.9) (84.5, 79.8, 82.0) (89.1, 79.3, 83.9) atemporal (p, r, f1) (58.2, 54.2, 56.1) (68.3, 79.2, 73.3) (58.9, 75.6, 66.2) (69.3, 82.6, 75.3) (81.3, 86.6, 83.8 ) (87.4, 90.8, 89.1)\nAccuracy 45.6 62.0 59.6 65.7 72.7 76.0 past (p, r, f1) (49.3, 46.7, 47.9) (61.2, 73.0, 66.5) (59.3, 79.1, 67.7) (63.1, 75.0, 68.0) (71.1, 79.5, 75.0) (81.2, 78.5, 79.8) present (p, r, f1) (55.3, 48.2, 51.5) (63.0, 75.2, 68.5) (58.0, 78.2 66.0 ) (77.4, 69.2, 73.0) (73.0, 71.5, 72.2) (85.1, 74.7, 79.0) future (p, r, f1) (48.5, 49.0, 48.7) (62.1, 71.9, 66.6) (57.0, 83.1, 67.6) (60.0, 75.6, 66.8) (79.4, 69.5, 74.0) (86.1, 70.0, 77.2)\ndata/training data (1064 synsets: 632 temporal and 632 atemporal) L1,L2,L3......Ln, and ensures that L1 \u2282 L2 \u2282 L3...... \u2282 Ln. As proposed in (Dias et al., 2014), binary classification models based on the generated subsets of labeled data are learned. Using the same subsets of labeled data, we formulate our best performing minimum cut (CFG2). Accuracies of both approaches are presented in Table 4. As can be seen from the table, semi-supervised Mincut performs consistently better than the previous semi-supervised non-graph classification approach (SVM). Moreover, our proposed graph classification framework with only 400 labeled data/training data examples achieves even higher accuracies than SVM with 1264 training items (73.7% vs 68.4% ).\nNumber of labeled data SVM (TWnH) Minncut (CFG2) 100 59.8 64.3 200 62.6 67.5 400 65.5 73.7 600 67.4 77.6 800 67.9 79.2 1000 68.0 79.0\n1264 (all) 68.4 79.7\nTable 4: Accuracy with different sizes of label data for temporal vs. atemporal classification.\n4.5 Extrinsic Evaluation\nFor extrinsic evaluation, we focus on the problem of classifying temporal relations task of TempEval-3, assuming that identification of events, times are already performed. The underlying idea is that proposed method to yield a timeenhanced WordNet has a greater positive impact on more applied temporal information extraction tasks, whose output is useful for many information retrieval applications.\nIn order to produce comparative results with best performing system at TempEval-3 namely\nUTTime (Laokulrat et al., 2013) for the above task, we follow the guidelines and use the same data sets provided by the evaluation campaign organizers. We restrict14 our experiment to a subset of relations namely BEFORE (CORR. Past), AFTER (CORR. Future), and INCLUDES (CORR. Present) with all other relation mapped to the \u2018NA-RELATION\u2019 for event to document creation time and event to same sentence event. For the task, we adopt a very simple strategy and implement the following features. \u2022 String features: The tokens and lemmas of each\nentity pair. \u2022 Grammatical features: The part-of-speech\n(PoS) tags of the entity pair (only for eventevent pairs). The grammatical information is obtained using the Standford CoreNLPtool15. \u2022 Entity attributes: Entity pair attributes as pro-\nvided in the data set. \u2022 Dependency relation: We used the information\nrelated to the dependency relation between two entities such as type of dependency, dependency order. \u2022 Textual context: The textual order of entity pair. \u2022 Lexica: The relative frequency of temporal cat-\negories, based on the resource developed in this research, in the text appearing between entity pair (event to same sentence event), text of all tokens in the time expression, 5 tokens following and preceding time expression/event. The features are encoded as the frequency with which a word from a temporal category (past, present, future) appeared in the text divided by the total number of tokens in the text.\n14Because of the complexity to map 14 relations of TempEval-3 into three temporal classes (past, present, future) considered for this experiment\n15http://stanfordnlp.github.io/CoreNLP/\n8\n701\n702\n703\n704\n705\n706\n707\n708\n709\n710\n711\n712\n713\n714\n715\n716\n717\n718\n719\n720\n721\n722\n723\n724\n725\n726\n727\n728\n729\n730\n731\n732\n733\n734\n735\n736\n737\n738\n739\n740\n741\n742\n743\n744\n745\n746\n747\n748\n749\n750\n751\n752\n753\n754\n755\n756\n757\n758\n759\n760\n761\n762\n763\n764\n765\n766\n767\n768\n769\n770\n771\n772\n773\n774\n775\n776\n777\n778\n779\n780\n781\n782\n783\n784\n785\n786\n787\n788\n789\n790\n791\n792\n793\n794\n795\n796\n797\n798\n799\nWe build our system namely T RelMincuts (composed of two classifiers) using Support Vector Machine (SVM) implementation of Weka over the features set and training data provided by the organizers. The best classifier for event to document creation time and event to same sentence event relation are selected via a grid search over parameter settings. The grid is evaluated with a 5- fold cross validation on the training data. We also measure the performance of UTTime for the above stated settings. Additionally, we build T RelTWnH by adopting same strategy and features set \u2018Lexica\u2019 is computed from prior time-tagging approach namely TWnH. Table 5 presents comparative evaluation results.\nApproaches Precision(%) Recall(%) F1(%) UT Time 57.5 58.7 58.1 T RelMincuts 66.9 68.7 67.7 T RelTWnH 61.2 62.5 61.8\nTable 5: Performance of different approaches on temporal relation classification based on TempEval-3 Evaluation strategy.\nResults evidence that T RelMincuts significantly outperforms all other approaches and achieve highest performance in terms of precision (+5.7), recall (+6.2), and F1 score (+5.9). Results also demonstrate that our approach achieves 9.6% improvement in terms of F1 score over the best performing system in TempEval-3.\nWe perform feature ablation analyses as presented in Table 6 in order to measure features contribution. As can be seen, every features set produced improvement over baseline (mfc) and highest improvement is achieved with features set (Lexica) that comes from the proposed temporal lexical resource. The result also implies that while each feature type contains certain temporal information, there is also some redundancy across the feature types.\nFeatures F1(%) Features F1(%) mfc baseline 33.55 all features 67.7 string alone 45.06 w/o string 65.70 grammatical alone 46.96 w/o grammatical 64.85 entity alone 52.23 w/o entity 62.08 dependency alone 48.65 w/o dependency 65.06 textual alone 46.82 w/o textual 64.96 lexica alone 51.62 w/o lexica 62.76\nTable 6: Feature ablation analysis of F1 score. The most frequent class baseline (mfc) indicates accuracy if only predicting the present class.\n5 Discussion\nOne important remark can be made related to the difficulty of the task. This is particularly due to the fact that the temporal dimension of synsets is mainly judged upon their definition. For example, \u201cdinosaur\u201d can be classified as temporal or atemporal as its gloss \u201cany of numerous extinct terrestrial reptiles of the Mesozoic era\u201d allows both interpretations. Apart from it, while digging into the results we observed that classifying the temporal synsets into past, present, or future is more difficult than temporal vs. atemporal classification. It is due to the fact that past, present and future connotations are only indicative of the temporal orientation of the synset but cannot be taken as a strict class. Indeed, there are many temporal synsets, which are neither past, present nor future (e.g.monthly \u2013 a periodical that is published every month).\n6 Conclusions\nIn this paper, we proposed a semi-supervised minimum cut framework to address the relatively unexplored problem of associating word senses with their underlying temporal dimensions. The basic idea is that instead of using single view, multiple views on the data would result in better temporal classification accuracy thus lead to a accurate and reliable temporal lexical resource. Thorough and comparative evaluations are performed to measure the quality of the resource. The results confirm the soundness of the proposed approach and the usefulness of the resource for temporal relation classification task. The resource is publicly available on https://www.anonymous.anonymous so that the community can benefit from it for relevant tasks and applications.\nFrom a resource point of view, we would like to explore the effect of other graph construction methods, such as the use of freely available online dictionaries including thesaurus and distributional similarity measures. We would also like to use the resource for various applicative scenarios such as automatic analysis of time-oriented clinical narratives. As an example, one can imagine a system that automatically analyzes medical discharge summaries including previous diseases related to the current conditions, treatments, and the family history for medical decision making, data modeling and biomedical research.\n9\n801\n802\n803\n804\n805\n806\n807\n808\n809\n810\n811\n812\n813\n814\n815\n816\n817\n818\n819\n820\n821\n822\n823\n824\n825\n826\n827\n828\n829\n830\n831\n832\n833\n834\n835\n836\n837\n838\n839\n840\n841\n842\n843\n844\n845\n846\n847\n848\n849\n850\n851\n852\n853\n854\n855\n856\n857\n858\n859\n860\n861\n862\n863\n864\n865\n866\n867\n868\n869\n870\n871\n872\n873\n874\n875\n876\n877\n878\n879\n880\n881\n882\n883\n884\n885\n886\n887\n888\n889\n890\n891\n892\n893\n894\n895\n896\n897\n898\n899\n", "cats": {"READ": 1.0, "DONTREAD": 0.0}, "reviews": [{"comments": "This paper presents an approach to tag word senses with temporal information\n(past, present, future or atemporal). They model the problem using a\ngraph-based semi-supervised classification algorithm that allows to combine\nitem specific information - such as the presence of some temporal indicators in\nthe glosses - and the structure of Wordnet - that is semantic relations between\nsynsets \u00e2\u0080\u0093, and to take into account unlabeled data. They perform a full\nannotation of Wordnet, based on a set of training data labeled in a previous\nwork and using the rest of Wordnet as unlabeled data. Specifically, they take\nadvantage of the structure of the label set by breaking the task into a binary\nformulation (temporal vs atemporal), then using the data labeled as temporal to\nperform a finer grained tagging (past, present or future). In order to\nintrinsically evaluate their approach, they annotate a subset of synsets in\nWordnet using crowd-sourcing. They compare their system to the results obtained\nby a state-of-the-art time tagger (Stanford's SUTime) using an heuristic as a\nbackup strategy, and to previous works. They obtain improvements around 11% in\naccuracy, and show that their approach allows performance higher than previous\nsystems using only 400 labeled data. Finally, they perform an evaluation of\ntheir resource on an existing task (TempEval-3) and show improvements of about\n10% in F1 on 4 labels.\n\nThis paper is well-constructed and generally clear, the approach seems sound\nand well justified. This work led to the development of a resource with fine\ngrained temporal information at the word sense level that would be made\navailable and could be used to improve various NLP tasks. I have a few remarks,\nespecially concerning the settings of the experiments.\n\nI think that more information should be given on the task performed in the\nextrinsic evaluation section. An example could be useful to understand what the\nsystem is trying to predict (the features describe \u00e2\u0080\u009centity pairs\u00e2\u0080\u009d but it\nhas not been made clear before what are these pairs) and what are the features\n(especially, what are the entity attributes? What is the POS for a pair, is it\none dimension or two? Are the lemmas obtained automatically?). The sentence\ndescribing the labels used is confusing, I'm not sure to understand what\n\u00e2\u0080\u009cevent to document creation time\u00e2\u0080\u009d and \u00e2\u0080\u009cevent to same sentence event\u00e2\u0080\u009d\nmeans, are they the kind of pairs considered? Are they relations (as they are\ndescribed as relation at the beginning of p.8)? I find unclear the footnote\nabout the 14 relations: why the other relations have to be ignored, what makes\na mapping too \u00e2\u0080\u009ccomplex\u00e2\u0080\u009d? Also, are the scores macro or micro averaged?\nFinally, the ablation study seems to indicate a possible redundancy between\nLexica and Entity with quite close scores, any clue about this behavior?\n\nI have also some questions about the use of the SVM.  For the extrinsic\nevaluation, the authors say that they optimized the parameters of the\nalgorithm: what are these parameters?  And since a SVM is also used within the\nMinCut framework, is it optimized and how? Finally, if it's the LibSVM library\nthat is used (Weka wrapper), I think a reference to LibSVM should be included. \n\nOther remarks:\n- It would be interesting to have the number of examples per label in the gold\ndata, the figures are given for coarse grained labels (127 temporal vs 271\natemporal), but not for the finer grained.\n- It would also be nice to have an idea of the number of words that are\nambiguous at the temporal level, words like \u00e2\u0080\u009cpresent\u00e2\u0080\u009d.\n- It is said in the caption of the table 3 that the results presented are\n\u00e2\u0080\u009csignificantly better\u00e2\u0080\u009d but no significancy test is indicated, neither any\np-value.\n\nMinor remarks:\n- Related work: what kind of task was performed in (Filannino and Nenadic,\n2014)?\n- Related work: \u00e2\u0080\u009crequires a post-calibration procedure\u00e2\u0080\u009d, needs a reference\n(and p.4 in 3.3 footnote it would be clearer to explain calibration)\n- Related work: \u00e2\u0080\u009ctheir model differ from ours\u00e2\u0080\u009d, in what?\n- Table 3 is really too small: maybe, remove the parenthesis, put the\n\u00e2\u0080\u009c(p,r,f1)\u00e2\u0080\u009d in the caption and give only two scores, e.g. prec and f1. The\ncaption should also be reduced.\n- Information in table 4 would be better represented using a graph.\n- Beginning of p.7: 1064 \u00e2\u0086\u0092 1264\n- TempEval-3: reference ?\n- table 6: would be made clearer by ordering the scores for one column\n- p.5, paragraph 3: atemporal) \u00e2\u0086\u0092 atemporal", "is_meta_review": null, "RECOMMENDATION": "4", "REPLICABILITY": "4", "PRESENTATION_FORMAT": "Oral Presentation", "CLARITY": "4", "MEANINGFUL_COMPARISON": "5", "SUBSTANCE": "4", "REVIEWER_CONFIDENCE": "2", "SOUNDNESS_CORRECTNESS": "4", "APPROPRIATENESS": "5", "IMPACT": "4", "ORIGINALITY": "3"}]}

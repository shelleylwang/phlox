{"text": "Neural Modeling of Multi-Predicate Interactions for Japanese Predicate Argument Structure Analysis\n1 000\n011\n012\n013\n014\n015\n016\n017\n018\n019\n020\n021\n022\n023\n024\n025\n026\n027\n028\n029\n030\n031\n032\n033\n034\n035\n036\n037\n038\n039\n040\n041\n042\n043\n044\n045\n046\n047\n048\n049\n061\n062\n063\n064\n065\n066\n067\n068\n069\n070\n071\n072\n075\n076\n077\n078\n079\n080\n081\n082\n083\n084\n085\n086\n087\n088\n089\n090\n091\n092\n093\n094\n095\n096\n097\n098\n099\n1 Introduction\nPredicate argument structure (PAS) analysis is a basic semantic analysis task, in which systems are required to identify semantic units of a sentence, such as who did what to whom. In pro-drop languages such as Japanese, Chinese and Italian, arguments are often omitted in text, and such argument omission is regarded as one of the most problematic issues of PAS analysis (Iida and Poesio, 2011; Sasano and Kurohashi, 2011).\nAs an approach to the argument omission problem, in Japanese PAS analysis, joint modeling of interactions between multiple predicates has been gaining popularity and achieved the state-of-theart result (Ouchi et al., 2015; Shibata et al., 2016). This approach is based on the linguistic intuition that the predicates in a sentence are semantically related to each other and the interaction information can be a clue for PAS analysis. However, to model such multi-predicate interactions, this ap-\nproach heavily relies on syntactic information predicted by parsers and suffers from the error propagation caused by the pipeline processing. To remedy this problem, we propose a neural model which automatically induces features sensitive to multi-predicate interactions from word sequence information of a sentence. This model takes as input all predicates and their argument candidates in a sentence at a time, and captures the interactions using grid-type recurrent neural networks (Grid-RNN) without syntactic information. In this paper, we firstly introduce a basic model using RNNs, which independently estimates arguments of each predicate without considering the multi-predicate interactions (Sec. 3). Then, extending this model, we propose a neural model using Grid-RNNs (Sec. 4). Performing experiments on the NAIST Text Corpus (Iida et al., 2007), we demonstrate that our neural models exceed the accuracy of the stateof-the-art Japanese PAS analyzer (Ouchi et al., 2015). In particular, the neural model using GridRNNs achieves the best result, which suggests that our grid-type neural architecture effectively captures multi-predicate interactions. 1\n1Our source code is publicly available at http:xxx\n2\n101\n102\n103\n104\n105\n106\n107\n108\n109\n110\n111\n112\n113\n114\n115\n116\n117\n118\n119\n120\n121\n122\n123\n124\n125\n126\n127\n128\n129\n130\n131\n132\n133\n134\n135\n136\n137\n138\n139\n140\n141\n142\n143\n144\n145\n146\n147\n148\n149\n150\n151\n152\n153\n154\n155\n156\n157\n158\n159\n160\n161\n162\n163\n164\n165\n166\n167\n168\n169\n170\n171\n172\n173\n174\n175\n176\n177\n178\n179\n180\n181\n182\n183\n184\n185\n186\n187\n188\n189\n190\n191\n192\n193\n194\n195\n196\n197\n198\n199\nFigure 2: An overview of the neural models: (i) single-sequence and (ii) multi-sequence models.\n2 Japanese Predicate Argument Structure Analysis\n2.1 Task Description\nIn Japanese PAS analysis, we identify arguments taking part in the three major case roles, nominative (NOM), accusative (ACC) and dative (DAT) cases, for each predicate. Arguments can be divided into the following three categories according to the positions relative to their predicates (Hayashibe et al., 2011; Ouchi et al., 2015):\nDep: The arguments that have direct syntactic dependency with the predicate.\nZero: The arguments referred to by zero pronouns within the same sentence, which have no direct syntactic dependency with the predicate.\nInter-Zero: The arguments referred to by zero pronouns out of the same sentence.\nFor example, in Figure 1, the nominative argument \u201c\u8b66\u5bdf (police)\u201d for the predicate \u201c\u902e\u6355\u3057\u305f (arrested)\u201d is regarded as a Dep argument since the argument has a direct syntactic dependency with the predicate. In contrast, the nominative argument \u201c\u7537 i (mani)\u201d for the predicate \u201c\u9003\u8d70\u3057\u305f (escaped)\u201d is regarded as a Zero argument since the argument has no direct syntactic dependency with the predicate.\nIn this paper, we focus on the analysis for these intra-sentential arguments, i.e., Dep and Zero. In order to identify inter-sentential arguments (Inter-Zero), it is required to search a much broader space, such as the whole document, resulting in a much harder analysis than intrasentential arguments.2 Thus, Ouchi et al. (2015)\n2The F-measure remains 10-20% (Taira et al., 2008; Imamura et al., 2009; Sasano and Kurohashi, 2011).\nand Shibata et al. (2016) focused on only intrasentential argument analysis. Following this trend, we focus on intra-sentential argument analysis.\n2.2 Challenging Problem\nArguments are often omitted in Japanese sentences. In Figure 1, \u03d5i represents the omitted argument, called zero pronoun. This zero pronoun \u03d5i refers to \u201c\u7537 i (mani)\u201d. In Japanese PAS analysis, when an argument of the target predicate is omitted, we have to identify the antecedent of the omitted argument (Zero argument). The analysis for such Zero arguments is much more difficult than that forDep arguments because of the lack of direct syntactic dependencies. For Dep arguments, the syntactic dependency between an argument and its predicate is a strong clue. In the sentence in Figure 1, for the predicate \u201c\u902e\u6355\u3057 \u305f (arrested)\u201d, the nominative argument is \u201c\u8b66\u5bdf (police)\u201d. This argument can easily be identified by relying on the syntactic dependency. In contrast, since the nominative argument \u201c\u7537 i (mani)\u201d has no syntactic dependency with its predicate \u201c\u9003 \u8d70\u3057\u305f (escaped)\u201d, we have to use other information for such zero argument identification. As an solution to this problem, we exploit two kinds of information: (i) context in the entire sentence and (ii) multi-predicate interactions. For the former, we introduce single-sequence model, which induces context-sensitive representations from a sequence of argument candidates of a predicate. For the latter, we introduce multi-sequence model, which induces predicate-sensitive representations from multiple sequences of argument candidates of all predicates in a sentence (shown in Figure 2).\n3\n201\n202\n203\n204\n205\n206\n207\n208\n209\n210\n211\n212\n213\n214\n215\n216\n217\n218\n219\n220\n221\n222\n223\n224\n225\n226\n227\n228\n229\n230\n231\n232\n233\n234\n235\n236\n237\n238\n239\n240\n241\n242\n243\n244\n245\n246\n247\n248\n249\n250\n251\n252\n253\n254\n255\n256\n257\n258\n259\n260\n261\n262\n263\n264\n265\n266\n267\n268\n269\n270\n271\n272\n273\n274\n275\n276\n277\n278\n279\n280\n281\n282\n283\n284\n285\n286\n287\n288\n289\n290\n291\n292\n293\n294\n295\n296\n297\n298\n299\nFigure 3: The overall architecture of the single sequence model. This model consists of three components: (i) Input Layer, (ii) RNN Layer and (iii) Output Layer.\n3 Single-Sequence Model\nThe single-sequence model exploits stacked bidirectional RNNs (Bi-RNN) (Schuster and Paliwal, 1997; Graves et al., 2005, 2013; Zhou and Xu, 2015). Figure 3 shows the overall architecture, which consists of the following three components:\nInput Layer: Map each word to a feature vector representation.\nRNN Layer: Produce high-level feature vectors using Bi-RNNs.\nOutput Layer: Compute the probability of each case label for each word using the softmax function.\nIn the following subsections, we describe each of them in more detail.\n3.1 Input Layer\nGiven an input sentence w1:T = (w1, \u00b7 \u00b7 \u00b7 , wT) and a predicate p, each word wt is mapped to a feature representation xt, which is the concatenation (\u2295) of three types of vectors:\nxt = x arg t \u2295 x pred t \u2295 xmarkt (1)\nwhere each vector is based on the following atomic features inspired by Zhou and Xu (2015):\nARG: Word index of each word. PRED: Word index of the target predicate and\nwords around the predicate.\nMARK: Binary index that represents whether the word is the predicate or not.\nFigure 4: An example of the feature extraction. The underlined word is the target predicate. From the sentence \u201c\u5f7c\u5973\u306f\u30d1\u30f3\u3092\u98df\u3079\u305f\u3002(She ate a bread.)\u201d, the three types of features are extracted for the target predicate \u201c\u98df\u3079\u305f (ate)\u201d.\nFigure 5: An example of the process of creating a feature vector. The extracted features are mapped to each vector, and all the vectors are concatenated into one feature vector.\nFigure 4 presents an example of the atomic features. As the ARG feature, we extract a word index xword \u2208 V of each word. Similarly, as the PRED feature, we extract each word index xword of the C words taking the target predicate at the center, where C is the window size. The MARK feature xmark \u2208 {0, 1} is a binary value that represents whether the word is the predicate or not. Then, using feature indices, we extract feature vector representations from each embedding matrix. Figure 5 shows the process of creating the feature vector x1 for the word w1 \u201c\u5f7c\u5973 (she)\u201d. We set two embedding matrices: (i) word embedding matrix Eword \u2208 Rdword\u00d7|V|, and (ii) mark embedding matrixEmark \u2208 Rdmark\u00d72. From each embedding matrix, we extract corresponding column vectors and concatenate them as a feature vector representation xt based on Eq. 1.\n4\n301\n302\n303\n304\n305\n306\n307\n308\n309\n310\n311\n312\n313\n314\n315\n316\n317\n318\n319\n320\n321\n322\n323\n324\n325\n326\n327\n328\n329\n330\n331\n332\n333\n334\n335\n336\n337\n338\n339\n340\n341\n342\n343\n344\n345\n346\n347\n348\n349\n350\n351\n352\n353\n354\n355\n356\n357\n358\n359\n360\n361\n362\n363\n364\n365\n366\n367\n368\n369\n370\n371\n372\n373\n374\n375\n376\n377\n378\n379\n380\n381\n382\n383\n384\n385\n386\n387\n388\n389\n390\n391\n392\n393\n394\n395\n396\n397\n398\n399\nEach feature vector xt is multiplied with a parameter matrix Wx:\nh (0) t = Wx xt (2)\nThe vector h(0)t is given to the first layer of the RNN layers as input.\n3.2 RNN Layer\nIn the RNN layers, feature vectors are updated recurrently using Bi-RNNs. Bi-RNNs process an input sequence from the left-to-right manner in odd-numbered layers and the opposite in evennumbered layers. By stacking these layers, we can construct the deeper network structures.\nStacked Bi-RNNs consist of L layers, and the hidden state in the layer \u2113 \u2208 (1, \u00b7 \u00b7 \u00b7 , L) is calculated as follows:\nh (\u2113) t =\n{ g(\u2113)(h\n(\u2113\u22121) t , h (\u2113) t\u22121) (\u2113 = odd)\ng(\u2113)(h (\u2113\u22121) t , h (\u2113) t+1) (\u2113 = even)\n(3)\nBoth of the odd and even-numbered layers receive h(\u2113\u22121)t , the t-th hidden state of the \u2113 \u2212 1 layer, as the first input of the function g(\u2113), which is an arbitrary function 3. As the second input of g(\u2113), odd-numbered layers receive h(\u2113)t\u22121 while even-numbered layers receive h(\u2113)t+1. By calculating the hidden states until the L-th layer, we obtain a hidden state sequence h(L)1:T = (h (L) 1 , \u00b7 \u00b7 \u00b7 ,h (L) T ). Using each vector h(L)t , we calculate the probability of case labels for each word in the output layer.\n3.3 Output Layer\nIn the output layer, multi-class classification is performed using the softmax function:\nyt = softmax(Wy h (L) t )\nwhere h(L)t is a vector representation propagated from the last RNN layer (Fig 3). Each element of yt is a probability value corresponding to each label. The label with the maximum probability among them is output as a result. In this task, there are five labels: NOM, ACC, DAT, PRED, null. The labels NOM, ACC and DAT indicate the nominative, accusative and dative case, respectively. PRED is the label for the predicate. null represents a word that does not play any case role.\n3In this work, we use the Gated Recurrent Unit (GRU) (Cho et al., 2014) as the function g(\u2113).\n4 Multi-Sequence Model\nWhile the single-sequence model assumes the independence between predicates, the multisequence model assumes the multi-predicate interactions. To capture such interactions between all predicates in a sentence, we extend the singlesequence model to the multi-sequence model using Grid-RNNs (Graves and Schmidhuber, 2009; Kalchbrenner et al., 2016). Figure 6 presents the overall architecture of the multi-sequence model, which consists of three components:\nInput Layer: Map words to M sequences of feature vectors for M predicates.\nGrid Layer: Update the hidden states over different sequences using Grid-RNNs.\nOutput Layer: Compute the probability of each case label for each word using the softmax function.\nIn the following subsections, we describe them in more detail.\n4.1 Input Layer\nThe multi-sequence model takes as input a sentence w1:T = (w1, \u00b7 \u00b7 \u00b7 , wT) and all predicates {pm}M1 in the sentence. For each predicate pm, the input layer creates a sequence of feature vectors Xm = (xm,1, \u00b7 \u00b7 \u00b7 ,xm,T) by mapping each input word wt to a feature vector xm,t based on Eq 1. That is, for M predicates, M sequences of feature vectors {Xm}M1 are created. Then, using Eq. 2, each feature vector xm,t is mapped to h(0)m,t, and a feature sequence is created for a predicate pm, i.e.,H (0) m = (h (0) m,1, \u00b7 \u00b7 \u00b7 ,h (0) m,T). Consequently, forM predicates, we obtainM feature sequences {H(0)m }M1 .\n4.2 Grid Layer\nInter-Sequence Connections\nIn the grid layers, we use Grid-RNNs to propagate the feature information over the different sequences (inter-sequence connections). The right figure in Figure 6 shows an odd-numbered layer of the Grid layers. The hidden state is recurrently calculated from upper-left (m = 1, t = 1) to lowerright (m = M, t = T). Formally, in the \u2113-th layer, the hidden state h(\u2113)m,t is calculated as follows:\nh (\u2113) m,t=\n{ g(\u2113)(h\n(\u2113\u22121) m,t \u2295 h (\u2113) m\u22121,t,h (\u2113) m,t\u22121) (\u2113 = odd)\ng(\u2113)(h (\u2113\u22121) m,t \u2295 h (\u2113) m+1,t,h (\u2113) m,t+1) (\u2113 = even)\n5\n401\n402\n403\n404\n405\n406\n407\n408\n409\n410\n411\n412\n413\n414\n415\n416\n417\n418\n419\n420\n421\n422\n423\n424\n425\n426\n427\n428\n429\n430\n431\n432\n433\n434\n435\n436\n437\n438\n439\n440\n441\n442\n443\n444\n445\n446\n447\n448\n449\n466\n467\n468\n469\n470\n471\n472\n473\n474\n475\n476\n477\n478\n479\n480\n481\n482\n483\n484\n485\n486\n487\n488\n489\n490\n491\n492\n493\n494\n495\n496\n497\n498\n499\nThis equation is similar to Eq. 3. The main difference is that the hidden state of a neighboring sequence, h(\u2113)m\u22121,t (or h (\u2113) m+1,t), is concatenated (\u2295) with the hidden state of the previous (\u2113\u2212 1) layer, h (\u2113\u22121) m,t , and is taken as input of the function g\n(\u2113). In the right figure in Figure 6, the blue curve lines represent the inter-sequence connections. Taking as input the hidden states of neighboring sequences, the network propagates feature information over multiple sequences (predicates). By calculating the hidden states until the L-th layer, we obtain M sequences of the hidden states, i.e., {H(L)m }M1 , in which H (L) m = (h (L) m,1, \u00b7 \u00b7 \u00b7 ,h (L) m,T).\nResidual Connections\nAs more layers are stacked, it gets more difficult to learn the model parameters due to some problems such as gradient vanishment (Pascanu et al., 2013). In this work, we integrate residual connections (He et al., 2015;Wu et al., 2016) with our networks to connect between layers. Specifically, the input vector h(\u2113\u22121)m,t of the \u2113-th layer is added to the output vector h(\u2113)m,t. Residual connections can also be applied to the single-sequence model, and thus we perform the experiments on both models with/without residual connections.\n4.3 Output Layer\nLike the single-sequence model, using the softmax function, we calculate the probability of case labels of each word wt for each predicate pm:\nym,t = softmax(Wy h (L) m,t)\nwhere h(L)m,t is a hidden state vector calculated in the last Grid Layer.\n5 Related Work\n5.1 Japanese PAS Analysis Approaches\nExisting approaches for Japanese PAS analysis are divided into two categories: (i) pointwise approach and (ii) joint approach. In the pointwise approach, we estimate the score of each argument candidate for one predicate, and select the argument candidate with the maximum score as an argument (Taira et al., 2008; Imamura et al., 2009; Hayashibe et al., 2011; Iida et al., 2016). In the joint approach, we select the score of all the predicate-argument combinations in one sentence, and select the combination with the highest score (Yoshikawa et al., 2011; Sasano and Kurohashi, 2011; Ouchi et al., 2015; Shibata et al., 2016). Compared with the pointwise approach, the joint approach has achieved better results.\n5.2 Multi-Predicate Interactions\nOuchi et al. (2015) reported that it is beneficial to Japanese PAS analysis to capture the interactions between all predicates in a sentence. This is based on the linguistic intuition that the predicates in a sentence are semantically related to each other, and the interaction information can be a clue for PAS analysis. Similarly, in semantic role labeling (SRL), Yang and Zong (2014) also reported that their reranking model capturing the multi-predicate interactions is effective for the English constituentbased SRL task (Carreras and Ma\u0300rquez, 2005). Taking a step further in this direction, we propose the neural architecture that effectively models the multi-predicate interactions.\n6\n501\n502\n503\n504\n505\n506\n507\n508\n509\n510\n511\n512\n513\n514\n515\n516\n517\n518\n519\n520\n521\n522\n523\n524\n525\n526\n527\n528\n529\n530\n531\n532\n533\n534\n535\n536\n537\n538\n539\n540\n541\n542\n543\n544\n545\n546\n547\n548\n549\n550\n551\n552\n553\n554\n555\n556\n557\n558\n559\n560\n561\n562\n563\n564\n565\n566\n567\n568\n569\n570\n571\n572\n573\n574\n575\n576\n577\n578\n579\n580\n581\n582\n583\n584\n585\n586\n587\n588\n589\n590\n591\n592\n593\n594\n595\n596\n597\n598\n599\n5.3 Neural Approaches\nJapanese PAS\nIn recent years, several attempts have been made to apply neural networks for Japanese PAS analysis (Shibata et al., 2016; Iida et al., 2016)4. Shibata et al. (2016) used a feed-forward neural network for the score calculation part of the joint model proposed by Ouchi et al. (2015). Iida et al. (2016) used multi-column convolutional neural networks for the zero anaphora resolution task.\nBoth models exploited syntactic and selectional preference information as atomic features of neural networks. Using neural networks, the good performance was realized with mitigating the cost of manually designing combination features. In this work, we demonstrate that even without such syntactic information, our neural models realize the state-of-the-art performance by using word sequence information of a sentence.\nEnglish SRL\nSome neural models achieved high performance without syntactic information in English SRL. Collobert et al. (2011) worked on the English constituent-based SRL task (Carreras and Ma\u0300rquez, 2005). Their model exploited a convolutional neural network and achieved 74.15% in F-measure without syntactic information. Zhou and Xu (2015) also worked on the same task using bidirectional RNNs with CRF and achieved the state-of-the-art result, 81.07% in F-measure. Our models can be regarded as an extension from their model.\nThe main differences between Zhou and Xu (2015) and our work are: (i) the constituent vs dependency-based argument identification and (ii) the multi-predicate consideration. In the constituent-based SRL, since systems are required to identify the spans of arguments for each predicate, Zhou and Xu (2015) used CRF to capture the IOB label dependencies. In contrast, in Japanese dependency-based PAS analysis, since arguments are infrequently adjacent to each other, we replaced the CRF with the softmax function. Also, while the model of Zhou and Xu (2015) predicts arguments for each predicate independently, our multi-sequence model jointly predicts arguments for all predicates in a sentence at a time by considering the multi-predicate interactions.\n4These previous studies used unpublished datasets and evaluated the performance with different experimental settings, so we cannot compare their models with ours.\n6 Experiments\n6.1 Experimental Settings\nDataset\nWe use the NAIST Text Corpus 1.5, which consists of 40,000 sentences of Japanese newspaper text (Iida et al., 2007). In the experiments, we adopt the standard data splits (Taira et al., 2008; Imamura et al., 2009; Ouchi et al., 2015):\nTrain: Articles: Jan 1-11, Editorials: Jan-Aug Dev: Articles: Jan 12-13, Editorials: Sept Test: Articles: Jan 14-17, Editorials: Oct-Dec\nWe use the word boundaries annotated to the NAIST Text Corpus and the target predicates that have at least one argument in the same sentence. We do not use any external resources.\nLearning\nWe train the model parameters by minimizing the cross-entropy loss function:\nL(\u03b8) = \u2212 \u2211 n \u2211 t logP (yt|xt) + \u03bb 2 ||\u03b8||2\nwhere \u03b8 is a set of model parameters, and the hyper-parameter \u03bb is the coefficient governing the L2 weight decay.\nImplementation Details\nWe implement our neural models using Theano (Bastien et al., 2012). The number of epochs is set to 50, and we report the result of the test set in the epoch with the best F-measure of the development set. Parameter optimization is done by stochastic gradient descent method (SGD) using mini-batch, whose size is selected from {2, 4, 8}. The learning rate is automatically adjusted using Adam (Kingma and Ba, 2014). For the L2 weight decay, the hyper-parameter \u03bb in Eq. 4 is selected from {0.001, 0.0005, 0.0001}. In the neural models, the number of the RNN and Grid layers are selected from {2, 4, 6, 8}. The window size C for the PRED feature (Sec. 3.1) is set to 5. Words with frequency 2 or more are mapped to each word index, and the remaining words are mapped to the unknown word index. The dimensions dword and dmark of the embeddings are set to 32. In the single-sequence model, the parameters of GRUs are set to 32\u00d7 32. In the multi-sequence model, the parameters of GRUs related to the input values are set to 64 \u00d7 32, and\n7\n601\n602\n603\n604\n605\n606\n607\n608\n609\n610\n611\n612\n613\n614\n615\n616\n617\n618\n619\n620\n621\n622\n623\n624\n625\n626\n627\n628\n629\n630\n631\n632\n633\n634\n635\n636\n637\n638\n639\n640\n641\n642\n643\n644\n645\n646\n647\n648\n649\n665\n666\n667\n668\n669\n670\n671\n672\n673\n674\n675\n676\n677\n678\n679\n680\n681\n682\n683\n684\n685\n686\n687\n688\n689\n690\n691\n692\n693\n694\n695\n696\n697\n698\n699\nDep Zero All Imamura+ 09 85.06 41.65 78.15\nOuchi+ 15 86.07 44.09 79.23 Single-Seq 88.10 46.10 81.15 Multi-Seq 88.17 \u2020 47.12 \u2020 81.42 \u2020\nTable 1: F-measures in the test set. Single-Seq is the single-sequence model, and Multi-Seq is the multi-sequence model. Imamura+ 09 is the model of Imamura et al. (2009) reimplemented by Ouchi et al. (2015), and Ouchi+ 15 is the ALLCases Joint Model of Ouchi et al. (2015). The mark \u2020 denotes the significantly better results with the significance level p < 0.05 comparing between Single-Seq and Multi-Seq.\nothers are 32\u00d7 32. The initial values of all the parameters are sampled according to the uniform distribution from [\u2212 \u221a 6\u221a\nrow+col ,\n\u221a 6\u221a\nrow+col ], where row\nand col are the number of rows and columns of each matrix, respectively.\nBaseline Models\nWe compare our models with the models in the previous works (Sec. 5.1) that use the NAIST Text Corpus 1.5. As a baseline of the pointwise approach, we use the pointwise model5 of Imamura et al. (2009). In addition, as a baseline of the joint approach, we use the model of Ouchi et al. (2015), which achieved the best result on the NAIST Text Corpus 1.5.\n6.2 Results\nNeural Models vs Baseline Models\nTable 1 presents F-measures of our neural sequence models with 8 RNN or Grid layers and the baseline models on the test set, in which as the significent test, we used the bootstrap resampling method. In all the metrics, both of the single-sequence (Single-Seq) and multisequence model (Multi-Seq) outperformed the baseline models. This confirms that our neural sequence models realize high-performance even without syntactic information by learning contextual information effective for PAS analysis from a word sequence of the sentence.\nIn particular, for zero arguments (Zero), our models achieved a considerable improvement compared with the state-of-the-art model of\n5We compared the results of the model reimplemented by Ouchi et al. (2015).\nOuchi et al. (2015), i.e., the single model improved around 2.0 points and the multi-sequence model improved around 3.0 points in F-measure. These results suggest that it is beneficial to Japanese PAS analysis, particularly to the zero argument identification, to model the context in the entire sentence using RNNs.\nEffects of Multiple Predicate Consideration\nAs Table 1 shows, the multi-sequence model significantly outperformed the single-sequence model in F-measure in total (81.42% vs 81.15%). This result demonstrates that the grid-type neural architecture can effectively capture the multipredicate interactions by connecting between the sequences of the argument candidates for all predicates in a sentence. Compared with the single-sequence model for different argument types, the multi-sequence model achieved slightly but significantly better result for the direct dependency arguments (Dep) (88.10% vs 88.17%). In addition, for zero arguments (Zero), which have no syntactic dependency with its predicate, the multi-sequence model significantly outperformed the single-sequence model by around 1.0 points in F-measure (46.10% vs 47.12%). This shows that capturing the multipredicate interactions is particularly effective for zero arguments, which is consistent with the results of Ouchi et al. (2015).\n8\n701\n702\n703\n704\n705\n706\n707\n708\n709\n710\n711\n712\n713\n714\n715\n716\n717\n718\n719\n720\n721\n722\n723\n724\n725\n726\n727\n728\n729\n730\n731\n732\n733\n734\n735\n736\n737\n738\n739\n740\n741\n742\n743\n744\n745\n746\n747\n748\n749\n750\n751\n752\n753\n754\n755\n756\n757\n758\n759\n760\n761\n762\n763\n764\n765\n766\n767\n768\n769\n770\n771\n772\n773\n774\n775\n776\n777\n778\n779\n780\n781\n782\n783\n784\n785\n786\n787\n788\n789\n790\n791\n792\n793\n794\n795\n796\n797\n798\n799\nDep Zero NOM ACC DAT NOM ACC DAT\nNAIST Text Corpus 1.5 Imamura+ 09 86.50 92.84 30.97 45.56 21.38 0.83\nOuchi+ 15 88.13 92.74 38.39 48.11 24.43 4.80 Single-Seq 88.32 93.89 65.91 49.51 35.07 9.83 Multi-Seq 88.75 93.68 64.38 50.65 32.35 7.52\nNAIST Text Corpus 1.4\u03b2 Taira+ 08* 75.53 88.20 89.51 30.15 11.41 3.66\nImamura+ 09* 87.0 93.9 80.8 50.0 30.8 0.0 Sasano+ 11* - - - 39.5 17.5 8.9\nTable 3: Performance comparison for different case roles on the test set in F-measures. NOM, ACC or DAT is the nominal, accusative or dative case, respectively. The mark * indicates that the model uses external resources.\nEffects of Network Depth\nTable 2 presents F-measures of the neural sequence models with different network depths and with/without residual connections. The performance tends to get better as the RNN or Grid layers get deeper with residual connections. In particular, the two models with 8 layers and residual connections achieved considerable improvements of around 1.0 point in F-measure compared the models without residual connections, which means that the residual connections contribute to the effective parameter learning of deeper models.\nComparison per Case Role\nTable 3 shows F-measures for each case role. For reference, we show the results of the previous studies using NAIST Text Corpus 1.4\u03b2 with external resources as well.6\nComparing between the models using the NAIST Text Corpus 1.5, the single-sequence and multi-sequence models outperformed the baseline models in all the metrics. In particular, for the dative case, the two neural models achieved much higher results by around 30 points. This suggests that although dative arguments appear infrequently compared with the other two case arguments, the neural models can robustly learn it.\nIn addition, for zero arguments (Zero), the neural models achieved better results than the baseline models. Especially, for zero arguments of\n6The major difference between NAIST Text Corpus 1.4\u03b2 and 1.5 is the revision of the annotation criterion for the dative case (DAT) (corresponding to Japanese case marker \u201c\u306b\u201d). Argument and adjunct usages of the case marker \u201c\u306b\u201d are not distinguished in 1.4\u03b2, making the identification of the dative case seemingly easy (Ouchi et al., 2015).\nthe nominative case (NOM), the multi-sequence model achieved a considerable improvement of around 2.5 points in F-measure compared with the state-of-the-art model of Ouchi et al. (2015). To achieve high accuracies for the analysis of such zero arguments, it is necessary to capture long distance dependencies (Iida et al., 2005; Sasano and Kurohashi, 2011; Iida et al., 2015). Therefore, the improvements of the results suggest that the neural models effectively capture long distance dependencies using RNNs that can encode the context in the entire sentence.\n7 Conclusion\nIn this work, we introduced neural sequence models that automatically induce effective feature representations from word sequence information of a sentence for Japanese PAS analysis. The experiments on NAIST Text Corpus 1.5 demonstrated that the models achieve the state-of-the-art result without syntactic information. In particular, our multi-sequence model improved the performance for zero argument identification, one of the problematic issues in Japanese PAS analysis, by considering the multi-predicate interactions using Grid-RNNs. Since our neural models are applicable to SRL, applying our models for multilingual SRL tasks is an interesting line of the future research. In addition, in this work, the model parameters were learned without any external resources. For future work, we plan to explore effective methods for exploiting large-scale unlabeled data to learn the neural models.\n9\n801\n802\n803\n804\n805\n806\n807\n808\n809\n810\n811\n812\n813\n814\n815\n816\n817\n818\n819\n820\n821\n822\n823\n824\n825\n826\n827\n828\n829\n830\n831\n832\n833\n834\n835\n836\n837\n838\n839\n840\n841\n842\n843\n844\n845\n846\n847\n848\n849\n850\n851\n852\n853\n854\n855\n856\n857\n858\n859\n860\n861\n862\n863\n864\n865\n866\n867\n868\n869\n870\n871\n872\n873\n874\n875\n876\n877\n878\n879\n880\n881\n882\n883\n884\n885\n886\n887\n888\n889\n890\n891\n892\n893\n894\n895\n896\n897\n898\n899\n", "cats": {"READ": 1.0, "DONTREAD": 0.0}, "reviews": [{"IMPACT": "3", "SUBSTANCE": "4", "APPROPRIATENESS": "5", "MEANINGFUL_COMPARISON": "3", "PRESENTATION_FORMAT": "Oral Presentation", "comments": "- Strengths:\n\nThis paper presents a sophisticated application of Grid-type Recurrent Neural\nNets to the task of determining predicate-argument structures (PAS) in\nJapanese.  The approach does not use any explicit syntactic structure, and\noutperforms the current SOA systems that do include syntactic structure.  The\nauthors give a clear and detailed description of the implementation and of the\nresults.  In particular, they pay close attention to the performance on dropped\narguments, zero pronouns, which are prevalent in Japanese and especially\nchallenging with respect to PAS. Their multi-sequence model, which takes all of\nthe predicates in the sentence into account, achieves the best performance for\nthese examples.  The paper is detailed and clearly written.\n\n- Weaknesses:\n\nI really only have minor comments. There are some typos listed below, the\ncorrection of which would improve English fluency. I think it would be worth\nillustrating the point about the PRED including context around the \"predicate\"\nwith the example from Fig 6 where the accusative marker is included with the\nverb in the PRED string.  I didn't understand the use of boldface in Table 2,\np. 7.\n\n- General Discussion:\n\nTypos:\n\np1 :  error propagation does not need a \"the\", nor does \"multi-predicate\ninteractions\"\np2: As an solution -> As a solution, single-sequence model -> a single-sequence\nmodel,                    multi-sequence model -> a multi-sequence model \np. 3 Example in Fig 4.                    She ate a bread -> She ate bread.\np. 4 assumes the independence -> assumed independence, the multi-predicate\ninteractions -> multi-predicate interactions, the multi-sequence model -> a\nmulti-sequence model\np.7: the residual connections -> residual connections, the multi-predicate\ninteractions -> multi-predicate interactions (twice)\np8 NAIST Text Corpus -> the NAIST Text Corpus, the state-of-the-art result ->\nstate-of-the-art results\n\nI have read the author response and am satisfied with it.", "SOUNDNESS_CORRECTNESS": "5", "ORIGINALITY": "5", "is_meta_review": null, "RECOMMENDATION": "4", "CLARITY": "4", "REVIEWER_CONFIDENCE": "3"}, {"IMPACT": "3", "SUBSTANCE": "4", "APPROPRIATENESS": "5", "MEANINGFUL_COMPARISON": "3", "PRESENTATION_FORMAT": "Oral Presentation", "comments": "This paper proposes new prediction models for Japanese SRL task by adopting the\nEnglish state-of-the-art model of (Zhou and Xu, 2015).\nThe authors also extend the model by applying the framework of Grid-RNNs in\norder to handle the interactions between the arguments of multiple predicates.\n\nThe evaluation is performed on the well-known benchmark dataset in Japanese\nSRL, and obtained a significantly better performance than the current state of\nthe art system.\n\nStrengths:\nThe paper is well-structured and well-motivated.\nThe proposed model obtains an improvement in accuracy compared with the current\nstate of the art system.\nAlso, the model using Grid-RNNs achieves a slightly better performance than\nthat of proposed single-sequential model, mainly due to the improvement on the\ndetection of zero arguments, that is the focus of this paper.\n\nWeakness:\nTo the best of my understanding, the main contribution of this paper is an\nextension of the single-sequential model to the multi-sequential model. The\nimpact of predicate interactions is a bit smaller than that of (Ouchi et al.,\n2015). There is a previous work (Shibata et al., 2016) that extends the (Ouchi\net al., 2015)'s model\nwith neural network modeling. I am curious about the comparison between them.", "SOUNDNESS_CORRECTNESS": "5", "ORIGINALITY": "5", "is_meta_review": null, "RECOMMENDATION": "4", "CLARITY": "4", "REVIEWER_CONFIDENCE": "5"}, {"IMPACT": "3", "SUBSTANCE": "4", "APPROPRIATENESS": "5", "MEANINGFUL_COMPARISON": "3", "PRESENTATION_FORMAT": "Poster", "comments": "This paper proposes a joint neural modelling approach to PAS analysis in\nJapanese, based on Grid-RNNs, which it compares variously with a conventional\nsingle-sequence RNN approach.\n\nThis is a solidly-executed paper, targeting a well-established task from\nJapanese but achieving state-of-the-art results at the task, and presenting\nthe task in a mostly accessible manner for those not versed in\nJapanese. Having said that, I felt you could have talked up the complexity of\nthe task a bit, e.g. wrt your example in Figure 1, talking through the\ninherent ambiguity between the NOM and ACC arguments of the first predicate,\nas the NOM argument of the second predicate, and better describing how the\ntask contrasts with SRL (largely through the ambiguity in zero pronouns). I\nwould also have liked to have seen some stats re the proportion of zero\npronouns which are actually intra-sententially resolvable, as this further\ncomplicates the task as defined (i.e. needing to implicitly distinguish\nbetween intra- and inter-sentential zero anaphors). One thing I wasn't sure of\nhere: in the case of an inter-sentential zero pronoun for the argument of a\ngiven predicate, what representation do you use? Is there simply no marking of\nthat argument at all, or is it marked as an empty argument? My reading of the\npaper is that it is the former, in which case there is no explicit\nrepresentation of the fact that there is a zero pronoun, which seems like a\nslightly defective representation (which potentially impacts on the ability of\nthe model to capture zero pronouns); some discussion of this would have been\nappreciated.\n\nThere are some constraints that don't seem to be captured in the model (which\nsome of the ILP-based methods for SRL explicitly model, e.g.): (1) a given\npredicate will generally have only one argument of a given type (esp. NOM and\nACC); and (2) a given argument generally only fills one argument slot for a\ngiven predicate. I would have liked to have seen some analysis of the output\nof the model to see how well the model was able to learn these sorts of\nconstraints. More generally, given the mix of numbers in Table 3 between\nSingle-Seq and Multi-Seq (where it is really only NOM where there is any\nimprovement for Multi-Seq), I would have liked to have seen some discussion of\nthe relative differences in the outputs of the two models: are they largely\nidentical, or very different but about the same in aggregate, e.g.? In what\ncontexts do you observe differences between the two models? Some analysis like\nthis to shed light on the internals of the models would have made the\ndifference between a solid and a strong paper, and is the main area where I\nbelieve the paper could be improved (other than including results for SRL, but\nthat would take quite a bit more work).\n\nThe presentation of the paper was good, with the Figures aiding understanding\nof the model. There were some low-level language issues, but nothing major:\n\nl19: the error propagation -> error propagation\nl190: an solution -> a solution\nl264 (and Figure 2): a bread -> bread\nl351: the independence -> independence\nl512: the good -> good\nl531: from their model -> of their model\nl637: significent -> significance\nl638: both of -> both\n\nand watch casing in your references (e.g. \"japanese\", \"lstm\", \"conll\", \"ilp\")", "SOUNDNESS_CORRECTNESS": "5", "ORIGINALITY": "5", "is_meta_review": null, "RECOMMENDATION": "4", "CLARITY": "4", "REVIEWER_CONFIDENCE": "4"}]}
{"text": "Detecting Lexical Entailment in Context\n1 000\n011\n012\n013\n014\n015\n016\n017\n018\n019\n020\n021\n022\n023\n024\n025\n026\n027\n028\n029\n030\n031\n032\n033\n034\n035\n036\n037\n038\n039\n040\n041\n042\n043\n044\n045\n046\n047\n048\n049\n061\n062\n063\n064\n065\n066\n067\n068\n069\n070\n071\n072\n073\n074\n075\n076\n077\n078\n079\n080\n081\n082\n083\n084\n085\n086\n087\n088\n089\n090\n091\n092\n093\n094\n095\n096\n097\n098\n099\n1 Introduction\nMany NLP applications require detecting relations between word meanings beyond synonymy and paraphrasing. For instance, given \u201cCarlsen plays chess.\u201d and the question \u201cWhich game does Carlsen play?\u201d, successfully answering the question requires knowing that chess is a kind of game, or more generally, that chess entails game.\nWhile prior work has defined lexical entailment as a relation between word types (Turney and Mohammad, 2013), we argue entailment relations are better defined when illustrating word meaning with an example in context. Ignoring context is problematic since entailment might hold between some senses of the words, but not others. Consider the word game in two distinct contexts:\n1. The championship game was played in NYC.\n2. The hunters were interested in the big game.\nGiven the sentence, Carlsen is the world chess champion, chess =\u21d2 game as used in the first context, while chess 6=\u21d2 game in the second context.\nIn this paper, we investigate how to represent and compare the meaning of words in context for lexical entailment. Since distributional representations for word types have proved useful to detect lexical entailment out of context in supervised settings (Baroni et al., 2012; Roller et al., 2014; Turney and Mohammad, 2013), we propose to transform context-agnostic word type representations into contextualized representations that highlight salient properties of the context (Section 3), and use these contextualized representations with a range of semantic similarity features (Section 4) to successfully detect entailment.\nAs we will see, these context representations significantly improve performance over contextagnostic baselines not only in English, but also between English and French words (Section 7) on two novel datasets (Section 5). We also show that our features are sensitive to word sense changes indicated by context, and adequately capture the direction of entailment relation (Section 8). Moreover, we establish a new state-of-the-art on an existing dataset that captures a broader range of semantic relations in context (Shwartz and Dagan, 2015), and show that the proposed features, induced solely from large amounts of raw text, yield systems that perform as well, or better than existing systems that require additional human annotation.\n2 Defining Lexical Entailment in Context\nWe frame the task of lexical entailment in context as a binary classification task on examples consisting of a 4-tuple (wl, wr, cl, cr), where wl and wr are two words, and cl and cr are sentences which\n2\n101\n102\n103\n104\n105\n106\n107\n108\n109\n110\n111\n112\n113\n114\n115\n116\n117\n118\n119\n120\n121\n122\n123\n124\n125\n126\n127\n128\n129\n130\n131\n132\n133\n134\n135\n136\n137\n138\n139\n140\n141\n142\n143\n144\n145\n146\n147\n148\n149\n150\n151\n152\n153\n154\n155\n156\n157\n171\n172\n173\n174\n175\n176\n177\n178\n179\n180\n181\n182\n183\n184\n185\n186\n187\n188\n189\n190\n191\n192\n193\n194\n195\n196\n197\n198\n199\nWords (wl, wr) Exemplars (cl,cr) Does wl =\u21d2 wr ?\nstaff , stick cl = He walked with the help of a wooden staff . Yes cr = The kid had a candied apple on a stick. staff , body cl = The hospital has an excellent nursing staff . Yes cr = The whole body filed out of the auditorium. staff , stick cl = The hospital has an excellent nursing staff . No cr = The kid had a candied apple on a stick.\nTable 1: Examples of the context-aware lexical entailment task\nillustrate each word usage. The example is treated as positive if wl =\u21d2 wr, given the meaning of each word exemplified by the contexts, and negative otherwise. Table 1 provides examples.\nWe say that entailment holds if the meaning of wl in the context of cl is more specific than the meaning of wr in the context of cr. The nature of entailment relations captured out-of-context can be broader depending on the test beds considered1. Zhitomirsky-Geffet and Dagan (2009) formalize lexical entailment as a substitutional relationship, which encompasses synonymy, hypernymy, some meronymy relations, and also cause-effect relations. However, we limit entailment to the specificity relation in this work to better understand the impact of context.\nNote that lexical entailment in context is not textual entailment. Recognizing textual entailment (Dagan et al., 2013) and natural language inference (Bowman et al., 2015) involves detecting entailment relations between sentences, while lexical entailment is a relation between words.\n3 Representing Words and their Contexts for Entailment\nHow can we construct vector representations of the meaning of target words wl and wr in their respective exemplar contexts cl and cr? We start from existing representations for word types which have proven useful for detecting lexical entailment and other semantic relations out of context (Baroni et al., 2012; Kruszewski and Baroni, 2015; Vylomova et al., 2016; Turney and Mohammad, 2013).\nGiven an example (wl, wr, cl, cr), let ~wl and ~wr refer to the context-agnostic representations of wl and wr, and let Cl and Cr represent the matrices obtained by row-wise stacking of the context-\n1We refer the reader to Turney and Mohammad (2013) and Shwartz et al (2017) for comprehensive surveys of supervised and unsupervised methods for the out-of-context task.\nagnostic representations of words in cl and cr respectively.\n3.1 Contextualized Word Representations\nFollowing Thater et al. (2011); Erk and Pado\u0301 (2008), our first approach is to apply a filter to word type representations to highlight the salient dimensions of the exemplar context, emphasizing relevant dimensions of and downplaying unimportant ones. However, while prior work represents context by averaging word vectors, we propose richer representations that better capture the salient geometrical properties of the exemplar context that might get lost by averaging (Figure 1).\nFirst, we construct fixed length representations for the contexts cl and cr by running convolutional filters over Cl and Cr. Specifically, we calculate the column-wise maximum, minimum and the mean over the matrices Cl and Cr, as done by (Tang et al., 2014) for supervised sentiment classification. This yields three ddimensional vectors for cl (~cl,max, ~cl,min, ~cl,mean), and three d-dimensional vectors for cr (~cr,max, ~cr,min, ~cr,mean). Computing the maximum and minimum across all vector dimensions captures the exterior surface of the \u201cinstance manifold\u201d\n3\n201\n202\n203\n204\n205\n206\n207\n208\n209\n210\n211\n212\n213\n214\n215\n216\n217\n218\n219\n220\n221\n222\n223\n224\n225\n226\n227\n228\n229\n230\n231\n232\n233\n234\n235\n236\n237\n238\n239\n240\n241\n242\n243\n244\n245\n246\n247\n248\n249\n257\n258\n259\n260\n261\n262\n263\n264\n265\n266\n267\n268\n269\n270\n271\n272\n273\n274\n275\n276\n277\n278\n279\n280\n281\n282\n283\n284\n285\n286\n287\n288\n289\n290\n291\n292\n293\n294\n295\n296\n297\n298\n299\n(the volume in embedding space within which all words in the instance reside), while the mean summarizes the density per-dimension within the manifold (Hovy, 2015).\nSecond, we transform initial context-agnostic representations for target word types by taking an element-wise product of the word type vectors ( ~w\u2217) with vectors representing salient dimensions of the exemplar context (~c\u2217,max, ~c\u2217,min, ~c\u2217,mean) where \u2217 \u2208 {l, r} . This yields three d-dimensional vectors for wl (~wl,max, ~wl,min, ~wl,mean), and three for wr (~wr,max, ~wr,min, ~wr,mean). We refer to our final word-in-context representations for wl and wr as ~wl,mask and ~wr,mask respectively, where ~wl,mask is the concatenation of ~wl,max, ~wl,min, ~wl,mean, and ~wr,mask is also similarly constructed.\n3.2 A Shared Vector Space for Words and Contexts: Context2vec\nAn alternative approach to contextualizing word representations is to directly compare the representations of words with representations of contexts. In a single language, this can be done using Context2Vec (Melamud et al., 2016), a neural model that, given a target word and its sentential context, embeds both the word and the context in the same low-dimensional space, with the objective of having the context predict the target word via a log linear model. This model approaches the state-of-the-art on lexical substitution, sentence completion, and supervised word sense disambiguation.\n4 Comparing Words and Contexts for Entailment\nGiven words and context representations described above, how can we predict entailment?\n4.1 Supervised Logistic Regression model\nPrior work on lexical entailment out of context suggests that the entailment relationship between words is a learnable function of the concatenation of their individual representations (Baroni et al., 2012; Turney and Mohammad, 2013). After concatenation, they are used as features for a logistic regression (Roller et al., 2014) or an SVM classifier (Baroni et al., 2012; Turney and Mohammad, 2013). We follow the same practice with our context-aware word representations. Intuitively, the classifier learns to weight the importance of each dimension of a word representation to detect\nentailment. By learning different weights for the same features describing wl or wr, the classifier can detect asymmetric behavior too, as we will see in Section 8.\n4.2 Similarity Features\nWe hypothesize that entailment relations hold between related words and introduce similarity features to capture this non-directional relation between words and contexts.\nGiven a pair of vectors (~l, ~r), we use three similarity measures: the cosine similarity cosine(~l, ~r), the dot product ~l \u00b7 ~r, and the euclidean distance \u2016~l \u2212 ~r\u2016. The cosine similarity captures the difference in the two vectors in terms of the angle between then, the euclidean distance measures the difference in magnitude, and the dot product captures both magnitude and angle.\nWe apply these measures to three types of representations. Our first set of similarities is intended to directly capture the similarities between contextualized word representations (Section 3.1). We calculate pairwise similarities between (~wl,max, ~wr,max), (~wl,min, ~wr,min), (~wl,mean, ~wr,mean), as well as between the context-agnostic representations (~wl, ~wr). Second, we add similarities based on Context2Vec representations, i.e. between each pair of ~wl,c2v, ~wr,c2v, ~cl,c2v, and ~cr,c2v. Finally, following Shwartz and Dagan (2015), we capture the similarity of the most relevant word to wl in cr, that to wr in cl, as well as between cl and cr. We therefore add the following three similarities, as well as their pairwise products:\nmax w\u2208cr ~wl \u00b7 ~w, max w\u2208cl ~wr \u00b7 ~w, max w\u2208cl,w\u2032\u2208cr ~w \u00b7 ~w\u2032\n4\n301\n302\n303\n304\n305\n306\n307\n308\n309\n310\n311\n312\n313\n314\n315\n316\n317\n318\n319\n320\n321\n322\n323\n324\n325\n326\n327\n328\n329\n330\n331\n332\n333\n334\n335\n336\n337\n338\n339\n340\n341\n342\n343\n344\n345\n346\n347\n348\n349\n350\n351\n352\n353\n354\n355\n356\n357\n358\n359\n360\n361\n362\n363\n364\n365\n366\n367\n368\n369\n370\n371\n372\n373\n374\n375\n376\n377\n378\n379\n380\n381\n382\n383\n384\n385\n386\n387\n388\n389\n390\n391\n392\n393\n394\n395\n396\n397\n398\n399\nWord : Study Example : he knocked on the door of the study\nWord : Study Example : he made several studies before the final painting\nWord : Room Example : the rooms were small but comfortable\nWord : Drawing Example : he did complicated pen-and-ink drawings\nFigure 2: Sample dataset creation process based on two synsets of the word study. The green/solid lines indicate positive examples, while the red/dashed lines indicate negative examples\n5 Evaluation Framework\nWe evaluate our models of lexical entailment in context on three complementary datasets.\n5.1 CONTEXT-PPDB\nThe first dataset, CONTEXT-PPDB, is a finegrained lexical inference dataset created using 375 word pairs from a subset of the English Paraphrase Database (Ganitkevitch et al., 2013; Pavlick et al., 2015b). These word pairs are semiautomatically labeled with semantic relations outof-context. Shwartz and Dagan (2015) augmented them with examples of word usage in context, and re-annotating word pairs given the extra contextual information. The final dataset consists of 3750 words/contexts tuples with a corresponding label from Table 2, one of which is entailment.\n5.2 CONTEXT-WN\nIn addition to CONTEXT-PPDB, we would like a test set for controlled analysis of lexical entailment in context. We would like to directly assess the sensitivity of our models to contexts that signal different word senses, as well as quantify the extent to which our models detect asymmetric entailment relations rather than semantic similarity.\nBased on the above criteria, we introduce a large-scale dataset automatically extracted from WordNet (Fellbaum, 1998a). We call this dataset CONTEXT-WN. WordNet groups synonyms into synsets. Most synsets are further accompanied by one or more short sentences illustrating the use of the members of the synset. The idea behind CONTEXT-WN is to use these example sentences as context for the words, and hypernymy relations to draw examples of lexical entailment relations. The entire process starts from a seed list of words W and proceeds as follows (see Figure 2 for an illustration) :\n1. For each word type w \u2208 W with multiple synsets, obtain all synsets Sw.\n2. For each synset i \u2208 Sw, pick a hypernym synset sih, with a corresponding word form wih. Also obtain c\ni and cih which are example sentences corresponding to wi and wih respectively - (wi, wih, c\ni, cih) serves as a positive example.\n3. Permute the positive examples to get negative examples. From (wi, wih, c\ni, cih) and (wj , wjh, c\nj , cjh), generate negative examples (wi, wjh, c i, cjh) and (w j , wih, c j , cih).\n4. Flip the positive examples to generate more negative examples. From (wi, wih, c\ni, cih) generate the negative example (wih, w i, cih, c i).\nWe run this process using the 9000 most frequent words from Wikipedia as W (after filtering the top 1000 as stopwords). This yields a total of 5239 positive examples. We sample an equal number of negative examples from Step 3, with another 5239 examples being generated in Step 4. The final dataset consists of 10478 negative examples.\nCONTEXT-WN satisfies our desiderata. The dataset has a very specific focus since we only pick hypernym-hyponym pairs. The negative examples generated in Steps 3 and 4 require discriminating between different word senses and entailment directions. Finally, with over 15000 examples distributed over 6000 word pairs, the dataset is almost five times as large as CONTEXT-PPDB.\nWe use a 70/5/25 train/dev/test split as in CONTEXT-PPDB. We also ensure that each set contains different word pairs, to avoid memorization and overfitting (Levy et al., 2015).\n5.3 Crosslingual dataset\nOur final dataset takes a cross-lingual view of lexical entailment in context. To create this dataset, we follow the same methodology that was used to create CONTEXT-PPDB, replacing word pairs from PPDB with a set of 100 English-French\n5\n401\n402\n403\n404\n405\n406\n407\n408\n409\n410\n411\n412\n413\n414\n415\n416\n417\n418\n419\n420\n421\n422\n423\n424\n425\n426\n427\n428\n429\n430\n431\n432\n433\n434\n435\n436\n437\n438\n439\n440\n441\n442\n443\n444\n445\n446\n447\n448\n449\n450\n451\n452\n453\n454\n455\n456\n457\n458\n459\n460\n461\n462\n463\n464\n465\n466\n467\n468\n469\n470\n471\n472\n473\n474\n475\n476\n477\n478\n479\n480\n481\n482\n483\n484\n485\n486\n487\n488\n489\n490\n491\n492\n493\n494\n495\n496\n497\n498\n499\nword pairs labeled as positive out-of-context in the cross-lingual lexical entailment dataset of Vyas and Carpuat (2016). Each word pair (wl, wr) consists of an English word and a French word. To add context, we extract 10 sentence pairs (cl, cr) per word pair from the Wikipedia in the corresponding language. These thousand examples are then given to annotators on a crowdsourcing platform 2. For each example (wl, wr, cl, cr), the annotators were asked to judge whether wl =\u21d2 wr given the contexts cl and cr. Each example was annotated by 5 workers3.\nFor each example, we assign the majority judgment as the correct label. 4 or more annotators agree on the label on 540 examples, and we retain only these examples as our test set. This yields a dataset of 374 positive examples and 166 negative examples.\n6 Experimental Set-up\n6.1 Monolingual\nAll experiments on CONTEXT-PPDB and CONTEXT-WN are with the default train/dev/test splits. We use 50 dimensional off-the-shelf GloVe embeddings (Pennington et al., 2014) to create ~wl, ~wr, Cl, and Cr. To obtain the Context2Vec representations ~wl,c2v, ~wl,c2v, ~cl,c2v, and ~cr,c2v, we use an existing model trained on the ukWaC corpus (Ferraresi et al., 2006) .\nWe use Logistic Regression as our classifier for both tasks, to allow for direct comparisons with previous work. We do not tune parameters of the classifier, except for adding class weights in the CONTEXT-WN experiments to account for the unbalanced data. As evaluation metric, we use weighted F1 score to compare against previous results, as well as to account for label imbalance in CONTEXT-WN.\n6.2 Crosslingual\nWe evaluate our models on cross-lingual test set in a transfer setting by training on the training set of CONTEXT-PPDB. To represent ~wl, ~wr, we experiment with two different pre-trained, 200- dimensional, bilingual embeddings - BiVec (Luong et al., 2015) and BiCVM (Hermann and Blunsom, 2014), both of which have been shown to be\n2www.crowdflower.com 3We will release further details of our annotation task including guidelines and the specific task after the review period.\nsuitable for cross-lingual semantic tasks such as dictionary induction and document classification (Upadhyay et al., 2016). The classifier is again a Logistic Regression classifier with class weights.\n7 Results\n7.1 CONTEXT-PPDB\nRepresentation-based features only A context-agnostic baseline (Baroni et al., 2012) based on the concatenation of the word type features for (wl, wr) reaches F1 scores of 53.5 using Glove and 54 with Context2Vec (Table 4). The contextualized word representations and similarity features yield the best result and improves performance by 14 points. The similarity features are also effective when used in combination with the Glove or Context2Vec representations, yielding improvements of 12 to 13 points on the baseline. We also note that using ~cl,c2v and ~cr,c2v to add contextual information performs very poorly (F = ~60), regardless of the word type feature, indicating the superiority of our masked representations.\nAdding PPDB-specific features CONTEXTPPDB comes with rich information about word pairs drawn from PPDB (Pavlick et al., 2015a,b). These features include scores for likelihood of context-agnostic entailment labels, distributional similarities, and probabilities of the word pair being paraphrases, among other scores. Shwartz and Dagan (2015) establish a strong baseline of 67 F1 using these features and the most salient word/context similarities described in Section 4.2. Augmenting this system with our proposed context-aware similarities, we observe an improvement of ~2 F1 points. The contextualized representations further gives a ~3 point boost. Overall, our new context aware features help in improving upon the previous state-of-the-art by 4.8 F1 points.\n7.2 CONTEXT-WN\nWe repeat all experiments on CONTEXT-WN (Table 4), except for those that require PPDB-specific features. The trends on CONTEXT-WN are similar to those on CONTEXT-PPDB. Using similarity features with the context-agnostic baseline, improves performance by ~4 F1 points. Adding the context-aware representations, further gives a tiny boost, enabling us to score 5 F1 points more than\n6\n501\n502\n503\n504\n505\n506\n507\n508\n509\n510\n511\n512\n513\n514\n515\n516\n517\n518\n519\n520\n521\n522\n523\n524\n525\n526\n527\n528\n529\n530\n531\n532\n533\n534\n535\n536\n537\n538\n539\n540\n541\n542\n543\n544\n545\n546\n547\n548\n549\n550\n551\n552\n553\n554\n555\n556\n557\n558\n559\n560\n561\n562\n563\n564\n565\n566\n567\n568\n569\n570\n571\n572\n573\n574\n575\n576\n577\n578\n579\n580\n581\n582\n583\n584\n585\n586\n587\n588\n589\n590\n591\n592\n593\n594\n595\n596\n597\n598\n599\nWord Type Features Context-aware Features Scores Similarities Representations P R F\n7 All [~wl,mask ; ~wr,mask] 0.674 0.694 0.677 [~wl ; ~wr] 7 7 0.531 0.539 0.535 [~wl ; ~wr] 7 [~wl,mask ; ~wr,mask] 0.512 0.521 0.516 [~wl ; ~wr] All 7 0.642 0.642 0.642 [~wl ; ~wr] All [~wl,mask ; ~wr,mask] 0.638 0.637 0.637 [~wl ; ~wr] All [~cl,c2v ; ~cr,c2v] 0.605 0.604 0.603\n[~wl,c2v ; ~wr,c2v] 7 7 0.533 0.569 0.540 [~wl,c2v ; ~wr,c2v] 7 [~wl,mask ; ~wr,mask] 0.518 0.545 0.526 [~wl,c2v ; ~wr,c2v] All 7 0.655 0.674 0.659 [~wl,c2v ; ~wr,c2v] All [~wl,mask ; ~wr,mask] 0.652 0.670 0.656 [~wl,c2v ; ~wr,c2v] All [~cl,c2v ; ~cr,c2v] 0.601 0.601 0.600 PPDB(wl, wr) Most salient only 7 0.677 0.685 0.670 PPDB(wl, wr) All 7 0.695 0.701 0.692 PPDB(wl, wr) All [~wl,mask ; ~wr,mask] 0.721 0.720 0.718\nTable 3: Experimental results on CONTEXT-PPDB. The evaluation metric is weighted F score.\nthe baseline. Again, we observe that using ~cl,c2v and ~cr,c2v does worse than the alternatives.\nThe absolute value of gains on CONTEXT-WN is smaller than on CONTEXT-PPDB, as can be expected, given that CONTEXT-WN was designed to be challenging (Section 5.2).\n7.3 Cross-lingual dataset\nResults on the cross-lingual dataset are similar to monolingual results (Table 5). Context-aware features outperform context-agnostic baselines - similarities boost the score by 3 and 6 points in the BiVec and BiCVM settings respectively. The contextualized word representations, when used instead of the context-agnostic representations, give a boost of almost 3 points, allowing us to score 6 points and 10 points above the context-agnostic baseline in the BiVec and BiCVM cases, respectively. The strength of these results attest to the generalizable nature of our features, which helps capture correspondences beyond a single language.\n8 Analysis\nThe experiments in Section 7 attest to the overall strength of our features. In this section, we aim to further test the assumptions underlying our proposed context representations.\n8.1 Sensitivity to context\nHow sensitive are our models to changes in contexts? To answer this, we focus on the subset of\nCONTEXT-WN that comprises of positive examples, and the equivalent number of negative examples from Step 3 of the dataset creation. These examples are created by permuting the contexts of the positive examples, and thus, can directly help in answering our question. We analyze the predictions made on this subset by our model using a metric we call \u2018Macro-F1\u2019, defined as the weighted F1 calculated over each (wl,wr) word pair, and then averaged over all word pairs.\nModels using context-aware features do consistently better on Macro-F1 than those without (Table 4). Interestingly though, the model that uses [~cl,c2v ; ~cr,c2v] does the best on this metric, indicating that while directly using the Context2Vec representations might not be ideal, these representation do capture some useful contextual knowledge.\n8.2 Sensitivity to Entailment Direction\nTo quantitatively evaluate our claim of our features learning to discriminate directionality 4, we report results on the subset of CONTEXT-WN that consists of all positive examples, and the equivalent number of flipped negative examples generated in Step 4. We measure directionality by looking at the number of example pairs ((wl, wr, cl, cr), (wr, wl, cr, cl)) where both examples are correctly labeled, i.e. the former is labeled as =\u21d2 and the latter as 6=\u21d2 .\nContext agnostic baselines detect direction significantly above chance (Table 4). Adding our\n7\n601\n602\n603\n604\n605\n606\n607\n608\n609\n610\n611\n612\n613\n614\n615\n616\n617\n618\n619\n620\n621\n622\n623\n624\n625\n626\n627\n628\n629\n630\n631\n632\n633\n634\n635\n636\n637\n638\n639\n640\n641\n642\n643\n644\n645\n646\n647\n648\n649\n650\n651\n652\n653\n654\n655\n656\n657\n658\n659\n660\n661\n662\n663\n664\n665\n666\n667\n668\n669\n670\n671\n672\n673\n674\n675\n676\n677\n678\n679\n680\n681\n682\n683\n684\n685\n686\n687\n688\n689\n690\n691\n692\n693\n694\n695\n696\n697\n698\n699\nWord Type Feat.s Context-aware Features Scores Similarities Representations P R F Macro F1 Pairwise Acc.\nRandom 0.5 0.5 0.5 0.5 0.25 7 All [~wl,mask ; ~wr,mask] 0.700 0.661 0.670 0.677 0.623\n[~wl ; ~wr] 7 7 0.653 0.615 0.625 0.594 0.582 [~wl ; ~wr] 7 [~wl,mask ; ~wr,mask] 0.656 0.618 0.628 0.608 0.592 [~wl ; ~wr] All 7 0.700 0.663 0.672 0.651 0.597 [~wl ; ~wr] All [~wl,mask ; ~wr,mask] 0.704 0.666 0.675 0.673 0.617 [~wl ; ~wr] All [~cl,c2v ; ~cr,c2v] 0.692 0.658 0.667 0.682 0.592\n[~wl,c2v ; ~wr,c2v] 7 7 0.695 0.655 0.664 0.663 0.667 [~wl,c2v ; ~wr,c2v] 7 [~wl,mask ; ~wr,mask] 0.694 0.655 0.665 0.668 0.660 [~wl,c2v ; ~wr,c2v] All 7 0.728 0.689 0.697 0.704 0.700 [~wl,c2v ; ~wr,c2v] All [~wl,mask ; ~wr,mask] 0.724 0.685 0.694 0.704 0.691 [~wl,c2v ; ~wr,c2v] All [~cl,c2v ; ~cr,c2v] 0.713 0.678 0.687 0.708 0.638\nTable 4: Results on CONTEXT-WN. Macro-F1 and Pairwise accuracy, are intended to capture contextawareness and directionality-discrimination abilities of our features, repsectively.\nWord Type Features Context-aware Features Scores Similarities Representations P R F Random 0.570 0.481 0.501\n[~wl,bivec ; ~wr,bivec] 7 7 0.618 0.537 0.560 [~wl,bivec ; ~wr,bivec] 7 [~wl,mask ; ~wr,mask] 0.618 0.537 0.560 [~wl,bivec ; ~wr,bivec] All 7 0.632 0.576 0.595 [~wl,bivec ; ~wr,bivec] All [~wl,mask ; ~wr,mask] 0.620 0.554 0.575\n7 All [~wl,mask ; ~wr,mask] 0.650 0.607 0.622 [~wl,bicvm ; ~wr,bicvm] 7 7 0.651 0.517 0.530 [~wl,bicvm ; ~wr,bicvm] 7 [~wl,mask ; ~wr,mask] 0.655 0.528 0.543 [~wl,bicvm ; ~wr,bicvm] All 7 0.681 0.574 0.590 [~wl,bicvm ; ~wr,bicvm] All [~wl,mask ; ~wr,mask] 0.682 0.566 0.581\n7 All [~wl,mask ; ~wr,mask] 0.676 0.614 0.629\nTable 5: Results on cross-lingual dataset with training on CONTEXT-PPDB using bilingual embeddings.\ncontext features improves by ~3, with the overall best performing model again scoring highest on this metric. It is reassuring to observe that improved detection of context with our features does not come at the cost of detecting entailment direction. Unlike in the previous section, the model that uses [~cl,c2v ; ~cr,c2v] is consistently poor, scoring lower than the corresponding context-agnostic baseline.\n8.3 Contextualized Masks\nWe also hypothesized that masked contextualized representations based on the full volume of the context using min and max operations (Section 3.1) better capture salient context dimensions than the more usual vector averaging approach. We test this hypothesis empirically by replacing\nmasked word-in-context representations ~wl,mask and ~wr,mask by two other ways to capture context. In the first method, we use the mean of the contexts (~cl,mean,~cr,mean). In the second method, we use (~wl,mean, ~wr,mean), i.e. the masked representations calculated by using only the mean. On both CONTEXT-PPDB and CONTEXT-WN, our preferred method outperforms the two alternatives by 2-3 F1 points. In fact, on CONTEXT-WN we can see that our method also captures directionality best.\n9 Related Work\nWordNet and lexical entailment The is-a hierarchy of WordNet (Fellbaum, 1998b) is a prominent source of information for unsupervised detection of hypernymy and entailment (Harabagiu\n8\n701\n702\n703\n704\n705\n706\n707\n708\n709\n710\n711\n712\n713\n714\n715\n716\n717\n718\n719\n720\n721\n722\n723\n724\n725\n726\n727\n728\n729\n730\n731\n732\n733\n734\n735\n736\n737\n738\n739\n740\n741\n742\n743\n744\n745\n746\n747\n748\n749\n750\n751\n752\n753\n754\n755\n756\n757\n758\n759\n760\n761\n762\n763\n764\n765\n766\n767\n768\n769\n770\n771\n772\n773\n774\n775\n776\n777\n778\n779\n780\n781\n782\n783\n784\n785\n786\n787\n788\n789\n790\n791\n792\n793\n794\n795\n796\n797\n798\n799\nDataset Context aware features Scores Similarities Representations P R F Macro F1 Pairwise Acc.\nCONTEXT-PPDB All [~wl,mean ; ~wr,mean] 0.646 0.672 0.643\nNA NAAll [~cl,mean ; ~cr,mean] 0.655 0.678 0.652 All [~wl,mask ; ~wr,mask] 0.674 0.694 0.677\nCONTEXT-WN All [~wl,mean ; ~wr,mean] 0.674 0.635 0.645 0.641 0.577 All [~cl,mean ; ~cr,mean] 0.688 0.648 0.657 0.665 0.608 All [~wl,mask ; ~wr,mask] 0.700 0.661 0.670 0.677 0.623\nTable 6: Impact of masks on CONTEXT-PPDB and CONTEXT-WN.\nand Moldovan, 1998; Shwartz et al., 2015), as well as a source of various datasets (Baroni and Lenci, 2011; Baroni et al., 2012). The dataset we introduce in this work is inspired by the latter line of work, but instead of just extracting word pairs we also obtain exemplar contexts from WordNet.\nModeling word meaning in context Several approaches have been proposed to model the meaning of a word in a given context to capture semantic equivalence in tasks such as lexical substitution, word sense disambiguation or paraphrase ranking (but not entailment).\nOne line of work (Dinu and Lapata, 2010; Reisinger and Mooney, 2010) treats each word as a set of latent word senses. These models methods start with token representations for individual occurrences of a word and then choose a set of token vectors based on the current context. An alternate set of models (Erk and Pado\u0301, 2008; Thater et al., 2011; Dinu et al., 2012) avoids defining a fixed set of word senses, and instead contextualizes word type vectors as we do in this paper. These models share the idea of using an element-wise multiplication to apply a context mask to word type representations. The nature of the context representation varies: Erk and Pado (2008) use inverse selectional preferences; Thater et al (2010) combine a first order representation for the context with a second order representation for the target, Thater et al. (2011) rely on syntactic dependencies to define context. Apidianaki (2016) shows that bag-ofword context representation within a small context window works as well as syntactic definitions of context for ranking paraphrases in context.\nThe use of convolution is motivated by success of similar models on sentence classification tasks. Tang et al (2014) uses convolution over embedding matrices for unigrams, bigrams, and trigrams, while Hovy (2015) uses just unigrams. However, all these works only use convolved representations\nto predict properties of the sentence (e.g., sentiment). We use them, instead, to contextualize our target word representations.\nIn-context lexical semantic tasks Besides entailment, other lexical semantic tasks studied in the presence of context include lexical substitution (McCarthy and Navigli, 2007), cross-lingual lexical substitution (Mihalcea et al., 2010) and paraphrase ranking (Apidianaki, 2016). The last work is also notable because of their successful use of models of word-meaning in context from Thater et al (2011), which is closely related to our methods.\n10 Conclusion\nWe proposed to address lexical entailment in context, providing exemplar sentences to ground the meaning of words being considered for entailment. We show that contextualized word representations constructed by transforming contextagnostic representations, combined with wordcontext similarity features, lead to large improvements over context-agnostic model, not only in English, but also between English and French words on two novel datasets. We also improve the state-of-the-art on a related task of detecting semantic relations in context. Our features are sensitive to changes in entailment based on context, and also capture the directionality of entailment.\nIn future work, we aim to further improve performance on both monolingual and cross-lingual datasets. We anticipate that richer features, capturing second-order comparisons used to detect lexical contrast (Mohammad et al., 2013) and entailment out of context (Turney and Mohammad, 2013), might be useful for this purpose, perhaps in combination with non-linear classifiers. Given the usefulness of similarities, we also plan to investigate asymmetric scoring functions(Kotlerman et al., 2010; Shwartz et al., 2017).\n9\n801\n802\n803\n804\n805\n806\n807\n808\n809\n810\n811\n812\n813\n814\n815\n816\n817\n818\n819\n820\n821\n822\n823\n824\n825\n826\n827\n828\n829\n830\n831\n832\n833\n834\n835\n836\n837\n838\n839\n840\n841\n842\n843\n844\n845\n846\n847\n848\n849\n850\n851\n852\n853\n854\n855\n856\n857\n858\n859\n860\n861\n862\n863\n864\n865\n866\n867\n868\n869\n870\n871\n872\n873\n874\n875\n876\n877\n878\n879\n880\n881\n882\n883\n884\n885\n886\n887\n888\n889\n890\n891\n892\n893\n894\n895\n896\n897\n898\n899\n", "cats": {"READ": 0.0, "DONTREAD": 1.0}, "reviews": [{"IMPACT": "3", "SUBSTANCE": "4", "APPROPRIATENESS": "5", "MEANINGFUL_COMPARISON": "4", "PRESENTATION_FORMAT": "Oral Presentation", "comments": "- Strengths: A well written paper, examining the use of context in lexical\nentailment task is a great idea, a well defined approach and experimental\nset-up and good analysis of the results \n\n- Weaknesses: Some information is missing or insufficient, e.g., the table\ncaptions should be more descriptive, a clear description for each of the word\ntype features should be given.\n\nGeneral Discussion: \n\nThe paper presents a proposal of consideration of context\nin lexical entailment task. The results from the experiments demonstrate that\ncontext-informed models do better than context-agnostic models on the\nentailment task. \n\nI liked the idea of creating negative examples to get negative annotations\nautomatically in the two ways described in the paper based on WordNet positive\nexamples. (new dataset; an interesting method to develop dataset)\n\nI also liked the idea of transforming already-used context-agnostic\nrepresentations into contextualized representations, experimenting with\ndifferent ways to get contextualized representations (i.e., mask vs\ncontetx2vec), and testing the model on 3 different datasets (generalizability\nnot just across different datasets but also cross-linguistically).\n\nMotivations for various decisions in the experimental design were good to\nsee, e.g., why authors used the split they used for CONTEXT-PPDB (it showed\nthat they thought out clearly what exactly they were doing and why).\n\nLines 431-434: authors might want to state briefly how the class weights were\ndetermined and added to account for the unbalanced data in the CONTEXT-WN\nexperiments. Would it affect direct comparisons with previous work, in what\nways? \n\nChange in Line 589: directionality 4 --> directionality, as in Table 4\n\nSuggested change in Line 696-697: is-a hierarchy of WordNet --> \"is-a\"\nhierarchy of WordNet \n\nFor the sake of completeness, represent \"mask\" also in Figure 1.\n\nI have read the author response.", "SOUNDNESS_CORRECTNESS": "3", "ORIGINALITY": "4", "is_meta_review": null, "RECOMMENDATION": "4", "CLARITY": "4", "REVIEWER_CONFIDENCE": "2"}, {"IMPACT": "3", "SUBSTANCE": "1", "APPROPRIATENESS": "5", "MEANINGFUL_COMPARISON": "4", "PRESENTATION_FORMAT": "Poster", "comments": "This paper proposes a method for recognizing lexical entailment (specifically,\nhypernymy) in context. The proposed method represents each context by\naveraging, min-pooling, and max-pooling its word embeddings. These\nrepresentations are combined with the target word's embedding via element-wise\nmultiplication. The in-context representation of the left-hand-side argument is\nconcatenated to that of the right-hand-side argument's, creating a single\nvectorial representation of the input. This input is then fed into a logistic\nregression classifier.\n\nIn my view, the paper has two major weaknesses. First, the classification model\nused in this paper (concat + linear classifier) was shown to be inherently\nunable to learn relations in \"Do Supervised Distributional Methods Really Learn\nLexical Inference Relations?\" (Levy et al., 2015). Second, the paper makes\nsuperiority claims in the text that are simply not substantiated in the\nquantitative results. In addition, there are several clarity and experiment\nsetup issues that give an overall feeling that the paper is still half-baked.\n\n= Classification Model =\n\nConcatenating two word vectors as input for a linear classifier was\nmathematically proven to be incapable of learning a relation between words\n(Levy et al., 2015). What is the motivation behind using this model in the\ncontextual setting?\n\nWhile this handicap might be somewhat mitigated by adding similarity features,\nall these features are symmetric (including the Euclidean distance, since |L-R|\n= |R-L|). Why do we expect these features to detect entailment?\n\nI am not convinced that this is a reasonable classification model for the task.\n\n= Superiority Claims =\n\nThe authors claim that their contextual representation is superior to\ncontext2vec. This is not evident from the paper, because:\n\n1) The best result (F1) in both table 3 and table 4 (excluding PPDB features)\nis the 7th row. To my understanding, this variant does not use the proposed\ncontextual representation; in fact, it uses the context2vec representation for\nthe word type.\n\n2) This experiment uses ready-made embeddings (GloVe) and parameters\n(context2vec) that were tuned on completely different datasets with very\ndifferent sizes. Comparing the two is empirically flawed, and probably biased\ntowards the method using GloVe (which was a trained on a much larger corpus).\n\nIn addition, it seems that the biggest boost in performance comes from adding\nsimilarity features and not from the proposed context representation. This is\nnot discussed.\n\n= Miscellaneous Comments =\n\n- I liked the WordNet dataset - using the example sentences is a nice trick.\n\n- I don\u2019t quite understand why the task of cross-lingual lexical entailment\nis interesting or even reasonable.\n\n- Some basic baselines are really missing. Instead of the \"random\" baseline,\nhow well does the \"all true\" baseline perform? What about the context-agnostic\nsymmetric cosine similarity of the two target words?\n\n- In general, the tables are very difficult to read. The caption should make\nthe tables self-explanatory. Also, it is unclear what each variant means;\nperhaps a more precise description (in text) of each variant could help the\nreader understand?\n\n- What are the PPDB-specific features? This is really unclear.\n\n- I could not understand 8.1.\n\n- Table 4 is overfull.\n\n- In table 4, the F1 of \"random\" should be 0.25.\n\n- Typo in line 462: should be \"Table 3\"\n\n= Author Response =\n\nThank you for addressing my comments. Unfortunately, there are still some\nstanding issues that prevent me from accepting this paper:\n\n- The problem I see with the base model is not that it is learning prototypical\nhypernyms, but that it's mathematically not able to learn a relation.\n\n- It appears that we have a different reading of tables 3 and 4. Maybe this is\na clarity issue, but it prevents me from understanding how the claim that\ncontextual representations substantially improve performance is supported.\nFurthermore, it seems like other factors (e.g. similarity features) have a\ngreater effect.", "SOUNDNESS_CORRECTNESS": "3", "ORIGINALITY": "4", "is_meta_review": null, "RECOMMENDATION": "2", "CLARITY": "3", "REVIEWER_CONFIDENCE": "5"}, {"IMPACT": "3", "SUBSTANCE": "3", "APPROPRIATENESS": "5", "MEANINGFUL_COMPARISON": "4", "PRESENTATION_FORMAT": "Poster", "comments": "This paper addresses the task of lexical entailment detection in context, e.g.\nis \"chess\" a kind of \"game\" given a sentence containing each of the words --\nrelevant for QA. The major contributions are:\n\n(1) a new dataset derived from WordNet using synset exemplar sentences, and \n\n(2) a \"context relevance mask\" for a word vector, accomplished by elementwise\nmultiplication with feature vectors derived from the context sentence. Fed to a\nlogistic regression classifier, the masked word vectors just beat state of the\nart on entailment prediction on a PPDB-derived dataset from previous\nliterature. Combined with other existing features, they beat state of the art\nby a few points. They also beats the baseline on the new WN-derived dataset,\nalthough the best-scoring method on that dataset doesn't use the masked\nrepresentations.\n\nThe paper also introduces some simple word similarity features (cosine,\neuclidean distance) which accompany other cross-context similarity features\nfrom previous literature. All of the similarity features, together, improve the\nclassification results by a large amount, but the features in the present paper\nare a relatively small contribution.\n\nThe task is interesting, and the work seems to be correct as far as it goes,\nbut incremental. The method of producing the mask vectors is taken from\nexisting literature on encoding variable-length sequences into min/max/mean\nvectors, but I don't think they've been used as masks before, so this is novel.\nHowever, excluding the PPDB features it looks like the best result does not use\nthe representation introduced in the paper.\n\nA few more specific points:\n\nIn the creation of the new Context-WN dataset, are there a lot of false\nnegatives resulting from similar synsets in the \"permuted\" examples? If you\ntake word w, with synsets i and j, is it guaranteed that the exemplar context\nfor a hypernym synset of j is a bad entailment context for i? What if i and j\nare semantically close?\n\nWhy does the masked representation hurt classification with the\ncontext-agnostic word vectors (rows 3, 5 in Table 3) when row 1 does so well?\nWouldn't the classifier learn to ignore the context-agnostic features?\n\nThe paper should make clearer which similarity measures are new and which are\nfrom previous literature. It currently says that previous lit used the \"most\nsalient\" similarity features, but that's not informative to the reader.\n\nThe paper should be clearer about the contribution of the masked vectors vs the\nsimilarity features. It seems like similarity is doing most of the work.\n\nI don't understand the intuition behind the Macro-F1 measure, or how it relates\nto \"how sensitive are our models to changes in context\" -- what changes? How do\nwe expect Macro-F1 to compare with F1?\n\nThe cross-language task is not well motivated.\n\nMissing a relevant citation: Learning to Distinguish Hypernyms and Co-Hyponyms.\nJulie Weeds, Daoud Clarke, Jeremy Reffin, David Weir and Bill Keller. COLING\n2014.\n\n==\n\nI have read the author response. As noted in the original reviews, a quick\nexamination of the tables shows that the similarity features make the largest\ncontribution to the improvement in F-score on the two datasets (aside from PPDB\nfeatures). The author response makes the point that similarities include\ncontextualized representations. However, the similarity features are a mixed\nbag, including both contextualized and non-contextualized representations. This\nwould need to be teased out more (as acknowledged in the response).\n\nNeither Table 3 nor 4 gives results using only the masked representations\nwithout the similarity features. This makes the contribution of the masked\nrepresentations difficult to isolate.", "SOUNDNESS_CORRECTNESS": "3", "ORIGINALITY": "4", "is_meta_review": null, "RECOMMENDATION": "2", "CLARITY": "2", "REVIEWER_CONFIDENCE": "4"}]}
{"text": "Neural Disambiguation of Causal Lexical Markers based on Context\n1 000\n011\n012\n013\n014\n015\n016\n017\n018\n019\n020\n021\n022\n023\n024\n025\n026\n027\n028\n029\n030\n031\n032\n033\n034\n035\n036\n037\n038\n039\n040\n041\n042\n043\n044\n045\n046\n047\n048\n049\n061\n062\n063\n064\n065\n066\n067\n068\n069\n070\n071\n072\n073\n074\n075\n076\n077\n078\n079\n080\n081\n082\n083\n084\n085\n086\n087\n088\n089\n090\n091\n092\n093\n094\n095\n096\n097\n098\n099\n1 Introduction\nCausation is a psychological tool of humans to understand the world independently of language, and it is one of the principles involved in the construction of the human mental model of reality (Neeleman and van de Koot, 2012). Causal reasoning is the process of relating two events, namely cause and its effect. Following the words of Reinhart (2002), causal relations are imposed by humans on the input from the world, and the (computational) linguist\u2019s task is to understand what is about language that enables speakers to use it to describe their causal perceptions.\nThere are different theories concerning how natural language approximates causation. Those theories based on generative semantics argue that the causal relation is encoded in the semantics of some verbs (Lakoff, 1970; McCawley, 1976) or in the syntactic structure of a sentence (Dowty, 1979; Ramchand, 2008). For instance, Pylkka\u0308nen (2008) argues that the causing event may be associated with the subject of the causal verb in a causal predicate. This principle is true in the sentence \u201cPe-\nter eventually killed John by hitting him with a hammer\u201d, in which the event of Peter hitting John with a hammer caused the death of John. But, the following sentence can be used as a counterexample: \u201cA hammer eventually killed John\u201d. In the later sentence the subject of \u201ckilled\u201d is the \u201chammer\u201d, which does not constitute a causing event for the event \u201ckill John\u201d. Therefore, the syntaxgrounded construction of causality defined by those theories is far away from the human mental model of causation. Indeed, Neeleman and van de Koot (2012) showed that the later approaches cannot be proven by standard syntactic tests, because neither the causing event nor causal relation correspond to a syntactic constituent. Neeleman and van de Koot (2012) concluded that the linguistic approximation of causation by culmination of events is not exclusive of causal verbs, and it is also found in non-causative verb classes.\nCausation or causality has also been studied in computational linguistics. There are some semantic and discourse resources that take into account causality in the range of linguistic phenomena that they annotate. PropBank (Palmer et al., 2005) is a semantic resource that annotates the argument structure of verbs. The type of causation annotated in PropBank is related to the syntax and semantics of the verbs. For the sentences John broke the window and \u201cThe hammer broke the window\u201d, \u201cJohn\u201d and \u201cthe hammer\u201d are the cause arguments of the verb \u201cbroke\u201d. Although, \u201cJohn\u201d and \u201cthe hammer\u201d are required arguments for the event expressed by \u201cbreak\u201d, they do not represent the causing event of the window breaking event.\nCausality is one of the discourse relations in Penn Discourse Tree Bank (PDTB) (Prasad et al., 2008). Causality is annotated in PDTB as either an explicit or an implicit relation between events. When causality is explicit, it is signalled by a lexical marker such as because or since between others.\n2\n101\n102\n103\n104\n105\n106\n107\n108\n109\n110\n111\n112\n113\n114\n115\n116\n117\n118\n119\n120\n121\n122\n123\n124\n125\n126\n127\n128\n129\n130\n131\n132\n133\n134\n135\n136\n137\n138\n139\n140\n141\n142\n143\n144\n145\n146\n147\n148\n149\n150\n151\n152\n153\n154\n155\n156\n157\n158\n159\n160\n161\n162\n163\n164\n165\n166\n167\n168\n169\n170\n171\n172\n173\n174\n175\n176\n177\n178\n179\n180\n181\n182\n183\n184\n185\n186\n187\n188\n189\n190\n191\n192\n193\n194\n195\n196\n197\n198\n199\nHowever, there are some scenarios where the causal discourse relation is not signalled by a known lexical marker, but by other expressions that are called Alternative Lexicalizations (AltLex). The existence of AltLex means that the expression of causality is not defined by a limited number of lexical constructions, so the coverage of the total number of possible causal expressions is restricted to the grade of coverage of the base corpus of PDTB.\nThe previous paragraphs and the works described in Section 2 show that causality is driven by a limited set of lexical elements and syntactic constructions. However, causality does not have a fixed list of lexical or syntactic constructions. Hidey and McKeown (2016) stressed the issue of the limited coverage of causal expressions and released a corpus with a larger amount of causal expressions than PDTB. They also proposed a classification method, which is based on the use of features from the corpus and features from lexical resources. The last set of features reduces the ability of the system to classify causality, because they restrict the system to the causal definition of the lexical resources.\nCausal meaning disambiguation is the task of identifying whether there is a causal relation between two events. We hypothesize that neural networks are able to encode the meaning of those events, and discover whether the underlying relation between them is causal. Herein, we contribute a neural network architecture for the task of causality disambiguation and we assess it in the corpus of Hidey and McKeown (2016). Empiric results on this corpus show that our claim indeed holds.\n2 Related Work\nKhoo et al. (1998) presented one of the first works related to the classification of causal relations. The authors defined a set of linguistic rules for the identification of cause-effect relations, and developed a pattern matching system based on those rules. The errors of the system show the common errors of a full linguistically grounded classification system, which are related to the ambiguity of some causal links such as by or as or the lexical and syntactic ambiguity of some causative verbs. Those errors are related to the lack of evidence of the syntax basis of causation (Neeleman and van de Koot, 2012).\nGirju (2003) proposed a decision tree learning method to classify causal relations between events. The type of relations studied by the author are those conveyed by a verb and two noun phrases. The clas-\nsification method uses as lexical features whether the verb of the sentence belongs to a list of ambiguous causative verbs, and whether the nouns belongs to one of the nine semantic hierarchies in WordNet. Again, the main source of errors is the ambiguity of the projection of causality in natural language.\nBethard and Martin (2008) defined a method for the classification of causal relations between verbal events in conjunction constructions, namely conjoined events. Since the method was defined for a specific kind of causal relation between events, the authors had to first create a corpus following that requirement. The authors developed a supervised classification system using syntactic and semantic features from WordNet, and they used SVM as a classification algorithm. The drawback of the method is its reduced flexibility, because it is only defined for a specific causal construction and its confidence in syntactic features. Moreover, the authors conclude that semantic features are more adequate for causality classification than syntactic features, which goes in the direction of our claim.\nThe first attempt of encoding the contextual information for causality classification between events is found in Do et al. (2011). The authors defined a measure of the causality relation between two events according to the co-occurrence frequency between the predicates and their arguments. The measure is inspired by the causality score defined by Riaz and Girju (2010).\nFollowing the work of Do et al. (2011), Riaz and Girju (2013, 2014) built a knowledge base for causal events, first for events related by a pair of verbs, and then for events conveyed by pairs of verbs and noun phrases. In these works, the authors attempted to go in the direction of using distributional semantics in order to encode the causal meaning, but they still rely on the use of lexical and syntactic features. Thus, the main drawbacks of those papers are the complex featuring engineering process to detect the causal relation, and the restricted coverage of the resulting knowledge bases.\nFollowing the previous approaches, Mirza and Tonelli (2016) described a causal classification system built upon a rule based system and a machine learning algorithm based on lexical, syntactic and semantic features. The novelty lies in the use of features of temporal relations. The work of Mirza and Tonelli (2016) has similar issues to the work of (Bethard and Martin, 2008), because they use similar features to represent causality.\n3\n201\n202\n203\n204\n205\n206\n207\n208\n209\n210\n211\n212\n213\n214\n215\n216\n217\n218\n219\n220\n221\n222\n223\n224\n225\n226\n227\n228\n229\n230\n231\n232\n233\n234\n235\n236\n237\n238\n239\n240\n241\n242\n243\n244\n245\n246\n247\n248\n249\n250\n251\n252\n253\n254\n255\n256\n257\n258\n259\n260\n261\n262\n263\n264\n265\n266\n267\n268\n269\n270\n271\n272\n273\n274\n275\n276\n277\n278\n279\n280\n281\n282\n283\n284\n285\n286\n287\n288\n289\n290\n291\n292\n293\n294\n295\n296\n297\n298\n299\nSentence Type\n(Cathay Pacific delayed both legs of its quadruple daily Hong Kong to London route)e1 ([due to]l this disruption in air traffic services.)e2 Explicit (The factory was not well equipped to handle the gas)e1 (created by the sudden addition of water to the MIC tank.)e2 Implicit\nTable 1: Explicit and implicit causal relations\nThe previous works support the idea that causality is defined by a specific syntactic construction or the semantics of verbs known as causative verbs. On the other hand, some of the works restricted their study to a narrow kind of syntactic constructions, such as the conjoined events of Bethard and Martin (2008). In contrast, we argue that the representation of causality should not be limited by syntactic patterns or the coverage of semantic resources, due to the lack of a particular linguistic construction for causality. Therefore, we propose the neural encoding of the context of the relation in order to disambiguate its causal meaning.\n3 Causality classification\nThe next sections expose the definition of the task (Section 3.1), the data used in the experiments (Section 3.2) and the proposed system (Section 3.3).\n3.1 Definition\nCausality is defined as the semantic relation between two events (e1, e2), namely causing event (cause, e1) and caused event (effect, e2). An event may be a verb, whose arguments may be explicitly present or not (subject, verb, object), or a noun phrase. The relation can be explicit or implicit, in case it is explicitly expressed, it is signalled by a lexical marker (l), which may be a verb, a preposition, an adverb or an expression such as as a result of, see Table 1.\nSome researchers impose two additional restrictions: the temporal restriction, which means that the causing event should take place before the caused event, and the counterfactual one, which says if the causing event did not occur, the caused event would not have occurred either. Thus, causation can be formally defined as e1 \u2192 e2. An example from the test set used in the experiments that follows the three restrictions is the following:\nA government affidavit in 2006 stated that (the leak)e1 (caused 558,125 injuries, including 38,478 temporary partial injuries and approximately 3,900 severely and permanently disabling injuries)e2\nIt is evident that e1 is before e2 and if the leak did not occur, the set of events in e2 would not occur. Nevertheless, we support the same opinion as Neeleman and van de Koot (2012) and we do not add the counterfactual constraint in our definition, because it is not true in all the cases. For example:\n(The argument between John\u2019s parents)e1 (broke the window)e2\nIf we follow the counterfactual constraint, the event of the argument between the John\u2019s parents should be the causing event. However, the human mental model of causality does not usually relate an argument with the breaking of a window. Causation is concerned with the human understanding of the world and not with the world itself.\nConcerning the temporal restriction, it should be interpreted as a physical ordering constraint and not as a positional one. That means, the causing event does not always appear before the caused event in the sentence. The caused event (e2) is before the causing event (e1) in the following example:\n(This water was diverted)e2 ([due to]l a combination of improper maintenance, leaking and clogging, and eventually ended up in the MIC storage tank.)e1\nSome related work attempted to identify the causing, the caused event and its direction (Mirza and Tonelli, 2016), but for that purpose, they restricted the study to a specific syntactic construction. We claim that the task of causality classification has not to be restricted to a syntactic construction and it has to be set up as a two steps task: causal meaning classification and causal arguments identification. The task of causal meaning classification is a binary classification task that is defined as the disambiguation of the causal meaning of the relation of two events. The input of the task is two events and the output is the meaning of the relation (Causal or Non Causal). The task of causal argument identification is focused on the identification of the causing (e1) and caused (e2) events. The aim of this paper is to contribute to the first subtask, so given two events (e1, e2) and the expression susceptible of signalling causality (l), the system returns the meaning of the relation (Causal or Non Causal). The following sentence is an example of the input of our system.\n(He fell in love with her and changed his life)e1 ([so]l he could help her)e2\n4\n301\n302\n303\n304\n305\n306\n307\n308\n309\n310\n311\n312\n313\n314\n315\n316\n317\n318\n319\n320\n321\n322\n323\n324\n325\n326\n327\n328\n329\n330\n331\n332\n333\n334\n335\n336\n337\n338\n339\n340\n341\n342\n343\n344\n345\n346\n347\n348\n349\n350\n351\n352\n353\n354\n355\n356\n357\n365\n366\n367\n368\n369\n370\n371\n372\n373\n374\n375\n376\n377\n378\n379\n380\n381\n382\n383\n384\n385\n386\n387\n388\n389\n390\n391\n392\n393\n394\n395\n396\n397\n398\n399\nSentence Meaning\nAn undercroft is traditionally a cellar or storage room, often brick-lined and vaulted, and used for storage in buildings since medieval times\nTemporal\nAdditionally if one is to use a large scan range then sensitivity of the instrument is decreased due to performing fewer scans per second since each scan will have to detect a wide range of mass fragments\nCausal\nIn stark contrast to his predecessor, five days after his election he spoke of his determination to do what he could to bring peace Temporal Bischoff in a round table discussion claimed he fired Austin after he refused to do a taping in Atlanta Causal\nTable 2: Examples of different meanings of the prepositions since and after taken from the AltLex corpus\n3.2 Data\nCausality may be signalled by a verb (cause), a preposition (because), an adverb (subsequently) or an expression such as as a result of. Some of those causal expressions are almost unambiguous, but the causal meaning of others totally depends on the context. For instance, according to PDTB, the preposition because is an unambiguous lexical marker of causation, but there are other expressions whose causal meaning is not unarguable, such as since or after that may have a temporal or a causal interpretation, see Table 2. Therefore, we need a corpus in which the events, the lexical marker and the meaning of the relation are annotated.\nTo the best of our knowledge, there are three available corpora. The first one was exposed in Bethard and Martin (2008) and it is built on top of the Penn TreeBank corpus. The corpus only provides the position of the related events in each sentence and whether the relation is causal. Unfortunately, the corpus is not freely available, and is also not large enough to train neural methods.\nThe second corpus is Causal-TimeBank (Mirza et al., 2014), which is built on top of the TimeML corpus. Annotation was restricted to explicit causal relations. They followed the same annotation schema of TimeML, so they defined the label CSIGNAL for the causal lexical markers, and CLINK to mark the causal relation between two events. The authors manually annotated the causal relations between the identified events in TimeML. The fact that the causal lexical markers are annotated agrees with our data requirements, but as the previous corpus, Causal-TimeBank is too small to train neural methods.\nRecently, the AltLex corpus has been freely released (Hidey and McKeown, 2016). The corpus was built on the idea that causation can be expressed by different kinds of linguistic constructions. This is validated by the fact that in PDTB there are explicit causal lexical markers, and other sort of expressions that have a discourse meaning,\nwhich are called AltLex. The relations signalled by an AltLex expression are implicit relations, in which the annotator did not find an appropriate connective to insert between the events, because the meaning of the relation is entailed by other expressions, namely AltLex. Furthermore, some causal lexical markers can be perceived as variations of the basic ones, but those variations are not usually in the list of discourse connectives. Thus, the authors of the AltLex corpus decided to develop a method to identify a larger amount of AltLex expressions with causal meaning. The corpus construction leveraged Simple Wikipedia, by aligning sentences from Wikipedia that consist of unknown lexical causal markers with sentences from Simple Wikipedia that contain corresponding known lexical causal markers. Once a first set of causal and non-causal sentences were identified, a bootsrapping method was applied to enhance the corpus. We call \u201cnon bootstrapping\u201d to the first version of the corpus and \u201cbootstrapping\u201d to the second one. The corpus size is shown in Table 3. For further details see (Hidey and McKeown, 2016).\nThe class distribution of the lexical markers should be studied, because one of the features of the corpus is the annotation of the lexical markers susceptible of expressing causation. Table 4 shows the class distribution of the lexical markers, as well as the the number of unarguable ones and the number of lexical markers with mostly a different meaning in the training and the test set. According to the Table 4, there are few ambiguous connectives, 121 in the \u201cnon bootstrapping\u201d corpus and 147 in the \u201cbootstrapping\u201d version. However, there is an important difference between the two versions of\n5\n401\n402\n403\n404\n405\n406\n407\n408\n409\n410\n411\n412\n413\n414\n415\n416\n417\n418\n419\n420\n421\n422\n423\n424\n425\n426\n427\n428\n429\n430\n431\n432\n433\n434\n435\n436\n437\n438\n439\n440\n441\n442\n443\n444\n445\n446\n447\n448\n449\n461\n462\n463\n464\n465\n466\n467\n468\n469\n470\n471\n472\n473\n474\n475\n476\n477\n478\n479\n480\n481\n482\n483\n484\n485\n486\n487\n488\n489\n490\n491\n492\n493\n494\n495\n496\n497\n498\n499\nBootstrapping\nNo Yes\nTotal 8214 8854 Unambiguous in Causal class 922 1034 Unambiguous in Non Causal class 7171 7673 Causal in train, Non Causal in test 0 27 Non Causal in train, Causal in test 0 8\nTable 4: Distribution of the lexical markers\nthe corpus: there are no differences between the training and the test set in the \u201cnon bootstrapping\u201d version, but there are in the \u201cbootstrapping\u201d one. This fact means that the \u201cbootstrapping\u201d version of the corpus has some instances that do not follow the class distribution of the training set, so they represent a higher difficulty for the classification system. Section 4.3 exposes the good performance of the proposed system in some of those instances.\n3.3 Disambiguation of the Causal Meaning\nKruengkrai et al. (2017) describe a Multi-column Convolutional Neural Network (CNN) that uses background knowledge for the identification of the causal meaning of an input sentence in Japanese. Convolutional networks excel at pattern learning in input data, however causation does not have a particular syntactic structure as it was mentioned. Moreover, CNN requires the definition of the kernel size, which means that we should know beforehand the length of the context that projects the causal meaning of the sentence. Nevertheless, the list of expressions that may represent a causal meaning is not limited and some sentences implicitly express causality. On the other hand, Long short-term memory (LSTM) recurrent neural networks (RNN) (Hochreiter and Schmidhuber, 1997) can encode the sequential information of the input text. Melamud et al. (2016) also showed that using an LSTM results in a more precise context encoding and substantially improve performance in several tasks upon the common average-of-wordsembeddings representation. Therefore, we propose the use of LSTM RNN instead of a CNN, because we argue that LSTM has a higher capacity of encoding meaning for the task of causality classification.\nWe propose a neural network architecture with two inputs, which is mainly based on the encoding of the two inputs with an LSTM and the use of several dense layers with a tanh activation function.\nFollowing the assumption that some sort of relation should exist between the two events of a causal relation, the first evaluated model consists of two\nconnected LSTMs, or in other words, the LSTM network of the second event of the causal relation is initialized with the last state of the first LSTM. We call to this architecture \u201cStated Pair LSTM\u201d. We also evaluated the same architecture but without the connection between the two LSTMs. We call to this second architecture \u201cPair LSTM\u201d and it is depicted in Figure 1.\nAccording to Figure 1, the first input of the system is the first event of the relation, and the second input is the concatenation of the lexical marker and the second event. The text processing starts with the tokenization of the two inputs, and the representation of them as a matrix of word embedding vectors. The set of pre-trained embeddings used was Glove (Pennington et al., 2014), specifically the 300 dimension reference vectors of the 840B cased tokens set.\nThe lengths of the first (n) and the second (m) input are not the same, so three zero-padding strategies were evaluated to measure which of them is the most convenient to encode the causal meaning. The maximum, the mean and mode of the length of the two inputs were calculated:\n\u2200x \u2208 IRn\u00d7d and \u2200y \u2208 IRm\u00d7d\npad(x) = {max(x),mean(x),mode(x)} (1)\nwhere x and y are the first and second input and d is the length of the word embedding vectors.\nSubsequently, each of the two outputs of the encoding layer are vectorized to a vector of length 100 by a dense layer with a tanh activation function. The context of the causal relation is represented by the concatenation of the two vectors.\n\u2200W \u2208 IR100\u00d7nd and \u2200 b \u2208 IR100\nvec(x) : IRn\u00d7d \u2192 IRnd\ntanh(W \u00b7 x+ b) context : (vec(x), vec(y))\n(2)\n6\n501\n502\n503\n504\n505\n506\n507\n508\n509\n510\n511\n512\n513\n514\n515\n516\n517\n518\n519\n520\n521\n522\n523\n524\n525\n526\n527\n528\n529\n530\n531\n532\n533\n534\n535\n536\n537\n538\n539\n540\n541\n542\n543\n544\n545\n546\n547\n548\n549\n550\n551\n552\n553\n554\n555\n556\n557\n558\n559\n560\n561\n562\n563\n564\n565\n566\n567\n568\n569\n570\n571\n572\n573\n574\n575\n576\n577\n578\n579\n580\n581\n582\n583\n584\n585\n586\n587\n588\n589\n590\n591\n592\n593\n594\n595\n596\n597\n598\n599\nThe output of the Equation 2 is processed by three dense layers activated by a tanh function. The last layer is composed of the softmax operation.\nThe performance of two learning optimizers was evaluated, specifically Adadelta (Zeiler, 2012) and Adam (Kingma and Ba, 2015). In order to avoid the overfitting of the classification process, different values for dropout ([0.5, 0.75]) and regularization were used ( [ 8 \u00b7 10\u22123, 8 \u00b7 10\u22126 ] ).\n4 Experiments and Results\nAs far as we know, the AltLex corpus has been only used in the work in which it was presented (Hidey and McKeown, 2016), therefore we consider their method as the state of the art on that corpus. Moreover, as it has been mentioned several times, the task of causality classification lacks of representative or large enough corpus that covers a wide range of causal expressions, as well as without the restriction of being composed of specific causal constructions. Therefore, the methods were only evaluated with the AltLex corpus. We have used the two versions of the corpus (\u201cnon bootstrapping\u201d and \u201cbootstrapping\u201d) in our experiments. The size of each of them is shown in Table 3.\n4.1 Baselines\nWe consider two baselines in order to compare our proposal. The first one (B1) is founded on the use of the most common class of each causal connective according to its class in the training data. For example, if the lexical marker since has mostly a causal meaning, B1 always classifies the relation signalled by since as Causal. A similar approach was also used in (Hidey and McKeown, 2016).\nThe second baseline (B2) is the system of Hidey and McKeown (2016), which is based on SVM with a large set of features generated from the combination of features from the original parallel corpus and some lexical resources (WordNet, VerbNet and PropBank). The fact of relying on lexical resources restricts the recall of the system to the linguistic coverage of the lexical resources. In contrast, we propose a neural network architecture fed only by a set of word embedding vectors, which has a higher ability of generalization as it is shown later in the paper.\n4.2 Results\nThe results reached by our system are shown in\nTable 5.1 The Precision, Recall and F1 values were used to measure the performance of the system in the Causal Class, and the Accuracy to measure the overall performance of the system.\nThe performance of B1 defines a hard baseline for the two versions of the corpus. On the other hand, the good performance of B1 might indicate that the training corpus is composed of few unambiguous causal connectives. Concerning the results reached by the proposed systems, all the configurations in Table 5 outperform the baseline B1, which means that the system learns beyond the class distribution of the lexical markers in the training data.\nThose configurations that use Adadelta as optimizer outperform the system B2 in the \u201cnon bootstrapping\u201d version of the corpus. The behaviour of B2 shows a big difference between Precision and Recall, indicating that the system has many false positives, i.e. a large number of sentences without a causal meaning. This is not a desirable behaviour. On the other hand, our system has a high Precision with lower Recall, indicating it mainly classifies correctly sentences with unambiguous connectives. Figure 2 shows that the most frequent connectives in the Causal class do not have any instance in the Non Causal class, which supports our last assertion. The best configuration (\u201cPair LSTM Max Adadelta\u201d) uses the maximum operation for the zero-padding strategy and Adadelta as learning optimizer. Our proposal in this scenario improves the state of the art by 2.13% according to F1 in the Causal class (C) and 7.72% according to Accuracy.\nWe observe a similar trend in the performance of the system with the \u201cbootstrapping\u201d version of the corpus, i.e. the system architecture \u201cPair LSTM\u201d tends to reach better results than the architecture \u201cStated LSTM\u201d. This behaviour means that our assumption about the relation between the meanings of the two input events does not hold, so it is better to encode each argument independently and then to measure the relation between the arguments by using dense layers. Concerning the optimizers, the results show a similar behaviour in both architectures. The Adadelta optimizer promotes the Recall and penalizes Precision, whereas the system reaches a higher value of Precision and a smaller difference between Precision and Recall\n1For the sake of brevity, those systems that performed worse than B1 and B2 are not in Table 5.\n7\n601\n602\n603\n604\n605\n606\n607\n608\n609\n610\n611\n612\n613\n614\n615\n616\n617\n618\n619\n620\n621\n622\n623\n624\n625\n626\n627\n628\n629\n630\n631\n632\n633\n634\n635\n636\n637\n638\n639\n640\n641\n642\n643\n644\n645\n646\n647\n648\n649\n650\n651\n652\n653\n654\n655\n656\n657\n658\n659\n660\n661\n662\n663\n664\n665\n666\n667\n668\n669\n670\n671\n672\n673\n674\n675\n676\n677\n680\n681\n682\n683\n684\n685\n686\n687\n688\n689\n690\n691\n692\n693\n694\n695\n696\n697\n698\n699\nCorpus Method Precision C. Recall C. F1 C. Accuracy\nNon bootstrapping B1 68.92% 54.92% 61.13% 63.99% B2 70.28% 77.60% 73.76% 71.86% Stated Pair LSTM Mean Adadelta 90.04% 60.31% 72.24% 76.10% Stated Pair LSTM Mode Adadelta 89.23% 63.17% 73.97% 77.08% Pair LSTM Max Adadelta 88.46% 65.71% 75.40% 77.90% Pair LSTM Mean Adadelta 89.33% 63.80% 74.44% 77.41%\nBootstrapping\nB1 74.38% 86.66% 80.05% 77.74% B2 77.29% 84.85% 80.90% 79.58% Stated Pair LSTM Max Adadelta 78.69% 84.44% 81.47% 80.19% Stated Pair LSTM Mean Adam 80.00% 82.53% 81.25% 80.36% Stated Pair LSTM Mode Adam 80.24% 82.53% 81.37% 80.52% Pair LSTM Max Adadelta 78.07% 84.76% 81.12% 79.86% Pair LSTM Mean Adadelta 78.48% 85.71% 81.94% 80.52% Pair LSTM Mean Adam 80.30% 82.85% 81.56% 80.68% Pair LSTM Mode Adam 77.24% 87.30% 81.96% 80.19%\nTable 5: Results of the baselines (B1 and B2) and the configurations evaluated\nb e ca u se b e ca u se o f so so t h a t th u s su b se q u e n tl y a s a r e su lt o f th e re fo re h e n ce in r e sp o n se t o a s d u e t o th e re b y so i n so a s a n d fo r d u e a n d s o co n se q u e n tl y si n ce th e n a n d b e ca u se a ft e r so f o r w h e n so o n b e ca u se i n B e ca u se o f S o so i m p re ss e d w it h T h u s so a t th a t b e ca u se so w it h ca n t h e re fo re so b y so i m p re ss e d b y so m u ch t h a t b e ca u se a t th u s : T h e re fo re so b e ca u se so a ft e r so s u cc e ss fu l th a t a cc o rd in g ly in o rd e r ca n t h u s so f a r a s so f ro m\n0\n500\n1000\n1500\n2000\nF re q u e n cy\nCausal Non Causal\nFigure 2: Distribution of the most frequent connectives in the class Cause\nwhen Adam optimizer is used, which is a more desirable behaviour. A high value of Precision and a high value of Recall mean that there is a good balance between the accuracy in the disambiguation of the causal meaning of the relation and the coverage of different ways of expressing causality. To conclude, the system that reached the highest performance is the one that used the mean strategy for zero-padding and the optimizer Adam.\nMost of the configurations of the proposed neural network outperform the baseline B2. If we look at the values of Precision, our proposal improves the Precision value of B2 by 3.89% . However, B2 is better 2.41% according to the Recall value. The good Recall value of B2 system entails a drop in the performance according to the Precision. This behaviour is also developed by the neural network configurations that use maximum strategy for zero-padding and Adadelta as optimizer, but in those cases the Recall and the Pre-\ncision are greater. As previously mentioned, this means that the systems have a large number of false positive instances, so they might not be correctly learning the characteristics of the Causal class. In contrast, \u201cPair LSTM Mean Adam\u201d reached the best Precision in the Causal class and the less difference between Precision and Recall, thus it also reached a better balance between the accuracy in the disambiguation of the causal relation and the coverage of different ways of expressing causality.\nCorpus Version Causal Non Causal Total\nTraining\nNon bootstrapping\n7,606 7,929 15,534\nBootstrapping 12,534 8,821 21,354\nThe class distribution of the AltLex corpus is not balanced (see Table 3), so we reduced the number of instances of the Non Causal by a factor of ten with the aim of evaluating our system with a more balanced dataset. The new corpus size is in Table 6. We assessed the performance of the baseline B1 and the system \u201cPair LSTM Mean Adam\u201d and the results are in Table 7. Regarding the performance of B1 in the \u201cnon bootstrapping version\u201d, the Precision is lower and the Recall is higher than in the previous experiments, which means that the number of false positives is large, so it is not learning the characteristics of the class Causal. As in the previous set of experiments, B1 promotes Recall and penalizes Precision in the \u201cbootstrapping\u201d version of the corpus, which is not a desirable behaviour. In contrast, our network architecture followed the same trend in both set of experiments and with the both version of the corpus. \u201cPair LSTM Mean Adam\u201d\n8\n701\n702\n703\n704\n705\n706\n707\n708\n709\n710\n711\n712\n713\n714\n715\n716\n717\n718\n719\n720\n721\n722\n723\n724\n725\n726\n727\n728\n729\n730\n731\n732\n733\n734\n735\n736\n737\n738\n739\n740\n741\n742\n743\n744\n745\n746\n747\n748\n749\n750\n751\n752\n753\n754\n755\n756\n757\n758\n759\n760\n761\n762\n763\n764\n765\n766\n767\n768\n769\n770\n771\n772\n773\n774\n775\n776\n777\n778\n779\n780\n781\n782\n783\n784\n785\n786\n787\n788\n789\n790\n791\n792\n793\n794\n795\n796\n797\n798\n799\nCorpus Method Precision C. Recall C. F1 C. Accuracy\nNon bootstrapping B1 63.70% 84.12% 72.50% 67.10%Pair LSTM Mean Adam 76.38% 69,84% 72,96% 73.32%\nBootstrapping B1 67.34% 94.28% 78.57% 73.48%Pair LSTM Mean Adam 72.65% 88.57% 79.82% 76.92%\nTable 7: Results of B1 and Pair LSTM Mean Adam with the reduced version of the AltLex corpus\nSentence Training Test B2 Our proposal\nThe United States decided to break off economic relations with Cuba (which means that they would stop buying things from them). Causal Non Causal Causal Non Causal\nAlthough Roosevelt had promised to keep the United States out of the war, he nevertheless took concrete steps to prepare for war. Causal Non Causal Causal Non Causal\nGreatly alarmed and with Hitler making further demands on the Free City of Danzig, Britain and France guaranteed their support for Polish independence; when Italy conquered Albania in April 1939, the same guarantee was extended to Romania and Greece. Causal Non Causal Causal Non Causal\nOne of these fragments gives rise to fibrils of amyloid beta, which then form clumps that deposit outside neurons in dense formations known as senile plaques. Ambiguous Causal Non Causal Causal\nTable 8: Some correctly classified examples by our best configuration and misclassified by B2\nsignificantly improves the performance of B1 according to the McNemar\u2019s test with a P-value of 0.005 and 0.05 respectively.\n4.3 Analysis\nAs mentioned in the previous sections, there are some linguistic constructions that can represent a causal meaning in some contexts, but not in other and vice-versa. A better balance between Precision and Recall in the Causal class means that the system learns the causal meaning better. Roughly speaking, the quality of the causal disambiguation is better, also for those lexical markers that are mostly Causal in the training data. We present some correctly classified examples by our best configuration and misclassified by B2 in Table 8. It shows the class of the instance in the test corpus and the value of the most frequent class in the training corpus. The behaviour of the verb break should be stressed since it is considered as a causative verb. However, there are some other uses of break that do not have a causal meaning (see Table 8).\nThe behaviour of the proposed system with the connective which then is also remarkable. That connective is totally ambiguous, because it only has one instance labelled as Causal and other as Non Causal in the training data. We have to take into account that we work with word embeddings, so if the individual words which and then are over-represented in one class, that fact can determine the behaviour of the classification system.\nThe individual term which does not appear in any instance of the training corpus, while the word then is mostly in sentences without a causal meaning. Despite the difficulty of the expression which then, we can see in Table 8 that our proposed system correctly disambiguate its causal meaning, while the system of Hidey and McKeown (2016) does not. Therefore, the results reached and those examples allow us to confirm our claim that the encoding of the context is required for the disambiguation of the causal meaning as we have shown in this paper.\n5 Conclusions and Future Work\nWe defined the task of causation classification as a task composed of another two subtasks: causal meaning classification and causal argument classification. The paper was focused on the first subtask, and we claim that the encoding of the two events of the relation is required for a suitable disambiguation of causality. We proposed an encoding system based on a neural network with two inputs, one for the first event and the other for the lexical marker and the second event. Our proposed system outperforms the state-of-the-art. We also showed the success of the system in some non-causative sentences but with commonly causative verbs (see Table 8).\nOne of the problems of the task is the lack of resources, so, as future work, we plan the creation of a new corpus for the two subtasks of causality classification, namely causality disambiguation and causality argument classification.\n9\n801\n802\n803\n804\n805\n806\n807\n808\n809\n810\n811\n812\n813\n814\n815\n816\n817\n818\n819\n820\n821\n822\n823\n824\n825\n826\n827\n828\n829\n830\n831\n832\n833\n834\n835\n836\n837\n838\n839\n840\n841\n842\n843\n844\n845\n846\n847\n848\n849\n850\n851\n852\n853\n854\n855\n856\n857\n858\n859\n860\n861\n862\n863\n864\n865\n866\n867\n868\n869\n870\n871\n872\n873\n874\n875\n876\n877\n878\n879\n880\n881\n882\n883\n884\n885\n886\n887\n888\n889\n890\n891\n892\n893\n894\n895\n896\n897\n898\n899\n", "cats": {"READ": 0.0, "DONTREAD": 1.0}, "reviews": [{"IMPACT": "3", "SUBSTANCE": "4", "APPROPRIATENESS": "5", "MEANINGFUL_COMPARISON": "3", "PRESENTATION_FORMAT": "Poster", "comments": "This paper develops an LSTM-based model for classifying connective uses for\nwhether they indicate that a causal relation was intended. The guiding idea is\nthat the expression of causal relations is extremely diverse and thus not\namenable to syntactic treatment, and that the more abstract representations\ndelivered by neural models are therefore more suitable as the basis for making\nthese decisions.\n\nThe experiments are on the AltLex corpus developed by Hidley and McKeown. The\nresults offer modest but consistent support for the general idea, and they\nprovide some initial insights into how best to translate this idea into a\nmodel. The paper distribution includes the TensorFlow-based models used for the\nexperiments.\n\nSome critical comments and questions:\n\n* The introduction is unusual in that it is more like a literature review than\na full overview of what the paper contains. This leads to some redundancy with\nthe related work section that follows it. I guess I am open to a non-standard\nsort of intro, but this one really doesn't work: despite reviewing a lot of\nideas, it doesn't take a stand on what causation is or how it is expressed, but\nrather only makes a negative point (it's not reducible to syntax). We aren't\nreally told what the positive contribution will be except for the very general\nfinal paragraph of the section.\n\n* Extending the above, I found it disappointing that the paper isn't really\nclear about the theory of causation being assumed. The authors seem to default\nto a counterfactual view that is broadly like that of David Lewis, where\ncausation is a modal sufficiency claim with some other counterfactual\nconditions added to it. See line 238 and following; that arrow needs to be a\nvery special kind of implication for this to work at all, and there are\nwell-known problems with Lewis's theory (see\nhttp://bcopley.com/wp-content/uploads/CopleyWolff2014.pdf). There are comments\nelsewhere in the paper that the authors don't endorse the counterfactual view,\nbut then what is the theory being assumed? It can't just be the temporal\nconstraint mentioned on page 3!\n\n* I don't understand the comments regarding the example on line 256. The\nauthors seem to be saying that they regard the sentence as false. If it's true,\nthen there should be some causal link between the argument and the breakage.\nThere are remaining issues about how to divide events into sub-events, and\nthese impact causal theories, but those are not being discussed here, leaving\nme confused.\n\n* The caption for Figure 1 is misleading, since the diagram is supposed to\ndepict only the \"Pair_LSTM\" variant of the model. My bigger complaint is that\nthis diagram is needlessly imprecise. I suppose it's okay to leave parts of the\nstandard model definition out of the prose, but then these diagrams should have\na clear and consistent semantics. What are all the empty circles between input\nand the \"LSTM\" boxes? The prose seems to say that the model has a look-up\nlayer, a Glove layer, and then ... what? How many layers of representation are\nthere? The diagram is precise about the pooling tanh layers pre-softmax, but\nnot about this. I'm also not clear on what the \"LSTM\" boxes represent. It seems\nlike it's just the leftmost/final representation that is directly connected to\nthe layers above. I suggest depicting that connection clearly.\n\n* I don't understand the sentence beginning on line 480. The models under\ndiscussion do not intrinsically require any padding. I'm guessing this is a\nrequirement of TensorFlow and/or efficient training. That's fine. If that's\ncorrect, please say that. I don't understand the final clause, though. How is\nthis issue even related to the question of what is \"the most convenient way to\nencode the causal meaning\"? I don't see how convenience is an issue or how this\nrelates directly to causal meaning.\n\n* The authors find that having two independent LSTMs (\"Stated_LSTM\") is\nsomewhat better than one where the first feeds into the second. This issue is\nreminiscent of discussions in the literature on natural language entailment,\nwhere the question is whether to represent premise and hypothesis independently\nor have the first feed into the second. I regard this as an open question for\nentailment, and I bet it needs further investigation for causal relations too.\nSo I can't really endorse the sentence beginning on line 587: \"This behaviour\nmeans that our assumption about the relation between the meanings of the two\ninput events does not hold, so it is better to encode each argument\nindependently and then to measure the relation between the arguments by using\ndense layers.\" This is very surprising since we are talking about subparts of a\nsentence that might share a lot of information.\n\n* It's hard to make sense of the hyperparameters that led to the best\nperformance across tasks. Compare line 578 with line 636, for example. Should\nwe interpret this or just attribute it to the unpredictability of how these\nmodels interact with data?\n\n* Section 4.3 concludes by saying, of the connective 'which then', that the\nsystem can \"correctly disambiguate its causal meaning\", whereas that of Hidey\nand McKeown does not. That might be correct, but one example doesn't suffice to\nshow it. To substantiate this point, I suggest making up a wide range of\nexamples that manifest the ambiguity and seeing how often the system delivers\nthe right verdict. This will help address the question of whether it got lucky\nwith the example from table 8.", "SOUNDNESS_CORRECTNESS": "5", "ORIGINALITY": "5", "is_meta_review": null, "RECOMMENDATION": "2", "CLARITY": "4", "REVIEWER_CONFIDENCE": "4"}, {"IMPACT": "3", "SUBSTANCE": "4", "APPROPRIATENESS": "5", "MEANINGFUL_COMPARISON": "3", "PRESENTATION_FORMAT": "Oral Presentation", "comments": "This paper proposes a method for detecting causal relations between clauses,\nusing neural networks (\"deep learning\", although, as in many studies, the\nnetworks are not particularly deep).  Indeed, while certain discourse\nconnectives are unambiguous regarding the relation they signal (e.g. 'because'\nis causal) the paper takes advantage of a recent dataset (called AltLex, by\nHidey and McKeown, 2016) to solve the task of identifying causal vs. non-causal\nrelations when the relation is not explicitly marked.  Arguing that\nconvolutional networks are not as adept as representing the relevant features\nof clauses as LSTMs, the authors propose a classification architecture which\nuses a Glove-based representation of clauses, input in an LSTM layer, followed\nby three densely connected layers (tanh) and a final decision layer with a\nsoftmax.\n\nThe best configuration of the system improves by 0.5-1.5% F1 over Hidey and\nMCkeown's 2016 one (SVM classifier).  Several examples of generalizations where\nthe system performs well are shown (indicator words that are always causal in\nthe training data, but are found correctly to be non causal in the test data).\nTherefore, I appreciate that the system is analyzed qualitatively and \nquantitatively.\n\nThe paper is well written, and the description of the problem is particularly\nclear. However a clarification of the differences between this task and the \ntask of implicit connective recognition would be welcome.  This could possibly \ninclude a discussion of why previous methods for implicit connective \nrecognition cannot be used in this case.\n\nIt is very appreciable that the authors uploaded their code to the submission\nsite (I inspected it briefly but did not execute it).  Uploading the (older)\ndata (with the code) is also useful as it provides many examples.  It was not\nclear to me what is the meaning of the 0-1-2 coding in the TSV files, given\nthat the paper mentions binary classification. I wonder also, given that this\nis the data from Hidey and McKeown, if the authors have the right to repost it\nas they do.  -- One point to clarify in the paper would be the meaning of\n\"bootstrapping\", which apparently extends the corpus by about 15%: while the\nconstruction of the corpus is briefly but clearly explained in the paper, the\nadditional bootstrapping is not. \n\nWhile it is certainly interesting to experiment with neural networks on this\ntask, the merits of the proposed system are not entirely convincing.  It seems\nindeed that the best configuration (among 4-7 options) is found on the test\ndata, and it is this best configuration that is announced as improving over\nHidey by \"2.13% F1\".  However, a fair comparison would involve selecting the\nbest configuration on the devset.\n\nMoreover, it is not entirely clear how significant the improvement is. On the\none hand, it should be possible, given the size of the dataset, to compute some\nstatistical significance indicators.  On the other hand, one should consider\nalso the reliability of the gold-standard annotation itself (possibly from the\ncreators of the dataset).  Upon inspection, the annotation obtained from the\nEnglish/SimpleEnglish Wikipedia is not perfect, and therefore the scores might\nneed to be considered with a grain of salt.\n\nFinally, neural methods have been previously shown to outperform human\nengineered features for binary classification tasks, so in a sense the results \nare rather a confirmation of a known property. It would be interesting to see\nexperiments with simpler networks used as baselines, e.g. a 1-layer LSTM.  The\nanalysis of results could try to explain why the neural method seems to favor \nprecision over recall.", "SOUNDNESS_CORRECTNESS": "5", "ORIGINALITY": "5", "is_meta_review": null, "RECOMMENDATION": "3", "CLARITY": "5", "REVIEWER_CONFIDENCE": "4"}]}
{"text": "Chunk-based Decoder for Neural Machine Translation\n1 000\n011\n012\n013\n014\n015\n016\n017\n018\n019\n020\n021\n022\n023\n024\n025\n026\n027\n028\n029\n030\n031\n032\n033\n034\n035\n036\n037\n038\n039\n040\n041\n042\n043\n044\n045\n046\n047\n048\n049\n061\n062\n063\n064\n065\n066\n067\n068\n069\n070\n071\n072\n073\n074\n075\n076\n077\n078\n079\n080\n081\n082\n083\n084\n085\n086\n087\n088\n089\n090\n091\n092\n093\n094\n095\n096\n097\n098\n099\n1 Introduction\nNeural machine translation (NMT) performs an end-to-end translation based on a simple encoderdecoder model (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Cho et al., 2014b), and now has overwhelmed the classical, complex statistical machine translation (SMT) (Sennrich et al., 2016; Luong and Manning, 2016; Cromieres et al., 2016; Neubig, 2016). In NMT, an encoder first maps a source sequence into vector representations and a decoder then maps the vectors into a target sequence (\u00a7 2). This simple framework allows researchers to incorporate the structure of the source\nsentence as in SMT by leveraging various architectures as the encoder (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Cho et al., 2014b; Eriguchi et al., 2016b). Most of the NMT models, however, still rely on a sequential decoder based on recurrent neural network (RNN), due to the difficulty in capturing the structure of a target sentence that is unseen during translation.\nWith the sequential decoder, however, there are two problems to be solved. First, it is difficult to model long-distance dependencies (Bahdanau et al., 2015). A hidden state ht in an RNN is only conditioned by its previous output yt\u22121, previous hidden state ht\u22121 and current input xt. This makes it difficult to capture the dependencies between an older output yt\u2212N if they are too far from the current output. This problem can become more serious when the target sequence become longer. For example in Figure 1, when one translates the English sentence into the Japanese one, after the decoder predicts the content word \u201c\u565b\u307e (bite)\u201d, it has to predict five function words \u201c\u308c (passive)\u201d, \u201c\u305f (past)\u201d, \u201c\u305d\u3046 (hearsay)\u201d, \u201c\u3060 (positive)\u201d, and \u201c\u3051\u308c\u3069 (but)\u201d and a punctuation mark \u201c\u3001\u201d before predicting the next content word \u201c\u541b (you)\u201d. In such a case, the decoder is required to capture the longer dependencies in a target sentence.\nAnother problem with the sequential decoder is that it is expected to cover possible word orders simply by memorizing the local word sequences in the limited training data. This problem can be more serious in free word-order languages such as Czech, German, Japanese, and Turkish. In the case of the example in Figure 1, the order of the subject phrase \u201c\u3060\u308c\u304b\u304c (someone was)\u201d and the modifier phrase \u201c\u72ac\u306b (by a dog)\u201d are flexible. This means that simply memorizing the word order in training data is not enough to train a model that can assign a high probability to a correct sentence regardless of its word order.\n2\n101\n102\n103\n104\n105\n106\n107\n108\n109\n110\n111\n112\n113\n114\n115\n116\n117\n118\n119\n120\n121\n122\n123\n124\n125\n126\n127\n128\n129\n130\n131\n132\n133\n134\n135\n136\n137\n138\n139\n140\n141\n142\n143\n144\n145\n146\n147\n148\n149\n150\n151\n152\n153\n154\n155\n156\n157\n158\n159\n160\n161\n162\n163\n164\n165\n166\n167\n168\n169\n170\n171\n172\n173\n174\n175\n176\n177\n178\n179\n180\n181\n182\n183\n184\n185\n186\n187\n188\n189\n190\n191\n192\n193\n194\n195\n196\n197\n198\n199\n!\" # $ % & '( \" ) *+ ! ,\"- . / 0 12 3 4 5 65 7 !\"#$\"%$ &\"' ()*$ +\", )%-,.$\n%,// %,//\n0%1 233 34$5.&3 33 *45* !\"#$\"%$3 3365! ()**$%3 33(+ 5 &\"'3 3373 336$.$%8* +\",333 )%-,.$&3 339\n:51\nFigure 1: Translation from English to Japanese. The aligned words are content words and the underlined words are function words.\nLooking back to the past, chunks (or phrases) are utilized to handle the above aforementioned problems in statistical machine translation (SMT) (Watanabe et al., 2003; Koehn et al., 2003) and in example-based machine translation (EBMT) (Kim et al., 2010). By using a chunk rather than a word as the basic translation unit, one can treat a sentence as a shorter sequence. This makes it easy to capture the longer dependencies in a target sentence. The order of words in a chunk is relatively fixed while that in a sentence is much more flexible. Thus, modeling intra-chunk (local) dependencies and inter-chunk (global) dependencies independently can help capture the difference of the flexibility between the word order and the chunk order in free word-order languages.\nIn this paper, we refine the original RNN decoder to consider chunk information in NMT. We propose three novel NMT models that capture and utilize the chunk structure in the target language (\u00a7 3). Our focus is the hierarchical structure of a sentence: each sentence consists of chunks, and each chunk consists of words. To encourage an NMT model to capture the hierarchical structure, we start from a hierarchical RNN that consists of a chunk-level decoder and a word-level decoder (Model 1). Then, we improve the word-level decoder by introducing inter-chunk connections to capture the interaction between chunks (Model 2). Finally, we introduce a feedback mechanism to the chunk-level decoder to enhance the memory capacity of previous outputs (Model 3).\nWe evaluate the three models on the WAT \u201916 English-to-Japanese translation task (\u00a7 4). The experimental results show that our best model outperforms the best single NMT model reported in WAT \u201916 (Eriguchi et al., 2016b).\nOur contributions are twofold: (1) chunk information is firstly introduced into NMT to improve translation performance, and (2) a novel hierarchical decoder is devised to model the properties of chunk structure in the encoder-decoder framework.\n2 Preliminaries: Attention-based Neural Machine Translation\nIn this section, we briefly introduce the architecture of the attention-based NMT model, which is the basis of our proposed models.\n2.1 Neural Machine Translation\nAn NMT model usually consists of two connected neural networks: an encoder and a decoder. After the encoder maps a source sentence into a fixed-length vector, the decoder maps the vector into a target sentence. The implementation of the encoder can be a convolutional neural network (CNN) (Kalchbrenner and Blunsom, 2013), a long short-term memory (LSTM) (Sutskever et al., 2014; Luong and Manning, 2016), a gated recurrent unit (GRU) (Cho et al., 2014b; Bahdanau et al., 2015), or a Tree-LSTM (Eriguchi et al., 2016b). While various architectures are leveraged as an encoder to capture the structural information in the source language, most of the NMT models rely on a standard sequential network such as LSTM or GRU as its decoder.\nFollowing (Bahdanau et al., 2015), we use GRU as the recurrent unit in this paper. A GRU unit computes its hidden state vector hi given an input vector xi and the previous hidden state hi\u22121:\nhi = GRU(hi\u22121, xi). (1)\nThe function GRU(\u00b7) is calculated as:\nri = \u03c3(Wrxi +Urhi\u22121 + br), (2)\nzi = \u03c3(Wzxi +Uzhi\u22121 + bz), (3)\nh\u0303i = tanh(Wxi +U(ri hi\u22121 + b)), (4) hi = (1\u2212 zi) h\u0303i + zi hi\u22121, (5)\nwhere the vectors ri and zi are reset gate and update gate, respectively. While the former gate allows the model to forget the previous states, the latter gate decides how much the model updates its content. All the W s and Us, or the bs above\n3\n201\n202\n203\n204\n205\n206\n207\n208\n209\n210\n211\n212\n213\n214\n215\n216\n217\n218\n219\n220\n221\n222\n223\n224\n225\n226\n227\n228\n229\n230\n231\n232\n233\n234\n235\n236\n237\n238\n239\n240\n241\n242\n243\n244\n245\n246\n247\n248\n249\n260\n261\n262\n263\n264\n265\n266\n267\n268\n269\n270\n271\n272\n273\n274\n275\n276\n277\n278\n279\n280\n281\n282\n283\n284\n285\n286\n287\n288\n289\n290\n291\n292\n293\n294\n295\n296\n297\n298\n299\n!\"\n!\"#$%&'( )*%%&\"( +,-,&+\n#\"\n#$\"\n\u3060\u308c \u304b \u304c %\"\n&'\n.\n&( &).\n/ )&-'% *)\n.\n.#(\n#$(\n#'\n#$'\nFigure 2: Standard word-based decoder.\nare trainable matrices or vectors. \u03c3(\u00b7) and denote the sigmoid function and element-wise multiplication operator, respectively.\nWe train a GRU that encodes a source sentence {x1, \u00b7 \u00b7 \u00b7 , xI} into a single vector hI . At the same time, we jointly train another GRU that decodes hI to the target sentence {y1, \u00b7 \u00b7 \u00b7 , yJ}. Here, the jth word in the target sentence yj can be predicted with this decoder GRU and a nonlinear function g(\u00b7) followed by a softmax layer, as\nc = hI , (6)\nsj = GRU(sj\u22121, [yj\u22121; c]), (7)\ns\u0303j = g(yj\u22121, sj , c), (8) P (yj |y<j ,x) = softmax(s\u0303j), (9)\nwhere c is a context vector of the encoded sentence and sj is a hidden state of the decoder GRU.\nFollowing Bahdanau et al. (2015), we use a mini-batch stochastic gradient descent (SGD) algorithm with ADADELTA (Zeiler, 2012) to train the above two GRUs (i.e., the encoder and the decoder) jointly. The objective is to minimize the cross-entropy loss of the training data D, as\nJ = \u2211\n(x,y)\u2208D\n\u2212 logP (y|x). (10)\n2.2 Attention Mechanism for Neural Machine Translation\nTo use all the hidden states of the encoder and improve the translation performance of long sentences, Bahdanau et al. (2015) proposed using an attention mechanism. In their model, the context vector is not simply the last encoder state hI but rather the weighted sum of all encoder states, as follows:\ncj = I\u2211\ni=1\n\u03b1jihi. (11)\nHere, the weight \u03b1ji decides how much a source word xi contributes to the target word yj . \u03b1ji is\ncomputed by a feedforward layer and a softmax layer as\neji = v \u00b7 tanh(Wehi +Uesj + be), (12) \u03b1ji = exp(eji)\u2211J\nj\u2032=1 exp(ej\u2032i) , (13)\nwhere We, Ue are trainable matrices and the be is a trainable vector. In a decoder using the attention mechanism, the obtained context vector cj in each timestep replaces cs in Eqs. (7) and (8). An illustration of the NMT model with the attention mechanism is shown in Figure 2.\nThe attention mechanism is expected to learn alignments between source and target words, and plays a similar role to the translation model in phrase-based SMT (Koehn et al., 2003).\n3 Chunk-based Neural Machine Translation\nTaking non-sequential information such as chunks (or phrases) structure into consideration is proved to be helpful for SMT (Watanabe et al., 2003; Koehn et al., 2003) and EBMT (Kim et al., 2010). We here focus on two important properties of chunks (Abney, 1991): (1) The word order in a chunk is almost always fixed; (2) A chunk consists of a few (typically one) content words surrounded by zero or more function words.\nTo fully utilize the above properties of a chunk, we propose to model the intra-chunk and the interchunk dependencies independently with a \u201cchunkby-chunk\u201d decoder (See Figure 3). In the standard word-by-word decoder described in \u00a7 2, a target word yj in the target sentence y is predicted by taking the previous outputs y<j and the source\n4\n301\n302\n303\n304\n305\n306\n307\n308\n309\n310\n311\n312\n313\n314\n315\n316\n317\n318\n319\n320\n321\n322\n323\n324\n325\n326\n327\n328\n329\n330\n331\n332\n333\n334\n335\n336\n337\n338\n339\n340\n341\n342\n343\n344\n345\n346\n347\n348\n349\n350\n351\n352\n353\n354\n355\n356\n357\n358\n359\n360\n361\n362\n363\n364\n365\n366\n367\n368\n369\n370\n371\n372\n373\n374\n375\n376\n377\n378\n379\n380\n381\n382\n383\n384\n385\n386\n387\n388\n389\n390\n391\n392\n393\n394\n395\n396\n397\n398\n399\n!\"\n!\"#$%&'('& )'*\"$'#+ ,!-./.-0\n1'23'4567&+84*\"$'#+ ,!-././0\n#$\n%$ &'( %) &'(\n9:34;+/ 9:34;+;%/ 9:34;+;\n%\" &'(\n%*\" &'(\n\u3060\u308c \u304b \u304c \u72ac \u565b\u307e \u308c\n%*$ &'( %*) &'(\n+\"\n<\n%,-./ &'(\n+,-./\n9:34;%&'('& )'*\"$'#+ ,!-./.=0\n#) #0<\n> :'7#$ 10\n<\n<\n<\n!\"#$%& '(&)*+$,-./0*1& .\"**$2+3\"*& 4\u00a756'7\n!\"#$%& 5(&8\",#-+\"-./0*1& 9$$#:;21& 4\u00a75657\n%$ &2(\n%3 &2( %34$ &2(\n%\" &2(\nFigure 4: Proposed model: Chunk-based NMT. A chunk-level decoder generates a chunk representation for each chunk while a word-level decoder uses the representation to predict each word. The solid lines in the figure illustrate Model 1. The dashed blue arrows in the word-level decoder denote the connections added in Model 2. The dotted red arrows in the chunk-level decoder denote the feedback states added in Model 3; the connections in the thick black arrows are replaced with the dotted red arrows.\nsentence x as input:\nP (y|x) = J\u220f\nj=1\nP (yj |y<j ,x), (14)\nwhere J is the length of the target sentence. Not assuming any structural information of the target language, the sequential decoder has to memorize long dependencies in a sequence. To release the model from the strong pressure of memorizing the long dependencies over a sentence, we redefine this problem as the combination of a word prediction problem and a chunk generation problem: P (y|x) = K\u220f k=1 P (ck|c<k,x) Jk\u220f j=1 P (yj |y<j , ck,x)  , (15) whereK is the number of chunks in the target sentence and Jk is the length of the k-th chunk (see Figure 3). The first term represents the generation probability of a chunk and the second term indicates the probability of a word in the chunk . We model the former term as a chunk-level decoder and the latter term as a word-level decoder. As we will later confirm in \u00a7 4, both K and Jk are much shorter than the sentence length J , which is why our decoders do not have to memorize the long dependencies like the standard decoder does.\nIn the above formulation, we model the information of the words and their orders in a chunk.\nNo matter which language we target, a chunk usually consists of some content words and function words, and the word order in the chunk is almost always fixed (Abney, 1991). Although our idea can be used in several languages, the optimal network architecture could be adapted depending on the word order of the target language. In this work, we design models for languages in which content words are followed by function words, such as Japanese and Korean. The details of our models are described in the following sections.\n3.1 Model 1: Standard Chunk-based NMT\nThe model described in this section is the basis of our proposed models. It consists of three parts: a sequential encoder (\u00a7 3.1.1), a chunk-level decoder (\u00a7 3.1.2), and a word-level decoder (\u00a7 3.1.3). The part drawn in black solid lines in Figure 4 illustrates the architecture of Model 1.\n3.1.1 Sequential Encoder\nWe adopt a standard single-layer bidirectional GRU (Cho et al., 2014b; Bahdanau et al., 2015) as our encoder (see the right part in Figure 4). By using a standard sequential encoder, we need not perform any additional preprocessing on test data such as syntactic parsing. This design prevents our model from being affected by any errors that may occur as a result of additional preprocessing during test time.\n5\n401\n402\n403\n404\n405\n406\n407\n408\n409\n410\n411\n412\n413\n414\n415\n416\n417\n418\n419\n420\n421\n422\n423\n424\n425\n426\n427\n428\n429\n430\n431\n432\n433\n434\n435\n436\n437\n438\n439\n440\n441\n442\n443\n444\n445\n446\n447\n448\n449\n450\n451\n452\n453\n454\n455\n456\n457\n458\n459\n460\n461\n462\n463\n464\n465\n466\n467\n468\n469\n470\n471\n472\n473\n474\n475\n476\n477\n478\n479\n480\n481\n482\n483\n484\n485\n486\n487\n488\n489\n490\n491\n492\n493\n494\n495\n496\n497\n498\n499\n3.1.2 Chunk-level Decoder\nOur chunk-level decoder (see Figure 3) outputs a chunk representation. The chunk representation contains the information about the words that should be predicted by the word-level decoder.\nTo generate the representation of the k-th chunk s\u0303 (c) k , the chunk-level decoder (see the bottom layer in Figure 4) takes the last states of the word-level decoder s(w)Jk\u22121 and updates its hidden state s (c) k as:\ns (c) k = GRU(s (c) k\u22121, s (w) Jk\u22121 ), (16) s\u0303 (c) k = Wcs (c) k + bc. (17)\nThe obtained chunk representation s\u0303(c)k continues to be fed into the word-level decoder until it outputs all the words in current chunk.\n3.1.3 Word-level Decoder\nOur word-level decoder (see Figure 4) differs from the standard sequential decoder described in \u00a7 2 in that it takes the chunk representation s\u0303(c)k as input:\ns (w) j = GRU(s (w) j\u22121, [s\u0303 (c) k ; yj\u22121; c (w) j\u22121]), (18) s\u0303 (w) j = g(yj\u22121, s (w) j , c (w) j ), (19)\nP (yj |y<j ,x) = softmax(s\u0303(w)j ). (20)\nIn a standard sequential decoder, the hidden state iterates over the length of a target sentence. In other words, its hidden layers are required to memorize the long-term dependencies in the target language. In contrast, in our word-level decoder, the hidden state iterates only over the length of a chunk. Thus, our word-level decoder is released from the pressure of memorizing the long (interchunk) dependencies and can focus on learning the short (intra-chunk) dependencies.\n3.2 Model 2: Inter-Chunk Connection\nThe second term in Eq. (15) only iterates over a chunk (j = 1 to Jk). This means that the last state and the last output of a chunk are not being fed into the word-level decoder at the next timestep (see the black part in Figure 4). In other words, s (w) 1 in Eq. (18) is always initialized before generating the first word in a chunk. This may affect the word-level decoder because it cannot access any previous information at the first word of each chunk.\nTo address this problem, we add new connections to Model 1 between the first state in a chunk\nand the last state in the previous chunk, as\ns (w) 1 = GRU(s (w) Jk\u22121 , [s\u0303 (c) k ; yJk\u22121 ; c (w) Jk\u22121 ]). (21)\nThe dashed blue arrows in Figure 4 illustrate the added inter-chunk connections.\n3.3 Model 3: Word-to-Chunk Feedback\nThe chunk-level decoder in Eq. (16) is only conditioned by s(w)Jk\u22121 , the last word state in each chunk (see the black part in Figure 4). This may affect the chunk-level decoder because it cannot memorize what kind of information has already been generated by the word-level decoder. The information about the words in a chunk should not be included in the representation of the next chunk; otherwise, it may generate the same chunks for multiple times, or forget to translate some words in the source sentence.\nTo encourage the chunk-level decoder to remove the information about the previous outputs more carefully, we add feedback states to our chunk-level decoder in Model 2. The feedback state in the chunk-level decoder is updated at every timestep j as:\ns (c) j = GRU(s (c) j\u22121, s (w) j ). (22)\nThe red lines in Figure 4 illustrate the added feedback states and their connection. The connections in the thick black arrows are replaced with the dotted red arrows in Model 3.\n4 Experiments\n4.1 Setup\nData To clarify the effectiveness of our decoders, we choose Japanese, a free word-order language, as the target language. Japanese sentences are easy to be broken into well-defined chunks (called bunsetsus (Hashimoto, 1934) in Japanese), and the accuracy of bunsetsu-chunking is over 99% (Murata et al., 2000; Yoshinaga and Kitsuregawa, 2014). The effect of chunking errors in training the decoder can be suppressed so we can evaluate the potential of our method. We use the English-Japanese training corpus in the Asian Scientific Paper Excerpt Corpus (ASPEC) (Nakazawa et al., 2016), which was provided in WAT \u201916. To remove inaccurate translation pairs, we extracted the first 2 million data from the 3 million translation pairs following the setting that gave the best performances in WAT \u201915 (Neubig et al., 2015).\n6\n501\n502\n503\n504\n505\n506\n507\n508\n509\n510\n511\n512\n513\n514\n515\n516\n517\n518\n519\n520\n521\n522\n523\n524\n525\n526\n527\n528\n529\n530\n531\n532\n533\n534\n535\n536\n537\n538\n539\n540\n541\n542\n543\n544\n545\n546\n547\n548\n549\n556\n557\n558\n559\n560\n561\n562\n563\n564\n565\n566\n567\n568\n569\n570\n571\n572\n573\n574\n575\n576\n577\n578\n579\n580\n581\n582\n583\n584\n585\n586\n587\n588\n589\n590\n591\n592\n593\n594\n595\n596\n597\n598\n599\nCorpus # words # chunks # sentences Train 44,286,317 13,707,397 1,505,871 Dev. 54,287 - 1,790 Test 54,088 - 1,812\nTable 1: Statistics of the target language (Japanese) in extracted corpus after preprocessing.\nPreprocessings For Japanese sentences, we performed tokenization using KyTea 0.4.7.1 (Neubig et al., 2011) Then we performed bunsetsuchunking with CaboCha 0.69.2 For English sentences, we performed the same preprocessings described on the WAT \u201916 Website.3 To suppress having possible chunking errors affect the translation quality, we removed extremely long chunks from the training data. Specifically, among the 2 million preprocessed translation pairs, we excluded the sentence pairs that matched any of following conditions: (1) The length of the source sentence or target sentence is larger than 64; (2) The maximum length of a chunk in the target sentence is larger than 8 (around 1% of whole data); (3) The maximum number of chunks in the target sentence is larger than 20 (around 2% of whole data). The amount of the excluded sentences by the condition (2) or (3) is negligible (less than 3% of whole data). Table 1 shows the details of the extracted data.\nPostprocessing To perform unknown word replacement (Luong et al., 2015), we built a bilingual English-Japanese dictionary from all of the 3 million translation pairs. The dictionary was extracted with the MGIZA++ 0.7.04 (Och and Ney, 2003; Gao and Vogel, 2008) word alignment tool by automatically extracting the alignments between English words and Japanese words.\nEvaluation Following the WAT \u201916 evaluation procedure, we used BLEU (Papineni et al., 2002) and RIBES (Isozaki et al., 2010) to evaluate our models. The BLEU scores were calculated with multi-bleu.pl in Moses 2.1.15 (Koehn et al., 2007); RIBES scores were calculated with RIBES.py 1.03.1.6 (Isozaki et al., 2010)\n1http://www.phontron.com/kytea/ 2http://taku910.github.io/cabocha/ 3http://lotus.kuee.kyoto-u.ac.jp/WAT/\nbaseline/dataPreparationJE.html 4https://github.com/moses-smt/mgiza 5http://www.statmt.org/moses/ 6http://www.kecl.ntt.co.jp/icl/lirg/ ribes/index.html\nTraining Details We use a single layer bidirectional GRU for the encoder and standard single layer GRUs for the word-level decoder and the chunk-level decoder. The vocabulary sizes are set to 30k for both source and target languages. The conditional probability of each target word is computed with a deep-output (Pascanu et al., 2014) layer with maxout (Goodfellow et al., 2013) units. The maximum number of output chunks is set to 20 and the maximum length of a chunk is set to 9.\nThe models are optimized using ADADELTA following (Bahdanau et al., 2015). The hyperparameters of the training procedure are fixed to the values given in Table 2. Note that the learning rate is halved when the BLEU score on the development set does not increase for 30,000 batches. All the parameters are initialized randomly with Gaussian distribution. It takes about a week to train each model with an NVIDIA TITAN X (Pascal) GPU.\n4.2 Results\nFollowing (Cho et al., 2014a), we perform beam search8 with length-normalized log-probability to decode target sentences. We saved the trained models that performed best on the development set during training, and use them to test the systems with the test set. Table 3 shows the results on the test set. Note that all the models listed in Table 3, including our three models, are single models without ensemble techniques. We set the word-based sequence-to-sequence model (Li et al., 2016) as our baseline, which is a standard implementation of the attention-based NMT described in \u00a7 2. We also compare our methods with the tree-to-sequence model (Eriguchi et al., 2016b) to compare the effectiveness of capturing the structure in the source language and that in the target language. Our improved models (Model 2 and Model 3) outperform all the single models reported in WAT \u201916. The best model (Model 3)\n7http://lotus.kuee.kyoto-u.ac.jp/WAT/ evaluation\n8Beam size is set to 20.\n7\n601\n602\n603\n604\n605\n606\n607\n608\n609\n610\n611\n612\n613\n614\n615\n616\n617\n618\n619\n620\n621\n622\n623\n624\n625\n626\n627\n628\n629\n630\n631\n632\n633\n634\n635\n636\n637\n638\n639\n640\n641\n642\n643\n644\n645\n646\n647\n648\n649\n650\n651\n652\n653\n654\n655\n656\n657\n658\n659\n660\n661\n662\n663\n664\n665\n666\n667\n668\n669\n670\n671\n672\n673\n674\n675\n676\n677\n678\n679\n680\n681\n682\n683\n684\n685\n686\n687\n688\n689\n690\n691\n692\n693\n694\n695\n696\n697\n698\n699\nSystem RNN |Vsrc| |Vtrg| BLEU RIBES Word-based Seq-to-Seq (Li et al., 2016) GRU 40k 30k 33.47 78.75 Word-based Tree-to-Seq (Eriguchi et al., 2016b) LSTM 88k 66k 34.87 81.58 Character-based Tree-to-Seq (Eriguchi et al., 2016a) LSTM 88k 3k 31.52 79.39 Proposed chunk-based model (Model 1) GRU 30k 30k 33.56 79.92 + Inter-chunk connection (Model 2) GRU 30k 30k 35.44 80.95 + Word-to-chunk feedback (Model 3) GRU 30k 30k 36.20 82.06\nTable 3: The results and settings of the baseline systems and our systems. |Vsrc| and |Vtrg| denote the vocabulary size of the source language and the target language, respectively. Only single NMT models (w/o ensembling) reported in WAT \u201916 are listed here. Full results are available on the WAT \u201916 Website.7\n!\"#$%&' ()*+, -.-&$,,/&+%$*0&/, ()&,,1&2, ,.#()\"$,,3+,,()&\"$4,,\"1,,56.++, ($.1+*(*\"1,,-\"*1(,,7$\"8,,%$\"++6*19&/ $&+*1,,(\",,6*1&.$,,-\"648&$, :,\n;&7&$&1%&' \u8457\u8005\u306e \u67b6\u6a4b \u6a39\u8102\u304b\u3089 \u7dda\u578b\u30dd\u30ea\u30de\u3078\u306e \u30ac\u30e9\u30b9\u8ee2\u79fb\u70b9 \u306b\u95a2 \u3059\u308b \u65b0\u3057\u3044 \u7406\u8ad6\u306b \u3064\u3044 \u3066 \u8ff0\u3079\u305f \u3002\n<\"/&6,=' \u67b6\u6a4b\u6a39\u8102\u304b\u3089 \u7dda\u72b6\u30dd\u30ea\u30de\u3078 \u306e \u30ac\u30e9\u30b9\u8ee2\u79fb\u70b9\u306b \u95a2\u3059\u308b \u8457\u8005\u306e \u7406\u8ad6\u306b\u3064 \u3044\u3066 \u8ff0\u3079\u305f\u3002 .#()\"$3+ ()&,()&\"$4 /&+%$*0&/7$\"8,%$\"++6*19&/ $&+*1 (\",6*1&.$,-\"648&$ .0\"#(,56.++,($.1+*(*\"1,-\"*1(\n<\"/&6,>' \u67b6\u6a4b\u6a39\u8102\u304b\u3089 \u7dda\u72b6\u30dd\u30ea\u30de\u30fc\u3078\u306e \u30ac\u30e9\u30b9\u8ee2\u79fb\u70b9\u306b \u95a2\u3059\u308b \u8457\u8005\u306e \u65b0\u3057\u3044 \u7406\u8ad6\u306b\u3064 \u3044\u3066 \u8ff0\u3079\u305f\u3002 1&2.#()\"$3+ /&+%$*0&/()&,()&\"$47$\"8,%$\"++6*19&/ $&+*1 (\",6*1&.$,-\"648&$ .0\"#(,56.++,($.1+*(*\"1,-\"*1(\n<\"/&6,?' \u67b6\u6a4b\u6a39\u8102\u304b\u3089 \u7dda\u72b6\u9ad8 \u5206\u5b50\u306b \u30ac\u30e9\u30b9\u8ee2\u79fb\u70b9 \u306b\u95a2 \u3059\u308b \u65b0\u3057\u3044 \u8457\u8005\u306e \u8457\u8005 \u306e \u7406\u8ad6\u306b \u3064\u3044 \u3066 \u8ff0\u3079\u305f \u3002 1&2 .#()\"$3+ .#()\"$3+ ()&,()&\"$4 /&+%$*0&/7$\"8,%$\"++6*19&/ $&+*1 (\",6*1&.$,-\"648&$ .0\"#(,56.++,($.1+*(*\"1,-\"*1(\nFigure 5: Translation examples of a long sentence. Each sequence of underlined words correspond to a chunk recognized by our decoder. Model 1 outputs a chunk \u201c\u8457\u8005\u306e (author\u2019s)\u201d twice by mistake. Model 2 does not output a chunk \u201c\u65b0\u3057\u3044 (new)\u201d by mistake. In contrast, Model 3 outputs a correct translation although there is a minor word order difference from the reference.\noutperforms the tree-to-sequence model (Eriguchi et al., 2016b) by +1.33 BLEU score and +0.48 RIBES score. The results show that capturing the chunk structure in the target language is more effective than capturing the syntax structure in the source language. Compared with the characterbased NMT model, our Model 3 outperformed the model of (Eriguchi et al., 2016a) by +4.68 BLEU score and +2.67 RIBES score. The characterbased model has a great advantage in that it does not require a large vocabulary size. Although the character-based model is less time-consuming thanks to the small target vocabulary size (|Vtrg| = 3k), our chunk-based model significantly outperformed it in terms of translation quality. One possible reason for this is that using a character-based model rather than a word-based model makes it more difficult to capture long-distance dependencies because the length of a target sequence becomes much longer in the character-based model.\nTo understand the qualitative difference between our three models, we show translation examples in Figure 5. While Model 3 outputs a cor-\nrect translation, there are some errors in the outputs of Model 1 and Model 2. Only Model 1 outputs a chunk \u201c\u8457\u8005\u306e (author\u2019s)\u201d twice continuously. This error indicates that the inter-chunk connections added in Model 2 play important roles in memorizing previous word states more efficiently. On the other hand, Model 2 does not output a chunk \u201c\u65b0\u3057\u3044 (new)\u201d by mistake, which is probably because it does not have a good ability to memorize the previous chunks. This phenomenon supports the importance of the feedback states that are added in Model 3.\n5 Related Work\nThere has been much work done on using chunk (or phrase) structure to improve machine translation quality. The most notable work was phrasebased SMT (Koehn et al., 2003), which has been the basis for a huge amount of work on SMT for more than ten years. Apart from this, Watanabe et al. (2003) proposed a chunk-based translation model that generates output sentences in a chunkby-chunk manner. The chunk structure is effective\n8\n701\n702\n703\n704\n705\n706\n707\n708\n709\n710\n711\n712\n713\n714\n715\n716\n717\n718\n719\n720\n721\n722\n723\n724\n725\n726\n727\n728\n729\n730\n731\n732\n733\n734\n735\n736\n737\n738\n739\n740\n741\n742\n743\n744\n745\n746\n747\n748\n749\n750\n751\n752\n753\n754\n755\n756\n757\n758\n759\n760\n761\n762\n763\n764\n765\n766\n767\n768\n769\n770\n771\n772\n773\n774\n775\n776\n777\n778\n779\n780\n781\n782\n783\n784\n785\n786\n787\n788\n789\n790\n791\n792\n793\n794\n795\n796\n797\n798\n799\nnot only for SMT but also for example-based machine translation (EBMT). Kim et al. (2010) proposed a chunk-based EBMT and showed that using chunk structures can help with finding better word alignments. Our work is different from their works in that our models are based on NMT, but not SMT or EBMT. The decoders in the above works can model the chunk structure by storing chunk pairs in a large table. On the other hand, we do that by separately training a chunk generation model and a word prediction model with two RNNs.\nWhile most of the NMT models focus on the conversion between sequential data, some works have tried to incorporate non-sequential information into NMT (Eriguchi et al., 2016b; Su et al., 2017). Eriguchi et al. (2016b) use tree-based LSTM (Tai et al., 2015) to encode input sentence into context vectors. Given a syntactic tree of a source sentence, their tree-based encoder encodes words from the leaf nodes to the root nodes recursively. Su et al. (2017) proposed a lattice-based encoder that considers multiple tokenization results while encoding the input sentence. To prevent the tokenization errors from propagating to the whole NMT system, their lattice-based encoder can utilize multiple tokenization results. These works focus on the encoding process and propose better encoders that can exploit the structures of the source language. In contrast, our work focuses on the decoding process to capture the structure of the target language. The encoders described above and our proposed decoders are complementary so they can be combined into a single network.\nConsidering that our Model 1 described in \u00a7 3.1 can be seen as a hierarchical RNN, our work is also related to previous studies that utilize multilayer RNNs to capture hierarchical structures in data. Hierarchical RNNs are used not only in the field of machine translation (Luong and Manning, 2016) but also for various NLP tasks such as document modeling (Li et al., 2015; Lin et al., 2015), dialog generation (Serban et al., 2017), image captioning (Krause et al., 2016), and video captioning (Yu et al., 2016). In particular, Li et al. (2015) and Luong and Manning (2016) use hierarchical sequence-to-sequence models, but not for the purpose of learning syntactic structures of target sentences. Li et al. (2015) build hierarchical models at the sentence-word level to obtain better document representations. Luong and Manning (2016) build the word-character level to cope with the out-\nof-vocabulary problem. In contrast, we build a chunk-word level model to explicitly capture the syntactic structure based on chunk segmentation.\nIn addition, the architecture of Model 3 is also related to stacked RNN, which has shown to be effective to improve the translation quality (Luong et al., 2015; Sutskever et al., 2014). Although these architectures look similar to each other, there is a fundamental difference between the directions of the connection between two layers. A stacked RNN consists of multiple RNN layers that are connected from the input side to the output side at every timestep. In contrast, our Model 3 has a different connection at each timestep. Before it generates a chunk, there is a feed-forward connection from the chunk-level decoder to the word-level decoder. However, after generating a chunk representation, the connection is to be reversed to feedback the information from the word-level decoder to the chunk-level decoder. By switching the connections between two layers, our model can capture the chunk structure explicitly. This is the first work that proposes the decoders for NMT that can capture plausible linguistic structures like chunk.\n6 Conclusion\nIn this paper, we propose chunk-based decoders for NMT. As the attention mechanism in NMT plays a similar role to the translation model in phrase-based SMT, our chunk-based decoders are intended to capture the notion of chunk in chunkbased (or phrase-based) SMT. We utilize the chunk structure to efficiently capture long-distance dependencies and cope with the problem of free word-order languages like Japanese. We design three models that have hierarchical RNN-like architectures, each of which consists of a word-level decoder and a chunk-level decoder. We performed experiments on the WAT \u201916 English-to-Japanese translation task and found that our best model outperforms all the single models that were reported in WAT \u201916 by +4.68 to +1.33 BLEU scores and by +3.31 to +0.48 RIBES scores.\nIn future work, we will apply our method to other target languages and evaluate the effectiveness on different languages such as Czech, German or Turkish. In addition, we plan to combine our decoder with other encoders that capture language structure, such as a Tree-LSTM (Eriguchi et al., 2016b), or an order-free encoder, such as a CNN (Kalchbrenner and Blunsom, 2013).\n9\n801\n802\n803\n804\n805\n806\n807\n808\n809\n810\n811\n812\n813\n814\n815\n816\n817\n818\n819\n820\n821\n822\n823\n824\n825\n826\n827\n828\n829\n830\n831\n832\n833\n834\n835\n836\n837\n838\n839\n840\n841\n842\n843\n844\n845\n846\n847\n848\n849\n850\n851\n852\n853\n854\n855\n856\n857\n858\n859\n860\n861\n862\n863\n864\n865\n866\n867\n868\n869\n870\n871\n872\n873\n874\n875\n876\n877\n878\n879\n880\n881\n882\n883\n884\n885\n886\n887\n888\n889\n890\n891\n892\n893\n894\n895\n896\n897\n898\n899\n", "cats": {"READ": 1.0, "DONTREAD": 0.0}, "reviews": [{"IMPACT": "3", "SUBSTANCE": "4", "APPROPRIATENESS": "5", "MEANINGFUL_COMPARISON": "5", "PRESENTATION_FORMAT": "Oral Presentation", "comments": "- Strengths:\nThe paper presents an interesting extension to attention-based neural MT\napproaches, which leverages source-sentence chunking as additional piece of\ninformation from the source sentence. The model is modified such that this\nchunking information is used differently by two recurrent layers: while one\nfocuses in generating a chunk at a time, the other focuses on generating the\nwords within the chunk. This is interesting. I believe readers will enjoy\ngetting to know this approach and how it performs.\nThe paper is very clearly written, and alternative approaches are clearly\ncontrasted. The evaluation is well conducted, has a direct contrast with other\npapers (and evaluation tables), and even though it could be strengthened (see\nmy comments below), it is convincing.\n\n- Weaknesses:\nAs always, more could be done in the experiments section to strengthen the case\nfor chunk-based models. For example, Table 3 indicates good results for Model 2\nand Model 3 compared to previous papers, but a careful reader will wonder\nwhether these improvements come from switching from LSTMs to GRUs. In other\nwords, it would be good to see the GRU tree-to-sequence result to verify that\nthe chunk-based approach is still best.\n\nAnother important aspect is the lack of ensembling results. The authors put a\nlot of emphasis is claiming that this is the best single NMT model ever\npublished. While this is probably true, in the end the best WAT system for\nEng-Jap is at 38.20 (if I'm reading the table correctly) - it's an ensemble of\n3. If the authors were able to report that their 3-way chunk-based ensemble\ncomes top of the table, then this paper could have a much stronger impact.\n\nFinally, Table 3 would be more interesting if it included decoding times. The\nauthors mention briefly that the character-based model is less time-consuming\n(presumably based on Eriguchi et al.'16), but no cite is provided, and no\nnumbers from chunk-based decoding are reported either. Is the chunk-based model\nfaster or slower than word-based? Similar? Who know... Adding a column to Table\n3 with decoding times would give more value to the paper.\n\n- General Discussion:\nOverall I think the paper is interesting and worth publishing. I have minor\ncomments and suggestions to the authors about how to improve their presentation\n(in my opinion, of course). \n\n* I think they should clearly state early on that the chunks are supplied\nexternally - in other words, that the model does not learn how to chunk. This\nonly became apparent to me when reading about CaboCha on page 6 - I don't think\nit's mentioned earlier, and it is important.\n\n* I don't see why the authors contrast against the char-based baseline so often\nin the text (at least a couple of times they boast a +4.68 BLEU gain). I don't\nthink readers are bothered... Readers are interested in gains over the best\nbaseline.\n\n* It would be good to add a bit more detail about the way UNKs are being\nhandled by the neural decoder, or at least add a citation to the\ndictionary-based replacement strategy being used here.\n\n* The sentence in line 212 (\"We train a GRU that encodes a source sentence into\na single vector\") is not strictly correct. The correct way would be to say that\nyou do a bidirectional encoder that encodes the source sentence into a set of\nvectors... at least, that's what I see in Figure 2.\n\n* The motivating example of lines 69-87 is a bit weird. Does \"you\" depend on\n\"bite\"? Or does it depend on the source side? Because if it doesn't depend on\n\"bite\", then the argument that this is a long-dependency problem doesn't really\napply.", "SOUNDNESS_CORRECTNESS": "5", "ORIGINALITY": "3", "is_meta_review": null, "RECOMMENDATION": "4", "CLARITY": "5", "REVIEWER_CONFIDENCE": "5"}, {"IMPACT": "3", "SUBSTANCE": "4", "APPROPRIATENESS": "5", "MEANINGFUL_COMPARISON": "5", "PRESENTATION_FORMAT": "Oral Presentation", "comments": "- Summary\n\nThis paper introduces chunk-level architecture for existing NMT models. Three\nmodels are proposed to model the correlation between word and chunk modelling\non the target side in the existing NMT models. \n\n- Strengths:\n\nThe paper is well-written and clear about the proposed models and its\ncontributions. \n\nThe proposed models to incorporating chunk information into NMT models are\nnovel and well-motivated. I think such models can be generally applicable for\nmany other language pairs. \n\n- Weaknesses:\n\nThere are some minor points, listed as follows:\n\n1) Figure 1: I am a bit surprised that the function words dominate the content\nones in a Japanese sentence. Sorry I may not understand Japanese. \n\n2) In all equations, sequences/vectors (like matrices) should be represented\nas bold texts to distinguish from scalars, e.g., hi, xi, c, s, ...\n\n3) Equation 12: s_j-1 instead of s_j.\n\n4) Line 244: all encoder states should be referred to bidirectional RNN states.\n\n5) Line 285: a bit confused about the phrase \"non-sequential information such\nas chunks\". Is chunk still sequential information???\n\n6) Equation 21: a bit confused, e.g, perhaps insert k into s1(w) like s1(w)(k)\nto indicate the word in a chunk.  \n\n7) Some questions for the experiments:\n\nTable 1: source language statistics? \n\nFor the baselines, why not running a baseline (without using any chunk\ninformation) instead of using (Li et al., 2016) baseline (|V_src| is\ndifferent)? It would be easy to see the effect of chunk-based models. Did (Li\net al., 2016) and other baselines use the same pre-processing and\npost-processing steps? Other baselines are not very comparable. After authors's\nresponse, I still think that (Li et al., 2016) baseline can be a reference but\nthe baseline from the existing model should be shown. \n\nFigure 5: baseline result will be useful for comparison? chunks in the\ntranslated examples are generated *automatically* by the model or manually by\nthe authors? Is it possible to compare the no. of chunks generated by the model\nand by the bunsetsu-chunking toolkit? In that case, the chunk information for\nDev and Test in Table 1 will be required. BTW, the authors's response did not\naddress my point here. \n\n8) I am bit surprised about the beam size 20 used in the decoding process. I\nsuppose large beam size is likely to make the model prefer shorter generated\nsentences. \n\n9) Past tenses should be used in the experiments, e.g.,\n\nLine 558: We *use* (used) ...\n\nLine 579-584: we *perform* (performed) ... *use* (used) ...\n\n...\n\n- General Discussion:\n\nOverall, this is a solid work - the first one tackling the chunk-based NMT;\nand it well deserves a slot at ACL.", "SOUNDNESS_CORRECTNESS": "5", "ORIGINALITY": "3", "is_meta_review": null, "RECOMMENDATION": "4", "CLARITY": "5", "REVIEWER_CONFIDENCE": "5"}]}
{"text": "Evaluation Metrics for Reading Comprehension: Prerequisite Skills and Readability\n1 000\n011\n012\n013\n014\n015\n016\n017\n018\n019\n020\n021\n022\n023\n024\n025\n026\n027\n028\n029\n030\n031\n032\n033\n034\n035\n036\n037\n038\n039\n040\n041\n042\n043\n044\n045\n046\n047\n048\n049\n074\n075\n076\n077\n078\n079\n080\n081\n082\n083\n084\n085\n086\n087\n088\n089\n090\n091\n092\n093\n094\n095\n096\n097\n098\n099\n1 Introduction\nA major goal of natural language processing (NLP) is to develop agents that can understand natural language. Such an ability can be tested with a reading comprehension (RC) task that requires the agent to read open-domain documents and answer questions about them. Building the RC ability is challenging because RC comprises multiple processes including parsing, understanding cohesion, and inference with linguistic and general knowledge.\nClarifying what a system achieves is important to the development of RC systems. To achieve robust improvement, systems need to be measured according to various metrics, not just simple accuracy. However, a current problem is that most RC datasets are presented only with superficial categories, such as question types (e.g., what, where, and who) and answer types (e.g., numeric, location, and person). In addition, Chen et al. (2016) revealed that some questions in datasets may not\nhave the quality to test RC systems. In these situations, it is difficult to assess systems accurately.\nAs Norvig (1989) stated, questions easy for humans often turn out to be difficult for systems. For example, see the two RC questions in Figure 1. In the first example from SQuAD (Rajpurkar et al., 2016), although the document is taken from a Wikipedia article and is thus written for adults, the question is easy to solve simply look at one sentence and select an entity. On the other hand, in the second example from MCTest (Richardson et al., 2013), the document is written for children and is easy to read, but the question requires reading multiple sentences and a combination of several skills, such as understanding of causal relations (Sara wanted... \u2192 they went to...), coreference resolution (Sara and Her Dad = they), and complementing ellipsis (baseball team = team). These two examples show that the readability of the text does not necessarily correlate with the difficulty of the questions. Nevertheless, the accompanying categories of existing RC datasets cannot\n2\n101\n102\n103\n104\n105\n106\n107\n108\n109\n110\n111\n112\n113\n114\n115\n116\n117\n118\n119\n120\n121\n122\n123\n124\n125\n126\n127\n128\n129\n130\n131\n132\n133\n134\n135\n136\n137\n138\n139\n140\n141\n142\n143\n144\n145\n146\n147\n148\n149\n150\n151\n152\n153\n154\n155\n156\n157\n158\n159\n160\n161\n162\n163\n164\n165\n166\n167\n168\n169\n170\n171\n172\n173\n174\n175\n176\n177\n178\n179\n180\n181\n182\n183\n184\n185\n186\n187\n188\n189\n190\n191\n192\n193\n194\n195\n196\n197\n198\n199\nprovide any way to analyze this issue.\nIn this study, our goal was to investigate how the two difficulties of \u201canswering questions\u201d and \u201creading text\u201d relate in RC. Therefore, we adopted two classes of evaluation metrics for RC datasets to analyze both the quality of datasets and the performance of systems. Our paper is divided into the following sections. First, we adopt the two classes: prerequisite skills and readability (Section 3). We then specify evaluation metrics for each (Sections 3.1 and 3.2). Next, we annotate six existing RC datasets with these metrics (Section 4). Finally, we present our annotation results (Section 5) and discuss them (Section 6).\nOur two classes of metrics are based on McNamara and Magliano (2009)\u2019s analysis of human text comprehension in psychology. The first class defines the difficulty of comprehending the context to answer questions. We adopted prerequisite skills proposed in a previous study by Sugawara et al. (2017). That study presented an important observation of the relation between the difficulty of an RC task and prerequisite skills: the more skills that are required to answer a question, the more difficult the question is. From this observation, we assume that the number of required skills corresponds to the difficulty of a question. This is because each skill corresponds to a function of a system, which has to be equipped with the system. However, a problem in previous studies, including that of Sugawara and Aizawa (2016), is that they analyzed only two datasets and that their categorization of knowledge reasoning is provisional with a weak theoretical background.\nTherefore, we reorganized the category of knowledge reasoning in terms of textual entailment and human text comprehension. In research on textual entailment, several methodologies have been proposed for precise analysis of entailment phenomena (Dagan et al., 2013; LoBue and Yates, 2011). In psychology research, Kintsch (1993) proposed dichotomies for the classification of human inferences: retrieved versus generated. In addition, McNamara and Magliano (2009) proposed a similar distinction for inferences: bridging versus elaboration. We utilized these insights in order to develop a comprehensive but not overly specific classification of knowledge reasoning.\nThe second class defines the difficulty of reading contents, readability, in documents considering syntactic and lexical complexity. We leverage\na wide range of linguistic features proposed by Vajjala and Meurers (2012).\nIn the annotation, annotators selected sentences needed for answering and then annotated them with prerequisite skills under the same condition for RC datasets with different task formulations. Therefore, our annotation was equivalent in that the datasets were annotated from the point of view of whether a context entails a hypothesis that can be made from a question and its answer. This means our methodology could not evaluate the competence of looking for sentences that need to be read and answer candidates from the context. Therefore, our methodology was used to evaluate the understanding of contextual entailments in a broader sense for RC.\nThe contributions of this paper are as follows:\n1. We adopt two classes of evaluation metrics to show the qualitative features of RC datasets. Through analyses of RC datasets, we demonstrate that there is only a weak correlation between the difficulty of questions and the readability of context texts in RC datasets.\n2. We revise the previous classification of prerequisite skills for RC. Specifically, skills of knowledge reasoning are organized in terms of entailment phenomena and human text comprehension in psychology.\n3. We annotate six existing RC datasets with our organized metrics for the comparison and make the results publicly available.\nWe believe that our annotation results will help researchers to develop a method for the stepby-step construction of better RC datasets and a method to improve RC systems.\n2 Related Work\n2.1 Reading Comprehension Datasets\nIn this section, we present a short chronicle of RC datasets. To our knowledge, Hirschman et al. (1999) were the first to use NLP methods for RC. Their dataset consisted of reading materials for grades 3\u20136 with simple 5W (wh-) questions. Afterwards, investigations into natural language understanding questions mainly focused on other formulations, such as question answering (Yang et al., 2015; Wang et al., 2007; Voorhees et al., 1999) and textual entailment (Bentivogli et al., 2010; Sammons et al., 2010; Dagan et al., 2006).\n3\n201\n202\n203\n204\n205\n206\n207\n208\n209\n210\n211\n212\n213\n214\n215\n216\n217\n218\n219\n220\n221\n222\n223\n224\n225\n226\n227\n228\n229\n230\n231\n232\n233\n234\n235\n236\n237\n238\n239\n240\n241\n242\n243\n244\n245\n246\n247\n248\n249\n250\n251\n252\n253\n254\n255\n256\n257\n258\n259\n260\n261\n262\n263\n264\n265\n266\n267\n268\n269\n270\n271\n272\n273\n274\n275\n276\n277\n278\n279\n280\n281\n282\n283\n284\n285\n286\n287\n288\n289\n290\n291\n292\n293\n294\n295\n296\n297\n298\n299\nOne of the RC tasks following it was QA4MRE (Sutcliffe et al., 2013). The top accuracy of this task remained 59% at that time, and the size of the dataset was very limited: there were only 224 gold-standard questions, which is not enough for machine learning methods.\nThus, an important issue for designing RC datasets is their scalability. Richardson et al. (2013) presented MCTest, which is an opendomain narrative dataset for gauging comprehension at a childs level. This dataset was created by crowdsourcing and is based on scalable methodology. Since then, more large-scale datasets have been proposed with the development of machine learning. For example, the CNN/Daily Mail dataset (Hermann et al., 2015) and CBTest (Hill et al., 2016) have approximately 1.4M and 688K passages, respectively. These context texts and questions were automatically curated and generated from large corpora. However, Chen et al. (2016) indicated that approximately 25% of the questions in the CNN/Daily Mail dataset are unsolvable or nonsense. This has highlighted the demand for more stable and robust sourcing methods regarding dataset quality.\nIn addition to this, several RC datasets were presented in the last half of 2016 with large documents and sensible queries that were guaranteed by crowdsourcing or human testing. We explain those datasets in Section 4.2. They were aimed at achieving large and good-quality content for machine learning models. Nonetheless, as shown in the examples of Figure 1, there is room for improvement, and there is still no methodology available for evaluating the quality.\n2.2 Reading Comprehension in Psychology\nIn psychology, there is a rich tradition of research on human text comprehension. The construction\u2013 integration (C\u2013I) model (Kintsch, 1988) is one of the most basic and influential theories. This model assumes connectional and computational architecture for text comprehension. It assumes that comprehension is the processing of information based on the following steps:1\n1. Construction: Read sentences or clauses as inputs; form and elaborate concepts and propositions corresponding to the inputs.\n2. Integration: Associate the contents to consistently understand them (e.g., coreference,\n1Note that this is only a very simplified overview.\ndiscourse, and coherence).\nDuring these steps, three levels of representation are constructed (Van Dijk and Kintsch, 1983): the surface code (i.e., wording and syntax), the textbase (i.e., text propositions with cohesion), and the situation model (i.e., mental representation). Based on the above assumptions, McNamara and Magliano (2009) proposed two aspects of text comprehension: \u201cstrategic/skilled comprehension\u201d and \u201ctext ease of processing.\u201d We employed these assumptions as the basis of two classes of evaluation metrics (Section 3).\nOn the other hand, Kintsch (1993) proposed two dichotomies for the classification of human inferences, including the knowledge-based ones that are performed in the C\u2013I model. The first is whether inferences are automatic or controlled. However, Graesser et al. (1994) indicated that this distinction is ambiguous because there is a continuum between the two states that depends on individuals. Therefore, this dichotomy is not suited for empirical evaluation, which we are working on attempting. The second is whether inferences are retrieved or generated. Retrieved means that the information used for inference is retrieved from context. In contrast, when inferences are generated, the reader uses external knowledge going beyond the context.\nA similar distinction was proposed by McNamara and Magliano (2009): bridging and elaboration. Bridging inference connects current information to other information that was previously encountered. Elaboration connects current information to external knowledge that is not in a context. We use these two types of inferences in the classification of knowledge reasoning.\n3 Evaluation Metrics for Datasets\nBased on McNamara and Magliano (2009)\u2019s depiction of text comprehension, we adopted two classes for the evaluation of RC datasets: prerequisite skills and readability.\nWe refined the prerequisite skills (Section 3.1) for RC that were proposed by Sugawara et al. (2017) and Sugawara and Aizawa (2016) using the insights mentioned in the previous section. This class covers the textbase and situation model, or understanding each fact and associating multiple facts in a text: relations of events, characters, topic of story, and so on. This class also includes knowledge reasoning; this is divided into several met-\n4\n301\n302\n303\n304\n305\n306\n307\n308\n309\n310\n311\n312\n313\n314\n315\n316\n317\n318\n319\n320\n321\n322\n323\n324\n325\n326\n327\n328\n329\n330\n331\n332\n333\n334\n335\n336\n337\n338\n339\n340\n341\n342\n343\n344\n345\n346\n347\n348\n349\n350\n351\n352\n353\n354\n355\n356\n357\n358\n359\n360\n361\n362\n363\n364\n365\n366\n367\n368\n369\n370\n371\n372\n373\n374\n375\n376\n377\n378\n379\n380\n381\n382\n383\n384\n385\n386\n387\n388\n389\n390\n391\n392\n393\n394\n395\n396\n397\n398\n399\nrics according to the distinctions of human inferences, as discussed by Kintsch (1993) and McNamara and Magliano (2009), and according to the classification of entailment phenomena by Dagan et al. (2013) and LoBue and Yates (2011).\nReadability metrics (Section 3.2) intuitively describe the difficulty of reading: vocabulary and the complexity of texts, or the surface code.\n3.1 Prerequisite Skills\nBy refining Sugawara et al. (2017)\u2019s ten reading comprehension skills, we reorganized thirteen prerequisite skills in total, which are presented below. Skills that have been modified/elaborated from the original definition are appended with an asterisk (\u2217), and new skills unique to this study are appended with a dagger (\u2020).\n1. Object tracking\u2217: Jointly tracking or grasping of multiple objects, including set or membership (Clark, 1975). This skill is a renamed version of list/enumeration used in the original classification to emphasize its scope for multiple objects.\n2. Mathematical reasoning\u2217: We merged statistical and quantitative reasoning with mathematical reasoning. This skill is a renamed version of mathematical operations.\n3. Coreference resolution\u2217: This skill has a small modification: it includes one anaphora (Dagan et al., 2013). This skill is similar to direct reference (Clark, 1975).\n4. Logical reasoning\u2217: We reorganized this skill as understanding predicate logic, e.g., conditionals, quantifiers, negation, and transitivity. Note that mathematical reasoning and this skill are intended to align with offline skills mentioned by Graesser et al. (1994).\n5. Analogy\u2217: Understanding of metaphors including metonymy and synecdoche. See LoBue and Yates (2011) for examples of synecdoche.\n6. Causal relation: Understanding of causality that is represented by explicit expressions of why, because, the reason... (only if they exist).\n7. Spatiotemporal relation: Understanding of spatial and/or temporal relationships between multiple entities, events, and states.\nFor commonsense reasoning in the original classification, we defined the following four categories, which can be jointly required.\n8. Ellipsis\u2020: Recognizing implicit/omitted information (argument, predicate, quantifier, time, place). This skill is inspired by Dagan et al. (2013)\nand the discussion of Sugawara et al. (2017). 9. Bridging\u2020: Inferences between two facts supported by grammatical and lexical knowledge (e.g., synonymy, hypernymy, thematic role, part of events, idioms, and apposition). This skill is inspired by the concept of indirect reference in the literature (Clark, 1975). Note that we excluded direct reference because it is coreference resolution (pronominalization) or elaboration (epithets).\n10. Elaboration\u2020: Inference using known facts, general knowledge (e.g., kinship, exchange, typical event sequence, and naming), and implicit relations (e.g., noun-compound and possessive) (see Dagan et al. (2013) for details). Bridging and elaboration are distinguished by whether knowledge used in inferences is grammatical/lexical versus general/commonsense.\n11. Meta-knowledge\u2020: Using knowledge including a reader, writer, and text genre (e.g., narratives and expository documents) from metaviewpoints (e.g., Who are the principal characters of the story? and What is the main subject of this article?). Although this skill can be regarded as part of elaboration, we defined an independent skill because this knowledge is specific to RC.\nThe last two skills are intended to be performed on a single sentence.\n12. Schematic clause relation: Understanding of complex sentences that have coordination or subordination, including relative clauses.\n13. Punctuation\u2217: Understanding of punctuation marks (e.g., parenthesis, dash, quotation, colon, and semicolon). This skill is a renamed version of special sentence structure. Concerning the original definition, we regarded \u201cscheme\u201d in figures of speech as ambiguous and excluded it (ellipsis was defined as a skill, and apposition was merged into bridging). We did the same with understanding of constructions, which was merged into idioms in bridging.\nNote that we did not construct this classification to be dependent on RC models. This is because our methodology is intended to be general and applicable to many kinds of architectures.\n3.2 Readability Metrics\nIn this study, we evaluated the readability of texts based on metrics in NLP. Several studies have examined readability for various applications, such\n4http://en.wikipedia.org/wiki/ Academic_Word_List\n5\n401\n402\n403\n404\n405\n406\n407\n408\n409\n410\n411\n412\n413\n414\n415\n416\n417\n418\n419\n420\n421\n422\n423\n424\n425\n426\n427\n428\n429\n430\n431\n432\n433\n434\n435\n436\n437\n438\n439\n440\n441\n442\n443\n444\n445\n446\n447\n448\n449\n462\n463\n464\n465\n466\n467\n468\n469\n470\n471\n472\n473\n474\n475\n476\n477\n478\n479\n480\n481\n482\n483\n484\n485\n486\n487\n488\n489\n490\n491\n492\n493\n494\n495\n496\n497\n498\n499\n- Ave. num. of characters per word (NumChar) - Ave. num. of syllables per word (NumSyll) - Ave. sentence length in words (MLS) - Proportion of words in AWL (AWL) - Modifier variation (ModVar) - Num. of coordinate phrases per sentence (CoOrd) - Coleman\u2013Liau index (Coleman) - Dependent clause to clause ratio (DC/C) - Complex nominals per clause (CN/C) - Adverb variation (AdvVar)\nFigure 2: Readability metrics. AWL refers to the Academic Word List.4\nas second language learning (Razon and Barnden, 2015) and text simplification (Aluisio et al., 2010), and various aspects, such as development measures of second language acquisition (Vajjala and Meurers, 2012) and discourse relations (Pitler and Nenkova, 2008).\nOf these, we used the classification of linguistic features proposed by Vajjala and Meurers (2012). This is because they presented a comparison of a wide range of linguistic features focusing on second language acquisition and their method can be applied to plain text.2\nWe list the readability metrics in Figure 2, which were reported by Vajjala and Meurers (2012) as the top ten features that affect human readability. To classify these metrics, we can place three classes: lexical features (NumChar, NumSyll, AWL, AdvVar, and ModVar), syntactic features (MLS, CoOrd, DC/C, and CN/C), and traditional features (Coleman). We applied these metrics only to sentences that needed to be read to answer questions. Nonetheless, because these metrics were proposed for human readability, they do not necessarily correlate with those for RC systems. Therefore, in a system analysis, we ideally have to consult various features.3\n4 Annotation of Reading Comprehension Datasets\nWe annotated six existing RC datasets with the prerequisite skills. We explain the annotation procedure in Section 4.1 and the specifications of the annotated RC datasets in Section 4.2.\n2Pitler and Nenkova (2008)\u2019s work is more suitable for measuring text quality. However, we could not use their results because we did not have discourse annotations.\n3We will make available the analysis of RC datasets on basic features as much as possible.\n4.1 Annotation Procedure\nWe asked three annotators to annotate questions of RC datasets with the prerequisite skills that are required to answer each question. We allowed multiple labeling. For each task that was curated from the datasets, the annotators saw the context, question, and its answer jointly. When a dataset consisted of multiple choice questions, we showed all candidates and labeled the correct one with an asterisk. The annotators then selected sentences that needed to be read to answer the question and decided if each prerequisite skill was required. The annotators were allowed to select nonsense for an unsolvable question to distinguish it from a solvable question that required no skills.\n4.2 Dataset Specifications\nFor the annotation, we randomly selected 100 questions from each dataset in Table 1. This amount of questions was considered to be sufficient for the analysis of RC datasets as performed by Chen et al. (2016). The questions were sampled from the gold-standard dataset of QA4MRE and the development sets of the other RC datasets. We explain the method of choosing questions for the annotation in Appendix A.\nThere were other datasets we did not annotate in this study. We decided not to annotate those datasets because of the following reasons. CNN/Daily Mail (Hermann et al., 2015) is anonymized and contains some errors (Chen et al., 2016), so it did not seem to be suitable for annotation. We considered CBTest (Hill et al., 2016) to be for language modeling tasks rather than RC task and excluded it. LAMBADA (Paperno et al., 2016)\u2019s texts are formatted for machine reading,\n6\n501\n502\n503\n504\n505\n506\n507\n508\n509\n510\n511\n512\n513\n514\n515\n516\n517\n518\n519\n520\n521\n522\n523\n524\n525\n526\n527\n528\n529\n530\n531\n532\n533\n534\n535\n536\n537\n538\n539\n540\n541\n542\n543\n544\n545\n546\n547\n548\n549\n559\n560\n561\n562\n563\n564\n565\n566\n567\n568\n569\n570\n571\n572\n573\n574\n575\n576\n577\n578\n579\n580\n581\n582\n583\n584\n585\n586\n587\n588\n589\n590\n591\n592\n593\n594\n595\n596\n597\n598\n599\nSkills QA4MRE MCTest SQuAD WDW MARCO NewsQA\n1. Tracking 11.0 6.0 3.0 8.0 6.0 2.0 2. Math. 4.0 4.0 0.0 3.0 0.0 1.0 3. Coref. resol. 32.0 49.0 13.0 19.0 15.0 24.0 4. Logical rsng. 15.0 2.0 0.0 8.0 1.0 2.0 5. Analogy 7.0 0.0 0.0 7.0 0.0 3.0 6. Causal rel. 1.0 6.0 0.0 2.0 0.0 4.0 7. Sptemp rel. 26.0 9.0 2.0 2.0 0.0 3.0 8. Ellipsis 13.0 4.0 3.0 16.0 2.0 15.0 9. Bridging 69.0 26.0 42.0 59.0 36.0 50.0 10. Elaboration 60.0 8.0 13.0 57.0 18.0 36.0 11. Meta 1.0 1.0 0.0 0.0 0.0 0.0 12. Clause rel. 52.0 40.0 28.0 42.0 27.0 34.0 13. Punctuation 34.0 1.0 24.0 20.0 14.0 25.0\nNonsense 10.0 1.0 3.0 27.0 14.0 1.0\nTable 2: Frequencies (%) of prerequisite skills needed for the RC datasets.\n#Skills QA4MRE MCTest SQuAD WDW MARCO NewsQA\n0 2.0 18.0 27.0 2.0 15.0 13.0 1 13.0 36.0 33.0 5.0 35.0 26.0 2 13.0 24.0 24.0 14.0 29.0 23.0 3 20.0 15.0 6.0 22.0 6.0 25.0 4 14.0 4.0 6.0 16.0 2.0 9.0 5 13.0 1.0 1.0 6.0 0.0 2.0 6 10.0 1.0 0.0 6.0 0.0 1.0 7 1.0 0.0 0.0 2.0 0.0 0.0 8 1.0 0.0 0.0 0.0 0.0 0.0 9 0.0 0.0 0.0 0.0 0.0 0.0 10 3.0 0.0 0.0 0.0 0.0 0.0\nAve. 3.25 1.56 1.28 2.43 1.19 1.99\nTable 3: Frequencies (%) of the number of required prerequisite skills for the RC datasets.\nand all tokens are lowercased, which seemingly prevents inferences based on proper nouns. Thus, we decided that its texts are not suitable for human reading and annotation.\n5 Results of the Dataset Analysis\nWe present the results from evaluating the RC datasets according to the two classes of metrics. The inter-annotator agreement was 90.1% for 62 randomly sampled questions. The evaluation was performed according to the following four points of view (i)\u2013(iv).\n(i) Frequencies of prerequisite skills (Table 2): QA4MRE had the highest scores for frequencies among the datasets. This seems to reflect the fact that QA4MRE has technical documents that contain a wide range of knowledge (bridging and elaboration), multiple clauses, and punctuation and that the questions are devised by experts.\nMCTest achieved high scores in several skills (first in causal relation and meta-knowledge and second in coreference resolution and spatiotemporal relation) and lower score in punctuation. These scores seem to be because the MCTest dataset consists of narratives.\nAnother dataset that achieved remarkable scores is Who-did-What. This dataset achieved the highest score for ellipsis. This is because the questions of Who-did-What are automatically generated from articles not used as context. This methodology can avoid textual overlap between a question and its context; therefore, the skills of ellipsis, bridging, and elaboration are frequently required.\nWith regard to nonsense, MS MARCO and Who-did-What received relatively high scores. This appears to have been caused by the automated curation, which may generate separation between the contents of the context and question (i.e., web segments and a search query in MS MARCO, and a context article and question article in Who-didWhat). In stark contrast, NewsQA had no nonsense questions. Although this result was affected by our filtering described in Appendix A, it is important to note that the NewsQA dataset includes annotations of meta information whether or not a question makes sense (is question bad).\n(ii) Number of required prerequisite skills (Table 3): QA4MRE had the highest score; on average, each question required 4.6 skills. There were few questions in QA4MRE that required zero or one skill, whereas the other datasets contained some questions that required zero or one skill. Table 3 also indicates that more than 90% of the MS MARCO questions required fewer than three skills, at least according to the annotation.\n(iii) Readability metrics for each dataset (Table 4): SQuAD and QA4MRE achieved the highest scores in most metrics; this reflects the fact that\n7\n601\n602\n603\n604\n605\n606\n607\n608\n609\n610\n611\n612\n613\n614\n615\n616\n617\n618\n619\n620\n621\n622\n623\n624\n625\n626\n627\n628\n629\n630\n631\n632\n633\n634\n635\n636\n637\n638\n639\n640\n641\n642\n643\n644\n645\n646\n647\n648\n649\n657\n658\n659\n660\n661\n662\n663\n664\n665\n666\n667\n668\n669\n670\n671\n672\n673\n674\n675\n676\n677\n678\n679\n680\n681\n682\n683\n684\n685\n686\n687\n688\n689\n690\n691\n692\n693\n694\n695\n696\n697\n698\n699\n1.0 1.5 2.0 2.5 3.0 3.5 Average number of required skills 2\n4\n6\n8\n10\n12\n14\n16\nFK\ng ra\nde le\nve l QA4MRE\nMCTest\nSQuAD WDW\nMARCO NewsQA\nFigure 3: Flesch\u2013Kincaid grade levels and average number of required prerequisite skills for the RC datasets.\n0 2 4 6 8 10 Number of required skills 10\n0\n10\n20\n30\n40\n50\nFK\ng ra\nde le\nve l\nQA4MRE MCTest SQuAD\nFigure 4: Flesch\u2013Kincaid grade levels and number of required prerequisite skills for all questions of the selected RC datasets.\nWikipedia articles and technical documents generally require a high grade level to understand. In contrast, MCTest had the lowest scores; its dataset consist of narratives for children.\n(iv) Correlation between numbers of required prerequisite skills and readability metrics (Figure 3, Figure 4, and Table 5): Our main interest was in the correlation between prerequisite skills and readability. To investigate this, we examined the relation between the number of required prerequisite skills and readability metrics (represented by the Flesch\u2013Kincaid grade level), as shown in Figure 3 for each dataset and in Figure 4 for each question. The first figure shows the trends of the datasets. QA4MRE was relatively difficult both to read and to answer, and SQuAD was difficult to read but easy to answer. For further investigation, we selected three datasets (QA4MRE, MCTest, and SQuAD) and plotted all of their questions in the second figure. Three separate domains can be seen.\nTable 5 presents Pearson\u2019s correlation coeffi-\ncients between the number of required prerequisite skills and each readability metric for all questions of the RC datasets. Although there are weak correlations from 0.025 to 0.416, the results highlight that there is not necessarily a strong correlation between the two values. This leads to the following two insights. First, the readability of RC datasets does not directly affect the difficulty of their questions. That is, RC datasets that are difficult to read are not necessary difficult to answer. Second, it is possible to create difficult questions from context that is easy to read. MCTest is a good example. The context texts of MCTest dataset are easy to read, but the difficulty of the questions is comparable to that of the other datasets.\nTo summarize our results in terms of each RC dataset, we present the following observations:\n- QA4MRE seemed to be the most difficult dataset among the RC datasets we analyzed, whereas MS MARCO seemed to be the easiest.\n- MCTest is a good example of an RC dataset that is easy to read but difficult to answer. The corpus\u2019 genre (i.e., narrative) seems to reflect the trend of required skills for the questions.\n- SQuAD was difficult to read along with QA4MRE but relatively easy to answer compared to other datasets.\n- Who-did-What performed well in terms of its query sourcing method. Although its questions are automatically created, they are sophisticated in terms of knowledge reasoning. However, an issue with the automated sourcing method is excluding nonsense questions.\n- MS MARCO was a relatively easy dataset in terms of prerequisite skills. A problem is that the dataset contained nonsense questions.\n- NewsQA is advantageous in that it provides meta information on the reliability of the ques-\n8\n701\n702\n703\n704\n705\n706\n707\n708\n709\n710\n711\n712\n713\n714\n715\n716\n717\n718\n719\n720\n721\n722\n723\n724\n725\n726\n727\n728\n729\n730\n731\n732\n733\n734\n735\n736\n737\n738\n739\n740\n741\n742\n743\n744\n745\n746\n747\n748\n749\n750\n751\n752\n753\n754\n755\n756\n757\n758\n759\n760\n761\n762\n763\n764\n765\n766\n767\n768\n769\n770\n771\n772\n773\n774\n775\n776\n777\n778\n779\n780\n781\n782\n783\n784\n785\n786\n787\n788\n789\n790\n791\n792\n793\n794\n795\n796\n797\n798\n799\ntions. Such information enabled us to avoid using nonsense questions, such as in the training of machine learning models.\n6 Discussion\nIn this section, we discuss several matters regarding the construction of RC datasets and the development of RC systems using our methodology.\nHow to utilize the two classes of metrics for system development: One example for the development of an RC system is that it should be built to solve an easy-to-read and easy-to-answer dataset. The next step is to improve the system so that it can solve an easy-to-read and difficult-toanswer dataset. Finally, only after it can solve such dataset should the system be applied to a difficultto-read and difficult-to-answer dataset. Appropriate datasets can be prepared for every step by measuring their properties using the metrics of this study. Such datasets can be placed in a continuum based on the grades of the metrics and applied to each step of the development, like in curriculum learning (Bengio et al., 2009) and transfer learning (Pan and Yang, 2010).\nCorpus genre: Attention should be paid to the genre of corpus used to construct a dataset. Expository documents like news articles tend to require factorial understanding. Most existing RC datasets use such texts because of their availability. On the other hand, narrative texts have a close correspondence to our everyday experience, such as the emotion and intention of characters (Graesser et al., 1994). If we want to build agents that work in the real world, RC datasets may have to be constructed from narratives.\nQuestion type: In contrast to factorial understanding, comprehensive understanding of natural language texts needs a better grasp of the global coherence (e.g., the main point or moral of the text, goal of a story, and intention of characters) from the broad context (Graesser et al., 1994). Most questions that are prevalent now require only local coherence (e.g., referential relations and thematic roles) with a narrow context. Such questions based on global coherence may be generated by techniques of abstractive text summarization (Rush et al., 2015; Ganesan et al., 2010).\nAnnotation issues: There were questions for which there were disagreements regarding the decision of nonsense. For example, some questions can be solved by external knowledge without seeing their context. Thus, we should clarify what a\nSentence QA4MRE MCTest SQuAD WDW MARCO NewsQA\nNumber 1.120 1.180 1.040 1.110 1.080 1.170 Distance 1.880 0.930 0.090 0.730 0.280 0.540\nTable 6: Average number and distance of sentences that need to be read to answer a question in the RC datasets.\n\u201csolvable\u201d or \u201creasonable\u201d question is in RC. In addition, annotators reported that the prerequisite skills did not easily treat questions whose answer was \u201cnone of the above\u201d in QA4MRE. We considered those \u201cno answer\u201d questions difficult in another sense, so our methodology failed to specify them.\nCompetence of selecting necessary sentences: As mentioned in Section 1, our methodology cannot evaluate the competence of selecting sentences that need to be read to answer questions. As a brief analysis, we further investigated sentences in the context of the datasets that were highlighted in the annotation. Analyses were performed in two ways: for each question, we counted the number of required sentences and their distance (see Appendix B for the calculation method). The values of the first row in Table 6 show the average number of required sentences per question for each RC dataset. Although the scores seemed to be approximately level, MCTest required multiple sentences the most frequently. The second row presents the average distance of required sentences. QA4MRE demonstrated the longest distance because readers had to look for clues in long context texts of the dataset. In contrast, SQuAD and MS MARCO showed lower scores: most of their questions seem only to require reading a single sentence to answer. Of course, the scores of distances should depend on the length of the context texts.\n7 Conclusion\nIn this study, we adopted evaluation metrics to analyze both the performance of a system and the quality of RC datasets. We assumed two classes\u2014 refined prerequisite skills and readability\u2014and defined evaluation metrics for each. Next, we annotated six existing RC datasets with those defined metrics. Our annotation highlights the characteristics of the datasets and provides a valuable guide for the construction of new datasets and the development of RC systems. For future work, we plan to use the analysis in the present study to construct a system that can be applied to multiple datasets.\n9\n801\n802\n803\n804\n805\n806\n807\n808\n809\n810\n811\n812\n813\n814\n815\n816\n817\n818\n819\n820\n821\n822\n823\n824\n825\n826\n827\n828\n829\n830\n831\n832\n833\n834\n835\n836\n837\n838\n839\n840\n841\n842\n843\n844\n845\n846\n847\n848\n849\n850\n851\n852\n853\n854\n855\n856\n857\n858\n859\n860\n861\n862\n863\n864\n865\n866\n867\n868\n869\n870\n871\n872\n873\n874\n875\n876\n877\n878\n879\n880\n881\n882\n883\n884\n885\n886\n887\n888\n889\n890\n891\n892\n893\n894\n895\n896\n897\n898\n899\nA Sampling Methods for Questions\nIn this appendix, we explain the method of choosing questions for the annotation.\nQA4MRE (Sutcliffe et al., 2013): The goldstandard dataset consists of four different topics and four documents for each topic. We randomly selected 100 main and auxiliary questions so that at least one question of each document was included.\nMCTest (Richardson et al., 2013): This dataset consists of two sets: MC160 and MC500. Their development sets have 80 tasks in total; each includes context texts and four questions. We randomly chose 25 tasks (100 questions) from the development sets.\nSQuAD (Rajpurkar et al., 2016): This dataset includes some Wikipedia articles from various topics, and those articles are divided into paragraphs. We randomly chose 100 paragraphs from\n1000\n1001\n1002\n1003\n1004\n1005\n1006\n1007\n1008\n1009\n1010\n1011\n1012\n1013\n1014\n1015\n1016\n1017\n1018\n1019\n1020\n1021\n1022\n1023\n1024\n1025\n1026\n1027\n1028\n1029\n1030\n1031\n1032\n1033\n1034\n1035\n1036\n1037\n1038\n1039\n1040\n1041\n1042\n1043\n1044\n1045\n1046\n1047\n1048\n1049\n1050\n1051\n1052\n1053\n1054\n1055\n1056\n1057\n1058\n1059\n1060\n1061\n1062\n1063\n1064\n1065\n1066\n1067\n1068\n1069\n1070\n1071\n1072\n1073\n1074\n1075\n1076\n1077\n1078\n1079\n1080\n1081\n1082\n1083\n1084\n1085\n1086\n1087\n1088\n1089\n1090\n1091\n1092\n1093\n1094\n1095\n1096\n1097\n1098\n1099\n15 articles and used only one question of each paragraph for the annotation.\nWho-did-What (WDW) (Onishi et al., 2016): This dataset is constructed from the English Gigaword newswire corpus (v5). Its questions are automatically created from an article that differs from one used for context. In addition, questions that can be solved by a simple baseline method are excluded from the dataset.\nMS MARCO (MARCO) (Nguyen et al., 2016): A task of this dataset comprises several segments, one question, and its answer. We randomly chose 100 tasks (100 questions) and only used segments whose attribute was is selected = 1 as context.\nNewsQA (Trischler et al., 2016): we randomly chose questions that satisfied the following conditions: is answer absent = 0, is question bad = 0, and validated answers do not include bad question or none.\nB Calculation of Sentence Distance\nAs mentioned in Section 6, the distance of sentences was calculated as follows. If a question required only one sentence to be read, its distance was zero. If a question required two adjacent sentences to be read, its distance was one. If a question required more than two sentences to be read, its distance was the sum of distances of any two sentences.\n", "cats": {"READ": 1.0, "DONTREAD": 0.0}, "reviews": [{"IMPACT": "2", "SUBSTANCE": "4", "APPROPRIATENESS": "4", "MEANINGFUL_COMPARISON": "3", "PRESENTATION_FORMAT": "Oral Presentation", "comments": "- Strengths:\n\n- this article puts two fields together: text readability for humans and\nmachine comprehension of texts\n\n- Weaknesses:\n\n- The goal of your paper is not entirely clear. I had to read the paper 4 times\nand I still do not understand what you are talking about!\n- The article is highly ambiguous what it talks about - machine comprehension\nor text readability for humans\n- you miss important work in the readability field\n- Section 2.2. has completely unrelated discussion of theoretical topics.\n- I have the feeling that this paper is trying to answer too many questions in\nthe same time, by this making itself quite weak. Questions such as \u201cdoes text\nreadability have impact on RC datasets\u201d should be analyzed separately from\nall these prerequisite skills.\n\n- General Discussion:\n\n- The title is a bit ambiguous, it would be good to clarify that you are\nreferring to machine comprehension of text, and not human reading\ncomprehension, because \u201creading comprehension\u201d and \u201creadability\u201d\nusually mean that.\n- You say that your \u201cdataset analysis suggested that the readability of RC\ndatasets does not directly affect the question difficulty\u201d, but this depends\non the method/features used for answer detection, e.g. if you use\nPOS/dependency parse features.\n- You need to proofread the English of your paper, there are some important\nomissions, like \u201cthe question is easy to solve simply look..\u201d on page 1.\n- How do you annotate datasets with \u201cmetrics\u201d??\n- Here you are mixing machine reading comprehension of texts and human reading\ncomprehension of texts, which, although somewhat similar, are also quite\ndifferent, and also large areas.\n- \u201creadability of text\u201d is not \u201cdifficulty of reading contents\u201d. Check\nthis:\nDuBay, W.H. 2004. The Principles of Readability. Costa Mesa, CA: Impact\ninformation. \n- it would be good if you put more pointers distinguishing your work from\nreadability of questions for humans, because this article is highly ambiguous.\nE.g. on page 1 \u201cThese two examples show that the readability of the text does\nnot necessarily correlate with the difficulty of the questions\u201d you should\nadd \u201cfor machine comprehension\u201d\n- Section 3.1. - Again: are you referring to such skills for humans or for\nmachines? If for machines, why are you citing papers for humans, and how sure\nare you they are referring to machines too?\n- How many questions the annotators had to annotate? Were the annotators clear\nthey annotate the questions keeping in mind machines and not people?", "SOUNDNESS_CORRECTNESS": "4", "ORIGINALITY": "3", "is_meta_review": null, "RECOMMENDATION": "4", "CLARITY": "2", "REVIEWER_CONFIDENCE": "3"}]}
{"text": "A Neural Local Coherence Model\n1 000\n011\n012\n013\n014\n015\n016\n017\n018\n019\n020\n021\n022\n023\n024\n025\n026\n027\n028\n029\n030\n031\n032\n033\n034\n035\n036\n037\n038\n039\n040\n041\n042\n043\n044\n045\n046\n047\n048\n049\n061\n062\n063\n064\n065\n066\n067\n068\n069\n070\n071\n072\n073\n074\n075\n076\n077\n078\n079\n080\n081\n082\n083\n084\n085\n086\n087\n088\n089\n090\n091\n092\n093\n094\n095\n096\n097\n098\n099\n1 Introduction and Motivation\nWhat distinguishes a coherent text from a random sequence of sentences is that it binds the sentences together to express a meaning as a whole \u2014 the interpretation of a sentence usually depends on the meaning of its neighbors. Coherence models that can distinguish a coherent from incoherent texts have a wide range of applications in text generation, summarization, and coherence scoring.\nSeveral formal theories of coherence have been proposed (Mann and Thompson, 1988a; Grosz et al., 1995; Asher and Lascarides, 2003), and their principles have inspired development of existing coherence models (Barzilay and Lapata, 2008; Lin et al., 2011; Li and Hovy, 2014). Among these models, the entity grid (Barzilay and Lapata, 2008), which is based on Centering Theory (Grosz et al., 1995), is arguably the most popular, and has seen a number of improvements over the years. As shown in Figure 2, the entity grid model represents a text by a grid that captures how grammatical roles of different entities change from\nsentence to sentence. The grid is then converted into a feature vector containing probabilities of local entity transitions, which enables machine learning models to learn the degree of text coherence. Extensions of this basic grid model incorporate entity-specific features (Elsner and Charniak, 2011), multiple ranks (Feng and Hirst, 2012), and coherence relations (Feng et al., 2014).\nWhile the entity grid and its extensions have been successful in many applications, they are limited in several ways. Firstly, they use discrete representation for grammatical roles and features, which limits the model to consider sufficiently long transitions (Bengio et al., 2003). Secondly, feature vector computation in existing models is decoupled from the target task, which limits the models to learn task-specific features.\nIn this paper, we propose a neural architecture for coherence assessment that can capture long range entity transitions along with arbitrary entityspecific features. Our model obtains generalization through distributed representations of entity transitions and entity features. We also present an end-to-end training method to learn task-specific high level features automatically in our model.\nWe evaluate our approach on three different evaluation tasks: discrimination, insertion, and summary coherence rating, proposed previously for evaluating coherence models (Barzilay and Lapata, 2008; Elsner and Charniak, 2011). Discrimination and insertion involve identifying the right order of the sentences in a text with different levels of difficulty. In summary coherence rating task, we compare the rankings, given by the model, against human pairwise judgments of coherence.\nThe experimental results show that our neural models consistently improve over the nonneural counterparts (i.e., existing entity grid models) yielding absolute gains of about 4% on discrimination, up to 2.5% on insertion, and more\n2\n101\n102\n103\n104\n105\n106\n107\n108\n109\n110\n111\n112\n113\n114\n115\n116\n117\n118\n119\n120\n121\n122\n123\n124\n125\n126\n127\n128\n129\n130\n131\n132\n133\n134\n135\n136\n137\n138\n139\n140\n141\n142\n143\n144\n145\n146\n147\n148\n149\n165\n166\n167\n168\n169\n170\n171\n172\n173\n174\n175\n176\n177\n178\n179\n180\n181\n182\n183\n184\n185\n186\n187\n188\n189\n190\n191\n192\n193\n194\n195\n196\n197\n198\n199\nthan 4% on summary coherence rating. Our model achieves state of the art results in all these tasks. We have released our code with the submission.\nThe remainder of this paper is organized as follows. We describe entity grid, its extensions, and its limitations in Section 2. In Section 3, we present our neural model. We describe evaluation tasks and results in Sections 4 and 5. We give a brief account of related work in Section 6. Finally, we conclude with future directions in Section 7.\n2 Entity Grid and Its Extensions\nMotivated by Centering Theory (Grosz et al., 1995), Barzilay and Lapata (2008) proposed an entity-based model for representing and assessing text coherence. Their model represents a text by a two-dimensional array called entity grid that captures transitions of discourse entities across sentences. As shown in Figure 2, the rows of the grid correspond to sentences, and the columns correspond to discourse entities appearing in the text. They consider noun phrases (NP) as entities, and employ a coreference resolver to detect mentions of the same entity (e.g., Obama, the president). Each entry Gi,j in the entity grid represents the syntactic role that entity ej plays in sentence si, which can be one of: subject (S), object (O), or other (X). In addition, entities not appearing in a sentence are marked by a special symbol (-). If an entity appears more than once with different grammatical roles in the same sentence, the role with the highest rank (S O X) is considered.\nTo represent an entity grid with a feature vector, Barzilay and Lapata (2008) compute probability for each local entity transition of length k (i.e., {S,O,X,\u2212}k), and represent each grid by a vector of 4k transitions probabilities. To distinguish between transitions of important entities from unimportant ones, they consider the salience of the entities, which they quantify by their occurrence frequency in the document. Assessment of text coherence is then formulated as a ranking problem in an SVM preference ranking framework (Joachims, 2002).\nSubsequent studies proposed to extend the basic entity grid model. Filippova and Strube (2007) attempted to improve the model by grouping entities based on semantic relatedness, but did not get significant improvement. Elsner and Charniak (2011) proposed a number of improvements. They initially show significant improvement by includ-\ning non-head nouns (i.e., nouns that do not head NPs) as entities in the grid.1 Then, they extend the grid to distinguish between entities of different types by incorporating entity-specific features like named entity, noun class, modifiers, etc. These extensions led to the best results reported so far.\nEntity grid and its extensions have been successfully applied to many downstream tasks including coherence rating (Barzilay and Lapata, 2008), essay scoring (Burstein et al., 2010), story generation (McIntyre and Lapata, 2010), and readability assessment (Pitler et al., 2010; Barzilay and Lapata, 2008). They have also been critical components in state-of-the-art sentence ordering models (Soricut and Marcu, 2006; Elsner and Charniak, 2011; Lin et al., 2011).\n2.1 Limitations of Entity Grid Models\nDespite its success, existing entity grid models are limited in several ways.\n\u2022 Existing models use discrete representation for grammatical roles and features, which leads to the so-called curse of dimensionality problem (Bengio et al., 2003). In particular, to model transitions of length k withR different grammatical roles, the basic entity grid model needs to computeRk transition probabilities from a grid. One can imagine that the estimated distribution becomes sparse as k increases. This limits the model to consider longer transitions \u2013 existing models use k \u2264 3.\n1They match the nouns to detect coreferent entities.\n3\n201\n202\n203\n204\n205\n206\n207\n208\n209\n210\n211\n212\n213\n214\n215\n216\n217\n218\n219\n220\n221\n222\n223\n224\n225\n226\n227\n228\n229\n230\n231\n232\n233\n234\n235\n236\n237\n238\n239\n240\n241\n242\n243\n244\n245\n246\n247\n248\n249\n250\n251\n252\n253\n254\n255\n256\n257\n258\n259\n260\n261\n262\n263\n264\n265\n266\n267\n268\n269\n270\n271\n272\n273\n274\n275\n276\n277\n278\n279\n280\n281\n282\n283\n284\n285\n286\n287\n288\n289\n290\n291\n292\n293\n294\n295\n296\n297\n298\n299\nThis problem is exacerbated when we want to include entity-specific features, as the number of parameters grows exponentially with the number of features (Elsner and Charniak, 2011).\n\u2022 Existing models compute feature representations from entity grids in a task-agnostic way. In other words, feature extraction is decoupled from the target downstream tasks. This can limit the models to learn task-specific features. Therefore, models that can be trained in an end-to-end fashion on different target tasks are desirable.\nIn the following section, we present a neural architecture that allows us to capture long range entity transitions along with arbitrary entity-specific features without loosing generalization. We also present an end-to-end training method to learn task-specific features automatically.\n3 The Neural Coherence Model\nFigure 2 summarizes our neural architecture for modeling local coherence, and how it can be trained in a pairwise fashion. The architecture takes a document as input, and first extracts its entity grid.2 The first layer of the neural network transforms each grammatical role in the grid into a distributed representation, a real-valued vector. The second layer computes high-level features by going over each column (transitions) of the grid. The following layer selects the most important high-level features, which are in turn used for coherence scoring. The features computed at different layers of the network are automatically trained by backpropagation to be relevant to the task. In the following, we elaborate on the layers of the neural network model.\n(I) Transforming grammatical roles into feature vectors: Grammatical roles are fed to our model as indices taken from a finite vocabulary V . In the simplest scenario, V contains {S,O,X,\u2212}. However, we will see in Section 3.1 that as we include more entity-specific features, V can contain more symbols. The first layer of our network maps each of these indices into a distributed representation Rd by looking up a shared embedding matrix E \u2208 R|V |\u00d7d. We consider E a model parameter to be learned by backpropagation on a given task. We can initialize E randomly or using pretrained vectors trained on a general coherence task.\n2For clarification, pairwise input as shown in the figure is required only to train the model.\nGiven an entity grid G with columns representing entity transitions over sentences in a document, the lookup layer extracts a d-dimensional vector for each entry Gi,j from E. More formally,\nL(G) = \u2329 E(G1,1) \u00b7 \u00b7 \u00b7 E(Gi,j) \u00b7 \u00b7 \u00b7 E(Gm,n) \u232a (1) where E(Gi,j) refers to the row in E that corresponds to the grammatical role Gi,j \u2208 V ; m is the total number of sentences and n is the total number of entities in the document. The output L(G) is a tensor in Rm\u00d7n\u00d7d, which is fed to the next layer of the network as we describe below.\n(II) Modeling entity transitions: The vectors produced by the lookup layer are combined by subsequent layers of the network to generate a coherence score for the document. To compose higher-level features from the embedding vectors, we make the following modeling assumptions:\n\u2022 Similar to existing entity grid models, we assume there is no spatio-temporal relation between the entities in a document. In other words, columns in a grid are treated independently.\n\u2022 We are interested in modeling entity transitions of arbitrary lengths in a location-invariant way. This means, we aim to compose local patches of entity transitions into higher-level representations, while treating the patches independently of their position in the entity grid.\nUnder these assumptions, the natural choice to tackle this problem is to use a convolutional approach, used previously to solve other NLP tasks (Collobert et al., 2011; Kim, 2014).\nConvolution layer: A convolution operation involves applying a filter w \u2208 Rk.d (i.e., a vector of weight parameters) to each entity transition of length k to produce a new abstract feature\nht = f(w TLt:t+k\u22121,j + bt) (2)\nwhere Lt:t+k\u22121,j denotes the concatenation of k vectors in the lookup layer representing a transition of length k for entity ej in the grid, bt is a bias term, and f is a nonlinear activation function, e.g., ReLU (Nair and Hinton, 2010) in our model.\nWe apply this filter to each possible k-length transitions of different entities in the grid to generate a feature map, hi = [h1, \u00b7 \u00b7 \u00b7 , hm.n+k\u22121]. We\n4\n301\n302\n303\n304\n305\n306\n307\n308\n309\n310\n311\n312\n313\n314\n315\n316\n317\n318\n319\n320\n321\n322\n323\n324\n325\n326\n327\n328\n329\n330\n331\n332\n333\n334\n335\n336\n337\n338\n339\n340\n341\n342\n343\n344\n345\n346\n347\n348\n349\n350\n351\n352\n353\n354\n355\n356\n357\n358\n359\n360\n361\n362\n363\n364\n365\n366\n367\n368\n369\n370\n371\n372\n373\n374\n375\n376\n377\n378\n379\n380\n381\n382\n383\n384\n385\n386\n387\n388\n389\n390\n391\n392\n393\n394\n395\n396\n397\n398\n399\nFigure 2: Neural architecture for modeling local coherence and the pairwise training method.\nrepeat this processN times withN different filters to get N different feature maps (Figure 2). Notice that we use a wide convolution (Kalchbrenner et al., 2014), as opposed to narrow, to ensure that the filters reach entire columns of a grid, including the boundary entities. This is done by performing zero-padding, where out-of-range (i.e., for t < 0 or t > {m,n}) vectors are assumed to be zero.\nConvolutional filters learn to compose local transition features of a grid into higher-level representations automatically. Since it operates over the distributed representation of grid entries, compared to traditional grid models, the transition length k can be sufficiently large (e.g., 5 \u2212 8 in our experiments) to capture long-range transitional dependencies without overfitting on the training data. Moreover, unlike existing grid models that compute transition probabilities from a single document, embedding vectors and convolutional filters are learned from all training documents, which helps the neural framework to obtain better generalization and robustness.\nPooling layer: After the convolution, we apply a max-pooling operation to each feature map.\nm = [\u00b5p(h 1), \u00b7 \u00b7 \u00b7 , \u00b5p(hN )] (3)\nwhere \u00b5p(hi) refers to the max operation applied to each non-overlapping3 window of p features in the feature map hi. Max-pooling reduces the output dimensionality by a factor of p, and it drives the model to capture the most salient local features\n3We set the stride size to be the same as the pooling length p to get non-overlapping regions.\nfrom each feature map in the convolutional layer.\nCoherence scoring: Finally, the max-pooled features are used in the output layer of the network to produce a coherence score y \u2208 R.\ny = vTm+ b (4)\nwhere v is the weight vector and b is a bias term.\nWhy it works: Intuitively, each filter detects a specific transition pattern (e.g., \u2018SS-O-X\u2019 for a coherent text), and if this pattern occurs somewhere in the grid, the resulting feature map will have a large value for that particular region and small values for other regions. By applying max pooling on this feature map, the network then discovers that the transition appeared in the grid.\n3.1 Incorporating Entity-Specific Features\nOur model as described above neuralizes the basic entity grid model that considers only entity transitions without distinguishing between types of the entities. However, as Elsner and Charniak (2011) pointed out entity-specific features could be crucial for modeling local coherence. One simple way to incorporate entity-specific features into our model is to attach the feature value (e.g., named entity type) with the grammatical role in the grid. For example, if an entity ej of type PERSON appears as a subject (S) in sentence si, the grid entry Gi,j can be encoded as PERSON-S.\n3.2 Training\nOur neural model assigns a coherence score to an input document d based on the degree of lo-\n5\n401\n402\n403\n404\n405\n406\n407\n408\n409\n410\n411\n412\n413\n414\n415\n416\n417\n418\n419\n420\n421\n422\n423\n424\n425\n426\n427\n428\n429\n430\n431\n432\n433\n434\n435\n436\n437\n438\n439\n440\n441\n442\n443\n444\n445\n446\n447\n448\n449\n450\n451\n452\n453\n454\n455\n456\n457\n458\n459\n460\n461\n462\n463\n464\n465\n466\n467\n468\n469\n470\n471\n472\n473\n474\n475\n476\n477\n478\n479\n480\n481\n482\n483\n484\n485\n486\n487\n488\n489\n490\n491\n492\n493\n494\n495\n496\n497\n498\n499\ncal coherence observed in its entity grid G. Let y = \u03c6(G|\u03b8) define our model that transforms an input grid G to a coherence score y through a sequence of lookup, convolutional, pooling, and linear projection layers with parameter set \u03b8. The parameter set \u03b8 includes the embedding matrix E, the filter matrix W , the weight vector v, and the biases. We use a pairwise ranking approach (Collobert et al., 2011) to learn \u03b8.\nThe training set comprises ordered pairs (di, dj), where document di exhibits a higher degree of coherence than document dj . As we will see in Section 4 such orderings can be obtained automatically or through manual annotation. In training, we seek to find \u03b8 that assigns a higher coherence score to di than to dj . We minimize the following ranking objective with respect to \u03b8:\nJ (\u03b8) = max{0, 1\u2212 \u03c6(Gi|\u03b8) + \u03c6(Gj |\u03b8)} (5)\nwhere Gi and Gj are the entity grids corresponding to documents di and dj , respectively. Notice that (also shown in Figure 2) the network shares its layers (and hence \u03b8) to obtain \u03c6(Gi|\u03b8) and \u03c6(Gj |\u03b8) from a pair of input grids (Gi, Gj).\nBarzilay and Lapata (2008) adopted a similar ranking criterion using an SVM preference kernel learner as they argue coherence assessment is best seen as a ranking problem as opposed to classification (coherent vs. incoherent). Also, the ranker gives a scoring function \u03c6 that a text generation system can use to compare alternative hypotheses.\n4 Evaluation Tasks\nWe evaluate the effectiveness of our coherence models on two different evaluation tasks: sentence ordering and summary coherence rating.\n4.1 Sentence Ordering\nFollowing (Elsner and Charniak, 2011), we evaluate our models on two sentence ordering tasks: discrimination and insertion.\nIn the discrimination task (Barzilay and Lapata, 2008), a document is compared to a random permutation of its sentences, and the model is considered correct if it scores the original document higher than the permuted one. We use 20 permutations of each document in the test set in accordance with previous work.\nIn the insertion task (Elsner and Charniak, 2011), we evaluate models based on their ability\nSections # Doc. # Pairs Avg. # Sen.\nTRAIN 00-13 1,378 26,422 21.5 TEST 14-24 1,053 20,411 22.3\nTable 1: Statistics on WSJ dataset.\nto locate the original position of a sentence previously removed from a document. To measure this, each sentence in the document is removed in turn, and an insertion place is located for which the model gives the highest coherence score to the document. The insertion score is then computed as the average fraction of sentences per document reinserted in their actual position.\nDiscrimination can be easier for longer documents, since a random permutation is likely to be different than the original one. Insertion is a much more difficult task since the candidate documents differ only by the position of one sentence.\nDataset: For sentence ordering tasks, we use the Wall Street Journal (WSJ) portion of Penn Treebank, as used by (Elsner and Charniak, 2008, 2011; Lin et al., 2011; Feng et al., 2014). Table 1 gives basic statistics about the dataset. Following previous works, we use 20 random permutations of each article, and we exclude permutations that match the original document.4 The fourth column (# Pairs) in Table 1 shows the resulting number of (original, permuted) pairs used for training our model and for testing in the discrimination task.\nSome previous studies (Barzilay and Lapata, 2008; Li and Hovy, 2014) used the AIRPLANES and the EARTHQUAKES corpora, which contain reports on airplane crashes and earthquakes, respectively. Each of these corpora contains 100 articles for training and 100 articles for testing. The average number of sentences per article in these two corpora is 10.4 and 11.5, respectively.\nWe preferred WSJ corpus for several reasons. First and most importantly, WSJ corpus is larger than other corpora (see Table 1). Large training set is crucial for learning effective deep learning models (Collobert et al., 2011), and a large enough test set is necessary to make a general comment about model performance. Secondly, as Elsner and Charniak (2011) pointed out, texts in AIRPLANES and EARTHQUAKES are constrained in style, whereas WSJ documents are more like normal informative articles. Thirdly, we could re-\n4Short articles may produce many matches.\n6\n501\n502\n503\n504\n505\n506\n507\n508\n509\n510\n511\n512\n513\n514\n515\n516\n517\n518\n519\n520\n521\n522\n523\n524\n525\n526\n527\n528\n529\n530\n531\n532\n533\n534\n535\n536\n537\n538\n539\n540\n541\n542\n543\n544\n545\n546\n547\n548\n549\n550\n551\n552\n553\n554\n555\n556\n557\n558\n559\n560\n561\n562\n563\n564\n565\n566\n567\n568\n569\n570\n571\n572\n573\n574\n575\n576\n577\n578\n579\n580\n581\n582\n583\n584\n585\n586\n587\n588\n589\n590\n591\n592\n593\n594\n595\n596\n597\n598\n599\nproduce results on this dataset for the competing systems (e.g., entity grid and its extensions) using publicly available Brown coherence toolkit.5\n4.2 Summary Coherence Rating\nWe further evaluate our models on the summary coherence rating task proposed by Barzilay and Lapata (2008), where we compare rankings given by a model to a pair of summaries against rankings elicited from human judges.\nDataset: The summary dataset was extracted from the Document Understanding Conference (DUC\u201903), which contains 6 clusters of multidocument summaries produced by human experts and 5 automatic summarization systems. Each cluster has 16 summaries of a document with pairwise coherence rankings given by humans judges; see (Barzilay and Lapata, 2008) for details on the annotation method. There are 144 pairs of summaries for training and 80 pairs for testing.\n5 Experiments\nIn this section, we present our experiments \u2014 the models we compare, their settings, and the results.\n5.1 Models Compared\nWe compare our coherence model against a random baseline and several existing models.\nRandom: The Random baseline makes a random decision for the evaluation tasks.\nGraph-based Model: This is the graph-based unsupervised model proposed by Guinaudeau and Strube (2013). We use the implementation from the cohere6 toolkit (Smith et al., 2016), and run it on the test set with syntactic projection (command line option \u2018projection=3\u2019) for graph construction. This setting yielded best scores for this model.\nGrid-all nouns (E&C): This is the simple extension of the original entity grid model, where all nouns are considered as entities. Elsner and Charniak (2011) report significant gains by considering all nouns as opposed to only head-nouns. Results for this model were obtained by training the baseline entity grid model (command line option \u2018-n\u2019) in Brown coherence toolkit on our dataset.\n5https://bitbucket.org/melsner/browncoherence 6https://github.com/karins/CoherenceFramework\nExtended grid (E&C): This represents the extended entity grid model of Elsner and Charniak (2011) that uses 9 entity-specific features; 4 of them were computed from external corpora. This model considers all nouns as entities. For this system, we train the extended grid model (command line option \u2018-f\u2019) in Brown coherence toolkit.\nGrid-CNN: This is our proposed neural extension of the basic entity grid (all nouns), where we only consider entity transitions as input.\nExtended Grid-CNN: This corresponds to our neural model that incorporates entity-specific features following the method described in Section 3.1. To keep the model simple, we include only three entity-specific features from (Elsner and Charniak, 2011) that are easy to compute and do not require any external corpus. The features are: (i) named entity type, (ii) salience as determined by occurrence frequency of the entity, and (iii) whether the entity has a proper mention.\nA remark: We also experimented with the distributed sentence model proposed recently by Li and Hovy (2014). We trained it on our WSJ corpus using their code7 with the same setting that produced the best results on the discrimination task. However, the results on our dataset were very disappointing \u2013 accuracy of only 19.34% in discrimination. We were not sure what could go wrong, therefore, we excluded it from our table of results.\n5.2 Settings for Neural Models\nWe held out 10% of the training documents to form a development set (DEV) on which we tune the hyper-parameters of our neural models. For discrimination and insertion tasks, the resulting DEV set contains 138 articles and 2,678 pairs after removing the permutations that match the original documents. For the summary rating task, DEV contains 14 pairs of summaries.\nWe implement our models in Theano (Theano Development Team, 2016). We use rectified linear units (ReLU) as activations (f ). The embedding matrix is initialized with samples from uniform distribution U(\u22120.01, 0.01), and the weight matrices are initialized with samples from glorotuniform distribution (Glorot and Bengio, 2010).\nWe train the models by optimizing the pairwise ranking loss in Equation 5 using the gradientbased online learning algorithm RMSprop with\n7http://cs.stanford.edu/ bdlijiwei/code/\n7\n601\n602\n603\n604\n605\n606\n607\n608\n609\n610\n611\n612\n613\n614\n615\n616\n617\n618\n619\n620\n621\n622\n623\n624\n625\n626\n627\n628\n629\n630\n631\n632\n633\n634\n635\n636\n637\n638\n639\n640\n641\n642\n643\n644\n645\n646\n647\n648\n649\n661\n662\n663\n664\n665\n666\n667\n668\n669\n670\n671\n672\n673\n674\n675\n676\n677\n678\n679\n680\n681\n682\n683\n684\n685\n686\n687\n688\n689\n690\n691\n692\n693\n694\n695\n696\n697\n698\n699\nBatch Emb. Dropout Filter Win. Pool\nGrid-CNN 128 100 0.5 150 6 6 Ext. Grid-CNN 32 100 0.5 150 5 6\nTable 2: Optimal hyper-parameter setting for our neural models based on development set accuracy.\nDiscr. Ins. Acc F1\nRandom 50.0 50.0 12.60\nGraph-based (G&S) 64.23 65.01 11.93\nGrid-all nouns (E&C) 81.58 81.60 22.13 Extended Grid (E&C) 84.95 84.95 23.28\nGrid-CNN 85.57\u2020 85.57\u2020 23.12 Extended Grid-CNN 88.69\u2020 88.69\u2020 25.95\u2020\nTable 3: Results on Discrimination and Insertion tasks. \u2020 indicates a neural model is significantly superior to its nonneural counterpart with p-value < 0.01.\nparameters (\u03c1 and ) set to the values suggested by Tieleman and Hinton (2012).8 We use up to 25 epochs. To avoid overfitting, we use dropout (Srivastava et al., 2014) of hidden units, and do early stopping by observing accuracy on the DEV set \u2013 if the accuracy does not increase for 10 consecutive epochs, we exit with the best model recorded so far. We search for optimal minibatch size in {16, 32, 64, 128}, embedding size in {80, 100, 200}, dropout rate in {0.2, 0.3, 0.5}, filter number in {100, 150, 200, 300}, window size in {2, 3, 4, 5, 6, 7, 8}, and pooling length in {3, 4, 5, 6, 7}. Table 2 shows the optimal hyperparameter setting for our models. The best model on DEV is then used for the final evaluation on the TEST set. We run each experiment five times, each time with a different random seed, and we report the average of the runs to avoid any randomness in results. Statistical significance tests are done using an approximate randomization test based on the accuracy. We used SIGF V.2 (Pado\u0301, 2006) with 10,000 iterations.\n5.3 Results on Sentence Ordering\nTable 3 shows the results on discrimination and insertion tasks. Among the existing models, the graph-based model gets the lowest scores, where the extended grid gets the highest scores on both tasks. By neuralizing the basic grid model (Grid-\n8Other adaptive algorithms, e.g., ADAM (Kingma and Ba, 2014), ADADELTA (Zeiler, 2012) gave similar results.\nall nouns), our Grid-CNN model delivers absolute improvements of about 4% in discrimination and 1% in insertion. When we compare our Extended Grid-CNN with its non-neural counterpart Extended Grid, we observe similar gains in discrimination and more gains (2.5%) in insertion. Note that the Extended Grid-CNN yields these improvements considering only a subset of the Extended Grid features. This demonstrates the effectiveness of distributed representation and convolutional feature learning method.\nCompared to discrimination, gain in insertion is less verbose. There could be two reasons. First, as mentioned before, insertion is a harder task than discrimination. Second, our models were not trained specifically on the insertion task. The model that is trained to distinguish an original document from its random permutation may learn features that are not specific enough to distinguish documents when only one sentence differs. It will be interesting to see how the model performs when we train it on the insertion task directly.\n5.4 Results on Summary Coherence Rating\nTable 4 presents the results on the summary coherence rating task, where we compare our models with the graph-based method and the reported results of Barzilay and Lapata (2008) on the same experimental setting.9 Since there are not many training instances, our neural models may not learn well for this task. Therefore, we also present versions of our model, where we use pre-trained models from discrimination task on WSJ corpus (last two rows in the table ). The pre-trained models are then fine-tuned on the summary rating task.\nWe can observe that even without pre-training\n9The extended grid model does not use pairwise training, therefore could not be trained on the summarization dataset.\n8\n701\n702\n703\n704\n705\n706\n707\n708\n709\n710\n711\n712\n713\n714\n715\n716\n717\n718\n719\n720\n721\n722\n723\n724\n725\n726\n727\n728\n729\n730\n731\n732\n733\n734\n735\n736\n737\n738\n739\n740\n741\n742\n743\n744\n745\n746\n747\n748\n749\n750\n751\n752\n753\n754\n755\n756\n757\n758\n759\n760\n761\n762\n763\n764\n765\n766\n767\n768\n769\n770\n771\n772\n773\n774\n775\n776\n777\n778\n779\n780\n781\n782\n783\n784\n785\n786\n787\n788\n789\n790\n791\n792\n793\n794\n795\n796\n797\n798\n799\nour models outperform existing models, and pretraining gives further improvements. Specifically, Pre-trained Grid-CNN gives an improvement of 2.5% over the Grid model, and including entity features pushes the improvement further to 3.7%.\n6 Related Work\nBarzilay and Lapata (2005, 2008) introduced the entity grid representation of discourse to model local coherence that captures the distribution of discourse entities across sentences in a text. They also introduced three tasks to evaluate the performance of coherence models: discrimination, summary coherence rating, and readability.\nA number of extensions of the basic entity grid model has been proposed. Elsner and Charniak (2011) included entity-specific features to distinguish between entities. Feng and Hirst (2012) used the basic grid representation, but improved its learning to rank scheme. Their model learns not only from original document and its permutations but also from ranking preferences among the permutations themselves. Guinaudeau and Strube (2013) convert a standard entity grid into a bipartite graph representing entity occurrences in sentences. To model local entity transition, the method constructs a directed projection graph representing the connection between adjacent sentences. Two sentences have a connected edge if they share at least one entity in common. The coherence score of the document is then computed as the average out-degree of sentence nodes.\nIn addition, there are some approaches that model text coherence based on coreferences and discourse relations. Elsner and Charniak (2008) proposed the discourse-new model by taking into account mentions of all referring expression (i.e., NPs) whether they are first mention (discoursenew) or subsequent (discourse-old) mentions. Given a document, they run a maximum-entropy classifier to detect each NP as a label Lnp \u2208 {new, old}. The coherence score of the document is then estimated by \u220f np:NPs P (Lnp|np). In this work, they also estimate text coherence through pronoun coreference modeling. Lin et al. (2011) assume that a coherent text has certain discourse relation patterns. Instead of modeling entity transitions, they model discourse role transitions between sentences. In a follow up work, Feng et al. (2014) trained the same model but using features derived from deep discourse struc-\ntures annotated with Rhetorical Structure Theory or RST (Mann and Thompson, 1988b) relations. Louis and Nenkova (2012) introduced a coherence model based on syntactic patterns in text by assuming that sentences in a coherent discourse should share the same structural syntactic patterns.\nIn recent years, there has been a growing interest in neuralizing traditional NLP approaches \u2013 language modeling (Bengio et al., 2003), sequence tagging (Collobert et al., 2011), syntactic parsing (Socher et al., 2013), and discourse parsing (Li et al., 2014), etc. Following this tradition, in this paper we propose to neuralize the popular entity grid models. Li and Hovy (2014) also proposed a neural framework to compute coherence score of a document by estimating coherence probability for every window of L sentences (in their experiments, L = 3). First, they use a recurrent or a recursive neural network to compute the representation for each sentence in L from its words and their pre-trained embeddings. Then the concatenated vector is passed through a non-linear hidden layer, and finally the output layer decides if the window of sentences is a coherent text or not. Our approach is fundamentally different from their approach; our model operates over entity grids, and we use convolutional architecture to model sufficiently long entity transitions.\n7 Conclusion and Future Work\nWe presented a local coherence model based on a convolutional neural network that operates over the distributed representation of entity transitions in the grid representation of a text. Our architecture can model sufficiently long entity transitions, and can incorporate entity-specific features without loosing generalization power. We described a pairwise ranking approach to train the model on a target task and learn task-specific features. Our evaluation on discrimination, insertion and summary coherence rating tasks demonstrates the effectiveness of our approach yielding the best results reported so far on these tasks.\nIn future, we would like to include other sources of information in our model. Our initial plan is to include rhetorical relations, which has been shown to benefit existing grid models (Feng et al., 2014). We would also like to extend our model to other forms of discourse, especially, asynchronous conversations, where participants communicate with each other at different times (e.g., forum, email).\n9\n801\n802\n803\n804\n805\n806\n807\n808\n809\n810\n811\n812\n813\n814\n815\n816\n817\n818\n819\n820\n821\n822\n823\n824\n825\n826\n827\n828\n829\n830\n831\n832\n833\n834\n835\n836\n837\n838\n839\n840\n841\n842\n843\n844\n845\n846\n847\n848\n849\n850\n851\n852\n853\n854\n855\n856\n857\n858\n859\n860\n861\n862\n863\n864\n865\n866\n867\n868\n869\n870\n871\n872\n873\n874\n875\n876\n877\n878\n879\n880\n881\n882\n883\n884\n885\n886\n887\n888\n889\n890\n891\n892\n893\n894\n895\n896\n897\n898\n899\n", "cats": {"READ": 0.0, "DONTREAD": 1.0}, "reviews": [{"IMPACT": "3", "SUBSTANCE": "4", "APPROPRIATENESS": "5", "MEANINGFUL_COMPARISON": "3", "PRESENTATION_FORMAT": "Poster", "comments": "The paper introduces an extension of the entity grid model. A convolutional\nneural network is used to learn sequences of entity transitions indicating\ncoherence, permitting better generalisation over longer sequences of entities\nthan the direct estimates of transition probabilities in the original model.\n\nThis is a nice and well-written paper. Instead of proposing a fully neural\napproach, the authors build on existing work and just use a neural network to\novercome specific issues in one step. This is a valid approach, but it would be\nuseful to expand the comparison to the existing neural coherence model of Li\nand Hovy. The authors admit being surprised by the very low score the Li and\nHovy model achieves on their task. This makes the reader wonder if there was an\nerror in the experimental setup, if the other model's low performance is\ncorpus-dependent and, if so, what results the model proposed in this paper\nwould achieve on a corpus or task where the other model is more successful. A\ndeeper investigation of these factors would strengthen the argument\nconsiderably.\n\nIn general the paper is very fluent and readable, but in many places definite\narticles are missing (e.g. on lines 92, 132, 174, 488, 490, 547, 674, 764 and\nprobably more). I would suggest proofreading the paper specifically with\narticle usage in mind. The expression \"...limits the model to do X...\", which\nis used repeatedly, sounds a bit unusual. Maybe \"limits the model's capacity to\ndo X\" or \"stops the model from doing X\" would be clearer.\n\n--------------\n\nFinal recommendation adjusted to 4 after considering the author response. I\nagree that objective difficulties running other people's software shouldn't be\nheld against the present authors. The efforts made to test the Li and Hovy\nsystem, and the problems encountered in doing so, should be documented in the\npaper. I would also suggest that the authors try to reproduce the results of Li\nand Hovy on their original data sets as a sanity check (unless they have\nalready done so), just to see if that works for them.", "SOUNDNESS_CORRECTNESS": "5", "ORIGINALITY": "5", "is_meta_review": null, "RECOMMENDATION": "4", "CLARITY": "4", "REVIEWER_CONFIDENCE": "3"}, {"IMPACT": "3", "SUBSTANCE": "3", "APPROPRIATENESS": "5", "MEANINGFUL_COMPARISON": "3", "PRESENTATION_FORMAT": "Poster", "comments": "The paper proposes a convolutional neural network approach to model the\ncoherence of texts. The model is based on the well-known entity grid\nrepresentation for coherence, but puts a CNN on top of it. \n\nThe approach is well motivated and described, I especially appreciate the clear\ndiscussion of the intuitions behind certain design decisions (e.g. why CNN and\nthe section titled 'Why it works').\n\nThere is an extensive evaluation on several tasks, which shows that the\nproposed approach beats previous methods. It is however strange that one\nprevious result could not be reproduced: the results on Li/Hovy (2014) suggest\nan implementation or modelling error that should be addressed.\n\nStill, the model is a relatively simple 'neuralization' of the entity grid\nmodel. I didn't understand why 100-dimensional vectors are necessary to\nrepresent a four-dimensional grid entry (or a few more in the case of the\nextended grid). How does this help? I can see that optimizing directly for\ncoherence ranking would help learn a better model, but the difference of\ntransition chains for up to k=3 sentences vs. k=6 might not make such a big\ndifference, especially since many WSJ articles may be very short.\n\nThe writing seemed a bit lengthy, the paper repeats certain parts in several\nplaces, for example the introduction to entity grids. In particular, section 2\nalso presents related work, thus the first 2/3 of section 6 are a repetition\nand should be deleted (or worked into section 2 where necessary). The rest of\nsection 6 should probably be added in section 2 under a subsection (then rename\nsection 2 as related work).\n\nOverall this seems like a solid implementation of applying a neural network\nmodel to entity-grid-based coherence. But considering the proposed\nconsolidation of the previous work, I would expect a bit more from a full\npaper, such as innovations in the representations (other features?) or tasks.\n\nminor points:\n\n- this paper may benefit from proof-reading by a native speaker: there are\narticles missing in many places, e.g. '_the_ WSJ corpus' (2x), '_the_ Brown ...\ntoolkit' (2x), etc.\n\n- p.1 bottom left column: 'Figure 2' -> 'Figure 1'\n\n- p.1 Firstly/Secondly -> First, Second\n\n- p.1 'limits the model to' -> 'prevents the model from considering ...' ?\n\n- Consider removing the 'standard' final paragraph in section 1, since it is\nnot necessary to follow such a short paper.", "SOUNDNESS_CORRECTNESS": "5", "ORIGINALITY": "5", "is_meta_review": null, "RECOMMENDATION": "3", "CLARITY": "4", "REVIEWER_CONFIDENCE": "3"}]}
{"text": "What do Neural Machine Translation Models Learn about Morphology?\n1 000\n011\n012\n013\n014\n015\n016\n017\n018\n019\n020\n021\n022\n023\n024\n025\n026\n027\n028\n029\n030\n031\n032\n033\n034\n035\n036\n037\n038\n039\n040\n041\n042\n043\n044\n045\n046\n047\n048\n049\n061\n062\n063\n064\n065\n066\n067\n068\n069\n070\n071\n072\n073\n074\n075\n076\n077\n078\n079\n080\n081\n082\n083\n084\n085\n086\n087\n088\n089\n090\n091\n092\n093\n094\n095\n096\n097\n098\n099\n1 Introduction\nNeural network models are quickly becoming the predominant approach to machine translation (MT). Training neural MT (NMT) models can be done in an end-to-end fashion, which is simpler and more elegant than traditional MT systems. Moreover, NMT systems have become competitive with, or better than, the previous state-of-the-art, especially since the introduction of sequence-to-sequence models and the attention mechanism (Bahdanau et al., 2014; Sutskever et al., 2014). The improved translation quality is often attributed to better handling of non-local dependencies and morphology generation (Luong and Manning, 2015; Bentivogli et al., 2016).\nHowever, little is known about what and how much these models learn about each language and its features. Recent work has started exploring the role of the NMT encoder in learning source syntax (Shi et al., 2016), but research studies are yet to answer important questions such as: (i) what do NMT models learn about word morphology? (ii) what is the effect on learning when translating into/from morphologically-rich languages? (iii) what impact do different representations (character vs. word) have on learning? and (iv) what do different modules learn about the syntactic and semantic structure of a language? Answering such questions is imperative for fully understanding the NMT architecture. In this paper, we strive towards exploring (i), (ii), and (iii) by providing quantitative, data-driven answers to the following specific questions:\n\u2022 Which parts of the NMT architecture capture word structure?\n\u2022 What is the division of labor between different components (e.g. different layers or encoder vs. decoder)?\n\u2022 How do different word representations help learn better morphology and modeling of infrequent words?\n\u2022 How does the target language affect the learning of word structure?\nTo achieve this, we follow a simple but effective procedure with three steps: (i) train a neural MT system on a parallel corpus; (ii) use the trained model to extract feature representations for words in a language of interest; and (iii) train a classifier using extracted features to make predictions for another task. We then evaluate the quality of the trained classifier on the given task as a proxy to the quality of the extracted representations. In\n2\n101\n102\n103\n104\n105\n106\n107\n108\n109\n110\n111\n112\n113\n114\n115\n116\n117\n118\n119\n120\n121\n122\n123\n124\n125\n126\n127\n128\n129\n130\n131\n132\n133\n134\n135\n136\n137\n138\n139\n140\n141\n142\n143\n144\n145\n146\n147\n148\n149\n159\n160\n161\n162\n163\n164\n165\n166\n167\n168\n169\n170\n171\n172\n173\n174\n175\n176\n177\n178\n179\n180\n181\n182\n183\n184\n185\n186\n187\n188\n189\n190\n191\n192\n193\n194\n195\n196\n197\n198\n199\nthis way, we obtain a quantitative measure of how well the original MT system learns features that are relevant to the given task.\nWe focus on the tasks of part-of-speech (POS) and full morphological tagging. We investigate how different neural MT systems capture POS and morphology through a series of experiments along several parameters. For instance, we contrast word-based and character-based representations, use different encoding layers, vary source and target languages, and compare extracting features from the encoder vs. the decoder.\nWe experiment with several languages with varying degrees of morphological richness: French, German, Czech, Arabic, and Hebrew. Our analysis reveals interesting insights such as:\n\u2022 Character-based representations are much better for learning morphology, especially for low-frequency words. This improvement is correlated with better BLEU scores. On the other hand, word-based models are sufficient for learning the structure of common words.\n\u2022 Lower layers of the MT encoder are better at capturing word structure, while higher layers are more focused on word meaning.\n\u2022 The target language impacts the kind of information learned by the MT system. Translating into morphologically-poorer languages leads to better source-side word representations. This is partly, but not completely, correlated with BLEU scores.\n\u2022 The neural decoder learns very little about word structure. The attention mechanism removes much of the burden of learning word representations from the decoder.\n2 Methodology\nGiven a source sentence s = {w1, w2, ..., wN} and a target sentence t = {u1, u2, ..., uM}, we first generate a vector representation for the source sentence using an encoder (Eqn. 1) and then map this vector to the target sentence using a decoder (Eqn. 2) (Sutskever et al., 2014):\nENC : s = {w1, w2, ..., wN} 7! s 2 Rk (1) DEC : s 2 Rk 7! t = {u1, u2, ..., uM} (2)\nIn this work, we use long short-term memory (LSTM) (Hochreiter and Schmidhuber, 1997)\nencoder-decoders with attention (Bahdanau et al., 2014), which we train on parallel data.\nAfter training the NMT system, we freeze the parameters of the encoder and use ENC as a feature extractor to generate vectors representing words in the sentence. Let ENCi(s) denote the encoded representation of word wi. For example, this may be the output of the LSTM after word wi. We feed ENCi(s) to a neural classifier that is trained to predict POS or morphological tags and evaluate the quality of the representation based on our ability to train a good classifier. By comparing the performance of classifiers trained with features from different instantiations of ENC, we can evaluate what MT encoders learn about word structure. Figure 1 illustrates this process. We follow a similar procedure for analyzing representation learning in DEC.\nThe classifier itself can be modeled in different ways. For example, it may be an LSTM over outputs of the encoder. However, as we are interested in assessing the quality of the representations learned by the MT system, we choose to model the classifier as a simple feed-forward network with one hidden layer and a ReLU non-linearity. Arguably, if the learned representations are good, then a non-linear classifier should be able to extract useful information from them.1 We emphasize that our goal is not to beat the state-of-the-art on a given task, but rather to analyze what NMT models learn about morphology. The classifier is trained with a cross-entropy loss; more details on its architecture are in the supplementary material.\n1We also experimented with a linear classifier and observed similar trends to the non-linear case, but overall lower results; Qian et al. (2016b) reported similar findings.\n3\n201\n202\n203\n204\n205\n206\n207\n208\n209\n210\n211\n212\n213\n214\n215\n216\n217\n218\n219\n220\n221\n222\n223\n224\n225\n226\n227\n228\n229\n230\n231\n232\n233\n234\n235\n236\n237\n238\n239\n240\n241\n242\n243\n244\n245\n246\n247\n248\n249\n259\n260\n261\n262\n263\n264\n265\n266\n267\n268\n269\n270\n271\n272\n273\n274\n275\n276\n277\n278\n279\n280\n281\n282\n283\n284\n285\n286\n287\n288\n289\n290\n291\n292\n293\n294\n295\n296\n297\n298\n299\nAr De Fr Cz\nGold/Pred Gold/Pred Pred Pred\nTrain 500K/2.7M 888K/4.0M 5.2M 2.0M Dev 63K/114K 45K/50K 55K 35K Test 62K/16K 44K/25K 23K 20K\nPOS Tags 42 54 33 368 Morph Tags 1969 214 \u2013 \u2013\nTable 1: Statistics for annotated corpora in Arabic (Ar), German (De), French (Fr), and Czech (Cz).\n3 Data\nLanguage pairs We experiment with several language pairs, including morphologically-rich languages, that have received relatively significant attention in the MT community. These include Arabic-, German-, French-, and Czech-English pairs. To broaden our analysis and study the effect of having morphologically-rich languages on both source and target sides, we also include ArabicHebrew, two languages with rich and similar morphological systems, and Arabic-German, two languages with rich but different morphologies.\nMT data Our translation models are trained on the WIT3 corpus of TED talks (Cettolo et al., 2012; Cettolo, 2016) made available for IWSLT 2016. This allows for comparable and crosslinguistic analysis. Statistics about each language pair are given in Table 1 (under Pred). We use official dev and test sets for tuning and testing. Reported figures are the averages over test sets.\nAnnotated data We use two kinds of datasets to train POS and morphological classifiers: goldstandard and predicted tags. For predicted tags, we simply used freely available taggers to annotate the MT data. For gold tags, we use gold-annotated datasets. Table 1 gives statistics for datasets with gold and predicted tags; see supplementary material for details on taggers and gold data. We train and test our classifiers on predicted annotations, and similarly on gold annotations, when we have them. We report both results wherever available.\n4 Encoder Analysis\nRecall that after training the NMT system we freeze its parameters and use it only to generate features for the POS/morphology classifier. Given a trained encoder ENC and a sentence s with POS/morphology annotation, we generate word features ENCi(s) for every word in the sentence.\nWe then train a classifier that uses the features ENCi(s) to predict POS or morphological tags.\n4.1 Effect of word representation\nIn this section we compare different word representations extracted with different encoders. Our word-based model uses a word embedding matrix which is initialized randomly and learned with other NMT parameters. For a character-based model we adopt a convolutional neural network (CNN) over character embeddings that is also learned during training (Kim et al., 2015; Costajussa\u0300 and Fonollosa, 2016); see appendix A.1 for specific settings. In both cases we run the encoder over these representations and use its output ENCi(s) as features for the classifier.\nTable 2 shows POS tagging accuracy using features from different NMT encoders. Charbased models always generate better representations for POS tagging, especially in the case of morphologically-richer languages like Arabic and Czech. We observed a similar pattern in the full morphological tagging task. For example, we obtain morphological tagging accuracy of 65.2/79.66 and 67.66/81.66 using word/charbased representations from the Arabic-Hebrew and Arabic-English encoders, respectively.2 The superior morphological power of the char-based model also manifests in better translation quality (measured by BLEU), as shown in Table 2.\nImpact of word frequency Let us look more closely at an example case: Arabic POS and morphological tagging. Figure 3 shows the effect of using word-based vs. char-based feature representations, obtained from the encoder of the Arabic-\n2The results are not far below dedicated taggers (e.g. 95.1/84.1 on Arabic POS/morphology (Pasha et al., 2014)), indicating that NMT models learn quite good representations.\n4\n312\n325\n326\n327\n328\n329\n330\n331\n332\n333\n334\n335\n336\n337\n338\n339\n340\n341\n342\n343\n344\n345\n346\n347\n348\n349\n362\n363\n364\n365\n366\n367\n368\n369\n370\n371\n372\n373\n374\n375\n376\n377\n378\n379\n380\n381\n382\n383\n384\n385\n386\n387\n388\n389\n390\n391\n392\n393\n394\n395\n396\n397\n398\n399\nHebrew system (other language pairs exhibit similar trends). Clearly, the char-based model is superior to the word-based one. This is true for the overall accuracy (+14.3% in POS, +14.5% in morphology), but more so on OOV words (+37.6% in POS, +32.7% in morphology). Figure 2 shows that the gap between word-based and char-based representations increases as the frequency of the word in the training data decreases. In other words, the more frequent the word, the less need there is for character information. These findings make intuitive sense: the char-based model is able to learn character n-gram patterns that are important for identifying word structure, but as the word becomes more frequent the word-based model has seen enough examples to make a decision.\nAnalyzing specific tags In Figure 5 we plot confusion matrices for POS tagging using wordbased and char-based representations (from Arabic encoders). While the char-based representations are overall better, the two models still\nFigure 4: Increase in POS accuracy with char- vs. word-based representations per tag frequency in the training set; larger bubbles reflect greater gaps.\nshare similar misclassified tags. Much of the confusion comes from wrongly predicting nouns (NN, NNP). In the word-based case, relatively many tags with determiner (DT+NNP, DT+NNPS, DT+NNS, DT+VBG) are wrongly predicted as non-determined nouns (NN, NNP). In the charbased case, this hardly happens. This suggests that char-based representations are predictive of the presence of a determiner, which in Arabic is expressed as the prefix \u201cAl-\u201d (the definite article), a pattern easily captured by a char-based model.\nIn Figure 4 we plot the difference in POS accuracy when moving from word-based to char-based representations, per POS tag frequency in the training data. Tags closer to the upper-right corner occur more frequently in the training set and are better predicted by char-based compared to wordbased representations. There are a few fairly frequent tags (in the middle-bottom part of the figure) whose accuracy does not improve much when moving from word- to char-based representations: mostly conjunctions, determiners, and certain par-\n5\n416\n417\n418\n419\n420\n421\n422\n423\n424\n425\n426\n427\n428\n429\n430\n431\n432\n433\n434\n435\n436\n437\n438\n439\n440\n441\n442\n443\n444\n445\n446\n447\n448\n449\n466\n467\n468\n469\n470\n471\n472\n473\n474\n475\n476\n477\n478\n479\n480\n481\n482\n483\n484\n485\n486\n487\n488\n489\n490\n491\n492\n493\n494\n495\n496\n497\n498\n499\nFigure 6: POS tagging accuracy using representations from layers 0 (word vectors), 1, and 2, taken from encoders of different language pairs.\nticles (CC, DT, WP). But there are several very frequent tags (NN, DT+NN, DT+JJ, VBP, and even PUNC) whose accuracy improves quite a lot. Then there are plural nouns (NNS, DT+NNS) where the char-based model really shines, which makes sense linguistically as plurality in Arabic is usually expressed by certain suffixes (\u201c-wn/yn\u201d for masc. plural, \u201c-At\u201d for fem. plural). The charbased model is thus especially good with frequent tags and infrequent words, which is understandable given that infrequent words typically belong to frequent open categories like nouns and verbs.\n4.2 Effect of encoder depth\nModern NMT systems use very deep architectures with up to 8 or 16 layers (Wu et al., 2016; Zhou et al., 2016). We would like to understand what kind of information different layers capture. Given\nFigure 7: POS and morphological tagging accuracy across layers. Layer 0: word vectors or charbased representations before the encoder; layers 1 and 2: representations after the 1st and 2nd layers.\na trained model with multiple layers, we extract representations from the different layers in the encoder. Let ENCli(s) denote the encoded representation of word wi after the l-th layer. We vary l and train different classifiers to predict POS or morphological tags. Here we focus on the case of a 2- layer encoder-decoder for simplicity (l 2 {1, 2}).\nFigure 6 shows POS tagging results using representations from different encoding layers across five language pairs. The general trend is that passing word vectors through the encoder improves POS tagging, which can be explained by contextual information contained in the representations after one layer. However, it turns out that representations from the 1st layer are better than those from the 2nd layer, at least for the purpose of capturing word structure. Figure 7 shows that the same\n6\n501\n502\n503\n504\n505\n506\n507\n508\n509\n510\n511\n512\n513\n514\n515\n516\n517\n518\n519\n520\n521\n522\n523\n524\n525\n526\n527\n528\n529\n530\n531\n532\n533\n534\n535\n536\n537\n538\n539\n540\n541\n542\n543\n544\n545\n546\n547\n548\n549\n561\n562\n563\n564\n565\n566\n567\n568\n569\n570\n571\n572\n573\n574\n575\n576\n577\n578\n579\n580\n581\n582\n583\n584\n585\n586\n587\n588\n589\n590\n591\n592\n593\n594\n595\n596\n597\n598\n599\npattern holds for both word-based and char-based representations, on Arabic POS and morphological tagging. In all cases, layer 1 representations are better than layer 2 representations.3 In contrast, BLEU scores actually increase when training 2-layer vs. 1-layer models (+1.11/+0.56 BLEU for Arabic-Hebrew word/char-based models). Thus translation quality improves when adding layers but morphology quality degrades. Intuitively, it seems that lower layers of the network learn to represent word structure while higher layers focus more on word meaning. A similar pattern was recently observed in a joint language-vision deep recurrent net (Gelderloos and Chrupa\u0142a, 2016).\n4.3 Effect of target language\nWhile translating from morphologically-rich languages is challenging, translating into such languages is even harder. For instance, our basic system obtains BLEU of 24.69/23.2 on Arabic/Czech to English, but only 13.37/13.9 on English to Arabic/Czech. How does the target language affect the learned source language representations? Does translating into a morphologically-rich language require more knowledge about source language morphology? In order to investigate these questions, we fix the source language and train NMT models on different target languages. For example, given an Arabic source we train Arabic-toEnglish/Hebrew/German systems. These target languages represent a morphologically-poor language (English), a morphologically-rich language with similar morphology to the source language (Hebrew), and a morphologically-rich language with different morphology (German). To make a fair comparison, we train the models on the intersection of the training data based on the source language. In this way the experimental setup is completely identical: the models are trained on the same Arabic sentences with different translations.\nFigure 8 shows POS and morphology accuracy of word-based representations from the NMT encoders, as well as corresponding BLEU scores. As expected, translating to English is easier than translating to the morphologically-richer Hebrew and German, resulting in higher BLEU. Despite their similar morphologies, translating Arabic to Hebrew is worse than Arabic to German, which can be attributed to the richer Hebrew morphology\n3We found this result to be also true in French, German, and Czech experiments (see the supplementary material).\ncompared to German. POS and morphology accuracies share an intriguing pattern: the representations that are learned when translating to English are better for predicting POS or morphology than those learned when translating to German, which are in turn better than those learned when translating to Hebrew. This is remarkable given that English is a morphologically-poor language that does not display many of the morphological properties that are found in the Arabic source. In contrast, German and Hebrew have richer morphologies, so one could expect that translating into them would make the model learn more about morphology.\nA possible explanation for this phenomenon is that the Arabic-English model is simply better than the Arabic-Hebrew and Arabic-German models, as hinted by the BLEU scores in Table 2. The inherent difficulty in translating Arabic to Hebrew/German may affect the ability to learn good representations of word structure. To probe this more, we trained an Arabic-Arabic autoencoder on the same training data. We found that it learns to recreate the test sentences extremely well, with very high BLEU scores (Figure 8). However, its word representations are actually inferior for the purpose of POS/morphological tagging. This implies that higher BLEU does not necessarily entail better morphological representations. In other words, a better translation model learns more informative representations, but only when it is actually learning to translate rather than merely memorizing the data as in the autoencoder case. We found this to be consistently true also for charbased experiments, and in other language pairs.\n7\n601\n602\n603\n604\n605\n606\n607\n608\n609\n610\n611\n612\n613\n614\n615\n616\n617\n618\n619\n620\n621\n622\n623\n624\n625\n626\n627\n628\n629\n630\n631\n632\n633\n634\n635\n636\n637\n638\n639\n640\n641\n642\n643\n644\n645\n646\n647\n648\n649\n650\n651\n652\n653\n654\n655\n656\n657\n658\n659\n660\n661\n662\n663\n664\n665\n666\n667\n668\n669\n670\n671\n672\n673\n674\n675\n676\n677\n678\n679\n680\n681\n682\n683\n684\n685\n686\n687\n688\n689\n690\n691\n692\n693\n694\n695\n696\n697\n698\n699\nPOS Accuracy BLEU Attn ENC DEC Ar-En En-Ar\n3 89.62 43.93 24.69 13.37 7 74.10 50.38 11.88 5.04\nTable 3: POS tagging accuracy using encoder and decoder representations with/without attention.\n5 Decoder Analysis\nSo far we only looked at the encoder. However, the decoder DEC is a crucial part in an MT system with access to both source and target sentences. In order to examine what the decoder learns about morphology, we first train an NMT system on the parallel corpus. Then, we use the trained model to encode a source sentence and extract features for words in the target sentence. These features are used to train a classifier on POS or morphological tagging on the target side.4 Note that in this case the decoder is given the correct target words oneby-one, similar to the usual NMT training regime.\nTable 3 (1st row) shows the results of using representations extracted with ENC and DEC from the Arabic-English and English-Arabic models, respectively. There is clearly a huge drop in representation quality with the decoder.5 At first, this drop seems correlated with lower BLEU in English to Arabic vs. Arabic to English. However, we observed similar low POS tagging accuracy using decoder representations from high-quality models. For instance, the French-to-English model obtains 37.8 BLEU, but its decoder representations give a mere 54.26 accuracy on English POS tagging.\nAs an alternative explanation for the poor quality of the decoder representations, consider the fundamental tasks of the two NMT modules: encoder and decoder. The encoder\u2019s task is to create a generic, close to language-independent representation of the source sentence, as shown by recent evidence from multilingual NMT (Johnson et al., 2016). The decoder\u2019s task is to use this representation to generate the target sentence in a specific language. Presumably, it is sufficient for the decoder to learn a strong language model to produce morphologically-correct output, without learning much about morphology, while the encoder needs to learn quite a lot about source language morphol-\n4In this section we only experiment with predicted tags for lack of available parallel data with gold POS/morph. tags.\n5Decoder results are above a majority baseline of 20%, so the decoder still learns something about the target language.\nPOS Accuracy BLEU ENC DEC Ar-En En-Ar\nWord 89.62 43.93 24.69 13.37 Char 95.35 44.54 28.42 13.00\nTable 4: POS tagging accuracy using word-based and char-based encoder/decoder representations.\nogy in order to create a good generic representation. In the following section we show that the attention mechanism also plays an important role in the division of labor between encoder and decoder.\n5.1 Effect of attention\nConsider the role of the attention mechanism in learning useful representations: during decoding, the attention weights are combined with the decoder\u2019s hidden states to generate the current translation. These two sources of information need to jointly point to the most relevant source word(s) and predict the next most likely word. Thus, the decoder puts significant emphasis on mapping back to the source sentence, which may come at the expense of obtaining a meaningful representation of the current word. We hypothesize that the attention mechanism hurts the quality of the target word representations learned by the decoder.\nTo test this hypothesis, we train NMT models with and without attention and compare the quality of their learned representations. As Table 3 shows (compare 1st and 2nd rows), removing the attention mechanism decreases the quality of the encoder representations, but improves the quality of the decoder representations. Without attention, the decoder is forced to learn more informative representations of the target language.\n5.2 Effect of word representation\nWe also conducted experiments to verify our findings regarding word-based versus character-based representations on the decoder side. By character representation we mean a character CNN on the input words. The decoder predictions are still done at the word-level, which enables us to use its hidden states as word representations.\nTable 4 shows POS accuracy of word- vs. charbased representations in the encoder and decoder. While char-based representations improve the encoder, they do not help the decoder. BLEU scores behave similarly: the char-based model leads to better translations in Arabic-to-English, but not\n8\n701\n702\n703\n704\n705\n706\n707\n708\n709\n710\n711\n712\n713\n714\n715\n716\n717\n718\n719\n720\n721\n722\n723\n724\n725\n726\n727\n728\n729\n730\n731\n732\n733\n734\n735\n736\n737\n738\n739\n740\n741\n742\n743\n744\n745\n746\n747\n748\n749\n750\n751\n752\n753\n754\n755\n756\n757\n758\n759\n760\n761\n762\n763\n764\n765\n766\n767\n768\n769\n770\n771\n772\n773\n774\n775\n776\n777\n778\n779\n780\n781\n782\n783\n784\n785\n786\n787\n788\n789\n790\n791\n792\n793\n794\n795\n796\n797\n798\n799\nin English-to-Arabic. A possible explanation for this is that the decoder\u2019s predictions are still done at word level even with the char-based model (which encodes the target input but not the output). In practice, this can lead to generating unknown words. Indeed, in Arabic-to-English the charbased model reduces the number of generated unknowns in the test set by 25%, while in English-toArabic the number of unknowns remains roughly the same between word- and char-based models.\n6 Related Work\nAnalysis of neural models The opacity of neural networks has motivated researchers to analyze such models in different ways. One line of work visualizes hidden unit activations in recurrent neural networks that are trained for a given task (Elman, 1991; Karpathy et al., 2015; Ka\u0301da\u0301r et al., 2016; Qian et al., 2016a). While such visualizations illuminate the inner workings of the network, they are often qualitative in nature and somewhat anecdotal. A different approach tries to provide quantitative analysis by correlating parts of the neural network with linguistic properties, for example by training a classifier to predict features of interest. Different units have been used, from word embeddings (Ko\u0308hn, 2015; Qian et al., 2016b), through LSTM gates or states (Qian et al., 2016a), to sentence embeddings (Adi et al., 2016). Our work is most similar to Shi et al. (2016), who use hidden vectors from a neural MT encoder to predict syntactic properties on the English source side. In contrast, we focus on representations in morphologically-rich languages and evaluate both source and target sides across several criteria. Vylomova et al. (2016) also analyze different representations for morphologically-rich languages in MT, but do not directly measure the quality of the learned representations.\nWord representations in MT Machine translation systems that deal with morphologically-rich languages resort to various techniques for representing morphological knowledge, such as word segmentation (Nieflen and Ney, 2000; Koehn and Knight, 2003) and factored translation models (Koehn and Hoang, 2007). Characters and other sub-word units have become increasingly popular in neural MT, although they had also been used in phrase-based MT (Luong et al., 2010). Such units can be obtained in a pre-processing step \u2013 e.g. by byte-pair encoding (Sennrich et al.,\n2016) or the word-piece model (Wu et al., 2016) \u2013 or learned during training with a character-based convolutional/recurrent sub-network (Costa-jussa\u0300 and Fonollosa, 2016; Luong and Manning, 2016; Vylomova et al., 2016). The latter approach has the advantage of keeping the original word boundaries without requiring pre- and post-processing. Here we focus on a character CNN which has been used in language modeling and machine translation (Kim et al., 2015; Costa-jussa\u0300 and Fonollosa, 2016; Jozefowicz et al., 2016). We evaluate the quality of different representations learned by an MT system augmented with a character CNN in terms of POS and morphological tagging, and contrast them with a purely word-based system.\n7 Conclusion\nNeural nets have become ubiquitous in machine translation due to their elegant architecture and good performance. The representations they use for linguistic units are crucial for obtaining highquality translation. In this work, we investigated how neural MT models learn word structure. We evaluated their representation quality on POS and morphological tagging in a number of languages. Our results lead to the following conclusions:\n\u2022 Character-based representations are better than word-based ones for learning morphology, especially in rare and unseen words.\n\u2022 Lower layers of the neural network are more focused on word structure, while higher ones are better for learning word meaning.\n\u2022 Translating into morphologically-poorer languages leads to better source-side representations. This is partly correlated with BLEU.\n\u2022 The attentional decoder learns impoverished representations that do not carry much information about morphology.\nThese insights can guide further development of neural MT systems. For instance, jointly learning translation and morphology can possibly lead to better representations and improved translation. Our analysis indicates that this kind of approach should take into account factors such as the encoding layer and the type of word representation.\nAnother area for future work is to extend the analysis to other representations (e.g. byte-pair encoding), deeper networks, and more semanticallyoriented tasks such as semantic parsing.\n9\n801\n802\n803\n804\n805\n806\n807\n808\n809\n810\n811\n812\n813\n814\n815\n816\n817\n818\n819\n820\n821\n822\n823\n824\n825\n826\n827\n828\n829\n830\n831\n832\n833\n834\n835\n836\n837\n838\n839\n840\n841\n842\n843\n844\n845\n846\n847\n848\n849\n850\n851\n852\n853\n854\n855\n856\n857\n858\n859\n860\n861\n862\n863\n864\n865\n866\n867\n868\n869\n870\n871\n872\n873\n874\n875\n876\n877\n878\n879\n880\n881\n882\n883\n884\n885\n886\n887\n888\n889\n890\n891\n892\n893\n894\n895\n896\n897\n898\n899\n", "cats": {"READ": 1.0, "DONTREAD": 0.0}, "reviews": [{"IMPACT": "3", "SUBSTANCE": "3", "APPROPRIATENESS": "5", "MEANINGFUL_COMPARISON": "3", "PRESENTATION_FORMAT": "Poster", "comments": "- Strengths: The authors have nice coverage of a different range of language\nsettings to isolate the way that relatedness and amount of morphology interact\n(i.e., translating between closely related morphologically rich languages vs\ndistant ones) in affecting what the system learns about morphology. They\ninclude an illuminating analysis of what parts of the architecture end up being\nresponsible for learning morphology, particularly in examining how the\nattention mechanism leads to more impoverished target side representations.\nTheir findings are of high interest and practical usefulness for other users of\nNMT. \n\n- Weaknesses: They gloss over the details of their character-based encoder.\nThere are many different ways to learn character-based representations, and\nomitting a discussion of how they do this leaves open questions about the\ngenerality of their findings. Also, their analysis could've been made more\ninteresting had they chosen languages with richer and more challenging\nmorphology such as Turkish or Finnish, accompanied by finer-grained morphology\nprediction and analysis.\n\n- General Discussion: This paper brings insight into what NMT models learn\nabout morphology by training NMT systems and using the encoder or decoder\nrepresentations, respectively, as input feature representations to a POS- or\nmorphology-tagging classification task. This paper is a straightforward\nextension of \"Does String-Based Neural MT Learn Source Syntax?,\" using the same\nmethodology but this time applied to morphology. Their findings offer useful\ninsights into what NMT systems learn.", "SOUNDNESS_CORRECTNESS": "5", "ORIGINALITY": "5", "is_meta_review": null, "RECOMMENDATION": "4", "CLARITY": "5", "REVIEWER_CONFIDENCE": "4"}, {"IMPACT": "3", "SUBSTANCE": "4", "APPROPRIATENESS": "5", "MEANINGFUL_COMPARISON": "3", "PRESENTATION_FORMAT": "Oral Presentation", "comments": "Strengths:\n\n- This paper describes experiments that aim to address a crucial\nproblem for NMT: understanding what does the model learn about morphology and\nsyntax, etc..\n- Very clear objectives and experiments effectively laid down.              Good\nstate\nof the art review and comparison. In general, this paper is a pleasure to read.\n- Sound experimentation framework. Encoder/Decoder Recurrent layer\noutputs are used to train POS/morphological classifiers. They show the effect\nof certain changes in the framework on the classifier accuracy (e.g. use\ncharacters instead of words).\n- Experimentation is carried out on many language pairs.\n- Interesting conclusions derived from this work, and not all agree with\nintuition.\n\nWeaknesses:\n\n -  The contrast of character-based vs word-based representations  is slightly\nlacking: NMT with byte-pair encoding is showing v. strong performance in the\nliterature. It would have been more relevant to have BPE in the mix, or replace\nword-based representations if three is too many.\n - Section 1: \"\u2026 while higher layers are more focused on word meaning\";\nsimilar sentence in Section 7. I am ready to agree with this intuition, but I\nthink the experiments in this paper do not support this particular sentence.\nTherefore it should not be included, or it should be clearly stressed that this\nis a reasonable hypothesis based on indirect evidence (translation performance\nimproves but morphology on higher layers does not).\n\nDiscussion:\n\nThis is a  fine paper that presents a thorough and systematic analysis of the\nNMT model, and derives several interesting conclusions based on many data\npoints across several language pairs. I find particularly interesting that (a)\nthe target language affects the quality of the encoding on the source side; in\nparticular, when the target side is a morphologically-poor language (English)\nthe pos tagger accuracy for the encoder improves. (b) increasing the depth of\nthe encoder does not improve pos accuracy (more experiments needed to determine\nwhat does it improve); (c) the attention layer hurts the quality of the decoder\nrepresentations.  I wonder if (a) and (c) are actually related? The attention\nhurts the decoder representation, which is more difficult to learn for a\nmorphologically rich language; in turn, the encoders learn based on the global\nobjective, and this backpropagates through the decoder. Would this not be a\nstrong\nindication that we need separate objectives to govern the encoder/decoder\nmodules of\nthe NMT model?", "SOUNDNESS_CORRECTNESS": "5", "ORIGINALITY": "5", "is_meta_review": null, "RECOMMENDATION": "4", "CLARITY": "5", "REVIEWER_CONFIDENCE": "4"}]}
{"text": "1 INTRODUCTION\nStochastic Gradient Descent (SGD) is an effective method for many regression and classification tasks. It is a simple algorithm with few hyper-parameters and its convergence rates are well understood both theoretically and empirically. However, its performance scalability is severely limited by its inherently sequential computation. SGD iteratively processes its input dataset where the computation at each iteration depends on the model parameters learned from the previous iteration.\nCurrent approaches for parallelizing SGD do not honor this inter-step dependence across threads. Each thread learns a local model independently and combine these models in ways that can break sequential behavior. For instance, threads in HOGWILD! Recht et al. (2011) racily update a shared global model without holding any locks. In parameter-server Li et al. (2014a), each thread (or machine) periodically sends its model deltas to a server that applies them to a global model. In ALLREDUCE Agarwal et al. (2014), threads periodically reach a barrier where they compute a weightedaverage of the local models. Although these asynchronous parallel approaches reach the optimal solution eventually, they can produce a model that is potentially different from what a sequential SGD would have produced after processing a certain number of examples. Our experiments indicate that this makes their convergence rate slower than sequential SGD in terms of total number of examples studied. Our experiments show that all these algorithms either do not scale or their accuracy on the same number of examples falls short of a sequential baseline.\nTo address this problem, this paper presents SYMSGD, a parallel SGD algorithm that seeks to retain its sequential semantics. The key idea is for each thread to generate a sound combiner that allows the local models to be combined into a model that is the same as the sequential model. This paper describes a method for generating sound combiners for a class of SGD algorithms in which the inter-step dependence is linear in the model parameters. This class includes linear regression, linear regression with L2 regularization, and polynomial regression. While logistic regression is not in this class, our experiments show that linear regression performs equally well in classification tasks as logistic regression for the datasets studied in this paper. Also, this approach works even if the SGD\ncomputation is non-linear on the input examples and other parameters such as the learning rate; only the dependence on model parameters has to be linear.\nGenerating sound combiners can be expensive. SYMSGD uses random projection techniques to reduce this overhead but still retaining sequential semantics in expectation. We call this approach probabilistically sound combiners. Even though SYMSGD is expected to produce the same answer as the sequential SGD, controlling the variance introduced by the random projection requires care \u2014 a large variance can result in reduced accuracy. This paper describes the factors that affect this variance and explores the ensuing design trade-offs.\nThe resulting algorithm is fast, scales well on multiple cores, and achieves the same accuracy as sequential SGD on sparse and dense datasets. When compared to our optimized sequential baseline, SYMSGD achieves a speedup of 3.5\u00d7 to 13\u00d7 on 16 cores, with the algorithm performing better for denser datasets. Moreover, the cost of computing combiners can be efficiently amortized in a multiclass regression as a single combiner is sufficient for all of the classes. Finally, SYMSGD (like ALLREDUCE) is deterministic, producing the same result for a given dataset, configuration, and random seed. Determinism greatly simplifies the task of debugging and optimizing learning.\n2 SOUND AND PROBABILISTIC MODEL COMBINERS\nStochastic gradient descent (SGD) is a robust method for finding the parameters of a model that minimize a given error function. Figure 1 shows an example of a (convex) error function over two dimensions x and y reaching the minimum at parameter w\u2217. SGD starts from some, not necessarily optimal, parameter wg (as shown in Figure 1), and repeatedly modifies w by taking a step along the gradient of the error function for a randomly selected example at the currentw. The magnitude of the step is called the learning rate and is usually denoted by \u03b1. The gradient computed from one example is not necessarily the true gradient at w. Nevertheless, SGD enjoys robust convergence behavior by moving along the \u201cright\u201d direction over a large number of steps. This is shown pictorially in Figure 1, where SGD processes examples in dataset D1 to reach w1 from wg . Subsequently, SGD starts from w1 and processes a different set D2 to reach wh. There is a clear dependence between the processing ofD1 and the processing ofD2 \u2014 the latter starts from w1, which is only determined after processing D1. Our goal is to parallelize SGD despite this dependence.\nState of the art parallelization techniques such as HOGWILD! and ALLREDUCE approach this problem by processing D1 and D2 starting from the same model wg (let us assume that there only two processors for now), and respectively reaching w1 and w2. Then, they combine their local models into a global model, but do so in an ad-hoc manner. For instance, ALLREDUCE computes a weighted average of w1 and w2, where the per-feature weights are chosen so as to prefer the processor that has larger update for that feature. This weighted average is depicted pictorially as wa. Similarly, in HOGWILD!, the two processors race to update the global model with their respective local model without any locking. (HOGWILD! performs this udpate after every example, thus the size of D1 and D2 is 1.) Both approaches do not necessarily reach wh, the model that a sequential SGD would have reached on D1 and D2. While SGD is algorithmically robust to errors, such ad-hoc combinations can result in slower convergence or poor performance, as we demonstrate in Section 4.\nSound Combiner: The goal of this paper is to soundly combine local models. Looking at Figure 1, a sound combiner combines local models w1 and w2, respectively generated from datasets D1 and D2, into a global model wh that is guaranteed to be the same as the model achieved by the sequential\nSGD processing D1 and then D2. In effect, a sound combiner allows us to parallelize the sequential computation without changing its semantics.\nIf we look at the second processor, it starts its computation at wg , while in a sequential execution it would have started at w1, the output of the first processor. To obtain sequential semantics, we need to \u201cadjust\u201d its computation from wg to w1. To do so, the second processor performs its computation starting fromwg +\u2206w, where \u2206w is an unknown symbolic vector. This allows the second processor to both compute a local model (resulting from the concrete part) and a sound combiner (resulting from the symbolic part) that accounts for changes in the initial state. Once both processors are done learning, second processor finds wh by setting \u2206w to w1 \u2212 wg where w1 is computed by the first processor. This parallelization approach of SGD can be extended to multiple processors where all processor produce a local model and a combiner (except for the first processor) and the local models are combined sequentially using the combiners.\nWhen the update to the model parameters is linear in a SGD computation, then the dependence on the unknown \u2206w can be concisely represented by a combiner matrix, as formally described in Section 3. Many interesting machine learning algorithms, such as linear regression, linear regression with L2 regularization, and polynomial regression already have linear update to the model parameters (but not necessarily linear on the input example).\nProbabilistically Sound Combiner: The main problem with generating a sound combiner is that the combiner matrix has as many rows and columns as the total number of features. Thus, it can be effectively generated only for datasets with modest number of features. Most interesting machine learning problems involve learning over tens of thousands to billions of features, for which maintaining a combiner matrix is clearly not feasible.\nWe solve this problem through dimensionality reduction. Johnson-Lindenstrauss (JL) lemma Johnson & Lindenstrauss (1984) allows us to project a set of vectors from a high-dimensional space to a random low-dimensional space while preserving distances. We use this property to reduce the size of the combiner matrix without losing the fidelity of the computation \u2014 our parallel algorithm produces the same result as the sequential SGD in expectation.\nOf course, a randomized SGD algorithm that generates the exact result in expectation is only useful if the resulting variance is small enough to maintain accuracy and the rate of convergence. We observe that for the variance to be small, the combiner matrix should have small singular values. Interestingly, the combiner matrix resulting from SGD is dominated by the diagonal entries as the learning rate has to be small for effective learning. We use this property to perform the JL projection only after subtracting the identity matrix. Also, other factors that control the singular values are the learning rate, number of processors, and the frequency of combining local models. This paper explores this design space and demonstrates the feasibility of efficient parallelization of SGD that retains the convergence properties of sequential SGD while enjoying parallel scalability.\n3 PARALLEL SYMSGD ALGORITHM\nConsider a training dataset (Xn\u00d7f , yn\u00d71), where f is the number of features, n is the number of examples in the dataset, the ith row of matrix X , Xi, represents the features of the ith example, and yi is the dependent value (or label) of that example. A linear model seeks to find a\nw\u2217 = arg min w\u2208Rf n\u2211 i=0 Q(Xi \u00b7 w, yi)\nthat minimizes an error function Q. For linear regression, Q(Xi \u00b7 w, yi) = (Xi \u00b7 w \u2212 yi)2. When (Xi, yi) is evident from the context, we will simply refer to the error function as Qi(w).\nSGD iteratively finds w\u2217 by updating the current model w with a gradient of Qr(w) for a randomly selected example r. For the linear regression error function above, this amounts to the update\nwi = wi\u22121 \u2212 \u03b1\u2207Qr(wi\u22121) = wi\u22121 \u2212 \u03b1(Xr \u00b7 wi\u22121 \u2212 yr)XTr (1) Here, \u03b1 is the learning rate that determines the magnitude of the update along the gradient. As it is clear from this equation, wi is dependent on wi\u22121 which creates a loop-carried dependence and consequently makes parallelization of SGD across iterations using na\u0131\u0308ve approaches impossible.\nThe complexity of SGD for each iteration is as follows. Assume thatXr has z non-zeros. Therefore, the computation in Equation 1 requires O(z) amount of time for the inner product computation, Xr \u00b7wi\u22121, and the sameO(z) amount of time for scalar-vector multiplication, \u03b1(Xr \u00b7wi\u22121\u2212yr)XTr . If the updates to the weight vector happen in-place meaning thatwi andwi\u22121 share the same memory location, the computation in Equation 1 takes O(z) amount of time.\n3.1 SYMBOLIC STOCHASTIC GRADIENT DESCENT\nThis section explains a new approach to parallelize the SGD algorithm despite its loop-carried dependence. As shown in Figure 1, the basic idea is to start each processor (except the first) on a concrete model w along with a symbolic unknown \u2206w that captures the fact that the starting model can change based on the output of the previous processor. If the dependence on \u2206w is linear during an SGD update, which is indeed the case for linear regression, then the symbolic dependence on \u2206w on the final output can be captured by an appropriate matrix Ma\u2192b that is a function of the input examples Xa, . . . , Xb processed (ya, . . . , yb do not affect this matrix). Specifically, as Lemma A.1 in the Appendix shows, this combiner matrix is given by\nMa\u2192b = a\u220f i=b (I \u2212 \u03b1XTi \u00b7Xi) (2)\nIn effect, the combiner matrix above is the symbolic representation of how a \u2206w change in the input will affect the output of a processor. Ma\u2192b is referred by M when the inputs are not evident.\nThe parallel SGD algorithm works as follows (see Figure 1). In the learning phase, each processor i starting from w0, computes both a local model li and a combiner matrix Mi. In a subsequent reduction phase, each processor in turn computes its true output using\nwi = li +Mi \u00b7 (wi\u22121 \u2212 w0) (3) Lemma A.1 ensures that this combination of local models will produce the same output as what these processors would have generated had they run sequentially. We call such combiners sound.\nOne can compute a sound model combiner for other SGD algorithms provided the loop-carried dependence on w is linear. In other words, there should exist a matrix Ai and vector bi in iteration i such that wi = Ai \u00b7 wi\u22121 + bi. Note that Ai and bi can be nonlinear in terms of input datasets.\n3.2 DIMENSIONALITY REDUCTION OF A SOUND COMBINER\nThe combiner matrixM generate above can be quite large and expensive to compute. The sequential SGD algorithm maintains and updates the weight vector w, and thus requires O(f) space and time, where f is the number of features. In contrast, M is a f \u00d7 f matrix and consequently, the space and time complexity of parallel SGD is O(f2). In practice, this would mean that we would need O(f) processors to see constant speedups, an infeasible proposition particularly for datasets that can have thousands if not millions of features.\nSYMSGD resolves this issue by projecting M into a smaller space while maintaining its fidelity. This projection is inspired by the Johnson-Lindenstrauss (JL) lemma Johnson & Lindenstrauss (1984) and follows the treatment of Achlioptas Achlioptas (2001). Lemma 3.1. 1 Let A be a random f \u00d7 k matrix with\naij = dij/ \u221a k\nwhere aij is the element of A at the ith row and jth column and dij is independently sampled from a random distribution D with IE[D] = 0 and Var[D] = 1. Then\nIE[A \u00b7AT ] = If\u00d7f\nThe matrix A from Lemma 3.1 projects from Rf \u2192 Rk where k can be much smaller than f . This allows us to approximate Equation 3 as\nwi \u2248 li +Mi \u00b7A \u00b7AT (wi\u22121 \u2212 w0) (4) 1See proof in Appendix A.2.\nLemma 3.1 guarantees that the approximation above is unbiased.\nIE[li +Mi \u00b7A \u00b7AT (wi\u22121 \u2212 w0)] = li +Mi \u00b7 IE[A \u00b7AT ](wi\u22121 \u2212 w0) = wi\nThis allows an efficient algorithm that only computes the projected version of the combiner matrix while still producing the same answer as the sequential algorithm in expectation. We call such combiners probabilistically sound.\nAlgorithm 1: SYMSGD learning a local model and a model combiner. 1 <vector,matrix,matrix> SymSGD( 2 float \u03b1, vector: w0, X1..Xn, 3 scalar: y1..yn) { 4 vector w = w0; 5 matrix A = 1\u221a\nk random(D,f,k);\n6 matrix MA = A; 7 for i in (1..n) { 8 w = w - \u03b1(Xi\u00b7w - yi)XiT; 9 MA = MA - \u03b1 Xi\u00b7(XiTMA); }\n10 return <w,MA,A>; }\nAlgorithm 2: SYMSGD combining local models using model combiners. 1 vector SymSGDCombine(vector w0, 2 vector w, vector l, 3 matrix MA, matrix A) { 4 parallel { 5 matrix NA = MA - A; 6 w = l+w-w0+NA\u00b7AT(w-w0); 7 } 8 return w; }\nAlgorithm 1 shows the resulting symbolic SGD learner. The random function in line 5 returns a f \u00d7 k matrix with elements chosen independently from the random distribution D according to Lemma 3.1. When compared to the sequential SGD, the additional work is the computation of MA in Line 9. It is important to note that this algorithm maintains the invariant that MA = M \u00b7 A at every step. This projection incurs a space and time overhead of O(z \u00d7 k) where z is the number of non-zeros in Xi. This overhead is acceptable for small k and infact in our experiments in Section 4, k is between 7 to 15 across all benchmarks. Most of the overhead for such a small k is hidden by utilizing SIMD hardware within a processor (SymSGD with one thread is only half as slow as the sequential SGD as discussed in Section 4.1). After learning a local model and a probabilistically sound combiner in each processor, Algorithm 2 combines the resulting local model using the combiners, but additionally employs the optimizations discussed in Section 3.3.\nNote that the correctness and performance of SYMSGD do not depend on the sparsity of a dataset and as Section 4 demonstrates, it works for very sparse and completely dense datasets. Also, note that X1, . . . ,Xn may contain a subset of size f\u2019 of all f features. Our implementation of Algorithm 1 takes advantage of this property and allocates and initializes A for only the observed features. This optimization is omitted from the pseudo code in Algorithm 1 for the sake of simplicity.\n3.3 CONTROLLING THE VARIANCE\nWhile the dimensionality reduction discussed above is expected to produce the right answer, this is useful only if the variance of the approximation is acceptably small. Computing the variance is involved and is discussed in the associated technical report SymSGDTR. But we discuss the main result that motivates the rest of the paper.\nConsider the approximation ofM \u00b7\u2206w with v = M \u00b7A \u00b7AT \u00b7\u2206w. Let C(v) be the covariance matrix of v. The trace of the covariance matrix tr(C(v)) is the sum of the variance of individual elements of v. Let \u03bbi(M) by the ith eigenvalue of M and \u03c3i(M) = \u221a \u03bbi(MTM) the ith singular value of M . Let \u03c3max(M) be the maximum singular value of M . Then the following holds SymSGDTR:\n\u2016\u2206w\u201622 k \u2211 i \u03c32i (M) \u2264 tr(C(v)) \u2264 \u2016\u2206w\u201622 k ( \u2211 i \u03c32i (M) + \u03c3 2 max(M))\nThe covariance is small if k, the dimension of the projected space, is large. But increasing k proportionally increases the overhead of the parallel algorithm. Similarly, covariance is small if the projection happens on small \u2206w. Looking at Equation 4, this means that wi\u22121 should be as close to w0 as possible, implying that processors should communicate frequently enough such that their\nmodels are roughly in sync. Finally, the singular values of M should be as small as possible. The next section describes a crucial optimization that achieves this.\nTaking Identity Off: Expanding Equation 2, we see that the combiner matrices are of the form\nI \u2212 \u03b1R1 + \u03b12R2 \u2212 \u03b13R3 + \u00b7 \u00b7 \u00b7\nwhere Ri matrices are formed from the sum of products of Xj \u00b7 XTj matrices. Since \u03b1 is a small number, this sum is dominated by I . In fact, for a combiner matrix M generated from n examples, M \u2212 I has at most n non-zero singular values SymSGDTR. We use these observation to lower the variance of dimensionality reduction by projecting matrix N = M \u2212 I instead of M . Appendix A.3 empirically shows the impact of this optimization. Rewriting Equations 3 and 4, we have\nwi = li + (Ni + I) \u00b7 (wi\u22121 \u2212 w0) = li + wi\u22121 \u2212 w0 +Ni \u00b7 (wi\u22121 \u2212 w0) \u2248 li + wi\u22121 \u2212 w0 +Ni \u00b7A \u00b7AT \u00b7 (wi\u22121 \u2212 w0) (5)\nLemma 3.1 guarantees that the approximation above is unbiased. Algorithm 2 shows the pseudo code for the resulting probabilistically sound combination of local models. The function SymSGDCombine is called iteratively to combine the model of the first processor with the local models of the rest. Note that each model combination is executed in parallel (Line 4) by parallelizing the underlying linear algebra operations.\nAn important factor in controlling the singular values of N is the frequency of model combinations which is a tunable parameter in SYMSGD. As it is shown in Appendix A.3, the fewer the number of examples learned, the smaller the singular values of N and the less variance (error) in Equation 5.\nImplementation For the implementation of SymSGD function, matrix M and weight vector w are stored next to each other. This enables better utilization of vector units in the processor and improves the performance of our approach significantly. Also, most of datasets are sparse and therefore, SGD and SymSGD only copy the observed features from w0 to their learning model w. Moreover, for the implementation of matrix A, we used Achlioptas (2001) theorem to minimize the overhead of creating A. In this approach, each element of A is independently chosen from { 13 ,\u2212 1 3 , 0} with probability { 16 , 1 6 , 2 3}, respectively.\n4 EVALUATION\nAll experiments described in this section were performed on an Intel Xeon E5-2630 v3 machine clocked at 2.4 GHz with 256 GB of RAM. The machine has two sockets with 8 cores each, allowing us to study the scalability of the algorithms across sockets. We disabled hyper-threading and turbo boost. We also explicitly pinned threads to cores in a compact way which means that thread i + 1 was placed as close as possible to thread i. The machine runs Windows 10. All of our implementations were compiled with Intel C/C++ compiler 16.0 and relied heavily on OpenMP primitives for parallelization and MKL for efficient linear algebra computations. And, finally, to measure runtime, we use the average of five independent runs on an otherwise idle machine.\nThere are several algorithms and implementations that we used for our comparison: Vowpal Wabbit Langford et al. (2007), a widely used public library, Baseline, a fast sequential implementation, HW-Paper, the implementation from Recht et al. (2011), HW-Release, an updated version, HogWild, which runs Baseline in multiple threads without any synchronization, and ALLREDUCE, the implementation from Agarwal et al. (2014). Each of these algorithms have different parameters and settings and we slightly modified to ensure a fair comparison; see Appendix A.4 for more details.\nWhen studying the scalability of a parallel algorithm, it is important to compare the algorithms against an efficient baseline Bailey (1991); McSherry et al. (2015). Otherwise, it is empirically not possible to differentiate between the scalability achieved from the parallelization of the inefficiencies and the scalability inherent in the algorithm. We spent a significant effort to implement a well-tuned sequential algorithm which we call Baseline in our comparisons. Baseline is between 1.97 to 7.62 (3.64 on average) times faster than Vowpal Wabbit and it is used for all speedup graphs in this paper.\nDatasets Table 1 describes the datasets used for evaluation. The number of features, training instances, test instances, classes and the sparsity of each dataset is shown in Table 1. We used Vowpal\nWabbit with the configurations discussed in Appendix A.4 to measure the maximum accuracies that can be achieved using linear and logistic regression and the result is presented in columns 8 and 9 of Table 1. In the case of aloi dataset, even after 500 passes (the default for our evaluation was 100 passes) the accuracies did not saturate to the maximum possible and we reported that both linear and logistic achieved at least 80% accuracy. The last two columns show the maximum speedup of SYMSGD and HOGWILD! over the baseline.\nParameters Hyper-parameters searching is essential for performance and accuracy. The learning rate, \u03b1, for each dataset was selected by searching for a constant value among {.5, .05, .005, . . . } where Baseline reached close to maximum accuracy for each benchmark. The parameters for the projection size, k, and the frequency of model combination were searched to pick the best performing configuration. The parameters for ALLREDUCE were similarly searched.\n4.1 RESULTS\nFigure 2 shows the accuracy and speedup measurements on three benchmarks: rcv1.binary, a sparse binary dataset, rcv1.multiclass, a sparse multiclass dataset, and epsilon, a dense binary dataset. The results for the other six benchmarks are presented in Appendix A.5.\nSparse Binary, rcv1.binary: Figure 2a compares the scalability of all the algorithms studied in this paper. HW-Paper is around six times slower than HW-Release. While this could potentially be a result of us running HW-Release on a Ubuntu VM, our primary aim of this comparison was to ensure that HogWild is a competitive implementation of HOGWILD!. Thus, we remove HW-Paper and HW-Release in our subsequent comparisons.\nSYMSGD is half as slow as the Baseline on one thread as it performs lot more computation, but scales to a 3.5\u00d7 speedup to 16 cores. Note, this represents a roughly 7\u00d7 strong-scaling speedup with respect to its own performance on one thread. Analysis of the hardware performance counters shows the current limit to SYMSGD\u2019s scalability arises from load-imbalance across barrier synchronization, which provides an opportunity for future improvement.\nFigure 2d shows the accuracy as a function of the number of examples processed by different algorithms. SYMSGD \u201cstutters\u201d at the beginning, but it too matches the accuracy of Baseline. The initial stuttering happens because the magnitude of the local models on each processor are large during the first set of examples. This directly affect the variance of the combiner matrix approximation. However, as more examples are given to SYMSGD, the magnitude of the local models are smaller and thus SYMSGD better matches the Baseline accuracy. One way to avoid this stuttering is to combine models more frequently (lower variance) or running single threaded for the first few iterations.\nHogWild does approach sequential accuracy, however, it does so at the cost of scalablity (i.e., see Figure 2a (a)). Likewise, ALLREDUCE scales slightly better but does so at the cost of accuracy.\nSparse Multiclass, rcv1.multiclass: Figure 2b shows the scalability on rcv1.multiclass. Since this is a multiclass dataset, SYMSGD is competitive with the baseline on one thread as it is able to amortize the combiner matrix computation across all of the classes (M is the same across different classes). Thus, it enjoys much better scalability of 7\u00d7 when compared to rcv1.binary. HogWild scales similar to SYMSGD up-to 8 threads but suffers when 16 threads across multiple sockets are used. Figure 2e shows that SYMSGD meets the sequential accuracy after an initial stutter. ALLREDUCE suffers from accuracy.\nDense Binary, epsilon: Figure 2c in Appendix A.5 shows that SYMSGD achieves a 7\u00d7 speedup over the baseline on 16 cores. This represents a 14\u00d7 strong scaling speedup over SYMSGD on one thread. As HOGWILD! is not designed for dense workloads, its speedup suffers when 16 cores across multiple sockets are used. This shows that SYMSGD scales to both sparse and dense datasets. Similarly, ALLREDUCE suffers from accuracy.\n5 RELATED WORK\nMost schemes for parallelizing SGD learn local models independently and communicate to update the global model. The algorithms differ in how and how often the update is performed. These choices determine the applicability of the algorithm to shared-memory or distributed systems.\nTo the best of our knowledge, our approach is the only one that retain the semantics of the sequential SGD algorithm. While some prior work provides theoretical analysis of the convergence rates that justify a specific parallelization, convergence properties of SYMSGD simply follow from the sequential SGD algorithm. On the other hand, SYMSGD is currently restricted to class of SGD computations where the inter-step dependence is linear in the model parameters.\nGiven a tight coupling of the processing units, Langford et al. Langford et al. (2009) suggest on a round-robin scheme to update the global model allowing for some staleness. However, as the SGD computation per example is usually much smaller when compared to the locking overhead, HOGWILD! Recht et al. (2011) improves on this approach to perform the update in a \u201cracy\u201d manner. While HOGWILD! is theoretically proven to achieve good convergence rates provided the dataset is sparse enough and the processors update the global model fast enough, our experiments show that the generated cache-coherence traffic limits its scalability particularly across multiple sockets. Moreover, as HOGWILD! does not update the model atomically, it potentially loses correlation among more frequent features resulting in loss of accuracy. Lastly, unlike SYMSGD, which works for both sparse and dense datasets, HOGWILD! is expclitly designed for sparse data. Recently, Sallinen et al. (2016) proposed applying lock-free HOGWILD! approach to mini-batch. However, mini-batch converges slower than SGD and also they did not study multi-socket scaling.\nZinkevich et al. Zinkevich et al. (2010) propose a MapReduce-friendly framework for SGD. The basic idea is for each machine/thread to run a sequential SGD on its local data. At the end, the global model is obtained by averaging these local models. Alekh et al. Agarwal et al. (2014) extend this approach by using MPI AllReduce operation. Additionally, they use the adagrad Duchi et al. (2011) approach for the learning rates at each node and use weighted averaging to combine local models\nwith processors that processed a feature more frequently having a larger weight. Our experiments on our datasets and implementation shows that it does not achieve the sequential accuracy.\nSeveral distributed frameworks for machine learning are based on parameter server Li et al. (2014b;a) where clients perform local learning and periodically send the changes to a central parameter server that applies the changes. For additional parallelism, the models themselves can be split across multiple servers and clients only contact a subset of the servers to perform their updates.\n6 CONCLUSION\nWith terabytes of memory available on multicore machines today, our current implementation has the capability of learning from large datasets without incurring the communication overheads of a distributed system. That said, we believe the ideas in this paper apply to distributed SGD algorithms and how to pursue in future work.\nMany machine learning SGD algorithms require a nonlinear dependence on the parameter models. While SYMSGD does not directly apply to such algorithms, it is an interesting open problem to devise linear approximations (say using Taylor expansion) to these problems and subsequently parallelize with probabilistically sound combiners. This is an interesting study for future work.\nA APPENDIX\nA.1 COMBINER MATRIX\nLemma A.1. If the SGD algorithm for linear regression processes examples (Xa, ya), (Xa+1, ya+1), . . . , (Xb, yb) starting from model ws to obtain wb, then its outcome starting on model ws + \u2206w is given by wb + Ma\u2192b \u00b7 \u2206w where the combiner matrix Ma\u2192b is given by\nMa\u2192b = a\u220f i=b (I \u2212 \u03b1XTi \u00b7Xi)\nProof. The proof follows from a simple induction. Starting from ws, let the models computed by SGD after processing (Xa, ya), (Xa+1, ya+1), . . . , (Xb, yb) respectively be wa, wa+1, . . . wb. Consider the base case of processing of (Xa, ya). Starting from ws + \u2206w, SGD computes the model w\u2032a using Equation 1 (reminder: wi = wi\u22121 \u2212 \u03b1(Xi \u00b7 wi\u22121 \u2212 yi)XTi ):\nw\u2032a = ws + \u2206w \u2212 \u03b1(Xa \u00b7 (ws + \u2206w)\u2212 ya)XTa = ws + \u2206w \u2212 \u03b1(Xa \u00b7 ws \u2212 ya)XTa \u2212 \u03b1(Xa \u00b7\u2206w)XTa = ws \u2212 \u03b1(Xa \u00b7 ws \u2212 ya)XTa + \u2206w \u2212 \u03b1(Xa \u00b7\u2206w)XTa = wa + \u2206w \u2212 \u03b1(Xa \u00b7\u2206w)XTa (6) = wa + \u2206w \u2212 \u03b1XTa (Xa \u00b7\u2206w) (7) = wa + \u2206w \u2212 \u03b1(XTa \u00b7Xa) \u00b7\u2206w (8) = wa + (I \u2212 \u03b1XTa \u00b7Xa) \u00b7\u2206w\nStep 6 uses Equation 1, Step 7 uses the fact that Xa \u00b7\u2206w is a scalar (allowing it to be rearranged), and Step 8 follows from associativity property of matrix multiplication.\nThe induction is very similar and follows from replacing \u2206w with Ma\u2192i\u22121\u2206w and the property that\nMa\u2192i = (I \u2212 \u03b1XTi \u00b7Xi) \u00b7Ma\u2192i\u22121\nA.2 PROOF OF LEMMA 3.1 Proof. Let\u2019s call B = A \u00b7 AT . Then bij , the element of B at row i and column j, is \u2211\ns aisajs. Therefore, IE[bij ] = \u2211k s=1 IE[aisajs] = ( 1\u221a k )2 \u2211k s=1 IE[disdjs] = 1 k \u2211k s=1 IE[disdjs]. For i 6= j, IE[bij ] = 1 k \u2211k s=1 IE[dis]IE[djs] because dij are chosen independently. Since IE[D] = 0 and dis, djs \u2208 D, IE[dis] = IE[djs] = 0 and consequently, IE[bij ] = 0. For i = j, IE[bii] = 1 k \u2211 s IE[disdis] = 1 k \u2211 s IE[d 2 is]. Since IE[D\n2] = 1 and dis \u2208 D, IE[d2is] = 1. As a result, IE[bii] = 1 k \u2211k s=1 IE[d 2 is] = 1 k \u2211k s=1 1 = 1.\nA.3 EMPIRICAL EVALUATING SINGULAR VALUES OF M\nFigure 3 empirically demonstrates the benefit of taking identity off. This figure plots the singular values of M for the rcv1.binary dataset (described in Section 4) after processing 64, 128, 192, 256 examples for two different learning rates. As it can be seen, the singular values are close to 1. However, the singular values of N = M \u2212 I are roughly the same as those of M minus 1 and consequently, are small. Finally, the smaller \u03b1, the closer the singular values of M are to 1 and the singular values of N are to 0. Also, note that the singular values of M decrease as the numbers of examples increase and therefore, the singular values of N increase. As a result, the more frequent the models are combined, the less variance (and error) is introduced into Equation 5.\nA.4 ALGORITHM DETAILS AND SETTINGS\nThis section provides details of all algorithms we used in this paper. Each algorithm required slight modification to ensure fair comparison.\nVowpal Wabbit: Vowpal Wabbit Langford et al. (2007) is one of the widely used public libraries for machine learning algorithms. We used this application as a baseline for accuracy of different datasets and as a comparison of logistic and linear regression and also an independent validation of the learners without any of our implementation bias. Vowpal Wabbit applies accuracy-improving optimizations such as adaptive and individual learning steps or per feature normalize updates. While all of these optimizations are applicable to SYMSGD, we avoided them since the focus of this paper is the running time performance of our learner. The non-default flags that we used are: --sgd, --power t 0, --holdout off, --oaa nc for multiclass datasets where nc is the number of classes, --loss function func where func is squared or logistic. For learning rate, we searched for \u03b1, the learning rate, in the set of {.1, .5, .01, .05, .001, .005, . . . } and used --learning rate \u03b1. We went through dataset 100 times for each dataset (--passes 100) and saved the learned model after each pass (--save per pass). At the end, for linear and logistic regressions, we reported the maximum accuracies achieved among different passes and different learning rates.\nBaseline: Baseline uses a mixture of MKL Intel and manually vectorized implementations of linear algebra primitives in order to deliver the fastest performance. Baseline processes up-to 3.20 billion features per second at 6.4 GFLOPS.\nHOGWILD!: HOGWILD! Recht et al. (2011) is a lock-free approach to parallelize SGD where multiple thread apply Equation 1 simultaneously. Although this approach may have race condition\nwhen two threads process instances with a shared feature but the authors discuss that this does not hurt the accuracy significantly for sparse datasets. There are multiple implementations of this approach that we studied and evaluated in this section. Below is a description of each:\n\u2022 HW-Paper: This is the implementation used to report the measurements in Recht et al. (2011) which is publicly available Hogwild. This code implements SVM algorithm. Therefore, we modified the update rule to linear regression. The modified code was compiled and run on our Windows machine described above using an Ubuntu VM since the code is configured for Linux systems.\n\u2022 HW-Release: This is an optimized implementation that the authors built after the HOGWILD! paper Recht et al. (2011) was published. Similar to HW-Paper, we changed the update rule accordingly and executed it on the VM.\n\u2022 HogWild: We implemented this version which runs Baseline by multiple threads without any synchronizations. This code runs natively on Windows and enjoys all the optimizations applied to our Baseline such as call to MKL library and manual vectorization of linear algebra primitives.\nALLREDUCE: ALLREDUCE Agarwal et al. (2014) is an approach where each thread makes a copy from the global model and applies the SGD update rule to the local model for certain number of instances. Along with the local model, another vector g is computed which indicates the confidence in an update for the weight of a feature in the local model. After the learning phase, the local weight vectors are averaged based on the confidence vectors from each thread. We implemented this approach similarly using MKL calls and manual vectorization.\nA.5 SPEEUPS ON REMAINING BENCHMARKS\n", "cats": {"READ": 1.0, "DONTREAD": 0.0}, "reviews": [{"IS_META_REVIEW": true, "comments": "This paper describes a correction technique to combine updates from multiple SGD to make it statistically equivalent to sequential technique.\nComments\n1) The proposed method is novel and interesting to allow update to be corrected even when the update is delayed.\n2) The proposed theory can only be applied to square loss setting (with linear update rule), making it somewhat limited. This paper would be much more interesting to ICLR community, if the technique is applicable to general objective function and settings of deep neural networks.\n3) The resulting technique requires book-keeping of a dimensional reduced combiner matrix, which causes more computation in terms of complexity. The authors argue that the overhead can be canceled with SIMD support for symbolic update. However, the normal update of SGD might also benefit from SIMD, especially when the dataset is dense.\n\nOverall, even though the practical value of this work is limited by 2) and 3), the technique(specifically the correction rule) proposed in the paper could be of interest to people scaling up learning. I would encourage the author to extend the method to the cases of non-linear objective function which could make it more interesting to the ICLR community"}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "The reviewers largely agree that this paper is well written and presents an interesting, novel approach to parallelizing Stochastic Gradient Descent. However, the current formulation is restricted to linear regression models and requires sketching techniques to handle large number of features, although it is striking that very aggressive sketching (k~10) still works well. In this setting though, there are specialized randomized solvers such as Blendenpick (Avron et al) which sketch the data upfront to construct a high quality pre-conditioner for use with iterative methods. \n The authors are encouraged to either compare with state of the art parallel randomized least squares solvers developed in the numerical linear algebra community (see papers by Michael Mahoney, Petros Drineas and others), or broaden the scope of the proposed methods to include models of current interest (e.g., DNNs). The latter would of course be of specific interest to the ICLR community.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"TITLE": "Why we can have speedup is unclear", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "This paper propose a parallel mechanism for stochastic gradient descent method (SGD) in case of gradient can be computed via linear operations (including least square linear regression and polynomial regression problems). The motivation is to recover the same effect compared with sequential SGD, by using a proposed sound combiner. To make such combiner more efficient, the authors also use a randomized projection matrix to do dimension reduction. Experiments shows the proposed method has better speedup than previous methods like Hogwild! and Allreduce.\n\nI feel that there might be some fundamental misunderstanding on SGD.\n\n''The combiner matrixM  generate above can be quite large and expensive to compute. The sequential SGD algorithm maintains and updates the weight vector w , and thus requires O(f)  space and time, where f  is the number of features. In contrast,M  is a f f  matrix and consequently, the space and time complexity of parallel SGD is O(f^2) . In practice, this would mean that we would need O(f) processors to see constant speedups, an infeasible proposition particularly for datasets that can have\nthousands if not millions of features.\"\n\nI do not think one needs O(f^2) space and complexity for updating M_i * v, where v is an f-dimensional vector. Note that M_i is a low rank matrix in the form of (I - a_i a_i'). The complexity and space can be reduced to O(f) if compute it by O(v - a_i (a_i' v)) equivalently. If M_i is defined in the form of the product of n number of rank 1 matrices. The complexity and space complexity is O(fn). In the context of this paper, n should be much smaller than f. I seriously doubt that all author's assumptions, experiments, and strategies in this paper are based on this incorrect assumption on space and complexity of SGD. \n\nWhy one can have speedup is unclear for me. It is unclear what computations are in parallel and why this sequential algorithms can bring speedup if M_i*v is computed in the most efficient way.\nI suggest authors to make the following changes to make this paper more clear and theoretically solid\n- provide computational complexity per step of the proposed algorithm\n- convergence rate analysis (convergence analysis is not enough): we would like to see how the dimension reduction can affect the complexity.\n", "IS_META_REVIEW": false, "RECOMMENDATION": 4, "DATE": "19 Dec 2016", "REVIEWER_CONFIDENCE": 5}, {"TITLE": "No Title", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "Overall, the idea in this paper is interesting and the paper is well-written and well-motivated.  However, I think it is not ready to publish in ICLR for the following reasons:\n\n- This paper is not related to representation learning. It may be more suitable for a general machine learning or data mining conference. \n\n- The proposed approach can only work for a small class of models and cannot apply to popular formulations,  such as SVM, logistic regression, and neural network. It is unclear why we want to use SGD for this specific type of formulations. For model like linear regression, the authors should compare their methods with linear programming approaches. Also, it is unclear why we need to develope parallel algorithm for linear regressio problems as they are relatively easy to solve unless the data are big (see next comment). \n\n- The dataset used in the paper are relatively small and can be only used for proving the concept. Most datasets considered in the paper can be solved in a few second using a single core CPU. Hogwild! is suitable for sparse dataset because of its asynchronized nature. On data that are very sparse, the proposed approach is only slightly better or is worse than Hogwild. For dense dataset, it is unclear why we need to use SYMSGD instead of simply parallelizing the gradient computation using GPUs. Put them together, the experiment results are not convincing. \n", "IS_META_REVIEW": false, "RECOMMENDATION": 4, "DATE": "17 Dec 2016", "REVIEWER_CONFIDENCE": 5}, {"TITLE": "review for Parallel Stochastic Gradient Descent with Sound Combiner", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "This paper describes a correction technique to combine updates from multiple SGD to make it statistically equivalent to sequential technique.\nComments\n1) The proposed method is novel and interesting to allow update to be corrected even when the update is delayed.\n2) The proposed theory can only be applied to square loss setting (with linear update rule), making it somewhat limited. This paper would be much more interesting to ICLR community, if the technique is applicable to general objective function and settings of deep neural networks.\n3) The resulting technique requires book-keeping of a dimensional reduced combiner matrix, which causes more computation in terms of complexity. The authors argue that the overhead can be canceled with SIMD support for symbolic update. However, the normal update of SGD might also benefit from SIMD, especially when the dataset is dense.\n\nOverall, even though the practical value of this work is limited by 2) and 3), the technique(specifically the correction rule) proposed in the paper could be of interest to people scaling up learning. I would encourage the author to extend the method to the cases of non-linear objective function which could make it more interesting to the ICLR community\n\n\n", "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "16 Dec 2016 (modified: 17 Dec 2016)", "REVIEWER_CONFIDENCE": 4}, {"DATE": "03 Dec 2016", "TITLE": "convergence of asynchronous parallel algorithms", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1"}, {"DATE": "01 Dec 2016", "TITLE": "How is sparse algorithm implemented", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3"}, {"IS_META_REVIEW": true, "comments": "This paper describes a correction technique to combine updates from multiple SGD to make it statistically equivalent to sequential technique.\nComments\n1) The proposed method is novel and interesting to allow update to be corrected even when the update is delayed.\n2) The proposed theory can only be applied to square loss setting (with linear update rule), making it somewhat limited. This paper would be much more interesting to ICLR community, if the technique is applicable to general objective function and settings of deep neural networks.\n3) The resulting technique requires book-keeping of a dimensional reduced combiner matrix, which causes more computation in terms of complexity. The authors argue that the overhead can be canceled with SIMD support for symbolic update. However, the normal update of SGD might also benefit from SIMD, especially when the dataset is dense.\n\nOverall, even though the practical value of this work is limited by 2) and 3), the technique(specifically the correction rule) proposed in the paper could be of interest to people scaling up learning. I would encourage the author to extend the method to the cases of non-linear objective function which could make it more interesting to the ICLR community"}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "The reviewers largely agree that this paper is well written and presents an interesting, novel approach to parallelizing Stochastic Gradient Descent. However, the current formulation is restricted to linear regression models and requires sketching techniques to handle large number of features, although it is striking that very aggressive sketching (k~10) still works well. In this setting though, there are specialized randomized solvers such as Blendenpick (Avron et al) which sketch the data upfront to construct a high quality pre-conditioner for use with iterative methods. \n The authors are encouraged to either compare with state of the art parallel randomized least squares solvers developed in the numerical linear algebra community (see papers by Michael Mahoney, Petros Drineas and others), or broaden the scope of the proposed methods to include models of current interest (e.g., DNNs). The latter would of course be of specific interest to the ICLR community.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"TITLE": "Why we can have speedup is unclear", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "This paper propose a parallel mechanism for stochastic gradient descent method (SGD) in case of gradient can be computed via linear operations (including least square linear regression and polynomial regression problems). The motivation is to recover the same effect compared with sequential SGD, by using a proposed sound combiner. To make such combiner more efficient, the authors also use a randomized projection matrix to do dimension reduction. Experiments shows the proposed method has better speedup than previous methods like Hogwild! and Allreduce.\n\nI feel that there might be some fundamental misunderstanding on SGD.\n\n''The combiner matrixM  generate above can be quite large and expensive to compute. The sequential SGD algorithm maintains and updates the weight vector w , and thus requires O(f)  space and time, where f  is the number of features. In contrast,M  is a f f  matrix and consequently, the space and time complexity of parallel SGD is O(f^2) . In practice, this would mean that we would need O(f) processors to see constant speedups, an infeasible proposition particularly for datasets that can have\nthousands if not millions of features.\"\n\nI do not think one needs O(f^2) space and complexity for updating M_i * v, where v is an f-dimensional vector. Note that M_i is a low rank matrix in the form of (I - a_i a_i'). The complexity and space can be reduced to O(f) if compute it by O(v - a_i (a_i' v)) equivalently. If M_i is defined in the form of the product of n number of rank 1 matrices. The complexity and space complexity is O(fn). In the context of this paper, n should be much smaller than f. I seriously doubt that all author's assumptions, experiments, and strategies in this paper are based on this incorrect assumption on space and complexity of SGD. \n\nWhy one can have speedup is unclear for me. It is unclear what computations are in parallel and why this sequential algorithms can bring speedup if M_i*v is computed in the most efficient way.\nI suggest authors to make the following changes to make this paper more clear and theoretically solid\n- provide computational complexity per step of the proposed algorithm\n- convergence rate analysis (convergence analysis is not enough): we would like to see how the dimension reduction can affect the complexity.\n", "IS_META_REVIEW": false, "RECOMMENDATION": 4, "DATE": "19 Dec 2016", "REVIEWER_CONFIDENCE": 5}, {"TITLE": "No Title", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "Overall, the idea in this paper is interesting and the paper is well-written and well-motivated.  However, I think it is not ready to publish in ICLR for the following reasons:\n\n- This paper is not related to representation learning. It may be more suitable for a general machine learning or data mining conference. \n\n- The proposed approach can only work for a small class of models and cannot apply to popular formulations,  such as SVM, logistic regression, and neural network. It is unclear why we want to use SGD for this specific type of formulations. For model like linear regression, the authors should compare their methods with linear programming approaches. Also, it is unclear why we need to develope parallel algorithm for linear regressio problems as they are relatively easy to solve unless the data are big (see next comment). \n\n- The dataset used in the paper are relatively small and can be only used for proving the concept. Most datasets considered in the paper can be solved in a few second using a single core CPU. Hogwild! is suitable for sparse dataset because of its asynchronized nature. On data that are very sparse, the proposed approach is only slightly better or is worse than Hogwild. For dense dataset, it is unclear why we need to use SYMSGD instead of simply parallelizing the gradient computation using GPUs. Put them together, the experiment results are not convincing. \n", "IS_META_REVIEW": false, "RECOMMENDATION": 4, "DATE": "17 Dec 2016", "REVIEWER_CONFIDENCE": 5}, {"TITLE": "review for Parallel Stochastic Gradient Descent with Sound Combiner", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "This paper describes a correction technique to combine updates from multiple SGD to make it statistically equivalent to sequential technique.\nComments\n1) The proposed method is novel and interesting to allow update to be corrected even when the update is delayed.\n2) The proposed theory can only be applied to square loss setting (with linear update rule), making it somewhat limited. This paper would be much more interesting to ICLR community, if the technique is applicable to general objective function and settings of deep neural networks.\n3) The resulting technique requires book-keeping of a dimensional reduced combiner matrix, which causes more computation in terms of complexity. The authors argue that the overhead can be canceled with SIMD support for symbolic update. However, the normal update of SGD might also benefit from SIMD, especially when the dataset is dense.\n\nOverall, even though the practical value of this work is limited by 2) and 3), the technique(specifically the correction rule) proposed in the paper could be of interest to people scaling up learning. I would encourage the author to extend the method to the cases of non-linear objective function which could make it more interesting to the ICLR community\n\n\n", "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "16 Dec 2016 (modified: 17 Dec 2016)", "REVIEWER_CONFIDENCE": 4}, {"DATE": "03 Dec 2016", "TITLE": "convergence of asynchronous parallel algorithms", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1"}, {"DATE": "01 Dec 2016", "TITLE": "How is sparse algorithm implemented", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3"}]}
{"text": "MENTS THROUGH CORRUPTION\n1 INTRODUCTION\nText understanding starts with the challenge of finding machine-understandable representation that captures the semantics of texts. Bag-of-words (BoW) and its N-gram extensions are arguably the most commonly used document representations. Despite its simplicity, BoW works surprisingly well for many tasks (Wang & Manning, 2012). However, by treating words and phrases as unique and discrete symbols, BoW often fails to capture the similarity between words or phrases and also suffers from sparsity and high dimensionality.\nRecent works on using neural networks to learn distributed vector representations of words have gained great popularity. The well celebrated Word2Vec (Mikolov et al., 2013a), by learning to predict the target word using its neighboring words, maps words of similar meanings to nearby points in the continuous vector space. The surprisingly simple model has succeeded in generating high-quality word embeddings for tasks such as language modeling, text understanding and machine translation. Word2Vec naturally scales to large datasets thanks to its simple model architecture. It can be trained on billions of words per hour on a single machine.\nParagraph Vectors (Le & Mikolov, 2014) generalize the idea to learn vector representation for documents. A target word is predicted by the word embeddings of its neighbors in together with a unique document vector learned for each document. It outperforms established document representations, such as BoW and Latent Dirichlet Allocation (Blei et al., 2003), on various text understanding tasks (Dai et al., 2015). However, two caveats come with this approach: 1) the number of parameters grows with the size of the training corpus, which can easily go to billions; and 2) it is expensive to generate vector representations for unseen documents at test time.\nWe propose an efficient model architecture, referred to as Document Vector through Corruption (Doc2VecC), to learn vector representations for documents. It is motivated by the observation that linear operations on the word embeddings learned by Word2Vec can sustain substantial amount of syntactic and semantic meanings of a phrase or a sentence (Mikolov et al., 2013b). For example, vec(\u201cRussia\u201d) + vec(\u201criver\u201d) is close to vec(\u201cVolga River\u201d) (Mikolov & Dean, 2013), and\nvec(\u201cking\u201d) - vec(\u201cman\u201d) + vec(\u201cwomen\u201d) is close to vec(\u201cqueen\u201d) (Mikolov et al., 2013b). In Doc2VecC, we represent each document as a simple average of the word embeddings of all the words in the document. In contrast to existing approaches which post-process learned word embeddings to form document representation (Socher et al., 2013; Mesnil et al., 2014), Doc2VecC enforces a meaningful document representation can be formed by averaging the word embeddings during learning. Furthermore, we include a corruption model that randomly remove words from a document during learning, a mechanism that is critical to the performance and learning speed of our algorithm.\nDoc2VecC has several desirable properties: 1. The model complexity of Doc2VecC is decoupled from the size of the training corpus, depending only on the size of the vocabulary; 2. The model architecture of Doc2VecC resembles that of Word2Vec, and can be trained very efficiently; 3. The new framework implicitly introduces a data-dependent regularization, which favors rare or informative words and suppresses words that are common but not discriminative; 4. Vector representation of a document can be generated by simply averaging the learned word embeddings of all the words in the document, which significantly boost test efficiency; 5. The vector representation generated by Doc2VecC matches or beats the state-of-the-art for sentiment analysis, document classification as well as semantic relatedness tasks.\n2 RELATED WORKS AND NOTATIONS\nText representation learning has been extensively studied. Popular representations range from the simplest BoW and its term-frequency based variants (Salton & Buckley, 1988), language model based methods (Croft & Lafferty, 2013; Mikolov et al., 2010; Kim et al., 2015), topic models (Deerwester et al., 1990; Blei et al., 2003), Denoising Autoencoders and its variants (Vincent et al., 2008; Chen et al., 2012), and distributed vector representations (Mesnil et al., 2014; Le & Mikolov, 2014; Kiros et al., 2015). Another prominent line of work includes learning task-specific document representation with deep neural networks, such as CNN (Zhang & LeCun, 2015) or LSTM based approaches (Tai et al., 2015; Dai & Le, 2015).\nIn this section, we briefly introduce Word2Vec and Paragraph Vectors, the two approaches that are most similar to ours. There are two well-know model architectures used for both methods, referred to as Continuous Bag-of-Words (CBoW) and Skipgram models (Mikolov et al., 2013a). In this work, we focus on CBoW. Extending to Skipgram is straightforward. Here are the notations we are going to use throughout the paper:\nD = {D1, \u00b7 \u00b7 \u00b7 , Dn}: a training corpus of size n, in which each document Di contains a variablelength sequence of words w1i , \u00b7 \u00b7 \u00b7 , w Ti i ;\nV : the vocabulary used in the training corpus, of sizes v;\nx \u2208 Rv\u00d71: BoW of a document, where xj = 1 iff word j does appear in the document. ct \u2208 Rv\u00d71: BoW of the local context wt\u2212k, \u00b7 \u00b7 \u00b7 , wt\u22121, wt+1, \u00b7 \u00b7 \u00b7 , wt+k at the target position t. ctj = 1 iff word j appears within the sliding window of the target;\nU \u2208 Rh\u00d7v: the projection matrix from the input space to a hidden space of size h. We use uw to denote the column in U for word w, i.e., the \u201cinput\u201c vector of word w;\nV> \u2208 Rv\u00d7h: the projection matrix from the hidden space to output. Similarly, we use vw to denote the column in V for word w, i.e., the \u201coutput\u201c vector of word w.\nWord2Vec. Word2Vec proposed a neural network architecture of an input layer, a projection layer parameterized by the matrix U and an output layer by V>. It defines the probability of observing the target word wt in a document D given its local context ct as\nP (wt|ct) = exp(v>wtUc t)\u2211 w\u2032\u2208V exp(v > w\u2032Uc t)\nThe word vectors are then learned to maximize the log likelihood of observing the target word at each position of the document. Various techniques (Mitchell & Lapata, 2010; Zanzotto et al., 2010; Yessenalina & Cardie, 2011; Grefenstette et al., 2013; Socher et al., 2013; Kusner et al., 2015)\nhave been studied to generate vector representations of documents from word embeddings, among which the simplest approach is to use weighted average of word embeddings. Similarly, our method forms document representation by averaging word embeddings of all the words in the document. Differently, as our model encodes the compositionality of words in the learned word embeddings, heuristic weighting at test time is not required.\nParagraph Vectors. Paragraph Vectors, on the other hands, explicitly learns a document vector with the word embeddings. It introduces another projection matrix D \u2208 Rh\u00d7n. Each column of D acts as a memory of the global topic of the corresponding document. It then defines the probability of observing the target word wt in a document D given its local context ct as\nP (wt|ct,d) = exp(v>wt(Uc t + d))\u2211 w\u2032\u2208V exp(v > w\u2032(Uc t + d))\nwhere d \u2208 D is the vector representation of the document. As we can see from this formula, the complexity of Paragraph Vectors grows with not only the size of the vocabulary, but also the size of the training corpus. While we can reasonably limit the size of a vocabulary to be within a million for most datasets, the size of a training corpus can easily go to billions. What is more concerning is that, in order to come up with the vector representations of unseen documents, we need to perform an expensive inference by appending more columns to D and gradient descent on D while fixing other parameters of the learned model.\n3 METHOD\nSeveral works (Mikolov & Dean, 2013; Mikolov et al., 2013b) showcased that syntactic and semantic regularities of phrases and sentences are reasonably well preserved by adding or subtracting word embeddings learned through Word2Vec. It prompts us to explore the option of simply representing a document as an average of word embeddings. Figure 1 illustrates the new model architecture.\nSimilar to Word2Vec or Paragraph Vectors, Doc2VecC consists of an input layer, a projection layer as well as an output layer to predict the target word, \u201cceremony\u201d in this example. The embeddings of neighboring words (\u201copening\u201d, \u201cfor\u201d, \u201cthe\u201d) provide local context while the vector representation of the entire document (shown in grey) serves as the global context. In contrast to Paragraph Vectors, which directly learns a unique vector for each document, Doc2VecC represents each document as an average of the embeddings of words randomly sampled from the document (\u201cperformance\u201d at position p, \u201cpraised\u201d at position q, and \u201cbrazil\u201d at position r).\nHuang et al. (2012) also proposed the idea of using average of word embeddings to represent the global context of a document. Different from their work, we choose to corrupt the original document by randomly removing significant portion of words, and represent the document using only the embeddings of the words remained. This corruption mechanism offers us great speedup during training as it significantly reduces the number of parameters to update in back propagation. At the same time, as we are going to detail in the next section, it introduces a special form of regularization, which brings great performance improvement.\nHere we describe the stochastic process we used to generate a global context at each update. The global context, which we denote as x\u0303, is generated through a unbiased mask-out/drop-out corruption, in which we randomly overwrites each dimension of the original document x with probability q. To make the corruption unbiased, we set the uncorrupted dimensions to 1/(1 \u2212 q) times its original value. Formally,\nx\u0303d = { 0, with probability q xd 1\u2212q , otherwise\n(1)\nDoc2VecC then defines the probability of observing a target word wt given its local context ct as well as the global context x\u0303 as\nP (wt|ct, x\u0303) = exp(v>wt(\nlocal context\ufe37\ufe38\ufe38\ufe37 Uct + global context\ufe37 \ufe38\ufe38 \ufe37 1\nT Ux\u0303 ))\u2211\nw\u2032\u2208V exp(v > w\u2032 ( Uct + 1T Ux\u0303) ) (2) Here T is the length of the document. Exactly computing the probability is impractical, instead we approximate it with negative sampling (Mikolov et al., 2013a).\nf(w, c, x\u0303) \u2261 logP (wt|ct, x\u0303) \u2248 log \u03c3 ( v>w(Uc + 1\nT Ux\u0303)\n) + \u2211\nw\u2032\u223cPv\nlog \u03c3 ( \u2212v>w\u2032(Uc + 1\nT Ux\u0303)\n) (3)\nhere Pv stands for a uniform distribution over the terms in the vocabulary. The two projection matrices U and V are then learned to minimize the loss:\n` = \u2212 n\u2211\ni=1 Ti\u2211 t=1 f(wti , c t i, x\u0303 t i) (4)\nGiven the learned projection matrix U, we then represent each document simply as an average of the embeddings of the words in the document,\nd = 1\nT \u2211 w\u2208D uw. (5)\nWe are going to elaborate next why we choose to corrupt the original document with the corruption model in eq.(1) during learning, and how it enables us to simply use the average word embeddings as the vector representation for documents at test time.\n3.1 CORRUPTION AS DATA-DEPENDENT REGULARIZATION\nWe approximate the log likelihood for each instance f(w, c, x\u0303) in eq.(4) with its Taylor expansion with respect to x\u0303 up to the second-order (Van Der Maaten et al., 2013; Wager et al., 2013; Chen et al., 2014). Concretely, we choose to expand at the mean of the corruption \u00b5x = Ep(x\u0303|x)[x\u0303]:\nf(w, c, x\u0303) \u2248 f(w, c, \u00b5x) + (x\u0303\u2212 \u00b5x)>\u2207x\u0303f + 1\n2 (x\u0303\u2212 \u00b5x)>\u22072x\u0303f(x\u0303\u2212 \u00b5x)\nwhere \u2207x\u0303f and \u22072x\u0303f are the first-order (i.e., gradient) and second-order (i.e., Hessian) of the log likelihood with respect to x\u0303. Expansion at the mean \u00b5x is crucial as shown in the following steps. Let us assume that for each instance, we are going to sample the global context x\u0303 infinitely many times, and thus compute the expected log likelihood with respect to the corrupted x\u0303.\nEp(x\u0303|x)[f(w, c, x\u0303)] \u2248 f(w, c, \u00b5x) + 1 2 tr ( E[(x\u0303\u2212 x)(x\u0303\u2212 x)>]\u22072x\u0303f ) The linear term disappears as Ep(x\u0303|x)[x\u0303 \u2212 \u00b5x] = 0. We substitute in x for the mean \u00b5x of the corrupting distribution (unbiased corruption) and the matrix \u03a3x = E[(x\u0303 \u2212 \u00b5x)(x\u0303 \u2212 \u00b5x)>] for the variance, and obtain\nEp(x\u0303|x)[f(w, c, x\u0303)] \u2248 f(w, c,x) + 1 2 tr ( \u03a3x\u22072x\u0303f ) (6)\nAs each word in a document is corrupted independently of others, the variance matrix \u03a3x is simplified to a diagonal matrix with jth element equals q1\u2212qx 2 j . As a result, we only need to compute the diagonal terms of the Hessian matrix\u22072x\u0303f .\nThe jth dimension of the Hessian\u2019s diagonal evaluated at the mean x is given by\n\u22022f \u2202x2j = \u2212\u03c3w,c,x(1\u2212 \u03c3w,c,x)( 1 T v>wuj)\n2 \u2212 \u2211\nw\u2032\u223cPv\n\u03c3w\u2032,c,x(1\u2212 \u03c3w\u2032,c,x)( 1\nT v>w\u2032uj) 2\nPlug the Hessian matrix and the variance matrix back into eq.(6), and then back to the loss defined in eq.(4), we can see that Doc2VecC intrinsically minimizes\n` = \u2212 n\u2211\ni=1 Ti\u2211 t=1 f(wti , c t i,xi) + q 1\u2212 q v\u2211 j=1 R(uj) (7)\nEach f(wti , c t i,xi) in the first term measures the log likelihood of observing the target word w t i given its local context cti and the document vector di = 1 T Uxi. As such, Doc2VecC enforces that a document vector generated by averaging word embeddings can capture the global semantics of the document, and fill in information missed in the local context.\nThe second term here is a data-dependent regularization. The regularization on the embedding uj of each word j takes the following form, R(uj) \u221d n\u2211\ni=1 Ti\u2211 t=1 x2ij [ \u03c3wti ,cti,xi(1\u2212 \u03c3wti ,cti,xi)( 1 T v>wti uj) 2 + \u2211 w\u2032\u223cPv \u03c3w\u2032,cti,xi(1\u2212 \u03c3w\u2032,cti,xi)( 1 T v>w\u2032uj) 2 ] where \u03c3w,c,x = \u03c3(v>w(Uc+ 1 T Ux)) prescribes the confidence of predicting the target wordw given its neighboring context c as well as the document vector d = 1T Ux.\nClosely examining R(uj) leads to several interesting findings: 1. the regularizer penalizes more on the embeddings of common words. A word j that frequently appears across the training corpus, i.e, xij = 1 often, will have a bigger regularization than a rare word; 2. on the other hand, the regularization is modulated by \u03c3w,c,x(1 \u2212 \u03c3w,c,x), which is small if \u03c3w,c,x \u2192 1 or 0. In other words, if uj is critical to a confident prediction \u03c3w,c,x when it is active, then the regularization is diminished. Similar effect was observed for dropout training for logistic regression model (Wager et al., 2013) and denoising autoencoders (Chen et al., 2014).\n4 EXPERIMENTS\nWe evaluate Doc2VecC on a sentiment analysis task, a document classification task and a semantic relatedness task, along with several document representation learning algorithms. All experiments can be reproduced using the code available at https://github.com/mchen24/iclr2017\n4.1 BASELINES\nWe compare against the following document representation baselines: bag-of-words (BoW); Denoising Autoencoders (DEA) (Vincent et al., 2008), a representation learned from reconstructing original document x using corrupted one x\u0303. SDAs have been shown to be the state-of-the-art for sentiment analysis tasks (Glorot et al., 2011). We used Kullback-Liebler divergence as the reconstruction error and an affine encoder. To scale up the algorithm to large vocabulary, we only take into account the non-zero elements of x in the reconstruction error and employed negative sampling for the remainings; Word2Vec (Mikolov et al., 2013a)+IDF, a representation generated through weighted average of word vectors learned using Word2Vec; Doc2Vec (Le & Mikolov, 2014); Skip-thought Vectors(Kiros et al., 2015), a generic, distributed sentence encoder that extends the Word2Vec skipgram model to sentence level. It has been shown to produce highly generic sentence representations that apply to various natural language processing tasks. We also include RNNLM (Mikolov et al., 2010), a recurrent neural network based language model in the comparison. In the semantic relatedness task, we further compare to LSTM-based methods (Tai et al., 2015) that have been reported on this dataset.\n4.2 SENTIMENT ANALYSIS\nFor sentiment analysis, we use the IMDB movie review dataset. It contains 100,000 movies reviews categorized as either positive or negative. It comes with predefined train/test split (Maas et al., 2011): 25,000 reviews are used for training, 25,000 for testing, and the rest as unlabeled data. The two classes are balanced in the training and testing sets. We remove words that appear less than 10 times in the training set, resulting in a vocabulary of 43,375 distinct words and symbols.\nSetup. We test the various representation learning algorithms under two settings: one follows the same protocol proposed in (Mesnil et al., 2014), where representation is learned using all the available data, including the test set; another one where the representation is learned using training and unlabeled set only. For both settings, a linear support vector machine (SVM) (Fan et al., 2008) is trained afterwards on the learned representation for classification. For Skip-thought Vectors, we used the generic model1 trained on a much bigger book corpus to encode the documents. A vector of 4800 dimensions, first 2400 from the uni-skip model, and the last 2400 from the bi-skip model, are generated for each document. In comparison, all the other algorithms produce a vector representation of size 100. The supervised RNN-LM is learned on the training set only. The hyper-parameters are tuned on a validation set subsampled from the training set.\nAccuracy. Comparing the two columns in Table 1, we can see that all the representation learning algorithms benefits from including the testing data during the representation learning phrase. Doc2VecC achieved similar or even better performance than Paragraph Vectors. Both methods outperforms the other baselines, beating the BOW representation by 15%. In comparison with Word2Vec+IDF, which applies post-processing on learned word embeddings to form document representation, Doc2VecC naturally enforces document semantics to be captured by averaged word embeddings during training. This leads to better performance. Doc2VecC reduces to Denoising Autoencoders (DEA) if the local context words are removed from the paradigm shown in Figure 1. By including the context words, Doc2VecC allows the document vector to focus more on capturing the global context. Skip-thought vectors perform surprisingly poor on this dataset comparing to other methods. We hypothesized that it is due to the length of paragraphs in this dataset. The average length of paragraphs in the IMDB movie review dataset is 296.5, much longer than the ones used for training and testing in the original paper, which is in the order of 10. As noted in (Tai et al., 2015), the performance of LSTM based method (similarly, the gated RNN used in Skip-thought vectors) drops significantly with increasing paragraph length, as it is hard to preserve state over long sequences of words.\nTime. Table 2 summarizes the time required by these algorithms to learn and generate the document representation. Word2Vec is the fastest one to train. Denoising Autoencoders and Doc2VecC second that. The number of parameters that needs to be back-propagated in each update was increased by the number of surviving words in x\u0303. We found that both models are not sensitive to the corruption rate q in the noise model. Since the learning time decreases with higher corruption rate, we used q = 0.9 throughout the experiments. Paragraph Vectors takes longer time to train as there are more parameters (linear to the number of document in the learning set) to learn. At test time, Word2Vec+IDF, DEA and Doc2VecC all use (weighted) averaging of word embeddings as document\n1available at https://github.com/ryankiros/skip-thoughts\nrepresentation. Paragraph Vectors, on the other hand, requires another round of inference to produce the vector representation of unseen test documents. It takes Paragraph Vectors 4 minutes and 17 seconds to infer the vector representations for the 25,000 test documents, in comparison to 7 seconds for the other methods. As we did not re-train the Skip-thought vector models on this dataset, the training time2 reported in the table is the time it takes to generate the embeddings for the 25,000 training documents. Due to repeated high-dimensional matrix operations required for encoding long paragraphs, it takes fairly long time to generate the representations for these documents. Similarly for testing. The experiments were conducted on a desktop with Intel i7 2.2Ghz cpu.\nData dependent regularization. As explained in Section 3.1, the corruption introduced in Doc2VecC acts as a data-dependent regularization that suppresses the embeddings of frequent but uninformative words. Here we conduct an experiment to exam the effect. We used a cutoff of 100 in this experiment. Table 3 lists the words having the smallest l2 norm of embeddings found by different algorithms. The number inside the parenthesis after each word is the number of times this word appears in the learning set. In word2Vec or Paragraph Vectors, the least frequent words have embeddings that are close to zero, despite some of them being indicative of sentiment such as debacle, bliss and shabby. In contrast, Doc2VecC manages to clamp down the representation of words frequently appear in the training set, but are uninformative, such as symbols and stop words.\nSubsampling frequent words. Note that for all the numbers reported, we applied the trick of subsampling of frequent words introduced in (Mikolov & Dean, 2013) to counter the imbalance between frequent and rare words. It is critical to the performance of simple Word2Vec+AVG as the sole remedy to diminish the contribution of common words in the final document representation. If we were to remove this step, the error rate of Word2Vec+AVG will increases from 12.1% to 13.2%. Doc2VecC on the other hand naturally exerts a stronger regularization toward embeddings of words that are frequent but uninformative, therefore does not rely on this trick.\n4.3 WORD ANALOGY\nIn table 3, we demonstrated that the corruption model introduced in Doc2VecC dampens the embeddings of words which are common and non-discriminative (stop words). In this experiment, we are going to quantatively compare the word embeddings generated by Doc2VecC to the ones generated by Word2Vec, or Paragraph Vectors on the word analogy task introduced by Mikolov et al. (2013a). The dataset contains five types of semantic questions, and nine types of syntactic questions, with a total of 8,869 semantic and 10,675 syntactic questions. The questions are answered through simple linear algebraic operations on the word embeddings generated by different methods. Please refer to the original paper for more details on the evaluation protocol.\n2As reported in the original paper, training of the skip-thought vector model on the book corpus dataset takes around 2 weeks on GPU.\nWe trained the word embeddings of different methods using the English news dataset released under the ACL workshop on statistical machine translation. The training set includes close to 15M paragraphs with 355M tokens. We compare the performance of word embeddings trained by different methods with increasing embedding dimensionality as well as increasing training data.\nWe observe similar trends as in Mikolov et al. (2013a). Increasing embedding dimensionality as well as training data size improves performance of the word embeddings on this task. However, the improvement is diminishing. Doc2VecC produces word embeddings which performs significantly better than the ones generated by Word2Vec. We observe close to 20% uplift when we train on the full training corpus. Paragraph vectors on the other hand performs surprisingly bad on this dataset. Our hypothesis is that due to the large capacity of the model architecture, Paragraph Vectors relies mostly on the unique document vectors to capture the information in a text document instead of learning the word semantic or syntactic similarities. This also explains why the PV-DBOW Le & Mikolov (2014) model architecture proposed in the original work, which completely removes word embedding layers, performs comparable to the distributed memory version.\nIn table 5, we list a detailed comparison of the performance of word embeddings generated by Word2Vec and Doc2VecC on the 14 subtasks, when trained on the full dataset with embedding of size 100. We can see that Doc2VecC significantly outperforms the word embeddings produced by Word2Vec across almost all the subtasks.\n4.4 DOCUMENT CLASSIFICATION\nFor the document classification task, we use a subset of the wikipedia dump, which contains over 300,000 wikipedia pages in 100 categories. The 100 categories includes categories under sports,\nTable 5: Classification error (%) of a linear classifier trained on various document representations on the Wikipedia dataset.\nModel BOW DEA Word2Vec + AVG Word2Vec + IDF ParagraphVectors Doc2VecC h = 100 36.03 32.30 33.2 33.16 35.78 31.92 h = 200 36.03 31.36 32.46 32.48 34.92 30.84 h = 500 36.03 31.10 32.02 32.13 33.93 30.43 h = 1000 36.03 31.13 31.78 32.06 33.02 30.24\n(a) Doc2Vec (b) Doc2VecC\nFigure 3: Visualization of document vectors on Wikipedia dataset using t-SNE.\nentertainment, literature, and politics etc. Examples of categories include American drama films, Directorial debut films, Major League Baseball pitchers and Sydney Swans players. Body texts (the second paragraph) were extracted for each page as a document. For each category, we select 1,000 documents with unique category label, and 100 documents were used for training and 900 documents for testing. The remaining documents are used as unlabeled data. The 100 classes are balanced in the training and testing sets. For this data set, we learn the word embedding and document representation for all the algorithms using all the available data. We apply a cutoff of 10, resulting in a vocabulary of size 107, 691.\nTable 5 summarizes the classification error of a linear SVM trained on representations of different sizes. We can see that most of the algorithms are not sensitive to the size of the vector representation. Doc2Vec benefits most from increasing representation size. Across all sizes of representations, Doc2VecC outperform the existing algorithms by a significant margin. In fact, Doc2VecC can achieve same or better performance with a much smaller representation vector.\nFigure 3 visualizes the document representations learned by Doc2Vec (left) and Doc2VecC (right) using t-SNE (Maaten & Hinton, 2008). We can see that documents from the same category are nicely clustered using the representation generated by Doc2VecC. Doc2Vec, on the other hand, does not produce a clear separation between different categories, which explains its worse performance reported in Table 5.\nFigure 4 visualizes the vector representation generated by Doc2VecC w.r.t. coarser categorization. we manually grouped the 100 categories into 7 coarse categories, television, albums, writers, musicians, athletes, species and actors. Categories that do no belong to any of these 7 groups are not included in the figure.\nWe can see that documents belonging to a coarser category are grouped together. This subset includes is a wide range of sports descriptions, ranging from football, crickets, baseball, and cycling etc., which explains why the athletes category are less concentrated. In the projection, we can see documents belonging to the musician category are closer to those belonging to albums category than those of athletes or species.\n4.5 SEMANTIC RELATEDNESS\nWe test Doc2VecC on the SemEval 2014 Task 1: semantic relatedness SICK dataset (Marelli et al., 2014). Given two sentences, the task is to determine how closely they are semantically related. The set contains 9,927 pairs of sentences with human annotated relatedness score, ranging from 1 to 5. A score of 1 indicates that the two sentences are not related, while 5 indicates high relatedness. The set is splitted into a training set of 4,500 instances, a validation set of 500, and a test set of 4,927.\nWe compare Doc2VecC with several winning solutions of the competition as well as several more recent techniques reported on this dataset, including bi-directional LSTM and Tree-LSTM3 trained from scratch on this dataset, Skip-thought vectors learned a large book corpus 4 (Zhu et al., 2015) and produced sentence embeddings of 4,800 dimensions on this dataset. We follow the same protocol as in skip-thought vectors, and train Doc2VecC on the larger book corpus dataset. Contrary to the vocabulary expansion technique used in (Kiros et al., 2015) to handle out-of-vocabulary words, we extend the vocabulary of the learned model directly on the target dataset in the following way: we use the pre-trained word embedding as an initialization, and fine-tune the word and sentence representation on the SICK dataset. Notice that the fine-tuning is done for sentence representation learning only, and we did not use the relatedness score in the learning. This step brings small improvement to the performance of our algorithm. Given the sentence embeddings, we used the exact same training and testing protocol as in (Kiros et al., 2015) to score each pair of sentences: with two sentence embedding u1 and u2, we concatenate their component-wise product, u1 \u00b7u2 and their absolute difference, |u1 \u2212 u2| as the feature representation. Table 6 summarizes the performance of various algorithms on this dataset. Despite its simplicity, Doc2VecC significantly out-performs the winning solutions of the competition, which are heavily feature engineered toward this dataset and several baseline methods, noticeably the dependency-tree RNNs introduced in (Socher et al., 2014), which relies on expensive dependency parsers to compose sentence vectors from word embeddings. The performance of Doc2VecC is slightly worse than the LSTM based methods or skip-thought vectors on this dataset, while it significantly outperforms skip-thought vectors on the IMDB movie review dataset (11.70% error rate vs 17.42%). As we hypothesized in previous section, while Doc2VecC is better at handling longer paragraphs, LSTMbased methods are superior for relatively short sentences (of length in the order of 10s). We would like to point out that Doc2VecC is much faster to train and test comparing to skip-thought vectors. It takes less than 2 hours to learn the embeddings on the large book corpus for Doc2VecC on a desktop with Intel i7 2.2Ghz cpu, in comparison to the 2 weeks on GPU required by skip-thought vectors.\n5 CONCLUSION\nWe introduce a new model architecture Doc2VecC for document representation learning. It is very efficient to train and test thanks to its simple model architecture. Doc2VecC intrinsically makes sure document representation generated by averaging word embeddings capture semantics of document during learning. It also introduces a data-dependent regularization which favors informative or rare words while dampening the embeddings of common and non-discriminative words. As such, each document can be efficiently represented as a simple average of the learned word embeddings. In comparison to several existing document representation learning algorithms, Doc2VecC outperforms not only in testing efficiency, but also in the expressiveness of the generated representations.\n3The word representation was initialized using publicly available 300-dimensional Glove vectors trained on 840 billion tokens of Common Crawl data\n4The dataset contains 11,038 books with over one billion words\n", "cats": {"READ": 1.0, "DONTREAD": 0.0}, "reviews": [{"DATE": "18 May 2017", "TITLE": "Better word embeddings", "IS_META_REVIEW": false, "comments": "Extended the paper with experiments on the word relationship dataset, showing Doc2VecC generates better word embeddings in comparison to Word2Vec or Paragraph Vectors. ", "OTHER_KEYS": "Minmin Chen"}, {"DATE": "22 Mar 2017", "TITLE": "About Table 3: Words with embeddings closest to 0 learned by different algorithms.", "IS_META_REVIEW": false, "comments": "I'm using ", "OTHER_KEYS": "(anonymous)"}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "The introduced method for producing document representations is simple, efficient and potentially quite useful. Though we could quibble a bit that the idea is just a combination of known techniques, the reviews generally agree that the idea is interesting.\n \n Pros:\n + interesting and simple algorithm\n + strong performance\n + efficient\n \n Cons:\n + individual ideas are not so novel\n \n This is a paper that will be well received at a poster presentation.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"DATE": "24 Dec 2016", "TITLE": "new dataset and baselines added", "IS_META_REVIEW": false, "comments": "Dear reviewers, \n\nI added another dataset to the draft (appendix): SemEval 2014 Task 1 semantic relatedness SICK dataset, as several of the recent works the reviewers pointed out have been reported on this dataset. Despite its simplicity, Doc2VecC significantly out-performs the winning solutions of the competition and several baseline methods, noticeably the dependency-tree RNNs introduced in [1], which relies on additional dependency parsers to compose sentence vectors from word embeddings. The performance of Doc2VecC is comparable (slightly worse) than the LSTM based methods or skip-thought vectors on this dataset. On the other hand, it is much faster to train and test. As reported in the original paper, training of the skip-thought vector models on the book corpus dataset takes around 2 weeks on GPU. In contrast, it takes less than 2 hours to learn the embeddings for Doc2VecC on a desktop with Intel i7 2.2Ghz cpu.\n\nI also provided more insight on why Skip-thought vectors did not perform well on the movie review dataset in Section 4.2 (Accuracy). \n\nI would greatly appreciate it if you could take another look at revisions and provide me with some feedbacks. \n\n[1] Socher, Richard, et al. \"Grounded compositional semantics for finding and describing images with sentences.\" Transactions of the Association for Computational Linguistics 2 (2014): 207-218.", "OTHER_KEYS": "Minmin Chen"}, {"DATE": "21 Dec 2016", "TITLE": "revisions", "IS_META_REVIEW": false, "comments": "Dear reviewers, \n\nThank you for your feedback. The updated manuscript included skip-thought as another baseline method. We will test this idea on more datasets, in particular the ones experimented in Skip-thought vectors in the submission. ", "OTHER_KEYS": "Minmin Chen"}, {"OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "TITLE": "Simple idea, nicely composed", "comments": "This paper discusses a method for computing vector representations for documents by using a skip-gram style learning mechanism with an added regularizer in the form of a global context vector with various bits of drop out. While none of the individual components proposed in this paper are new, I believe that the combination in this fashion is. Further, I appreciated the detailed analysis of model behaviour in section 3.\n\nThe main downside to this submission is in its relative weakness on the empirical front. Arguably there are more interesting tasks than sentiment analysis and k-way classification! Likewise, why waste 2/3 of a page on t-sne projections rather than use that space for further analysis?\n\nWhile I am a bit disappointed by this reduced evaluation and agree with the other reviewers concerning soft baselines, I think this paper should be accepted: it's an interesting algorithm, nicely composed and very efficient, so it's reasonable to assume that other readers might have use for some of the ideas presented here.", "SOUNDNESS_CORRECTNESS": 4, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "19 Dec 2016", "CLARITY": 5, "REVIEWER_CONFIDENCE": 3}, {"TITLE": "review", "RECOMMENDATION_UNOFFICIAL": 5, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "This paper presents a framework for creating document representations. \nThe main idea is to represent a document as an average of its word embeddings with a data-dependent regularization that favors informative or rare words while forcing common words to be close to 0. \nExperiments on sentiment analysis and document classification show that the proposed method has the lowest error rates compared to baseline document embedding methods. \n\nWhile I like the motivation of finding the best way to encode a document into a vector, the paper does not offer significant technical contributions.\nMost of the techniques are not new, and the main selling point is the simplicity and speed of the proposed method. \nFor this reason, I would like to see good results for more than two tasks to be convinced that this is the best way to learn document representations.  \nFor RNN-LM, is the LM trained to minimize classification error, or is it trained  as a language model? Did you use the final hidden state as the representation, or the average of all hidden states?\nOne of the most widely used method to represent documents now is to have a bidirectional LSTM and concatenate the final hidden states as the document representation. \nI think it would be useful to know how the proposed method compares to this approach for tasks such as document classification or sentiment analysis.", "IS_ANNOTATED": true, "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "19 Dec 2016 (modified: 04 Jan 2017)", "CLARITY": 5, "REVIEWER_CONFIDENCE": 4}, {"DATE": "16 Dec 2016", "TITLE": "Need more recent baselines", "IS_META_REVIEW": false, "comments": "Unsupervised document representations is an active area of research, so it would be useful to benchmark against something more recent than doc2vec, which was in ICML 2014. Skip-thought vectors, in particular, should really be included.", "OTHER_KEYS": "(anonymous)"}, {"TITLE": "Interesting corruption mechanism for document representation", "RECOMMENDATION_UNOFFICIAL": 5, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "This paper proposes learning document embeddings as a sum of the constituent word embeddings, which are jointly learned and randomly dropped out ('corrupted') during training. While none of the pieces of this model are particularly novel, the result is an efficient learning algorithm for document representation with good empirical performance.\n\nJoint training of word and document embeddings is not a new idea, nor is the idea of enforcing the document to be represented by the sum of its word embeddings (see, e.g. '\u201cThe Sum of Its Parts\u201d: Joint Learning of Word and Phrase Representations with Autoencoders' by Lebret and Collobert). Furthermore, the corruption mechanism is nothing other than traditional dropout on the input layer. Coupled with the word2vec-style loss and training methods, this paper offers little on the novelty front.\n\nOn the other hand, it is very efficient at generation time, requiring only an average of the word embeddings rather than a complicated inference step as in Doc2Vec. Moreover, by construction, the embedding captures salient global information about the document -- it captures specifically that information that aids in local-context prediction. For such a simple model, the performance on sentiment analysis and document classification is quite encouraging.\n\nOverall, despite the lack of novelty, the simplicity, efficiency, and performance of this model make it worthy of wider readership and study, and I recommend acceptance.", "SOUNDNESS_CORRECTNESS": 5, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "16 Dec 2016", "CLARITY": 4, "REVIEWER_CONFIDENCE": 4}, {"TITLE": "Effect of document length", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "RECOMMENDATION_UNOFFICIAL": 5, "comments": "", "SOUNDNESS_CORRECTNESS": 5, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "DATE": "02 Dec 2016", "CLARITY": 4}, {"TITLE": "pre review questions", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "RECOMMENDATION_UNOFFICIAL": 5, "comments": "", "IS_ANNOTATED": true, "IS_META_REVIEW": false, "DATE": "01 Dec 2016", "CLARITY": 5}, {"DATE": "18 May 2017", "TITLE": "Better word embeddings", "IS_META_REVIEW": false, "comments": "Extended the paper with experiments on the word relationship dataset, showing Doc2VecC generates better word embeddings in comparison to Word2Vec or Paragraph Vectors. ", "OTHER_KEYS": "Minmin Chen"}, {"DATE": "22 Mar 2017", "TITLE": "About Table 3: Words with embeddings closest to 0 learned by different algorithms.", "IS_META_REVIEW": false, "comments": "I'm using ", "OTHER_KEYS": "(anonymous)"}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "The introduced method for producing document representations is simple, efficient and potentially quite useful. Though we could quibble a bit that the idea is just a combination of known techniques, the reviews generally agree that the idea is interesting.\n \n Pros:\n + interesting and simple algorithm\n + strong performance\n + efficient\n \n Cons:\n + individual ideas are not so novel\n \n This is a paper that will be well received at a poster presentation.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"DATE": "24 Dec 2016", "TITLE": "new dataset and baselines added", "IS_META_REVIEW": false, "comments": "Dear reviewers, \n\nI added another dataset to the draft (appendix): SemEval 2014 Task 1 semantic relatedness SICK dataset, as several of the recent works the reviewers pointed out have been reported on this dataset. Despite its simplicity, Doc2VecC significantly out-performs the winning solutions of the competition and several baseline methods, noticeably the dependency-tree RNNs introduced in [1], which relies on additional dependency parsers to compose sentence vectors from word embeddings. The performance of Doc2VecC is comparable (slightly worse) than the LSTM based methods or skip-thought vectors on this dataset. On the other hand, it is much faster to train and test. As reported in the original paper, training of the skip-thought vector models on the book corpus dataset takes around 2 weeks on GPU. In contrast, it takes less than 2 hours to learn the embeddings for Doc2VecC on a desktop with Intel i7 2.2Ghz cpu.\n\nI also provided more insight on why Skip-thought vectors did not perform well on the movie review dataset in Section 4.2 (Accuracy). \n\nI would greatly appreciate it if you could take another look at revisions and provide me with some feedbacks. \n\n[1] Socher, Richard, et al. \"Grounded compositional semantics for finding and describing images with sentences.\" Transactions of the Association for Computational Linguistics 2 (2014): 207-218.", "OTHER_KEYS": "Minmin Chen"}, {"DATE": "21 Dec 2016", "TITLE": "revisions", "IS_META_REVIEW": false, "comments": "Dear reviewers, \n\nThank you for your feedback. The updated manuscript included skip-thought as another baseline method. We will test this idea on more datasets, in particular the ones experimented in Skip-thought vectors in the submission. ", "OTHER_KEYS": "Minmin Chen"}, {"OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "TITLE": "Simple idea, nicely composed", "comments": "This paper discusses a method for computing vector representations for documents by using a skip-gram style learning mechanism with an added regularizer in the form of a global context vector with various bits of drop out. While none of the individual components proposed in this paper are new, I believe that the combination in this fashion is. Further, I appreciated the detailed analysis of model behaviour in section 3.\n\nThe main downside to this submission is in its relative weakness on the empirical front. Arguably there are more interesting tasks than sentiment analysis and k-way classification! Likewise, why waste 2/3 of a page on t-sne projections rather than use that space for further analysis?\n\nWhile I am a bit disappointed by this reduced evaluation and agree with the other reviewers concerning soft baselines, I think this paper should be accepted: it's an interesting algorithm, nicely composed and very efficient, so it's reasonable to assume that other readers might have use for some of the ideas presented here.", "SOUNDNESS_CORRECTNESS": 4, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "19 Dec 2016", "CLARITY": 5, "REVIEWER_CONFIDENCE": 3}, {"TITLE": "review", "RECOMMENDATION_UNOFFICIAL": 5, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "This paper presents a framework for creating document representations. \nThe main idea is to represent a document as an average of its word embeddings with a data-dependent regularization that favors informative or rare words while forcing common words to be close to 0. \nExperiments on sentiment analysis and document classification show that the proposed method has the lowest error rates compared to baseline document embedding methods. \n\nWhile I like the motivation of finding the best way to encode a document into a vector, the paper does not offer significant technical contributions.\nMost of the techniques are not new, and the main selling point is the simplicity and speed of the proposed method. \nFor this reason, I would like to see good results for more than two tasks to be convinced that this is the best way to learn document representations.  \nFor RNN-LM, is the LM trained to minimize classification error, or is it trained  as a language model? Did you use the final hidden state as the representation, or the average of all hidden states?\nOne of the most widely used method to represent documents now is to have a bidirectional LSTM and concatenate the final hidden states as the document representation. \nI think it would be useful to know how the proposed method compares to this approach for tasks such as document classification or sentiment analysis.", "IS_ANNOTATED": true, "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "19 Dec 2016 (modified: 04 Jan 2017)", "CLARITY": 5, "REVIEWER_CONFIDENCE": 4}, {"DATE": "16 Dec 2016", "TITLE": "Need more recent baselines", "IS_META_REVIEW": false, "comments": "Unsupervised document representations is an active area of research, so it would be useful to benchmark against something more recent than doc2vec, which was in ICML 2014. Skip-thought vectors, in particular, should really be included.", "OTHER_KEYS": "(anonymous)"}, {"TITLE": "Interesting corruption mechanism for document representation", "RECOMMENDATION_UNOFFICIAL": 5, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "This paper proposes learning document embeddings as a sum of the constituent word embeddings, which are jointly learned and randomly dropped out ('corrupted') during training. While none of the pieces of this model are particularly novel, the result is an efficient learning algorithm for document representation with good empirical performance.\n\nJoint training of word and document embeddings is not a new idea, nor is the idea of enforcing the document to be represented by the sum of its word embeddings (see, e.g. '\u201cThe Sum of Its Parts\u201d: Joint Learning of Word and Phrase Representations with Autoencoders' by Lebret and Collobert). Furthermore, the corruption mechanism is nothing other than traditional dropout on the input layer. Coupled with the word2vec-style loss and training methods, this paper offers little on the novelty front.\n\nOn the other hand, it is very efficient at generation time, requiring only an average of the word embeddings rather than a complicated inference step as in Doc2Vec. Moreover, by construction, the embedding captures salient global information about the document -- it captures specifically that information that aids in local-context prediction. For such a simple model, the performance on sentiment analysis and document classification is quite encouraging.\n\nOverall, despite the lack of novelty, the simplicity, efficiency, and performance of this model make it worthy of wider readership and study, and I recommend acceptance.", "SOUNDNESS_CORRECTNESS": 5, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "16 Dec 2016", "CLARITY": 4, "REVIEWER_CONFIDENCE": 4}, {"TITLE": "Effect of document length", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "RECOMMENDATION_UNOFFICIAL": 5, "comments": "", "SOUNDNESS_CORRECTNESS": 5, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "DATE": "02 Dec 2016", "CLARITY": 4}, {"TITLE": "pre review questions", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "RECOMMENDATION_UNOFFICIAL": 5, "comments": "", "IS_ANNOTATED": true, "IS_META_REVIEW": false, "DATE": "01 Dec 2016", "CLARITY": 5}]}
{"text": "1 INTRODUCTION\nControlling artificial agents using only raw high-dimensional input data such as image or sound is a difficult and important task in the field of Reinforcement Learning (RL). Recent breakthroughs in the field allow its utilization in real-world applications such as autonomous driving (Shalev-Shwartz et al., 2016), navigation (Bischoff et al., 2013) and more. Agent interaction with the real world is usually either expensive or not feasible, as the real world is far too complex for the agent to perceive. Therefore in practice the interaction is simulated by a virtual environment which receives feedback on a decision made by the algorithm. Traditionally, games were used as a RL environment, dating back to Chess (Campbell et al., 2002), Checkers (Schaeffer et al., 1992), backgammon (Tesauro, 1995) and the more recent Go (Silver et al., 2016). Modern games often present problems and tasks which are highly correlated with real-world problems. For example, an agent that masters a racing game, by observing a simulated driver\u2019s view screen as input, may be usefull for the development of an autonomous driver. For high-dimensional input, the leading benchmark is the Arcade Learning Environment (ALE) (Bellemare et al., 2013) which provides a common interface to dozens of Atari 2600 games, each presents a different challenge. ALE provides an extensive benchmarking platform, allowing a controlled experiment setup for algorithm evaluation and comparison. The main challenge posed by ALE is to successfully play as many Atari 2600 games as possible (i.e., achieving a score higher than an expert human player) without providing the algorithm any game-specific information (i.e., using the same input available to a human - the game screen and score). A key work to tackle this problem is the Deep Q-Networks algorithm (Mnih et al., 2015), which made a breakthrough in the field of Deep Reinforcement Learning by achieving human level performance on 29 out of 49 games. In this work we present a new environment \u2014 the Retro Learning Environment (RLE). RLE sets new challenges by providing a unified interface for Atari 2600 games as well as more advanced gaming consoles. As a start we focused on the Super Nintendo Entertainment\nSystem (SNES). Out of the five SNES games we tested using state-of-the-art algorithms, only one was able to outperform an expert human player. As an additional feature, RLE supports research of multi-agent reinforcement learning (MARL) tasks (Bus\u0327oniu et al., 2010). We utilize this feature by training and evaluating the agents against each other, rather than against a pre-configured in-game AI. We conducted several experiments with this new feature and discovered that agents tend to learn how to overcome their current opponent rather than generalize the game being played. However, if an agent is trained against an ensemble of different opponents, its robustness increases. The main contributions of the paper are as follows:\n\u2022 Introducing a novel RL environment with significant challenges and an easy agent evaluation technique (enabling agents to compete against each other) which could lead to new and more advanced RL algorithms.\n\u2022 A new method to train an agent by enabling it to train against several opponents, making the final policy more robust.\n\u2022 Encapsulating several different challenges to a single RL environment.\n2 RELATED WORK\n2.1 ARCADE LEARNING ENVIRONMENT\nThe Arcade Learning Environment is a software framework designed for the development of RL algorithms, by playing Atari 2600 games. The interface provided by ALE allows the algorithms to select an action and receive the Atari screen and a reward in every step. The action is the equivalent to a human\u2019s joystick button combination and the reward is the difference between the scores at time stamp t and t\u2212 1. The diversity of games for Atari provides a solid benchmark since different games have significantly different goals. Atari 2600 has over 500 games, currently over 70 of them are implemented in ALE and are commonly used for algorithm comparison.\n2.2 INFINITE MARIO\nInfinite Mario (Togelius et al., 2009) is a remake of the classic Super Mario game in which levels are randomly generated. On these levels the Mario AI Competition was held. During the competition, several algorithms were trained on Infinite Mario and their performances were measured in terms of the number of stages completed. As opposed to ALE, training is not based on the raw screen data but rather on an indication of Mario\u2019s (the player\u2019s) location and objects in its surrounding. This environment no longer poses a challenge for state of the art algorithms. Its main shortcoming lie in the fact that it provides only a single game to be learnt. Additionally, the environment provides hand-crafted features, extracted directly from the simulator, to the algorithm. This allowed the use of planning algorithms that highly outperform any learning based algorithm.\n2.3 OPENAI GYM\nThe OpenAI gym (Brockman et al., 2016) is an open source platform with the purpose of creating an interface between RL environments and algorithms for evaluation and comparison purposes. OpenAI Gym is currently very popular due to the large number of environments supported by it. For example ALE, Go, MouintainCar and VizDoom (Zhu et al., 2016), an environment for the learning of the 3D first-person-shooter game \u201dDoom\u201d. OpenAI Gym\u2019s recent appearance and wide usage indicates the growing interest and research done in the field of RL.\n2.4 OPENAI UNIVERSE\nUniverse (Universe, 2016) is a platform within the OpenAI framework in which RL algorithms can train on over a thousand games. Universe includes very advanced games such as GTA V, Portal as well as other tasks (e.g. browser tasks). Unlike RLE, Universe doesn\u2019t run the games locally and requires a VNC interface to a server that runs the games. This leads to a lower frame rate and thus longer training times.\n2.5 MALMO\nMalmo (Johnson et al., 2016) is an artificial intelligence experimentation platform of the famous game \u201dMinecraft\u201d. Although Malmo consists of only a single game, it presents numerous challenges since the \u201dMinecraft\u201d game can be configured differently each time. The input to the RL algorithms include specific features indicating the \u201dstate\u201d of the game and the current reward.\n2.6 DEEPMIND LAB\nDeepMind Lab (Dee) is a first-person 3D platform environment which allows training RL algorithms on several different challenges: static/random map navigation, collect fruit (a form of reward) and a laser-tag challenge where the objective is to tag the opponents controlled by the in-game AI. In LAB the agent observations are the game screen (with an additional depth channel) and the velocity of the character. LAB supports four games (one game - four different modes).\n2.7 DEEP Q-LEARNING\nIn our work, we used several variant of the Deep Q-Network algorithm (DQN) (Mnih et al., 2015), an RL algorithm whose goal is to find an optimal policy (i.e., given a current state, choose action that maximize the final score). The state of the game is simply the game screen, and the action is a combination of joystick buttons that the game responds to (i.e., moving ,jumping). DQN learns through trial and error while trying to estimate the \u201dQ-function\u201d, which predicts the cumulative discounted reward at the end of the episode given the current state and action while following a policy \u03c0. The Q-function is represented using a convolution neural network that receives the screen as input and predicts the best possible action at it\u2019s output. The Q-function weights \u03b8 are updated according to:\n\u03b8t+1(st, at) = \u03b8t + \u03b1(Rt+1 + \u03b3max a\n(Qt(st+1, a; \u03b8 \u2032 t))\u2212Qt(st, at; \u03b8t))\u2207\u03b8Qt(st, at; \u03b8t), (1)\nwhere st, st+1 are the current and next states, at is the action chosen, \u03b1 is the step size, \u03b3 is the discounting factor Rt+1 is the reward received by applying at at st. \u03b8\u2032 represents the previous weights of the network that are updated periodically. Other than DQN, we examined two leading algorithms on the RLE: Double Deep Q-Learning (D-DQN) (Van Hasselt et al., 2015), a DQN based algorithm with a modified network update rule. Dueling Double DQN (Wang et al., 2015), a modification of D-DQN\u2019s architecture in which the Q-function is modeled using a state (screen) dependent estimator and an action dependent estimator.\n3 THE RETRO LEARNING ENVIRONMENT\n3.1 SUPER NINTENDO ENTERTAINMENT SYSTEM\nThe Super Nintendo Entertainment System (SNES) is a home video game console developed by Nintendo and released in 1990. A total of 783 games were released, among them, the iconic Super Mario World, Donkey Kong Country and The Legend of Zelda. Table (1) presents a comparison between Atari 2600, Sega Genesis and SNES game consoles, from which it is clear that SNES and Genesis games are far more complex.\n3.2 IMPLEMENTATION\nTo allow easier integration with current platforms and algorithms, we based our environment on the ALE, with the aim of maintaining as much of its interface as possible. While the ALE is highly coupled with the Atari emulator, Stella1, RLE takes a different approach and separates the learning environment from the emulator. This was achieved by incorporating an interface named LibRetro (libRetro site), that allows communication between front-end programs to game-console emulators. Currently, LibRetro supports over 15 game consoles, each containing hundreds of games, at an estimated total of over 7,000 games that can potentially be supported using this interface. Examples of supported game consoles include Nintendo Entertainment System, Game Boy, N64, Sega Genesis,\n1http://stella.sourceforge.net/\nSaturn, Dreamcast and Sony PlayStation. We chose to focus on the SNES game console implemented using the snes9x2 as it\u2019s games present interesting, yet plausible to overcome challenges. Additionally, we utilized the Genesis-Plus-GX3 emulator, which supports several Sega consoles: Genesis/Mega Drive, Master System, Game Gear and SG-1000.\n3.3 SOURCE CODE\nRLE is fully available as open source software for use under GNU\u2019s General Public License4. The environment is implemented in C++ with an interface to algorithms in C++, Python and Lua. Adding a new game to the environment is a relatively simple process.\n3.4 RLE INTERFACE\nRLE provides a unified interface to all games in its supported consoles, acting as an RL-wrapper to the LibRetro interface. Initialization of the environment is done by providing a game (ROM file) and a gaming-console (denoted by \u2019core\u2019). Upon initialization, the first state is the initial frame of the game, skipping all menu selection screens. The cores are provided with the RLE and installed together with the environment. Actions have a bit-wise representation where each controller button is represented by a one-hot vector. Therefore a combination of several buttons is possible using the bit-wise OR operator. The number of valid buttons combinations is larger than 700, therefore only the meaningful combinations are provided. The environments observation is the game screen, provided as a 3D array of 32 bit per pixel with dimensions which vary depending on the game. The reward can be defined differently per game, usually we set it to be the score difference between two consecutive frames. By setting different configuration to the environment, it is possible to alter in-game properties such as difficulty (i.e easy, medium, hard), its characters, levels, etc.\n3.5 ENVIRONMENT CHALLENGES\nIntegrating SNES and Genesis with RLE presents new challenges to the field of RL where visual information in the form of an image is the only state available to the agent. Obviously, SNES games are significantly more complex and unpredictable than Atari games. For example in sports games, such as NBA, while the player (agent) controls a single player, all the other nine players\u2019 behavior is determined by pre-programmed agents, each exhibiting random behavior. In addition, many SNES games exhibit delayed rewards in the course of their play (i.e., reward for an actions is given many time steps after it was performed). Similarly, in some of the SNES games, an agent can obtain a reward that is indirectly related to the imposed task. For example, in platform games, such as Super Mario, reward is received for collecting coins and defeating enemies, while the goal of the challenge is to reach the end of the level which requires to move to keep moving to the right. Moreover, upon completing a level, a score bonus is given according to the time required for its completion. Therefore collecting coins or defeating enemies is not necessarily preferable if it consumes too much time. Analysis of such games is presented in section 4.2. Moreover, unlike Atari that consists of\n2http://www.snes9x.com/ 3https://github.com/ekeeke/Genesis-Plus-GX 4https://github.com/nadavbh12/Retro-Learning-Environment\neight directions and one action button, SNES has eight-directions pad and six actions buttons. Since combinations of buttons are allowed, and required at times, the actual actions space may be larger than 700, compared to the maximum of 18 actions in Atari. Furthermore, the background in SNES is very rich, filled with details which may move locally or across the screen, effectively acting as non-stationary noise since it provided little to no information regarding the state itself. Finally, we note that SNES utilized the first 3D games. In the game Wolfenstein, the player must navigate a maze from a first-person perspective, while dodging and attacking enemies. The SNES offers plenty of other 3D games such as flight and racing games which exhibit similar challenges. These games are much more realistic, thus inferring from SNES games to \u201dreal world\u201d tasks, as in the case of self driving cars, might be more beneficial. A visual comparison of two games, Atari and SNES, is presented in Figure (1).\n4 EXPERIMENTS\n4.1 EVALUATION METHODOLOGY\nThe evaluation methodology that we used for benchmarking the different algorithms is the popular method proposed by (Mnih et al., 2015). Each examined algorithm is trained until either it reached convergence or 100 epochs (each epoch corresponds to 50,000 actions), thereafter it is evaluated by performing 30 episodes of every game. Each episode ends either by reaching a terminal state or after 5 minutes. The results are averaged per game and compared to the average result of a human player. For each game the human player was given two hours for training, and his performances were evaluated over 20 episodes. As the various algorithms don\u2019t use the game audio in the learning process, the audio was muted for both the agent and the human. From both, humans and agents\nscore, a random agent score (an agent performing actions randomly) was subtracted to assure that learning indeed occurred. It is important to note that DQN\u2019s -greedy approach (select a random action with a small probability ) is present during testing thus assuring that the same sequence of actions isn\u2019t repeated. While the screen dimensions in SNES are larger than those of Atari, in our experiments we maintained the same pre-processing of DQN (i.e., downscaling the image to 84x84 pixels and converting to gray-scale). We argue that downscaling the image size doesn\u2019t affect a human\u2019s ability to play the game, therefore suitable for RL algorithms as well. To handle the large action space, we limited the algorithm\u2019s actions to the minimal button combinations which provide unique behavior. For example, on many games the R and L action buttons don\u2019t have any use therefore their use and combinations were omitted.\n4.1.1 RESULTS\nA thorough comparison of the four different agents\u2019 performances on SNES games can be seen in Figure (). The full results can be found in Table (3). Only in the game Mortal Kombat a trained agent was able to surpass a expert human player performance as opposed to Atari games where the same algorithms have surpassed a human player on the vast majority of the games.\nOne example is Wolfenstein game, a 3D first-person shooter game, requires solving 3D vision tasks, navigating in a maze and detecting object. As evident from figure (2), all agents produce poor results indicating a lack of the required properties. By using -greedy approach the agents weren\u2019t able to explore enough states (or even other rooms in our case). The algorithm\u2019s final policy appeared as a random walk in a 3D space. Exploration based on visited states such as presented in Bellemare et al. (2016) might help addressing this issue. An interesting case is Gradius III, a side-scrolling, flight-shooter game. While the trained agent was able to master the technical aspects of the game, which includes shooting incoming enemies and dodging their projectiles, it\u2019s final score is still far from a human\u2019s. This is due to a hidden game mechanism in the form of \u201dpower-ups\u201d, which can be accumulated, and significantly increase the players abilities. The more power-ups collected without being use \u2014 the larger their final impact will be. While this game-mechanism is evident to a human, the agent acts myopically and uses the power-up straight away5.\n4.2 REWARD SHAPING\nAs part of the environment and algorithm evaluation process, we investigated two case studies. First is a game on which DQN had failed to achieve a better-than-random score, and second is a game on which the training duration was significantly longer than that of other games.\nIn the first case study, we used a 2D back-view racing game \u201dF-Zero\u201d. In this game, one is required to complete four laps of the track while avoiding other race cars. The reward, as defined by the score of the game, is only received upon completing a lap. This is an extreme case of a reward delay. A lap may last as long as 30 seconds, which span over 450 states (actions) before reward is received. Since DQN\u2019s exploration is a simple -greedy approach, it was not able to produce a useful strategy. We approached this issue using reward shaping, essentially a modification of the reward to be a function of the reward and the observation, rather than the reward alone. Here, we define the reward to be the sum of the score and the agent\u2019s speed (a metric displayed on the screen of the game). Indeed when the reward was defined as such, the agents learned to finish the race in first place within a short training period.\nThe second case study is the famous game of Super Mario. In this game the agent, Mario, is required to reach the right-hand side of the screen, while avoiding enemies and collecting coins. We found this case interesting as it involves several challenges at once: dynamic background that can change drastically within a level, sparse and delayed rewards and multiple tasks (such as avoiding enemies and pits, advancing rightwards and collecting coins). To our surprise, DQN was able to reach the end of the level without any reward shaping, this was possible since the agent receives rewards for events (collecting coins, stomping on enemies etc.) that tend to appear to the right of the player, causing the agent to prefer moving right. However, the training time required for convergence was significantly longer than other games. We defined the reward as the sum of the in-game reward and a bonus granted according the the player\u2019s position, making moving right preferable. This reward\n5A video demonstration can be found at https://youtu.be/nUl9XLMveEU\nproved useful, as training time required for convergence decreased significantly. The two games above can be seen in Figure (3).\nFigure (4) illustrates the agent\u2019s average value function . Though both were able complete the stage trained upon, the convergence rate with reward shaping is significantly quicker due to the immediate realization of the agent to move rightwards.\n4.3 MULTI-AGENT REINFORCEMENT LEARNING\nIn this section we describe our experiments with RLE\u2019s multi-agent capabilities. We consider the case where the number of agents, n = 2 and the goals of the agents are opposite, as in r1 = \u2212r2. This scheme is known as fully competitive (Bus\u0327oniu et al., 2010). We used the simple singleagent RL approach (as described by Bus\u0327oniu et al. (2010) section 5.4.1) which is to apply to single agent approach to the multi-agent case. This approach was proved useful in Crites and Barto (1996) and Mataric\u0301 (1997). More elaborate schemes are possible such as the minimax-Q algorithm (Littman, 1994), (Littman, 2001). These may be explored in future works. We conducted three experiments on this setup: the first use was to train two different agents against the in-game AI, as done in previous sections, and evaluate their performance by letting them compete against each other. Here, rather than achieving the highest score, the goal was to win a tournament which consist of 50 rounds, as common in human-player competitions. The second experiment was to initially train two agents against the in-game AI, and resume the training while competing against each other. In this case, we evaluated the agent by playing again against the in-game AI, separately. Finally, in our last experiment we try to boost the agent capabilities by alternated it\u2019s opponents, switching between the in-game AI and other trained agents.\n4.3.1 MULTI-AGENT REINFORCEMENT LEARNING RESULTS\nWe chose the game Mortal Kombat, a two character side viewed fighting game (a screenshot of the game can be seen in Figure (1), as a testbed for the above, as it exhibits favorable properties: both players share the same screen, the agent\u2019s optimal policy is heavily dependent on the rival\u2019s behavior, unlike racing games for example. In order to evaluate two agents fairly, both were trained using the same characters maintaining the identity of rival and agent. Furthermore, to remove the impact of the starting positions of both agents on their performances, the starting positions were initialized randomly.\nIn the first experiment we evaluated all combinations of DQN against D-DQN and Dueling D-DQN. Each agent was trained against the in-game AI until convergence. Then 50 matches were performed between the two agents. DQN lost 28 out of 50 games against Dueling D-DQN and 33 against D-DQN. D-DQN lost 26 time to Dueling D-DQN. This win balance isn\u2019t far from the random case, since the algorithms converged into a policy in which movement towards the opponent is not\nrequired rather than generalize the game. Therefore, in many episodes, little interaction between the two agents occur, leading to a semi-random outcome.\nIn our second experiment, we continued the training process of a the D-DQN network by letting it compete against the Dueling D-DQN network. We evaluated the re-trained network by playing 30 episodes against the in-game AI. After training, D-DQN was able to win 28 out of 30 games, yet when faced again against the in-game AI its performance deteriorated drastically (from an average of 17000 to an average of -22000). This demonstrated a form of catastrophic forgetting (Goodfellow et al., 2013) even though the agents played the same game.\nIn our third experiment, we trained a Dueling D-DQN agent against three different rivals: the ingame AI, a trained DQN agent and a trained Dueling-DQN agent, in an alternating manner, such that in each episode a different rival was playing as the opponent with the intention of preventing the agent from learning a policy suitable for just one opponent. The new agent was able to achieve a score of 162,966 (compared to the \u201dnormal\u201d dueling D-DQN which achieved 169,633). As a new and objective measure of generalization, we\u2019ve configured the in-game AI difficulty to be \u201dvery hard\u201d (as opposed to the default \u201dmedium\u201d difficulty). In this metric the alternating version achieved 83,400 compared to -33,266 of the dueling D-DQN which was trained in default setting. Thus, proving that the agent learned to generalize to other policies which weren\u2019t observed while training.\n4.4 FUTURE CHALLENGES\nAs demonstrated, RLE presents numerous challenges that have yet to be answered. In addition to being able to learn all available games, the task of learning games in which reward delay is extreme, such as F-Zero without reward shaping, remains an unsolved challenge. Additionally, some games, such as Super Mario, feature several stages that differ in background and the levels structure. The task of generalizing platform games, as in learning on one stage and being tested on the other, is another unexplored challenge. Likewise surpassing human performance remains a challenge since current state-of-the-art algorithms still struggling with the many SNES games.\n5 CONCLUSION\nWe introduced a rich environment for evaluating and developing reinforcement learning algorithms which presents significant challenges to current state-of-the-art algorithms. In comparison to other environments RLE provides a large amount of games with access to both the screen and the ingame state. The modular implementation we chose allows extensions of the environment with new consoles and games, thus ensuring the relevance of the environment to RL algorithms for years to come (see Table (2)). We\u2019ve encountered several games in which the learning process is highly dependent on the reward definition. This issue can be addressed and explored in RLE as reward definition can be done easily. The challenges presented in the RLE consist of: 3D interpretation, delayed reward, noisy background, stochastic AI behavior and more. Although some algorithms were able to play successfully on part of the games, to fully overcome these challenges, an agent must incorporate both technique and strategy. Therefore, we believe, that the RLE is a great platform for future RL research.\n6 ACKNOWLEDGMENTS\nThe authors are grateful to the Signal and Image Processing Lab (SIPL) staff for their support, Alfred Agrell and the LibRetro community for their support and Marc G. Bellemare for his valuable inputs.\n", "cats": {"READ": 1.0, "DONTREAD": 0.0}, "reviews": [{"IS_META_REVIEW": true, "comments": "The paper presents a new environment, called Retro Learning Environment (RLE), for reinforcement learning. The authors focus on Super Nintendo but claim that the interface supports many others (including ALE). Benchmark results are given for standard algorithms in 5 new Super Nintendo games, and some results using a new \"rivalry metric\".\n\nThese environments (or, more generally, standardized evaluation methods like public data sets, competitions, etc.) have a long history of improving the quality of AI and machine learning research. One example in the past few years was the Atari Learning Environment (ALE) which has now turned into a standard benchmark for comparison of algorithms and results. In this sense, the RLE could be a worthy contribution to the field by encouraging new challenging domains for research.\n\nThat said, the main focus of this paper is presenting this new framework and showcasing the importance of new challenging domains. The results of experiments themselves are for existing algorithms. There are some new results that show reward shaping and policy shaping (having a bias toward going right in Super Mario) help during learning. And, yes, domain knowledge helps, but this is obvious. The rivalry training is an interesting idea, when training against a different opponent, the learner overfits to that opponent and forgets to play against the in-game AI; but then oddly, it gets evaluated on how well it does against the in-game AI! \n\nAlso the part of the paper that describes the scientific results (especially the rivalry training) is less polished, so this is disappointing. In the end, I'm not very excited about this paper.\n\nI was hoping for a more significant scientific contribution to accompany in this new environment. It's not clear if this is necessary for publication, but also it's not clear that ICLR is the right venue for this work due to the contribution being mainly about the new code (for example, mloss.org could be a better 'venue', JMLR has an associated journal track for accompanying papers:"}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "The authors present a new set of environments, similar to ALE but based on Super Nintendo rather than Atari. This is a great asset and could be important for RL research, but it doesn't merit ICLR publication because of the lack of novel research ideas. Hopefully the authors will consider another venue to publish this paper, such as perhaps a journal or workshop.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"TITLE": "Final review: Nice software contribution, expected more significant scientific contributions", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "The paper presents a new environment, called Retro Learning Environment (RLE), for reinforcement learning. The authors focus on Super Nintendo but claim that the interface supports many others (including ALE). Benchmark results are given for standard algorithms in 5 new Super Nintendo games, and some results using a new \"rivalry metric\".\n\nThese environments (or, more generally, standardized evaluation methods like public data sets, competitions, etc.) have a long history of improving the quality of AI and machine learning research. One example in the past few years was the Atari Learning Environment (ALE) which has now turned into a standard benchmark for comparison of algorithms and results. In this sense, the RLE could be a worthy contribution to the field by encouraging new challenging domains for research.\n\nThat said, the main focus of this paper is presenting this new framework and showcasing the importance of new challenging domains. The results of experiments themselves are for existing algorithms. There are some new results that show reward shaping and policy shaping (having a bias toward going right in Super Mario) help during learning. And, yes, domain knowledge helps, but this is obvious. The rivalry training is an interesting idea, when training against a different opponent, the learner overfits to that opponent and forgets to play against the in-game AI; but then oddly, it gets evaluated on how well it does against the in-game AI! \n\nAlso the part of the paper that describes the scientific results (especially the rivalry training) is less polished, so this is disappointing. In the end, I'm not very excited about this paper.\n\nI was hoping for a more significant scientific contribution to accompany in this new environment. It's not clear if this is necessary for publication, but also it's not clear that ICLR is the right venue for this work due to the contribution being mainly about the new code (for example, mloss.org could be a better 'venue', JMLR has an associated journal track for accompanying papers: ", "IS_META_REVIEW": false, "RECOMMENDATION": 5, "DATE": "19 Dec 2016 (modified: 23 Jan 2017)", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "Ok but limited contributions", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "This paper introduces a new reinforcement learning environment called \u00ab The Retro Learning Environment\u201d, that interfaces with the open-source LibRetro API to offer access to various emulators and associated games (i.e. similar to the Atari 2600 Arcade Learning Environment, but more generic). The first supported platform is the SNES, with 5 games (more consoles and games may be added later). Authors argue that SNES games pose more challenges than Atari\u2019s (due to more complex graphics, AI and game mechanics). Several DQN variants are evaluated in experiments, and it is also proposed to compare learning algorihms by letting them compete against each other in multiplayer games.\n\nI like the idea of going toward more complex games than those found on Atari 2600, and having an environment where new consoles and games can easily be added sounds promising. With OpenAI Universe and DeepMind Lab that just came out, though, I am not sure we really need another one right now. Especially since using ROMs of emulated games we do not own is technically illegal: it looks like this did not cause too much trouble for Atari but it might start raising eyebrows if the community moves to more advanced and recent games, especially some Nintendo still makes money from.\n\nBesides the introduction of the environment, it is good to have DQN benchmarks on five games, but this does not add a lot of value. The authors also mention as contribution \"A new benchmarking technique, allowing algorithms to compete against each other, rather than playing against the in-game AI\", but this seems a bit exaggerated to me: the idea of pitting AIs against each other has been at the core of many AI competitions for decades, so it is hardly something new. The finding that reinforcement learning algorithms tend to specialize to their opponent is also not particular surprising.\n\nOverall I believe this is an ok paper but I do not feel it brings enough to the table for a major conference. This does not mean, however, that this new environment won't find a spot in the (now somewhat crowded) space of game-playing frameworks.\n\nOther small comments:\n- There are lots of typos (way too many to mention them all)\n- It is said that Infinite Mario \"still serves as a benchmark platform\", however as far as I know it had to be shutdown due to Nintendo not being too happy about it\n- \"RLE requires an emulator and a computer version of the console game (ROM file) upon initialization rather than a ROM file only. The emulators are provided with RLE\" => how is that different from ALE that requires the emulator Stella which is also provided with ALE?\n- Why is there no DQN / DDDQN result on Super Mario?\n- It is not clear if Figure 2 displays the F-Zero results using reward shaping or not\n- The Du et al reference seems incomplete", "IS_META_REVIEW": false, "RECOMMENDATION": 4, "DATE": "17 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "No Title", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "This paper presents a valuable new collection of video game benchmarks, in an extendable framework, and establishes initial baselines on a few of them.\n\nReward structures: for how many of the possible games have you implemented the means to extract scores and incremental reward structures? From the github repo it looks like about 10 -- do you plan to add more, and when?\n\n\u201crivalry\u201d training: this is one of the weaker components of the paper, and it should probably be emphasised less. On this topic, there is a vast body of (uncited) multi-agent literature, it is a well-studied problem setup (more so than RL itself). To avoid controversy, I would recommend not claiming any novel contribution on the topic (I don\u2019t think that you really invented \u201ca new method to train an agent by enabling it to train against several opponents\u201d nor \u201ca new benchmarking technique for agents evaluation, by enabling them to compete against each other, rather than playing against the in-game AI\u201d). Instead, just explain that you have established single-agent and multi-agent baselines for your new benchmark suite.\n\nYour definition of Q-function (\u201cpredicts the score at the end of the game given the current state and selected action\u201d) is incorrect. It should read something like: it estimates the cumulative discounted reward that can be obtained from state s, starting with action a (and then following a certain policy).\n\nMinor:\n* Eq (1): the Q-net inside the max() is the target network, with different parameters theta\u2019\n* the Du et al. reference is missing the year\n* some of the other references should point at the corresponding published papers instead of the arxiv versions", "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"DATE": "11 Dec 2016", "TITLE": "Legal issues & most recent game environments", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2"}, {"DATE": "03 Dec 2016", "TITLE": "Rivalry", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1"}, {"DATE": "02 Dec 2016", "TITLE": "benchmarks", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3"}, {"IS_META_REVIEW": true, "comments": "The paper presents a new environment, called Retro Learning Environment (RLE), for reinforcement learning. The authors focus on Super Nintendo but claim that the interface supports many others (including ALE). Benchmark results are given for standard algorithms in 5 new Super Nintendo games, and some results using a new \"rivalry metric\".\n\nThese environments (or, more generally, standardized evaluation methods like public data sets, competitions, etc.) have a long history of improving the quality of AI and machine learning research. One example in the past few years was the Atari Learning Environment (ALE) which has now turned into a standard benchmark for comparison of algorithms and results. In this sense, the RLE could be a worthy contribution to the field by encouraging new challenging domains for research.\n\nThat said, the main focus of this paper is presenting this new framework and showcasing the importance of new challenging domains. The results of experiments themselves are for existing algorithms. There are some new results that show reward shaping and policy shaping (having a bias toward going right in Super Mario) help during learning. And, yes, domain knowledge helps, but this is obvious. The rivalry training is an interesting idea, when training against a different opponent, the learner overfits to that opponent and forgets to play against the in-game AI; but then oddly, it gets evaluated on how well it does against the in-game AI! \n\nAlso the part of the paper that describes the scientific results (especially the rivalry training) is less polished, so this is disappointing. In the end, I'm not very excited about this paper.\n\nI was hoping for a more significant scientific contribution to accompany in this new environment. It's not clear if this is necessary for publication, but also it's not clear that ICLR is the right venue for this work due to the contribution being mainly about the new code (for example, mloss.org could be a better 'venue', JMLR has an associated journal track for accompanying papers:"}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "The authors present a new set of environments, similar to ALE but based on Super Nintendo rather than Atari. This is a great asset and could be important for RL research, but it doesn't merit ICLR publication because of the lack of novel research ideas. Hopefully the authors will consider another venue to publish this paper, such as perhaps a journal or workshop.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"TITLE": "Final review: Nice software contribution, expected more significant scientific contributions", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "The paper presents a new environment, called Retro Learning Environment (RLE), for reinforcement learning. The authors focus on Super Nintendo but claim that the interface supports many others (including ALE). Benchmark results are given for standard algorithms in 5 new Super Nintendo games, and some results using a new \"rivalry metric\".\n\nThese environments (or, more generally, standardized evaluation methods like public data sets, competitions, etc.) have a long history of improving the quality of AI and machine learning research. One example in the past few years was the Atari Learning Environment (ALE) which has now turned into a standard benchmark for comparison of algorithms and results. In this sense, the RLE could be a worthy contribution to the field by encouraging new challenging domains for research.\n\nThat said, the main focus of this paper is presenting this new framework and showcasing the importance of new challenging domains. The results of experiments themselves are for existing algorithms. There are some new results that show reward shaping and policy shaping (having a bias toward going right in Super Mario) help during learning. And, yes, domain knowledge helps, but this is obvious. The rivalry training is an interesting idea, when training against a different opponent, the learner overfits to that opponent and forgets to play against the in-game AI; but then oddly, it gets evaluated on how well it does against the in-game AI! \n\nAlso the part of the paper that describes the scientific results (especially the rivalry training) is less polished, so this is disappointing. In the end, I'm not very excited about this paper.\n\nI was hoping for a more significant scientific contribution to accompany in this new environment. It's not clear if this is necessary for publication, but also it's not clear that ICLR is the right venue for this work due to the contribution being mainly about the new code (for example, mloss.org could be a better 'venue', JMLR has an associated journal track for accompanying papers: ", "IS_META_REVIEW": false, "RECOMMENDATION": 5, "DATE": "19 Dec 2016 (modified: 23 Jan 2017)", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "Ok but limited contributions", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "This paper introduces a new reinforcement learning environment called \u00ab The Retro Learning Environment\u201d, that interfaces with the open-source LibRetro API to offer access to various emulators and associated games (i.e. similar to the Atari 2600 Arcade Learning Environment, but more generic). The first supported platform is the SNES, with 5 games (more consoles and games may be added later). Authors argue that SNES games pose more challenges than Atari\u2019s (due to more complex graphics, AI and game mechanics). Several DQN variants are evaluated in experiments, and it is also proposed to compare learning algorihms by letting them compete against each other in multiplayer games.\n\nI like the idea of going toward more complex games than those found on Atari 2600, and having an environment where new consoles and games can easily be added sounds promising. With OpenAI Universe and DeepMind Lab that just came out, though, I am not sure we really need another one right now. Especially since using ROMs of emulated games we do not own is technically illegal: it looks like this did not cause too much trouble for Atari but it might start raising eyebrows if the community moves to more advanced and recent games, especially some Nintendo still makes money from.\n\nBesides the introduction of the environment, it is good to have DQN benchmarks on five games, but this does not add a lot of value. The authors also mention as contribution \"A new benchmarking technique, allowing algorithms to compete against each other, rather than playing against the in-game AI\", but this seems a bit exaggerated to me: the idea of pitting AIs against each other has been at the core of many AI competitions for decades, so it is hardly something new. The finding that reinforcement learning algorithms tend to specialize to their opponent is also not particular surprising.\n\nOverall I believe this is an ok paper but I do not feel it brings enough to the table for a major conference. This does not mean, however, that this new environment won't find a spot in the (now somewhat crowded) space of game-playing frameworks.\n\nOther small comments:\n- There are lots of typos (way too many to mention them all)\n- It is said that Infinite Mario \"still serves as a benchmark platform\", however as far as I know it had to be shutdown due to Nintendo not being too happy about it\n- \"RLE requires an emulator and a computer version of the console game (ROM file) upon initialization rather than a ROM file only. The emulators are provided with RLE\" => how is that different from ALE that requires the emulator Stella which is also provided with ALE?\n- Why is there no DQN / DDDQN result on Super Mario?\n- It is not clear if Figure 2 displays the F-Zero results using reward shaping or not\n- The Du et al reference seems incomplete", "IS_META_REVIEW": false, "RECOMMENDATION": 4, "DATE": "17 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "No Title", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "This paper presents a valuable new collection of video game benchmarks, in an extendable framework, and establishes initial baselines on a few of them.\n\nReward structures: for how many of the possible games have you implemented the means to extract scores and incremental reward structures? From the github repo it looks like about 10 -- do you plan to add more, and when?\n\n\u201crivalry\u201d training: this is one of the weaker components of the paper, and it should probably be emphasised less. On this topic, there is a vast body of (uncited) multi-agent literature, it is a well-studied problem setup (more so than RL itself). To avoid controversy, I would recommend not claiming any novel contribution on the topic (I don\u2019t think that you really invented \u201ca new method to train an agent by enabling it to train against several opponents\u201d nor \u201ca new benchmarking technique for agents evaluation, by enabling them to compete against each other, rather than playing against the in-game AI\u201d). Instead, just explain that you have established single-agent and multi-agent baselines for your new benchmark suite.\n\nYour definition of Q-function (\u201cpredicts the score at the end of the game given the current state and selected action\u201d) is incorrect. It should read something like: it estimates the cumulative discounted reward that can be obtained from state s, starting with action a (and then following a certain policy).\n\nMinor:\n* Eq (1): the Q-net inside the max() is the target network, with different parameters theta\u2019\n* the Du et al. reference is missing the year\n* some of the other references should point at the corresponding published papers instead of the arxiv versions", "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"DATE": "11 Dec 2016", "TITLE": "Legal issues & most recent game environments", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2"}, {"DATE": "03 Dec 2016", "TITLE": "Rivalry", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1"}, {"DATE": "02 Dec 2016", "TITLE": "benchmarks", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3"}]}
{"text": "DROPOUT WITH EXPECTATION-LINEAR REGULARIZATION\n1 INTRODUCTION\nDeep neural networks (DNNs, e.g., LeCun et al., 2015; Schmidhuber, 2015), if trained properly, have been demonstrated to significantly improve the benchmark performances in a wide range of application domains. As neural networks go deeper and deeper, naturally, its model complexity also increases quickly, hence the pressing need to reduce overfitting in training DNNs. A number of techniques have emerged over the years to address this challenge, among which dropout (Hinton et al., 2012; Srivastava, 2013) has stood out for its simplicity and effectiveness. In a nutshell, dropout randomly \u201cdrops\u201d neural units during training as a means to prevent feature co-adaptation\u2014a sign of overfitting (Hinton et al., 2012). Simple as it appears to be, dropout has led to several record-breaking performances (Hinton et al., 2012; Ma & Hovy, 2016), and thus spawned a lot of recent interests in analyzing and justifying dropout from the theoretical perspective, and also in further improving dropout from the algorithmic and practical perspective.\nIn their pioneering work, Hinton et al. (2012) and Srivastava et al. (2014) interpreted dropout as an extreme form of model combination (aka. model ensemble) with extensive parameter/weight sharing, and they proposed to learn the combination through minimizing an appropriate expected loss. Interestingly, they also pointed out that for a single logistic neural unit, the output of dropout is in fact the geometric mean of the outputs of the model ensemble with shared parameters. Subsequently, many theoretical justifications of dropout have been explored, and we can only mention a few here due to space limits. Building on the weight sharing perspective, Baldi & Sadowski (2013; 2014) analyzed the ensemble averaging property of dropout in deep non-linear logistic networks, and supported the view that dropout is equivalent to applying stochastic gradient descent on some regularized\nloss function. Wager et al. (2013) treated dropout as an adaptive regularizer for generalized linear models (GLMs). Helmbold & Long (2016) discussed the differences between dropout and traditional weight decay regularization. In terms of statistical learning theory, Gao & Zhou (2014) studied the Rademacher complexity of different types of dropout, showing that dropout is able to reduce the Rademacher complexity polynomially for shallow neural networks (with one or no hidden layers) and exponentially for deep neural networks. This latter work (Gao & Zhou, 2014) formally demonstrated that dropout, due to its regularizing effect, contributes to reducing the inherent model complexity, in particular the variance component in the generalization error.\nSeen as a model combination technique, it is intuitive that dropout contributes to reducing the variance of the model performance. Surprisingly, dropout has also been shown to play some role in reducing the model bias. For instance, Jain et al. (2015) studied the ability of dropout training to escape local minima, hence leading to reduced model bias. Other studies (Chen et al., 2014; Helmbold & Long, 2014; Wager et al., 2014) focus on the effect of the dropout noise on models with shallow architectures. We noted in passing that there are also some work (Kingma et al., 2015; Gal & Ghahramani, 2015; 2016) trying to understand dropout from the Bayesian perspective.\nIn this work, we first formulate dropout as a tractable approximation of a latent variable model, and give a clean view of weight sharing (\u00a73). Then, we focus on an inference gap in dropout that has somehow gotten under-appreciated: In the inference phase, for computational tractability considerations, the model ensemble generated by dropout is approximated by a single model with scaled weights, resulting in a gap between training and inference, and rendering the many previous theoretical findings inapplicable. In general, this inference gap can be very large and no attempt (to our best knowledge) has been made to control it. We make three contributions in bridging this gap: Theoretically, we introduce expectation-linear dropout neural networks, through which we are able to explicitly quantify the inference gap (\u00a74). In particular, our theoretical results explain why the max-norm constraint on the network weights, a standard practice in training DNNs, can lead to a small inference gap hence potentially improve performance. Algorithmically, we propose to add a sampled version of the inference gap to regularize the standard dropout training objective (expectationlinearization), hence allowing explicit control of the inference gap, and analyze the interaction between expectation-linearization and the model accuracy (\u00a75). Experimentally, through three benchmark datasets we show that our regularized dropout is not only as simple and efficient as standard dropout but also consistently leads to improved performance (\u00a76).\n2 DROPOUT NEURAL NETWORKS\nIn this section we set up the notations, review the dropout neural network model, and discuss the inference gap in standard dropout training that we will attempt to study in the rest of the paper.\n2.1 DNNS AND NOTATIONS\nThroughout we use uppercase letters for random variables (and occasionally for matrices as well), and lowercase letters for realizations of the corresponding random variables. Let X \u2208 X be the input of the neural network, Y \u2208 Y be the desired output, and D = {(x1, y1), . . . , (xN , yN )} be our training sample, where xi, i = 1, . . . , N, (resp. yi) are usually i.i.d. samples of X (resp. Y ).\nLet M denote a deep neural network with L hidden layers, indexed by l \u2208 {1, . . . , L}. Let h(l) denote the output vector from layer l. As usual, h(0) = x is the input, and h(L) is the output of the neural network. Denote \u03b8 = {\u03b8l : l = 1, . . . , L} as the set of parameters in the network M, where \u03b8l assembles the parameters in layer l. With dropout, we need to introduce a set of dropout random variables S = {\u0393(l) : l = 1, . . . , L}, where \u0393(l) is the dropout random variable for layer l. Then the deep neural network M can be described as:\nh(l) = fl(h (l\u22121) \u03b3(l); \u03b8l), l = 1, . . . , L, (1)\nwhere is the element-wise product and fl is the transformation function of layer l. For example, if layer l is a fully connected layer with weight matrix W , bias vector b, and sigmoid activation function \u03c3(x) = 11+exp(\u2212x) , then fl(x) = \u03c3(Wx+ b)). We will also use h\n(l)(x, s; \u03b8) to denote the output of layer l with input x and dropout value s, under parameter \u03b8.\nIn the simplest form of dropout, which is also called standard dropout, \u0393(l) is a vector of independent Bernoulli random variables, each of which has probability pl of being 1 and 1\u2212 pl of being 0. This corresponds to dropping each of the weights independently with probability pl.\n2.2 DROPOUT TRAINING\nThe standard dropout neural networks can be trained using stochastic gradient decent (SGD), with a sub-network sampled by dropping neural units for each training instance in a mini-batch. Forward and backward pass for that training instance are done only on the sampled sub-network. Intuitively, dropout aims at, simultaneously and jointly, training an ensemble of exponentially many neural networks (one for each configuration of dropped units) while sharing the same weights/parameters.\nThe goal of the stochastic training procedure of dropout can be understood as minimizing an expected loss function, after marginalizing out the dropout variables (Srivastava, 2013; Wang & Manning, 2013). In the context of maximal likelihood estimation, dropout training can be formulated as:\n\u03b8\u2217 = argmin \u03b8 ESD [\u2212l(D,SD; \u03b8)] = argmin \u03b8 ESD\n[ \u2212 N\u2211 i=1 log p(yi|xi, Si; \u03b8) ] , (2)\nwhere recall that D is the training sample, SD = {S1, . . . , SN} is the dropout variable (one for each training instance), and l(D,SD; \u03b8) is the (conditional) log-likelihood function defined by the conditional distribution p(y|x, s; \u03b8) of output y given input x, under parameter \u03b8 and dropout variable s. Throughout we use the notation EZ to denote the conditional expectation where all random variables except Z are conditioned on.\nDropout has also been shown to work well with regularization, such as L2 weight decay (Tikhonov, 1943), Lasso (Tibshirani, 1996), KL-sparsity(Bradley & Bagnell, 2008; Hinton, 2010), and max-norm regularization (Srebro et al., 2004), among which the max-norm regularization \u2014 that constrains the norm of the incoming weight matrix to be bounded by some constant \u2014 was found to be especially useful for dropout (Srivastava, 2013; Srivastava et al., 2014).\n2.3 DROPOUT INFERENCE AND GAP\nAs mentioned before, dropout is effectively training an ensemble of neural networks with weight sharing. Consequently, at test time, the output of each network in the ensemble should be averaged to deliver the final prediction. This averaging over exponentially many sub-networks is, however, intractable, and standard dropout typically implements an approximation by introducing a deterministic scaling factor for each layer to replace the random dropout variable:\nES [H (L)(x, S; \u03b8)] ? \u2248 h(L)(x,E[S]; \u03b8), (3)\nwhere the right-hand side is the output of a single deterministic neural network whose weights are scaled to match the expected number of active hidden units on the left-hand side. Importantly, the right-hand side can be easily computed since it only involves a single deterministic network.\nBul\u00f2 et al. (2016) combined dropout with knowledge distillation methods (Hinton et al., 2015) to better approximate the averaging processing of the left-hand side. However, the quality of the approximation in (3) is largely unknown, and to our best knowledge, no attempt has been made to explicitly control this inference gap. The main goal of this work is to explicitly quantify, algorithmically control, and experimentally demonstrate the inference gap in (3), in the hope of improving the generalization performance of DNNs eventually. To this end, in the next section we first present a latent variable model interpretation of dropout, which will greatly facilitate our later theoretical analysis.\n3 DROPOUT AS LATENT VARIABLE MODELS\nWith the end goal of studying the inference gap in (3) in mind, in this section, we first formulate dropout neural networks as a latent variable model (LVM) in \u00a7 3.1. Then, we point out the relation between the training procedure of LVM and that of standard dropout in \u00a7 3.2. The advantage of formulating dropout as a LVM is that we need only deal with a single model (with latent structure), instead of an ensemble of exponentially many different models (with weight sharing). This much\nsimplified view of dropout enables us to understand and analyze the model parameter \u03b8 in a much more straightforward and intuitive way.\n3.1 AN LVM FORMULATION OF DROPOUT\nA latent variable model consists of two types of variables: the observed variables that represent the empirical (observed) data and the latent variables that characterize the hidden (unobserved) structure. To formulate dropout as a latent variable model, the input x and output y are regarded as observed variables, while the dropout variable s, representing the sub-network structure, is hidden. Then, upon fixing the input space X , the output space Y , and the latent space S for dropout variables, the conditional probability of y given x under parameter \u03b8 can be written as\np(y|x; \u03b8) = \u222b S p(y|x, s; \u03b8)p(s)d\u00b5(s), (4)\nwhere p(y|x, s; \u03b8) is the conditional distribution modeled by the neutral network with configuration s (same as in Eq. (2)), p(s) is the distribution of dropout variable S (e.g. Bernoulli), here assumed to be independent of the input x, and \u00b5(s) is the base measure on the space S.\n3.2 LVM DROPOUT TRAINING VS. STANDARD DROPOUT TRAINING\nBuilding on the above latent variable model formulation (4) of dropout, we are now ready to point out a simple relation between the training procedure of LVM and that of standard dropout. Given an i.i.d. training sample D, the maximum likelihood estimate for the LVM formulation of dropout in (4) is equivalent to minimizing the following negative log-likelihood function:\n\u03b8\u2217 = argmin \u03b8 \u2212l(D; \u03b8) = argmin \u03b8 \u2212 N\u2211 i=1 log p(yi|xi; \u03b8), (5)\nwhere p(y|x; \u03b8) is given in Eq. (4). Recall the dropout training objective ESD [\u2212l(D,SD; \u03b8)] in Eq. (2). We have the following theorem as a simple consequence of Jensen\u2019s inequality (details in Appendix A): Theorem 1. The expected loss function of standard dropout (Eq. (2)) is an upper bound of the negative log-likelihood of LVM dropout (Eq. (5)):\n\u2212l(D; \u03b8) \u2264 ESD [\u2212l(D,SD; \u03b8)]. (6)\nTheorem 1, in a rigorous sense, justifies dropout training as a convenient and tractable approximation of the LVM formulation in (4). Indeed, since directly minimizing the marginalized negative loglikelihood in (5) may not be easy, a standard practice is to replace the marginalized (conditional) likelihood p(y|x; \u03b8) in (4) with its empirical Monte carlo average through drawing samples from the dropout variable S. The dropout training objective in (2) corresponds exactly to this Monte carlo approximation when a single sample Si is drawn for each training instance (xi, yi). Importantly, we note that the above LVM formulation involves only a single network parameter \u03b8, which largely simplifies the picture and facilitates our subsequent analysis.\n4 EXPECTATION-LINEAR DROPOUT NEURAL NETWORKS\nBuilding on the latent variable model formulation in \u00a7 3, we introduce in this section the notion of expectation-linearity that essentially measures the inference gap in (3). We then characterize a general class of neural networks that exhibit expectation-linearity, either exactly or approximately over a distribution p(x) on the input space.\nWe start with defining expectation-linearity in the simplest single-layer neural network, then we extend the notion into general deep networks in a natural way. Definition 1 (Expectation-linear Layer). A network layer h = f(x \u03b3; \u03b8) is expectation-linear with respect to a set X \u2032 \u2286 X , if for all x \u2208 X \u2032 we have\u2225\u2225E[f(x \u0393; \u03b8)]\u2212 f(x E[\u0393]; \u03b8)\u2225\u2225\n2 = 0. (7)\nIn this case we say that X \u2032 is expectation-linearizable, and \u03b8 is expectation-linearizing w.r.t X \u2032.\nObviously, the condition in (7) will guarantee no gap in the dropout inference approximation (3)\u2014an admittedly strong condition that we will relax below. Clearly, if f is an affine function, then we can choose X \u2032 = X and expectation-linearity is trivial. Note that expectation-linearity depends on the network parameter \u03b8 and the dropout distribution \u0393.\nExpectation-linearity, as defined in (7), is overly strong: under standard regularity conditions, essentially the transformation function f has to be affine over the set X \u2032, ruling out for instance the popular sigmoid or tanh activation functions. Moreover, in practice, downstream use of DNNs are usually robust to small errors resulting from approximate expectation-linearity (hence the empirical success of dropout), so it makes sense to define an inexact extension. We note also that the definition in (7) is uniform over the set X \u2032, while in a statistical setting it is perhaps more meaningful to have expectation-linearity \u201con average,\u201d since inputs from lower density regions are not going to play a significant role anyway. Taking into account the aforementioned motivations, we arrive at the following inexact extension: Definition 2 (Approximately Expectation-linear Layer). A network layer h = f(x \u03b3; \u03b8) is \u03b4-approximately expectation-linear with respect to a distribution p(x) over X if\nEX [\u2225\u2225E\u0393[f(X \u0393; \u03b8)|X]\u2212 f(X E[\u0393]; \u03b8)\u2225\u22252] < \u03b4. (8) In this case we say that p(x) is \u03b4-approximately expectation-linearizable, and \u03b8 is \u03b4-approximately expectation-linearizing.\nTo appreciate the power of cutting some slack from exact expectation-linearity, we remark that even non-affine activation functions often have approximately linear regions. For example, the logistic function, a commonly used non-linear activation function in DNNs, is approximately linear around the origin. Naturally, we can ask whether it is sufficient for a target distribution p(x) to be well-approximated by an approximately expectation-linearizable one. We begin by providing an appropriate measurement of the quality of this approximation. Definition 3 (Closeness, (Andreas et al., 2015)). A distribution p(x) is C-close to a set X \u2032 \u2286 X if\nE [\ninf x\u2217\u2208X \u2032 sup \u03b3\u2208S \u2016X \u03b3 \u2212 x\u2217 \u03b3\u20162\n] \u2264 C, (9)\nwhere recall that S is the (bounded) space that the dropout variable lives in.\nIntuitively, p(x) is C-close to a set X \u2032 if a random sample from p is no more than a distance C from X \u2032 in expectation and under the worst \u201cdropout perturbation\u201d. For example, a standard normal distribution is close to an interval centering at origin ([\u2212\u03b1, \u03b1]) with some constant C. Our definition of closeness is similar to that in Andreas et al. (2015), who used this notion to analyze self-normalized log-linear models.\nWe are now ready to state our first major result that quantifies approximate expectation-linearity of a single-layered network (proof in Appendix B.1): Theorem 2. Given a network layer h = f(x \u03b3; \u03b8), where \u03b8 is expectation-linearizing w.r.t. X \u2032 \u2286 X . Suppose p(x) is C-close to X \u2032 and for all x \u2208 X , \u2016\u2207xf(x)\u2016op \u2264 B, where \u2016 \u00b7 \u2016op is the usual operator norm. Then, p(x) is 2BC-approximately expectation-linearizable.\nRoughly, Theorem 2 states that the input distribution p(x) that place most of its mass on regions close to expectation-linearizable sets are approximately expectation-linearizable on a similar scale. The bounded operator norm assumption on the derivative \u2207f is satisfied in most commonly used layers. For example, for a fully connected layer with weight matrix W , bias vector b, and activation function \u03c3, \u2016\u2207f(\u00b7)\u2016op = |\u03c3\u2032(\u00b7)| \u00b7 \u2016W\u2016op is bounded by \u2016W\u2016op and the supremum of |\u03c3\u2032(\u00b7)| (1/4 when \u03c3 is sigmoid and 1 when \u03c3 is tanh).\nNext, we extend the notion of approximate expectation-linearity to deep dropout neural networks. Definition 4 (Approximately Expectation-linear Network). A deep neural network with L layers (cf. Eq. (1)) is \u03b4-approximately expectation-linear with respect to p(x) over X if\nEX [\u2225\u2225ES[H(L)(X,S; \u03b8)|X]\u2212 h(L)(X,E[S]; \u03b8)\u2225\u22252] < \u03b4. (10) where h(L)(X,E[S]; \u03b8) is the output of the deterministic neural network in standard dropout.\nLastly, we relate the level of approximate expectation-linearity of a deep neural network to the level of approximate expectation-linearity of each of its layers: Theorem 3. Given an L-layer neural network as in Eq. (1), and suppose that each layer l \u2208 {1, . . . , L} is \u03b4-approximately expectation-linear w.r.t. p(h(l)), E[\u0393(l)] \u2264 \u03b3, supx \u2016\u2207fl(x)\u2016op \u2264 B, and E [ Var[H(l)|X] ] \u2264 \u03c32. Then the network is \u2206-approximately expectation-linear with\n\u2206 = (B\u03b3)L\u22121\u03b4 + (\u03b4 +B\u03b3\u03c3)\n( 1\u2212 (B\u03b3)L\u22121\n1\u2212B\u03b3\n) . (11)\nFrom Theorem 3 (proof in Appendix B.2) we observe that the level of approximate expectationlinearity of the network mainly depends on four factors: the level of approximate expecatationlinearity of each layer (\u03b4), the expected variance of each layer (\u03c3), the operator norm of the derivative of each layer\u2019s transformation function (B), and the mean of each layer\u2019s dropout variable (\u03b3). In practice, \u03b3 is often a constant less than or equal to 1. For example, if \u0393 \u223c Bernoulli(p), then \u03b3 = p. According to the theorem, the operator norm of the derivative of each layer\u2019s transformation function is an important factor in the level of approximate expectation-linearity: the smaller the operator norm is, the better the approximation. Interestingly, the operator norm of a layer often depends on the norm of the layer\u2019s weight (e.g. for fully connected layers). Therefore, adding max-norm constraints to regularize dropout neural networks can lead to better approximate expectation-linearity hence smaller inference gap and the often improved model performance.\nIt should also be noted that when B\u03b3 < 1, the approximation error \u2206 tends to be a constant when the network becomes deeper. When B\u03b3 = 1, \u2206 grows linearly with L, and when B\u03b3 > 1, the growth of \u2206 becomes exponential. Thus, it is essential to keep B\u03b3 < 1 to achieve good approximation, particularly for deep neural networks.\n5 EXPECTATION-LINEAR REGULARIZED DROPOUT\nIn the previous section we have managed to bound the approximate expectation-linearity, hence the inference gap in (3), of dropout neural networks. In this section, we first prove a uniform deviation bound of the sampled approximate expectation-linearity measure from its mean, which motivates adding the sampled (hence computable) expectation-linearity measure as a regularization scheme to standard dropout, with the goal of explicitly controlling the inference gap of the learned parameter, hence potentially improving the performance. Then we give the upper bounds on the loss in accuracy due to expectation-linearization, and describe classes of distributions that expectation-linearize easily.\n5.1 A UNIFORM DEVIATION BOUND FOR THE SAMPLED EXPECTATION-LINEAR MEASURE\nWe now show that an expectation-linear network can be found by expectation-linearizing the network on the training sample. To this end, we prove a uniform deviation bound between the empirical expectation-linearization measure using i.i.d. samples (Eq. (12)) and its mean (Eq. (13)).\nTheorem 4. Let H = { h(L)(x, s; \u03b8) : \u03b8 \u2208 \u0398 } denote a space of L-layer dropout neural networks\nindexed with \u03b8, where h(L) : X \u00d7 S \u2192 R and \u0398 is the space that \u03b8 lives in. Suppose that the neural networks inH satisfy the constraints: 1) \u2200x \u2208 X , \u2016x\u20162 \u2264 \u03b1; 2) \u2200l \u2208 {1, . . . , L},E(\u0393(l)) \u2264 \u03b3 and \u2016\u2207fl\u2016op \u2264 B; 3) \u2016h(L)\u2016 \u2264 \u03b2. Denote empirical expectation-linearization measure and its mean as:\n\u2206\u0302 = 1\nn n\u2211 i=1 \u2225\u2225ESi[H(L)(Xi, Si; \u03b8)]\u2212 h(L)(Xi,E[Si]; \u03b8)\u2225\u22252, (12) \u2206 = EX\n[\u2225\u2225ES[H(L)(X,S; \u03b8)]\u2212 h(L)(X,E[S]; \u03b8)\u2225\u22252]. (13) Then, with probability at least 1\u2212 \u03bd, we have\nsup \u03b8\u2208\u0398 |\u2206\u2212 \u2206\u0302| < 2\u03b1B L(\u03b3L/2 + 1)\u221a n + \u03b2\n\u221a log(1/\u03bd)\nn . (14)\nFrom Theorem 4 (proof in Appendix C.1) we observe that the deviation bound decreases exponentially with the number of layers L when the operator norm of the derivative of each layer\u2019s transformation\nfunction (B) is less than 1 (and the contrary if B \u2265 1). Importantly, the square root dependence on the number of samples (n) is standard and cannot be improved without significantly stronger assumptions.\nIt should be noted that Theorem 4 per se does not imply anything between expectation-linearization and the model accuracy (i.e. how well the expectation-linearized neural network actually achieves on modeling the data). Formally studying this relation is provided in \u00a7 5.3. In addition, we provide some experimental evidences in \u00a7 6 on how improved approximate expectation-linearity (equivalently smaller inference gap) does lead to better empirical performances.\n5.2 EXPECTATION-LINEARIZATION AS REGULARIZATION\nThe uniform deviation bound in Theorem 4 motivates the possibility of obtaining an approximately expectation-linear dropout neural networks through adding the empirical measure (12) as a regularization scheme to the standard dropout training objective, as follows:\nloss(D; \u03b8) = \u2212l(D; \u03b8) + \u03bbV (D; \u03b8), (15) where \u2212l(D; \u03b8) is the negative log-likelihood defined in Eq. (5), \u03bb > 0 is a regularization constant, and V (D; \u03b8) measures the level of approximate expectation-linearity:\nV (D; \u03b8) = 1\nN N\u2211 i=1 \u2225\u2225ESi[H(L)(xi, Si; \u03b8)]\u2212 h(L)(xi,E[Si]; \u03b8)\u2225\u222522. (16) To solve (15), we can minimize loss(D; \u03b8) via stochastic gradient descent as in standard dropout, and approximate V (D; \u03b8) using Monte carlo:\nV (D; \u03b8) \u2248 1 N N\u2211 i=1 \u2225\u2225h(L)(xi, si; \u03b8)\u2212 h(L)(xi,E[Si]; \u03b8)\u2225\u222522, (17) where si is the same dropout sample as in l(D; \u03b8) for each training instance in a mini-batch. Thus, the only additional computational cost comes from the deterministic term h(L)(xi,E[Si]; \u03b8). Overall, our regularized dropout (15), in its Monte carlo approximate form, is as simple and efficient as the standard dropout.\n5.3 ON THE ACCURACY OF EXPECTATION-LINEARIZED MODELS\nSo far our discussion has concentrated on the problem of finding expectation-linear neural network models, without any concerns on how well they actually perform at modeling the data. In this section, we characterize the trade-off between maximizing \u201cdata likelihood\u201d and satisfying an expectationlinearization constraint.\nTo achieve the characterization, we measure the likelihood gap between the classical maximum likelihood estimator (MLE) and the MLE subject to a expectation-linearization constraint. Formally, given training data D = {(x1, y1), . . . , (xn, yn)}, we define\n\u03b8\u0302 = argmin \u03b8\u2208\u0398\n\u2212l(D; \u03b8) (18)\n\u03b8\u0302\u03b4 = argmin \u03b8\u2208\u0398,V (D;\u03b8)\u2264\u03b4\n\u2212l(D; \u03b8) (19)\nwhere \u2212l(D; \u03b8) is the negative log-likelihood defined in Eq. (5), and V (D; \u03b8) is the level of approximate expectation-linearity in Eq. (16).\nWe would like to control the loss of model accuracy by obtaining a bound on the likelihood gap defined as:\n\u2206l(\u03b8\u0302, \u03b8\u0302\u03b4) = 1\nn (l(D; \u03b8\u0302)\u2212 l(D; \u03b8\u0302\u03b4)) (20)\nIn the following, we focus on neural networks with softmax output layer for classification tasks.\np(y|x, s; \u03b8) = h(L)y (x, s; \u03b8) = fL(h(L\u22121)(x, s); \u03b7) = e\u03b7\nT y h (L\u22121)(x,s)\u2211 y\u2032\u2208Y e \u03b7T y\u2032h (L\u22121)(x,s) (21)\nwhere \u03b8 = {\u03b81, . . . , \u03b8L\u22121, \u03b7}, Y = {1, . . . , k} and \u03b7 = {\u03b7y : y \u2208 Y}. We claim:\nTheorem 5. Given an L-layer neural network h(L)(x, s; \u03b8) with softmax output layer in (21), where parameter \u03b8 \u2208 \u0398, dropout variable s \u2208 S, input x \u2208 X and target y \u2208 Y . Suppose that for every x and s, p(y|x, s; \u03b8\u0302) makes a unique best prediction\u2014that is, for each x \u2208 X , s \u2208 S, there exists a unique y\u2217 \u2208 Y such that \u2200y 6= y\u2217, \u03b7\u0302Ty h(L\u22121)(x, s) < \u03b7\u0302Ty\u2217h(L\u22121)(x, s). Suppose additionally that \u2200x, s, \u2016h(L\u22121)(x, s; \u03b8\u0302)\u2016 \u2264 \u03b2, and \u2200y, p(y|x; \u03b8\u0302) > 0. Then\n\u2206l(\u03b8\u0302, \u03b8\u0302\u03b4) \u2264 c1\u03b22 ( \u2016\u03b7\u0302\u20162 \u2212 \u03b4\n4\u03b2\n)2 e\u2212c2\u03b4/4\u03b2 (22)\nwhere c1 and c2 are distribution-dependent constants.\nFrom Theorem 5 (proof in Appendix C.2) we observe that, at one extreme, distributions closed to deterministic can be expectation-linearized with little loss of likelihood.\nWhat about the other extreme \u2014 distributions \u201cas close to uniform distribution as possible\u201d? With suitable assumptions about the form of p(y|x, s; \u03b8\u0302) and p(y|x; \u03b8\u0302), we can achieve an accuracy loss bound for distributions that are close to uniform: Theorem 6. Suppose that \u2200x, s, \u2016h(L\u22121)(x, s; \u03b8\u0302)\u2016 \u2264 \u03b2. Additionally, for each (xi, yi) \u2208 D, s \u2208 S , log 1k \u2264 log p(yi|xi, s; \u03b8\u0302) \u2264 1 k \u2211 y\u2208Y log p(y|xi, s; \u03b8\u0302). Then asymptotically as n\u2192\u221e:\n\u2206l(\u03b8\u0302, \u03b8\u0302\u03b4) \u2264 (\n1\u2212 \u03b4 4\u03b2\u2016\u03b7\u0302\u20162\n) E [KL (p(\u00b7|X; \u03b8)\u2016Unif(Y))] (23)\nTheorem 6 (proof in Appendix C.3) indicates that uniform distributions are also an easy class for expectation-linearization.\nThe next question is whether there exist any classes of conditional distributions p(y|x) for which all distributions are provably hard to expectation-linearize. It remains an open problem and might be an interesting direction for future work.\n6 EXPERIMENTS\nIn this section, we evaluate the empirical performance of the proposed regularized dropout in (15) on a variety of network architectures for the classification task on three benchmark datasets\u2014MNIST, CIFAR-10 and CIFAR-100. We applied the same data preprocessing procedure as in Srivastava et al. (2014). To make a thorough comparison and provide experimental evidence on how the expectationlinearization interacts with the predictive power of the learned model, we perform experiments of Monte Carlo (MC) dropout, which approximately computes the final prediction (left-hand side of (3)) via Monte Carlo sampling, w/o the proposed regularizer. In the case of MC dropout, we average m = 100 predictions using randomly sampled configurations. In addition, the network architectures and hyper-parameters for each experiment setup are the same as those in Srivastava et al. (2014), unless we explicitly claim to use different ones. Following previous works, for each data set We held out 10,000 random training images for validation to tune the hyper-parameters, including \u03bb in Eq. (15). When the hyper-parameters are fixed, we train the final models with all the training data, including the validation data. A more detailed description of the conducted experiments can be provided in Appendix D. For each experiment, we report the mean test errors with corresponding standard deviations over 5 repetitions.\n6.1 MNIST\nThe MNIST dataset (LeCun et al., 1998) consists of 70,000 handwritten digit images of size 28\u00d728, where 60,000 images are used for training and the rest for testing. This task is to classify the images into 10 digit classes. For the purpose of comparison, we train 6 neural networks with different architectures. The experimental results are shown in Table 1.\n6.2 CIFAR-10 AND CIFAR-100\nThe CIFAR-10 and CIFAR-100 datasets (Krizhevsky, 2009) consist of 60,000 color images of size 32\u00d7 32, drawn from 10 and 100 categories, respectively. 50,000 images are used for training and the\nrest for testing. The neural network architecture we used for these two datasets has 3 convolutional layers, followed by two fully-connected (dense) hidden layers (again, same as that in Srivastava et al. (2014)). The experimental results are recorded in Table 1, too.\nFrom Table 1 we can see that on MNIST data, dropout network training with expectation-linearization outperforms standard dropout on all 6 neural architectures. On CIFAR data, expectation-linearization reduces error rate from 12.82% to 12.20% for CIFAR-10, achieving 0.62% improvement. For CIFAR-100, the improvement in terms of error rate is 0.97% with reduction from 37.22% to 36.25%.\nFrom the results we see that with or without expectation-linearization, the MC dropout networks achieve similar results. It illustrates that by achieving expectation-linear neural networks, the predictive power of the learned models has not degraded significantly. Moreover, it is interesting to see that with the regularization, on MNIST dataset, standard dropout networks achieve even better accuracy than MC dropout. It may be because that with expectation-linearization, standard dropout inference achieves better approximation of the final prediction than MC dropout with (only) 100 samples. On CIFAR datasets, MC dropout networks achieve better accuracy than the ones with the regularization. But, obviously, MC dropout requires much more inference time than standard dropout (MC dropout with m samples requires about m times the inference time of standard dropout).\n6.3 EFFECT OF REGULARIZATION CONSTANT \u03bb\nIn this section, we explore the effect of varying the hyper-parameter for the expectation-linearization rate \u03bb. We train the network architectures in Table 1 with the \u03bb value ranging from 0.1 to 10.0. Figure 1 shows the test errors obtained as a function of \u03bb on three datasets. In addition, Figure 1, middle and right panels, also measures the empirical expectation-linearization risk \u2206\u0302 of Eq. (12) with varying \u03bb on CIFAR-10 and CIFAR-100, where \u2206\u0302 is computed using Monte carlo with 100 independent samples.\nFrom Figure 1 we can see that when \u03bb increases, better expectation-linearity is achieved (i.e. \u2206\u0302 decreases). The model accuracy, however, has not kept growing with increasing \u03bb, showing that in practice considerations on the trade-off between model expectation-linearity and accuracy are needed.\n6.4 COMPARISON WITH DROPOUT DISTILLATION\nTo make a thorough empirical comparison with the recently proposed Dropout Distillation method (Bul\u00f2 et al., 2016), we also evaluate our regularization method on CIFAR-10 and CIFAR-100 datasets with the All Convolutional Network (Springenberg et al., 2014) (AllConv). To facilitate comparison, we adopt the originally reported hyper-parameters and the same setup for training.\nTable 2 gives the results comparison the classification error percentages on test data under AllConv using standard dropout, Monte Carlo dropout, standard dropout with our proposed expectationlinearization, and recently proposed dropout distillation on CIFAR-10 and CIFAR-100 1. According to Table 2, our proposed expectation-linear regularization method achieves comparable performance to dropout distillation.\n7 CONCLUSIONS\nIn this work, we attempted to establish a theoretical basis for the understanding of dropout, motivated by controlling the gap between dropout\u2019s training and inference phases. Through formulating dropout as a latent variable model and introducing the notion of (approximate) expectation-linearity, we have formally studied the inference gap of dropout, and introduced an empirical measure as a regularization scheme to explicitly control the gap. Experiments on three benchmark datasets demonstrate that reducing the inference gap can indeed improve the end performance. In the future, we intend to formally relate the inference gap to the generalization error of the underlying network, hence providing further justification of regularized dropout.\nACKNOWLEDGEMENTS\nThis research was supported in part by DARPA grant FA8750-12-2-0342 funded under the DEFT program. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of DARPA.\nAPPENDIX: DROPOUT WITH EXPECTATION-LINEAR REGULARIZATION\nA LVM DROPOUT TRAINING VS. STANDARD DROPOUT TRAINING\nProof of Theorem 1\nProof.\nESD [l(D,SD; \u03b8)] = \u222b S N\u220f i=1 p(si) ( N\u2211 i=1 log p(yi|xi, si; \u03b8) ) d\u00b5(s1) . . . d\u00b5(sN )\n= N\u2211 i=1 \u222b S p(si) log p(yi|xi, si; \u03b8)d\u00b5(si)\nBecause log(\u00b7) is a concave function, from Jensen\u2019s Inequality,\u222b S p(s) log p(y|x, s; \u03b8)d\u00b5(s) \u2264 log \u222b S p(s)p(y|x, s; \u03b8)d\u00b5(s)\nThus\nESD [\u2212l(D,SD; \u03b8)] \u2265 N\u2211 i=1 log \u222b S p(si)p(yi|xi, si; \u03b8)d\u00b5(si) = \u2212l(D; \u03b8).\nB EXPECTATION-LINEAR DROPOUT NEURAL NETWORKS\nB.1 PROOF OF THEOREM 2\nProof. Let \u03b3\u2217 = E[\u0393], and\nA \u2206 = {x : \u2016E[f(x \u0393; \u03b8)]\u2212 f(x \u03b3\u2217; \u03b8)\u20162 = 0}\nLet X\u2217 = argmin x\u2208A sup \u03b3\u2208S \u2016X \u03b3 \u2212 x \u03b3\u20162, and X\u2212 = X \u2212X\u2217. Then,\nX \u03b3 = X\u2217 \u03b3 +X\u2212 \u03b3\nIn the following, we omit the parameter \u03b8 for convenience. Moreover, we denote E\u0393 [ f(X \u0393; \u03b8) ] \u2206 = E [ f(X \u0393; \u03b8)|X ] From Taylor Series, there exit some X \u2032, X \u2032\u2032 \u2208 X satisfy that\nf(X \u0393) = f(X\u2217 \u0393) + f \u2032(X \u2032 \u0393)(X\u2212 \u0393) f(X \u03b3\u2217) = f(X\u2217 \u03b3\u2217) + f \u2032(X \u2032\u2032 \u03b3\u2217)(X\u2212 \u03b3\u2217)\nwhere we denote f \u2032(x) = (\u2207xf(x))T . Then,\nE\u0393[f(X \u0393)\u2212 f(X \u03b3\u2217)] = E\u0393[f(X\n\u2217 \u0393 +X\u2212 \u0393)\u2212 f(X\u2217 \u03b3\u2217 +X\u2212 \u03b3\u2217)] = E\u0393[f(X\n\u2217 \u0393)\u2212 f(X\u2217 \u03b3\u2217) + f \u2032(X \u2032 \u0393)(X\u2212 \u0393)\u2212 f \u2032(X \u2032\u2032 \u03b3\u2217)(X\u2212 \u03b3\u2217)] = E\u0393[f(X \u2217 \u0393)\u2212 f(X\u2217 \u03b3\u2217)] + E\u0393[f \u2032(X \u2032 \u0393)(X\u2212 \u0393)\u2212 f \u2032(X \u2032\u2032 \u03b3\u2217)(X\u2212 \u03b3\u2217)]\nSince X\u2217 \u2208 A, we have E\u0393[f(X\n\u2217 \u0393)\u2212 f(X\u2217 \u03b3\u2217)] = 0. Then,\nE\u0393[f(X \u0393)\u2212 f(X \u03b3\u2217)] = E\u0393[f\n\u2032(X \u2032 \u0393)(X\u2212 \u0393)\u2212 f \u2032(X \u2032\u2032 \u03b3\u2217)(X\u2212 \u03b3\u2217)] = E\u0393[(f\n\u2032(X \u2032 \u0393)\u2212 f \u2032(X \u2032\u2032 \u03b3\u2217))(X\u2212 \u0393)] + E\u0393[f \u2032(X \u2032\u2032 \u03b3\u2217)(X\u2212 \u0393\u2212X\u2212 \u03b3\u2217)] = E\u0393[(f \u2032(X \u2032 \u0393)\u2212 f \u2032(X \u2032\u2032 \u03b3\u2217))(X\u2212 \u0393)]\nThen, \u2016E\u0393[f(X \u0393)]\u2212 f(X \u03b3\u2217)\u20162 = \u2016E\u0393[(f \u2032(X \u2032 \u0393)\u2212 f \u2032(X \u2032\u2032 \u03b3\u2217))(X\u2212 \u0393)]\u20162 Since \u2016X\u2212 \u03b3\u2032\u20162 \u2264 sup\n\u03b3\u2208S \u2016X\u2212 \u03b3\u20162 = inf x\u2208A sup \u03b3\u2208S \u2016X \u03b3 \u2212 x \u03b3\u20162, and from Jensen\u2019s inequality\nand property of operator norm,\n\u2016E\u0393[(f \u2032(X \u2032 \u0393)\u2212 f \u2032(X \u2032\u2032 \u03b3\u2217))(X\u2212 \u0393)]\u20162 \u2264 E\u0393 [ \u2016f \u2032(X \u2032 \u0393)\u2212 f \u2032(X \u2032\u2032 \u03b3\u2217)\u2016op\u2016X\u2212 \u0393\u20162 ] \u2264 2BE\u0393 [ \u2016X\u2212 \u0393\u20162\n] \u2264 2B inf\nx\u2208A sup \u03b3\u2208S \u2016X \u03b3 \u2212 x \u03b3\u20162\nFinally we have,\nEX [ \u2016E\u0393[(f \u2032(X \u2032 \u0393)\u2212 f \u2032(X \u2032\u2032 \u03b3\u2217))(X\u2212 \u0393)]\u20162 ] \u2264 2BE [ inf x\u2208A sup \u03b3\u2208S \u2016X \u03b3 \u2212 x \u03b3\u20162 ] \u2264 2BC\nB.2 PROOF OF THEOREM 3\nProof. Induction on the number of the layers L. As before, we omit the parameter \u03b8. Initial step: when L = 1, the statement is obviously true. Induction on L: Suppose that the statement is true for neural networks with L layers. Now we prove the case L+ 1. From the inductive assumption, we have,\nEX [\u2225\u2225ESL[H(L)(X,SL)]\u2212 h(L)(X,E[SL])\u2225\u22252] \u2264 \u2206L (1) where SL = {\u0393(1), . . . ,\u0393(L)} is the dropout random variables for the first L layers, and\n\u2206L = (B\u03b3) L\u22121\u03b4 + (\u03b4 +B\u03b3\u03c3)\n( 1\u2212 (B\u03b3)L\u22121\n1\u2212B\u03b3 ) In addition, the L+ 1 layer is \u03b4-approximately expectation-linear, we have:\nEH(L) [\u2225\u2225E\u0393(L+1)[fL+1(H(L) \u0393(L+1))]\u2212 fL+1(H(L) \u03b3(L+1))\u2225\u22252] \u2264 \u03b4 (2)\nLet E[\u0393(l)] = \u03b3(l),\u2200l \u2208 {1, . . . , L + 1}, and let H(l) and h(l) be short for H(l)(X,Sl) and h(l)(X,E(Sl)), respectively, when there is no ambiguity. Moreover, we denote\nES [ H(L)(X,S; \u03b8) ] = ES [ H(L)(X,S; \u03b8) \u2223\u2223X] for convenience. Then,\nEX [\u2225\u2225ESL+1[H(L+1)]\u2212 h(L+1)\u2225\u22252] = EX\n[\u2225\u2225\u2225ESL[E\u0393(L+1)[fL+1(H(L) \u0393(L+1))]\u2212 fL+1(h(L) \u03b3(L+1))] +ESL [ fL+1(H (L) \u03b3(L+1)) ] \u2212 fL+1(h(L) \u03b3(L+1)) \u2225\u2225\u2225 2\n] \u2264 EX [\u2225\u2225\u2225ESL[E\u0393(L+1)[fL+1(H(L) \u0393(L+1))]\u2212 fL+1(h(L) \u03b3(L+1))]\u2225\u2225\u2225 2\n] +EX [\u2225\u2225\u2225ESL[fL+1(H(L) \u03b3(L+1))]\u2212 fL+1(h(L) \u03b3(L+1))\u2225\u2225\u2225 2\n] From Eq. 2 and Jensen\u2019s inequality, we have\nEX [\u2225\u2225\u2225ESL[E\u0393(L+1)[fL+1(H(L) \u0393(L+1))]\u2212 fL+1(h(L) \u03b3(L+1))]\u2225\u2225\u2225 2 ] \u2264 EH(L) [\u2225\u2225\u2225E\u0393(L+1)[fL+1(H(L) \u0393(L+1))]\u2212 fL+1(h(L) \u03b3(L+1))\u2225\u2225\u2225 2 ] \u2264 \u03b4\n(3)\nand\nEX [\u2225\u2225\u2225ESL[fL+1(H(L) \u03b3(L+1))]\u2212 fL+1(h(L) \u03b3(L+1))\u2225\u2225\u2225 2 ] = EX\n[\u2225\u2225\u2225ESL[fL+1(H(L) \u03b3(L+1))]\u2212 fL+1(ESL[H(L)] \u03b3(L+1)) +fL+1(ESL [ H(L) ] \u03b3(L+1))\u2212 fL+1(h(L) \u03b3(L+1)) \u2225\u2225\u2225 2\n] \u2264 EX [\u2225\u2225\u2225ESL[fL+1(H(L) \u03b3(L+1))]\u2212 fL+1(ESL[H(L)] \u03b3(L+1))\u2225\u2225\u2225 2\n] +EX [\u2225\u2225\u2225fL+1(ESL[H(L)] \u03b3(L+1))\u2212 fL+1(h(L) \u03b3(L+1))\u2225\u2225\u2225 2 ] (4)\nUsing Jensen\u2019s inequality, property of operator norm and E [ Var[H(l)|X] ] \u2264 \u03c32, we have\nEX [\u2225\u2225\u2225ESL[fL+1(H(L) \u03b3(L+1))]\u2212 fL+1(ESL[H(L)] \u03b3(L+1))\u2225\u2225\u2225 2 ] \u2264 EH(L) [\u2225\u2225\u2225fL+1(H(L) \u03b3(L+1))\u2212 fL+1(ESL[H(L)] \u03b3(L+1))\u2225\u2225\u2225 2\n] \u2264 B\u03b3EH(L)\n[\u2225\u2225H(L) \u2212 ESL[H(L)]\u2225\u22252] \u2264 B\u03b3 ( EH(L) [\u2225\u2225H(L) \u2212 ESL[H(L)]\u2225\u222522]) 12 \u2264 B\u03b3\u03c3 (5)\nFrom Eq. 1\nEX [\u2225\u2225\u2225fL+1(ESL[H(L)] \u03b3(L+1))\u2212 fL+1(h(L) \u03b3(L+1))\u2225\u2225\u2225 2 ] = B\u03b3EX\n[\u2225\u2225ESL[H(L)]\u2212 h(L)\u2225\u22252] \u2264 B\u03b3\u2206L (6) Finally, to sum up with Eq. 3, Eq. 4, , Eq. 5, , Eq. 6, we have\nEX [\u2225\u2225ESL+1[H(L+1)]\u2212 h(L+1)\u2225\u22252] \u2264 \u03b4 +B\u03b3\u03c3 +B\u03b3\u2206L = (B\u03b3)L\u03b4 + (\u03b4 +B\u03b3\u03c3) ( 1\u2212(B\u03b3)L\n1\u2212B\u03b3\n) = \u2206L+1\nC EXPECTATION-LINEARIZATION\nC.1 PROOF OF THEOREM 4: UNIFORM DEVIATION BOUND\nBefore proving Theorem 4, we first define the notations.\nLet Xn = {X1, . . . , Xn} be a set of n samples of input X . For a function space F : X \u2192 R, we use Radn(F , Xn) to denote the empirical Rademacher complexity of F ,\nRadn(F , Xn) = E\u03c3 [\nsup f\u2208F ( 1 n n\u2211 i=1 \u03c3if(Xi) )]\nand the Rademacher complexity is defined as Radn(F) = EXn [ Radn(F , Xn) ] In addition, we import the definition of dropout Rademacher complexity from Gao & Zhou (2014):\nRn(H, Xn, Sn) = E\u03c3 [\nsup h\u2208H\n( 1 n n\u2211 i=1 \u03c3ih(Xi, Si) )]\nRn(H) = EXn,Sn [ Radn(H, Xn, Sn) ]\nwhere H : X \u00d7 S \u2192 R is a function space defined on input space X and dropout variable space S. Rn(H, Xn, Sn) and Rn(H) are the empirical dropout Rademacher complexity and dropout Rademacher complexity, respectively. We further denoteRn(H, Xn) \u2206 = ESn [ Radn(H, Xn, Sn) ] .\nNow, we define the following function spaces: F = { f(x; \u03b8) : f(x; \u03b8) = ES [ H(L)(x, S; \u03b8) ] , \u03b8 \u2208 \u0398 } G = { g(x; \u03b8) : g(x; \u03b8) = h(L)(x,E[S]; \u03b8), \u03b8 \u2208 \u0398\n} H = { h(x, s; \u03b8) : h(x, s; \u03b8) = h(L)(x, s; \u03b8), \u03b8 \u2208 \u0398\n} Then, the function space of v(x) = f(x)\u2212 g(x) is V = {f(x)\u2212 g(x) : f \u2208 F , g \u2208 G}. Lemma 7.\nRadn(F , Xn) \u2264 Rn(H, Xn)\nProof. Rn(H, Xn) = ESn [ Radn(H, Xn, Sn) ] = ESn [ E\u03c3 [ sup h\u2208H ( 1 n n\u2211 i=1 \u03c3ih(Xi, Si) )]]\n= E\u03c3 [ ESn [ sup h\u2208H ( 1 n n\u2211 i=1 \u03c3ih(Xi, Si) )]]\n\u2265 E\u03c3 [\nsup h\u2208H\nESn [( 1 n n\u2211 i=1 \u03c3ih(Xi, Si) )]]\n= E\u03c3 [ sup h\u2208H ( 1 n n\u2211 i=1 \u03c3iESi [ h(Xi, Si) ])] = E\u03c3 [ sup h\u2208H ( 1 n n\u2211 i=1 \u03c3iESi [ H(L)(Xi, Si; \u03b8) ])] = Radn(F , Xn)\nFrom Lemma 7, we have Radn(F) \u2264 Rn(H). Lemma 8.\nRn(H) \u2264 \u03b1B L\u03b3L/2\u221a n\nRadn(G) \u2264 \u03b1B L\n\u221a n\nProof. See Theorem 4 in Gao & Zhou (2014).\nNow, we can prove Theorem 4.\nProof of Theorem 4\nProof. From Rademacher-based uniform bounds theorem, with probability \u2265 1\u2212 \u03b4,\nsup v\u2208V |\u2206\u2212 \u2206\u0302| < 2Radn(V) + \u03b2\n\u221a log(1/\u03b4)\nn\nSince V = F \u2212 G, we have\nRadn(V) = Radn(F \u2212 G) \u2264 Radn(F) +Radn(G) \u2264 \u03b1BL(\u03b3L/2 + 1)\u221a\nn\nThen, finally, we have that with probability \u2265 1\u2212 \u03b4,\nsup \u03b8\u2208\u0398 |\u2206\u2212 \u2206\u0302| < 2\u03b1B L(\u03b3L/2 + 1)\u221a n + \u03b2\n\u221a log(1/\u03b4)\nn\nC.2 PROOF OF THEOREM 5: NON-UNIFORM BOUND OF MODEL ACCURACY\nFor convenience, we denote \u03bb = {\u03b81, . . . , \u03b8L\u22121}. Then \u03b8 = {\u03bb, \u03b7}, and MLE \u03b8\u0302 = {\u03bb\u0302, \u03b7\u0302} Lemma 9.\n\u2016\u2207fL(\u00b7; \u03b7)T \u2016op \u2264 2\u2016\u03b7\u20162 (7)\nProof. denote\nA = \u2207fL(\u00b7; \u03b7)T = [ py(\u03b7y \u2212 \u03b7)T ] \u2223\u2223\u2223k y=1\nwhere py = p(y|x, s; \u03b8), \u03b7 = E [\u03b7Y ] = k\u2211 y=1 py\u03b7y .\nFor each v such that \u2016v\u20162 = 1,\n\u2016Av\u201622 = \u2211 y\u2208Y ( py (\u03b7y \u2212 \u03b7)T v )2 \u2264 \u2211 y\u2208Y \u2016py (\u03b7y \u2212 \u03b7) \u201622\u2016v\u201622 = \u2211 y\u2208Y \u2016py (\u03b7y \u2212 \u03b7) \u201622\n\u2264 \u2211 y\u2208Y py\u2016\u03b7y \u2212 \u03b7\u201622 \u2264 \u2211 y\u2208Y 2py ( \u2016\u03b7\u201622 + \u2211 y\u2032\u2208Y py\u2032\u2016\u03b7y\u2032\u201622 ) = 4\n\u2211 y\u2208Y py\u2016\u03b7y\u201622 \u2264 4\u2016\u03b7\u201622\nSo we have \u2016A\u2016op \u2264 2\u2016\u03b7\u20162.\nLemma 10. If parameter \u03b8\u0303 = {\u03bb\u0302, \u03b7} satisfies that \u2016\u03b7\u20162 \u2264 \u03b44\u03b2 , then V (D; \u03b8\u0303) \u2264 \u03b4, where V (D; \u03b8) is defined in Eq. (16).\nProof. Let SL = {\u0393(1), . . . ,\u0393(L)}, and let H(l) and h(l) be short for H(l)(X,Sl; \u03b8\u0303) and h(l)(X,E(Sl); \u03b8\u0303), respectively.\nFrom lemma 9, we have \u2016fL(x; \u03b7)\u2212 fL(y; \u03b7)\u20162 \u2264 2\u2016\u03b7\u20162\u2016x\u2212 y\u20162. Then,\u2225\u2225ESL [HL]\u2212 hL\u2225\u22252 = \u2225\u2225ESL\u22121 [fL(H(L\u22121); \u03b7)]\u2212 fL(h(L\u22121); \u03b7)\u2225\u22252 \u2264 ESL\u22121\n\u2225\u2225fL(H(L\u22121); \u03b7)\u2212 fL(h(L\u22121); \u03b7)\u2225\u22252 \u2264 2\u2016\u03b7\u20162 \u2225\u2225H(L\u22121) \u2212 h(L\u22121)\u2225\u2225 2 \u2264 4\u03b2\u2016\u03b7\u20162 \u2264 \u03b4\nLemma 10 says that we can get \u03b8 satisfying the expectation-linearization constrain by explicitly scaling down \u03b7\u0302 while keeping \u03bb\u0302.\nIn order to prove Theorem 5, we make the following assumptions:\n\u2022 The dimension of h(L\u22121) is d, i.e. h(L\u22121) \u2208 Rd. \u2022 Since \u2200y \u2208 Y, p(y|x; \u03b8\u0302) > 0, we assume p(y|x; \u03b8\u0302) \u2265 1/b, where b \u2265 |Y| = k. \u2022 As in the body text, let p(y|x, s; \u03b8\u0302) be nonuniform, and in particular let \u03b7\u0302Ty\u2217h (L\u22121)(x, s; \u03bb\u0302)\u2212 \u03b7\u0302Ty h(L\u22121)(x, s; \u03bb\u0302) > c\u2016\u03b7\u0302\u20162,\u2200y 6= y\u2217.\nFor convenience, we denote \u03b7Th(L\u22121)(x, s;\u03bb) = \u03b7Tuy(x, s;\u03bb), where uTy (x, s;\u03bb) = (v T 0 , . . . , v T k ) and\nvi = { h(L\u22121)(x, s;\u03bb) if i = y 0 otherwise\nTo prove Theorem 5, we first prove the following lemmas.\nLemma 11. If p(y|x; \u03b8\u0302) \u2265 1/b, then \u2200\u03b1 \u2208 [0, 1], for parameter \u03b8\u0303 = {\u03bb\u0302, \u03b1\u03b7\u0302}, we have\np(y|x; \u03b8\u0303) \u2265 1 b\nProof. We define\nf(\u03b1) \u2206 = (y|x, s; \u03b8\u0303) = e \u03b1\u03b7Ty h (L\u22121)(x,s;\u03bb\u0302)\u2211\ny\u2032\u2208Y e \u03b1\u03b7T y\u2032h (L\u22121)(x,s;\u03bb\u0302)\n=\n( e\u03b7 T y h (L\u22121)(x,s;\u03bb\u0302) )\u03b1\n\u2211 y\u2032\u2208Y ( e \u03b7T y\u2032h (L\u22121)(x,s;\u03bb\u0302) )\u03b1\nSince Y = {1, . . . , k}, for fixed x \u2208 X , s \u2208 S, log f(\u03b1) is a concave function w.r.t \u03b1. Since b \u2265 k, we have\nlog f(\u03b1) \u2265 (1\u2212 \u03b1) log f(0) + \u03b1 log f(1) \u2265 \u2212 log b\nSo we have \u2200x, s, p(y|x, s; \u03b8\u0303) \u2265 1/b. Then\np(y|x; \u03b8\u0303) = ES [ p(y|x, S; \u03b8\u0302) ] \u2265 1 b\nLemma 12. if y is not the majority class, i.e. y 6= y\u2217, then for parameter \u03b8\u0303 = {\u03bb\u0302, \u03b1\u03b7\u0302}\np(y|x, s, \u03b8\u0303) \u2264 e\u2212c\u03b1\u2016\u03b7\u0302\u20162\nProof.\np(y|x, s, \u03b8\u0303) = e \u03b1\u03b7\u0302Tuy\u2211\ny\u2032\u2208Y e\u03b1\u03b7\u0302\nTuy\u2032 \u2264 e\n\u03b1\u03b7\u0302Tuy\ne\u03b1\u03b7\u0302 Tuy\u2217\n\u2264 e\u2212c\u03b1\u2016\u03b7\u0302\u20162\nLemma 13. For a fixed x and s, the absolute value of the entry of the vector under the parameter \u03b8\u0303 = {\u03bb\u0302, \u03b1\u03b7\u0302}:\n|p(y|x, s; \u03b8\u0303)(uy \u2212 EY [uY ])|i \u2264 \u03b2(k \u2212 1)e\u2212c\u03b1\u2016\u03b7\u0302\u20162\nProof. Suppose y is the majority class of p(y|x, s; \u03b8\u0303). Then,\nuy \u2212 Ey[uY ] = (vy\u2032)ky\u2032=1\nwhere\nvy = { (1\u2212 p(y|x, s; \u03b8\u0303)h(L\u22121) if y = y\u2217 \u2212p(y|x, s; \u03b8\u0303)h(L\u22121) otherwise\nFrom Lemma 12, we have\n|p(y|x, s; \u03b8\u0303)(uy \u2212 EY [uY ])|i \u2264 |(uy \u2212 EY [uY ])|i \u2264 \u03b2(k \u2212 1)e\u2212c\u03b1\u2016\u03b7\u0302\u20162\nNow, we suppose y is not the majority class of p(y|x, s; \u03b8\u0303). Then,\n|p(y|x, s; \u03b8\u0303)(uy \u2212 EY [uY ])|i \u2264 p(y|x, s; \u03b8\u0303)\u03b2 \u2264 \u03b2e\u2212c\u03b1\u2016\u03b7\u0302\u20162\nOverall, the lemma follows.\nLemma 14. We denote the matrix\nA \u2206 = ES [ p(y|x,s;\u03b8\u0303) p(y|x;\u03b8\u0303) (uy \u2212 EY [uY ])(uy \u2212 EY [uY ]) T ]\n\u2212ES [ p(y|x,s;\u03b8\u0303) p(y|x;\u03b8\u0303) (uy \u2212 EY [uY ]) ] ES [ p(y|x,s;\u03b8\u0303) p(y|x;\u03b8\u0303) (uy \u2212 EY [uY ]) ]T Then the absolute value of the entry of A under the parameter \u03b8\u0303 = {\u03bb\u0302, \u03b1\u03b7\u0302}:\n|Aij | \u2264 2b(k \u2212 1)\u03b22e\u2212c\u03b1\u2016\u03b7\u0302\u20162\nProof. From Lemma 11, we have p(y|x; \u03b8\u0303) \u2265 1/b. Additionally, the absolute value of the entry of uy \u2212 EY [uY ] is bounded by \u03b2. We have for each i\u2223\u2223\u2223\u2223\u2223ES [ p(y|x, s; \u03b8\u0303) p(y|x; \u03b8\u0303) (uy \u2212 EY [uY ]) ]\u2223\u2223\u2223\u2223\u2223 i \u2264 ES [ p(y|x, s; \u03b8\u0303) p(y|x; \u03b8\u0303) \u03b2 ] = \u03b2\nThen from Lemma 13 |Aij | \u2264 2b(k \u2212 1)\u03b22e\u2212c\u03b1\u2016\u03b7\u0302\u20162\nLemma 15. We denote the matrix\nB \u2206 = ES [ p(y|x, s; \u03b8\u0303) p(y|x; \u03b8\u0303) ( EY [ uY u T Y ] \u2212 EY [uY ]EY [uY ]T )]\nThen the absolute value of the entry of B under the parameter \u03b8\u0303 = {\u03bb\u0302, \u03b1\u03b7\u0302}:\n|Bij | \u2264 2(k \u2212 1)\u03b22e\u2212c\u03b1\u2016\u03b7\u0302\u20162\nProof. We only need to prove that for fixed x and s, for each i, j:\u2223\u2223EY [uY uTY ]\u2212 EY [uY ]EY [uY ]T \u2223\u2223ij \u2264 2(k \u2212 1)\u03b22e\u2212c\u03b1\u2016\u03b7\u0302\u20162 Since \u2223\u2223EY [uY uTY ]\u2212 EY [uY ]EY [uY ]T \u2223\u2223ij = |CovY [(uY )i, (uY )j ]| \u2264 \u03b22 k\u2211 y=1 p(y|x, s; \u03b8\u0303)\u2212 p(y|x, s; \u03b8\u0303)2\nSuppose y is the majority class. Then from Lemma 12,\np(y|x, s; \u03b8\u0303)\u2212 p(y|x, s; \u03b8\u0303)2 \u2264 1\u2212 p(y|x, s; \u03b8\u0303) \u2264 (k \u2212 1)e\u2212c\u03b1\u2016\u03b7\u0302\u20162\nIf y is not the majority class. Then,\np(y|x, s; \u03b8\u0303)\u2212 p(y|x, s; \u03b8\u0303)2 \u2264 p(y|x, s; \u03b8\u0303) \u2264 e\u2212c\u03b1\u2016\u03b7\u0302\u20162\nSo we have k\u2211 y=1 p(y|x, s; \u03b8\u0303)\u2212 p(y|x, s; \u03b8\u0303)2 \u2264 2(k \u2212 1)e\u2212c\u03b1\u2016\u03b7\u0302\u20162\nThe lemma follows.\nLemma 16. Under the parameter \u03b8\u0303 = {\u03bb\u0302, \u03b1\u03b7\u0302}, the largest eigenvalue of the matrix\n1\nn n\u2211 i=1 (A(xi, yi)\u2212B(xi, yi)) (8)\nis at most 2dk(k \u2212 1)(b+ 1)\u03b22e\u2212c\u03b1\u2016\u03b7\u0302\u20162\nProof. From Lemma 14 and Lemma 15, each entry of the matrix in (8) is at most 2(k \u2212 1)(b + 1)\u03b22e\u2212c\u03b1\u2016\u03b7\u0302\u20162 . Thus, by Gershgorin\u2019s theorem, the maximum eigenvalue of the matrix in (8) is at most 2dk(k \u2212 1)(b+ 1)\u03b22e\u2212c\u03b1\u2016\u03b7\u0302\u20162 .\nNow, we can prove Theorem 5 by constructing a scaled version of \u03b8\u0302 that satisfies the expectationlinearization constraint.\nProof of Theorem 5\nProof. Consider the likelihood evaluated at \u03b8\u0303 = {\u03bb\u0302, \u03b1\u03b7\u0302}, where \u03b1 = \u03b44\u03b2\u2016\u03b7\u0302\u20162 . If \u03b1 > 1, then \u2016\u03b7\u20162 > \u03b44\u03b2 . We know the MLE \u03b8\u0302 already satisfies the expectation-linearization constraint. So we can assume that 0 \u2264 \u03b1 \u2264 1, and we know that \u03b8\u0303 satisfies V (D; \u03b8\u0303) \u2264 \u03b4. Then,\n\u2206l(\u03b8\u0302, \u03b8\u0302\u03b4) \u2264 \u2206l(\u03b8\u0302, \u03b8\u0303) = 1\nn (l(D; \u03b8\u0302)\u2212 l(D; \u03b8\u0303)) = g(\u03bb\u0302, \u03b7\u0302)\u2212 g(\u03bb\u0302, \u03b1\u03b7\u0302)\nwhere g(\u03bb, \u03b7) = 1n l(D; (\u03bb, \u03b7)). Taking the second-order Taylor expansion about \u03b7, we have\ng(\u03bb\u0302, \u03b1\u03b7\u0302) = g(\u03bb\u0302, \u03b7\u0302) +\u2207T\u03b7 g(\u03bb\u0302, \u03b7\u0302)(\u03b1\u03b7\u0302 \u2212 \u03b7\u0302) + (\u03b1\u03b7\u0302 \u2212 \u03b7\u0302)T\u22072\u03b7g(\u03bb\u0302, \u03b7\u0302)(\u03b1\u03b7\u0302 \u2212 \u03b7\u0302)\nSince \u03b8\u0302 is the MLE, the first-order term \u2207T\u03b7 g(\u03bb\u0302, \u03b7\u0302)(\u03b1\u03b7\u0302 \u2212 \u03b7\u0302) = 0. The Hessian in the second-order term is just Eq.(8). Thus, from Lemma 16 we have\ng(\u03bb\u0302, \u03b1\u03b7\u0302) \u2264 g(\u03bb\u0302, \u03b7\u0302)\u2212 (1\u2212 \u03b1)2\u2016\u03b7\u0302\u2016222dk(k \u2212 1)(b+ 1)\u03b22e\u2212c\u03b1\u2016\u03b7\u0302\u20162 = g(\u03bb\u0302, \u03b7\u0302)\u2212 2dk(k \u2212 1)(b+ 1)\u03b22 ( \u2016\u03b7\u0302\u20162 \u2212 \u03b44\u03b2 )2 e\u2212c\u03b4/4\u03b2\n= g(\u03bb\u0302, \u03b7\u0302)\u2212 c1\u03b22 ( \u2016\u03b7\u0302\u20162 \u2212 \u03b44\u03b2 )2 e\u2212c2\u03b4/4\u03b2\nwith setting c1 = 2dk(k \u2212 1)(b+ 1) and c2 = c. Then the theorem follows.\nC.3 PROOF OF THEOREM 6: UNIFORM BOUND OF MODEL ACCURACY\nIn the following, we denote \u03b8\u0303 = {\u03bb\u0302, \u03b1\u03b7\u0302}.\nLemma 17. For each y \u2208 Y , if p(y|x, s; \u03b8\u0302) \u2265 1/k, then \u2200\u03b1 \u2208 [0, 1]\np(y|x, s; \u03b8\u0303) \u2265 1 k\nProof. This lemma can be regarded as a corollary of Lemma 11.\nLemma 18. For a fixed x and s, we denote e\u03b7\u0302 T y h (L\u22121)(x,s;\u03bb\u0302) = wy . Then we have\np(y|x, s, \u03b8\u0303) = e \u03b1\u03b7\u0302Ty h (L\u22121)(x,s;\u03bb\u0302)\u2211 y\u2032\u2208Y e \u03b1\u03b7\u0302T y\u2032h (L\u22121)(x,s;\u03bb\u0302) = (wy) \u03b1\u2211 y\u2032\u2208Y (wy\u2032)\u03b1\nAdditionally, we denote gs(\u03b1) = \u2211 y\u2032\u2208Y p(y\u2032|x, s; \u03b8\u0303) logwy\u2032 \u2212 logwy . We assume gs(0) \u2265 0. Then we have \u2200\u03b1 \u2265 0 gs(\u03b1) \u2265 0\nProof.\n\u2202gs(\u03b1) \u2202\u03b1 = \u2211 y\u2032\u2208Y logwy\u2032 \u2202p(y\u2032|x, s; \u03b8\u0303) \u2202\u03b1 = VarY [logwY |X \u2212 x, S = s] \u2265 0\nSo gs(\u03b1) is non-decreasing. Since gs(0) \u2265 0, we have gs(\u03b1) \u2265 0 when \u03b1 \u2265 0.\nFrom above lemma, we have for each training instance (xi, yi) \u2208 D, and \u2200\u03b1 \u2208 [0, 1],\nEY [ log p(Y |xi, s; \u03b8\u0303) ] \u2265 log p(yi|xi, s; \u03b8\u0303) (9)\nFor convenience, we define m(s, y) = log p(y|x, s; \u03b8\u0303)\u2212 EY [ log p(Y |x, s; \u03b8\u0303) ]\nLemma 19. If y satisfies Lemma 17 and gs(\u03b1) \u2265 0, then\nVarY [m(s, Y )] \u2265 m(s, y)2\nProof. First we have m(s, y) = log p(y|x, s; \u03b8\u0303)\u2212 log 1/k \u2212KL ( p(\u00b7|x, s; \u03b8\u0303)|Unif(Y) ) \u2264 0\nSo we have\n(VarY [m(s, Y )]) 1/2 = \u221a EY [( log p(Y |x, s; \u03b8\u0303)\u2212 EY [ log p(Y |x, s; \u03b8\u0303) ])2] \u2265 EY\n[\u2223\u2223\u2223log p(Y |x, s; \u03b8\u0303)\u2212 EY [log p(Y |x, s; \u03b8\u0303)]\u2223\u2223\u2223] = EY\n[\u2223\u2223\u2223KL(p(\u00b7|x, s; \u03b8\u0303)|Unif(Y))+ log 1/k \u2212 log p(Y |x, s; \u03b8\u0303)\u2223\u2223\u2223] = EY [ KL ( p(\u00b7|x, s; \u03b8\u0303)|Unif(Y) ) + \u2223\u2223\u2223log 1/k \u2212 log p(Y |x, s; \u03b8\u0303)\u2223\u2223\u2223]\n\u2265 KL ( p(\u00b7|x, s; \u03b8\u0303)|Unif(Y) ) + EY [ log p(Y |x, s; \u03b8\u0303)\u2212 log 1/k ] = 2KL ( p(\u00b7|x, s; \u03b8\u0303)|Unif(Y)\n) As KL ( p(\u00b7|x, s; \u03b8\u0303)|Unif(Y) ) \u2265 0 and log p(y|x, s; \u03b8\u0303) \u2265 log 1/k. So we have 2KL ( p(\u00b7|x, s; \u03b8\u0303)|Unif(Y) ) \u2265 KL ( p(\u00b7|x, s; \u03b8\u0303)|Unif(Y) ) +log 1/k\u2212log p(y|x, s; \u03b8\u0303) = \u2212m(s, y)\nThen the lemma follows.\nFrom Lemma 19 and Eq. (9), we have for each training instance (xi, yi) \u2208 D, and \u2200\u03b1 \u2208 [0, 1],\nVarY [m(s, Y )] \u2265 m(s, yi)2 (10)\nLemma 20. For each training instance (xi, yi) \u2208 D, and \u2200\u03b1 \u2208 [0, 1], we have\nlog p(yi|xi; {\u03bb\u0302, \u03b1\u03b7\u0302}) \u2265 (1\u2212 \u03b1) log p(yi|xi; {\u03bb\u0302, 0}) + \u03b1 log p(yi|xi; {\u03bb\u0302, \u03b7\u0302})\nProof. We define\nf(\u03b1) = log p(yi|xi; {\u03bb\u0302, \u03b1\u03b7\u0302})\u2212 (1\u2212 \u03b1) log p(yi|xi; {\u03bb\u0302, 0})\u2212 \u03b1 log p(yi|xi; {\u03bb\u0302, \u03b7\u0302})\nBecause f(0) = f(1) = 0, we only need to prove that f(\u03b1) is concave on [0, 1]. We have\n\u22072f(\u03b1) = \u2212ES|Y=yi [VarY [m(S, Y )]] + VarS|Y=yi [m(S, yi)]\nwhere S|Y = yi is under the probability distribution p(s|Y = yi, xi; \u03b8\u0303) = p(yi|xi,S;\u03b8\u0303)p(s)p(yi|xi;\u03b8\u0303) From Eq. (10), we have\nES|Y=yi [VarY [m(S, Y )]] \u2265 ES|Y=yi [ m(S, yi) 2 ] \u2265 VarS|Y=yi [m(S, yi)]\nSo we have \u22072f(\u03b1) \u2264 0. The lemma follows.\nNow, we can prove Theorem 6 by using the same construction of an expectation-linearizing parameter as in Theorem 5.\nProof of Theorem 6\nProof. Consider the same parameter \u03b8\u0303 = {\u03bb\u0302, \u03b1\u03b7\u0302}, where \u03b1 = \u03b44\u03b2\u2016\u03b7\u0302\u20162 \u2264 1. we know that \u03b8\u0303 satisfies V (D; \u03b8\u0303) \u2264 \u03b4. Then,\n\u2206l(\u03b8\u0302, \u03b8\u0302\u03b4) \u2264 \u2206l(\u03b8\u0302, \u03b8\u0303) = 1\nn (l(D; \u03b8\u0302)\u2212 l(D; \u03b8\u0303))\nFrom Lemma 20 we have:\nl(D; \u03b8\u0303) = l(D; {\u03bb\u0302, \u03b1\u03b7\u0302}) \u2265 (1\u2212 \u03b1)l(D; {\u03bb\u0302, 0}) + \u03b1l(D; {\u03bb\u0302, \u03b7\u0302})\nSo \u2206l(\u03b8\u0302, \u03b8\u0302\u03b4) \u2264 (1\u2212 \u03b1) 1n ( l(D; \u03b8\u0302)\u2212 l(D; {\u03bb\u0302, 0}) ) = (1\u2212 \u03b1) 1n n\u2211 i=1 log p(yi|xi; \u03b8\u0302)\u2212 log Unif(Y)\n(1\u2212 \u03b1)E [KL (p(\u00b7|X; \u03b8)\u2016Unif(Y))] \u2264 ( 1\u2212 \u03b44\u03b2\u2016\u03b7\u0302\u20162 ) E [KL (p(\u00b7|X; \u03b8)\u2016Unif(Y))]\nD DETAILED DESCRIPTION OF EXPERIMENTS\nD.1 NEURAL NETWORK ARCHITECTURES\nMNIST For MNIST, we train 6 different fully-connected (dense) neural networks with 2 or 3 layers (see Table 1). For all architectures, we used dropout rate p = 0.5 for all hidden layers and p = 0.2 for the input layer.\nCIFAR-10 and CIFAR-100 For the two CIFAR datasets, we used the same architecture in Srivastava et al. (2014) \u2014 three convolutional layers followed by two fully-connected hidden layers. The convolutional layers have 96, 128, 265 filters respectively, with a 5\u00d7 5 receptive field applied with a stride of 1. Each convolutional layer is followed by a max pooling layer pools 3\u00d7 3 regions at strides of 2. The fully-connected layers have 2048 units each. All units use the rectified linear activation function. Dropout was applied to all the layers with dropout rate p = (0.1, 0.25, 0.25, 0.5, 0.5, 0.5) for the layers going from input to convolutional layers to fully-connected layers.\nD.2 NEURAL NETWORK TRAINING\nNeural network training in all the experiments is performed with mini-batch stochastic gradient descent (SGD) with momentum. We choose an initial learning rate of \u03b70, and the learning rate is updated on each epoch of training as \u03b7t = \u03b70/(1 + \u03c1t), where \u03c1 is the decay rate and t is the number of epoch completed. We run each experiment with 2,000 epochs and choose the parameters achieving the best performance on validation sets.\nTable 3 summarizes the chosen hyper-parameters for all experiments. Most of the hyper-parameters are chosen from Srivastava et al. (2014). But for some experiments, we cannot reproduce the performance reported in Srivastava et al. (2014) (We guess one of the possible reasons is that we used different library for implementation.). For these experiments, we tune the hyper-parameters on the validation sets by random search. Due to time constrains it is infeasible to do a random search across the full hyper-parameter space. Thus, we try to use as many hyper-parameters reported in Srivastava et al. (2014) as possible.\nD.3 EFFECT OF EXPECTATION-LINEARIZATION RATE \u03bb\nTable 4 illustrates the detailed results of the experiments on the effect of \u03bb. For MNIST, it lists the error rates under different \u03bb values for six different network architectures. For two datasets of CIFAR, it gives the error rates under different \u03bb values, among with the empirical expectation-linearization risk \u2206\u0302.\n", "cats": {"READ": 1.0, "DONTREAD": 0.0}, "reviews": [{"IS_META_REVIEW": true, "comments": "This paper puts forward a not entirely new, but also not sufficiently understood interpretation of dropout regularization. The authors derive useful theorems that estimate or put bounds on key quantities that are of interest when analyzing dropout regularized networks from their perspective. They furthermore introduce an explicit regularization term that should have a well understood impact on these key quantities. In the experimental section they convincingly show that the proposed regularization indeed has the expected effect and that their perspective on dropout is therefore useful and meaningful.\n\nTheir proposed regularization also seems to have a positive impact on the models performance but they demonstrate this only on rel. small scale benchmark problems. I therefore don\u2019t belief that this approach will have a large impact on how practitioner train models.  But their general perspective is well aligned with the recently proposed idea of \u201cDropout as a bayesian approximation\u201d and the insights and theorems in this paper might enable future work in that direction."}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "This paper presents a theoretical underpinning of dropout, and uses this derivation to both characterize its properties and to extend the method. A solid contribution. I am surprised that none of the reviewers mentioned that this work is closely related to the uncited 2015 paper \"Variational Dropout and the Local Reparameterization Trick\" by Diederik P. Kingma, Tim Salimans, Max Welling.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"DATE": "01 Jan 2017", "TITLE": "Revision of the paper", "IS_META_REVIEW": false, "comments": "We made the following revisions:\n\n1. We switched the section 6.3 and 6.4 to make the paper more clear.\n\n2. We added the definition of MC dropout on page 8.\n\n3. We fixed all the typos in the three reviewers' comments.", "OTHER_KEYS": "Xuezhe Ma"}, {"TITLE": "Good paper", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "summary\n\nThe paper explains dropout with a latent variable model where the dropout variable (0 or 1 depending on which units should be dropped) is not observed and is accordingly marginalised. Maximum likelihood under this model is not tractable but standard dropout then corresponds to a simple Monte Carlo approximation of ML for this model.\n\nThe paper then introduces a theoretical framework for analysing the discrepancy (called inference gap) between the model at training (model ensemble, or here the latent variable model), and the model at testing (where usually what should be an expectation over the activations over many models becomes the activation of one model with averaged weights).\nThis framework introduces several notions (e.g. expectation linearity) which allow the study of which transition functions (and more generally layers) can have a small inference gap. Theorem 3 gives a bound on the inference gap.\n\nFinally a new regularisation term is introduced to account for minimisation of the inference gap during learning.\n\nExperiments are performed on MNIST, CIFAR-10 and CIFAR-100 and show that the method has the potential to perform better than standard dropout and at the level of Monte Carlo Dropout (the standard method to compute the real dropout outputs consistently with the training assumption of an ensemble, of course quite expensive computationally)\n\n\nThe study gives a very interesting theoretical model for dropout as a latent variable model where standard dropout is then a monte carlo approximation. This is very probably widely applicable to further studies of dropout.\n\nThe framework for the study of the inference gap is interesting although maybe somewhat less widely applicable.\n\nThe proposed model is convincing although 1. it is tested on simple datasets 2. the gains are relatively small and 3. there is an increased computational cost during training because a new hyper-parameter is introduced.\n\np6 line 8 typo: expecatation", "IS_META_REVIEW": false, "RECOMMENDATION": 8, "DATE": "23 Dec 2016", "REVIEWER_CONFIDENCE": 3}, {"TITLE": "No Title", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "This paper introduces dropout as a latent variable model (LVM). Leveraging this formulation authors analyze the dropout \u201cinference gap\u201d which they define to be the gap between network output during training (where an instance of dropout is used for every training sample) and test (where expected dropout values are used to scale node outputs).  They introduce the notion of expectation linearity and use this to derive bounds on the inference gap under some (mild) assumptions.  Furthermore, they propose use of per-sample based inference gap as a regularizer, and present analysis of accuracy of models with expectation-linearization constraints as compared to those without.\n\nOne relatively minor issue I see with the LVM view of dropout is that it seems applicable only to probabilistic models whereas dropout is more generally applicable to deep networks.  However I\u2019d expect that the regularizer formulation of dropout would be effective even in non-probabilistic models.\n\nMC dropout on page 8 is not defined, please define.\n\nOn page 9 it is mentioned that with the proposed regularizer the standard dropout networks achieve better results than when Monte Carlo dropout is used.  This seems to be the case only on MNIST dataset and not on CIFAR?\n\nFrom Tables 1 and 2 it also appears that MC dropout achieves best performance across tasks and methods but it is of course an expensive procedure.  Comments on the computational efficiency of various dropout procedures - to go with the accuracy results - would be quite valuable.\n\nCouple of typos:\n- Pg. 2 \u201c \u2026 x is he input \u2026\u201d -> \u201c \u2026 x is the input \u2026\u201d\n- Pg. 5 \u201c \u2026 as defined in (1), is \u2026\u201d -> ref. to (1) is not right at two places in this paragraph\n\nOverall it is a good paper, I think should be accepted and discussed at the conference.\n\n", "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "17 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "Good paper", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "This paper puts forward a not entirely new, but also not sufficiently understood interpretation of dropout regularization. The authors derive useful theorems that estimate or put bounds on key quantities that are of interest when analyzing dropout regularized networks from their perspective. They furthermore introduce an explicit regularization term that should have a well understood impact on these key quantities. In the experimental section they convincingly show that the proposed regularization indeed has the expected effect and that their perspective on dropout is therefore useful and meaningful.\n\nTheir proposed regularization also seems to have a positive impact on the models performance but they demonstrate this only on rel. small scale benchmark problems. I therefore don\u2019t belief that this approach will have a large impact on how practitioner train models.  But their general perspective is well aligned with the recently proposed idea of \u201cDropout as a bayesian approximation\u201d and the insights and theorems in this paper might enable future work in that direction.\n ", "IS_META_REVIEW": false, "RECOMMENDATION": 8, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 3}, {"DATE": "02 Dec 2016", "TITLE": "precisions on the details", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1"}, {"IS_META_REVIEW": true, "comments": "This paper puts forward a not entirely new, but also not sufficiently understood interpretation of dropout regularization. The authors derive useful theorems that estimate or put bounds on key quantities that are of interest when analyzing dropout regularized networks from their perspective. They furthermore introduce an explicit regularization term that should have a well understood impact on these key quantities. In the experimental section they convincingly show that the proposed regularization indeed has the expected effect and that their perspective on dropout is therefore useful and meaningful.\n\nTheir proposed regularization also seems to have a positive impact on the models performance but they demonstrate this only on rel. small scale benchmark problems. I therefore don\u2019t belief that this approach will have a large impact on how practitioner train models.  But their general perspective is well aligned with the recently proposed idea of \u201cDropout as a bayesian approximation\u201d and the insights and theorems in this paper might enable future work in that direction."}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "This paper presents a theoretical underpinning of dropout, and uses this derivation to both characterize its properties and to extend the method. A solid contribution. I am surprised that none of the reviewers mentioned that this work is closely related to the uncited 2015 paper \"Variational Dropout and the Local Reparameterization Trick\" by Diederik P. Kingma, Tim Salimans, Max Welling.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"DATE": "01 Jan 2017", "TITLE": "Revision of the paper", "IS_META_REVIEW": false, "comments": "We made the following revisions:\n\n1. We switched the section 6.3 and 6.4 to make the paper more clear.\n\n2. We added the definition of MC dropout on page 8.\n\n3. We fixed all the typos in the three reviewers' comments.", "OTHER_KEYS": "Xuezhe Ma"}, {"TITLE": "Good paper", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "summary\n\nThe paper explains dropout with a latent variable model where the dropout variable (0 or 1 depending on which units should be dropped) is not observed and is accordingly marginalised. Maximum likelihood under this model is not tractable but standard dropout then corresponds to a simple Monte Carlo approximation of ML for this model.\n\nThe paper then introduces a theoretical framework for analysing the discrepancy (called inference gap) between the model at training (model ensemble, or here the latent variable model), and the model at testing (where usually what should be an expectation over the activations over many models becomes the activation of one model with averaged weights).\nThis framework introduces several notions (e.g. expectation linearity) which allow the study of which transition functions (and more generally layers) can have a small inference gap. Theorem 3 gives a bound on the inference gap.\n\nFinally a new regularisation term is introduced to account for minimisation of the inference gap during learning.\n\nExperiments are performed on MNIST, CIFAR-10 and CIFAR-100 and show that the method has the potential to perform better than standard dropout and at the level of Monte Carlo Dropout (the standard method to compute the real dropout outputs consistently with the training assumption of an ensemble, of course quite expensive computationally)\n\n\nThe study gives a very interesting theoretical model for dropout as a latent variable model where standard dropout is then a monte carlo approximation. This is very probably widely applicable to further studies of dropout.\n\nThe framework for the study of the inference gap is interesting although maybe somewhat less widely applicable.\n\nThe proposed model is convincing although 1. it is tested on simple datasets 2. the gains are relatively small and 3. there is an increased computational cost during training because a new hyper-parameter is introduced.\n\np6 line 8 typo: expecatation", "IS_META_REVIEW": false, "RECOMMENDATION": 8, "DATE": "23 Dec 2016", "REVIEWER_CONFIDENCE": 3}, {"TITLE": "No Title", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "This paper introduces dropout as a latent variable model (LVM). Leveraging this formulation authors analyze the dropout \u201cinference gap\u201d which they define to be the gap between network output during training (where an instance of dropout is used for every training sample) and test (where expected dropout values are used to scale node outputs).  They introduce the notion of expectation linearity and use this to derive bounds on the inference gap under some (mild) assumptions.  Furthermore, they propose use of per-sample based inference gap as a regularizer, and present analysis of accuracy of models with expectation-linearization constraints as compared to those without.\n\nOne relatively minor issue I see with the LVM view of dropout is that it seems applicable only to probabilistic models whereas dropout is more generally applicable to deep networks.  However I\u2019d expect that the regularizer formulation of dropout would be effective even in non-probabilistic models.\n\nMC dropout on page 8 is not defined, please define.\n\nOn page 9 it is mentioned that with the proposed regularizer the standard dropout networks achieve better results than when Monte Carlo dropout is used.  This seems to be the case only on MNIST dataset and not on CIFAR?\n\nFrom Tables 1 and 2 it also appears that MC dropout achieves best performance across tasks and methods but it is of course an expensive procedure.  Comments on the computational efficiency of various dropout procedures - to go with the accuracy results - would be quite valuable.\n\nCouple of typos:\n- Pg. 2 \u201c \u2026 x is he input \u2026\u201d -> \u201c \u2026 x is the input \u2026\u201d\n- Pg. 5 \u201c \u2026 as defined in (1), is \u2026\u201d -> ref. to (1) is not right at two places in this paragraph\n\nOverall it is a good paper, I think should be accepted and discussed at the conference.\n\n", "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "17 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "Good paper", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "This paper puts forward a not entirely new, but also not sufficiently understood interpretation of dropout regularization. The authors derive useful theorems that estimate or put bounds on key quantities that are of interest when analyzing dropout regularized networks from their perspective. They furthermore introduce an explicit regularization term that should have a well understood impact on these key quantities. In the experimental section they convincingly show that the proposed regularization indeed has the expected effect and that their perspective on dropout is therefore useful and meaningful.\n\nTheir proposed regularization also seems to have a positive impact on the models performance but they demonstrate this only on rel. small scale benchmark problems. I therefore don\u2019t belief that this approach will have a large impact on how practitioner train models.  But their general perspective is well aligned with the recently proposed idea of \u201cDropout as a bayesian approximation\u201d and the insights and theorems in this paper might enable future work in that direction.\n ", "IS_META_REVIEW": false, "RECOMMENDATION": 8, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 3}, {"DATE": "02 Dec 2016", "TITLE": "precisions on the details", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1"}]}
{"text": "TREE-STRUCTURED DECODING WITH DOUBLY- RECURRENT NEURAL NETWORKS\nWe propose a neural network architecture for generating tree-structured objects from encoded representations. The core of the method is a doubly recurrent neural network model comprised of separate width and depth recurrences that are combined inside each cell (node) to generate an output. The topology of the tree is modeled explicitly together with the content. That is, in response to an encoded vector representation, co-evolving recurrences are used to realize the associated tree and the labels for the nodes in the tree. We test this architecture in an encoderdecoder framework, where we train a network to encode a sentence as a vector, and then generate a tree structure from it. The experimental results show the effectiveness of this architecture at recovering latent tree structure in sequences and at mapping sentences to simple functional programs.\n1 INTRODUCTION\nRecurrent neural networks have become extremely popular for modeling structured data. Key to their success is their ability to learn long-range temporal dependencies, their flexibility, and ease of customization. These architectures are naturally suited for modeling sequences since the underlying state evolution resulting from successive operations follows an inherently linear order (Williams & Zipser, 1995; Hochreiter & Schmidhuber, 1997). Indeed, they have been successfully adapted to language modeling (Zaremba et al., 2015), machine translation (Sutskever et al., 2014) and conversational agents (Vinyals & Le, 2015), among other applications.\nAlthough sequences arise frequently in practice, other structures such as trees or graphs do not naturally conform to a linear ordering. For example, natural language sentences or associated parse trees, programs, hierarchical structures in biology, or molecules are not inherently linear structures. While sentences in natural language can be modeled as if they were linear sequences, the underlying process is compositional (Frege, 1892). Models that construct sentences compositionally should derive an advantage from adopting a more appropriate inductive bias.\nThe flexibility and success of recurrent neural networks in modeling and generating sequential data has prompted efforts to adapt them to non-sequential data too. Recent work has focused on the application of neural architectures to hierarchical structures, albeit in limited ways. Much of this work has assumed that either the full tree structure is given (Socher et al., 2012; Tai et al., 2015) or at least the nodes are (Socher & Lin, 2011; Chen & Manning, 2014; Kiperwasser & Goldberg, 2016). In the former scenario, the network aggregates the node information in a manner that is coherent with a given tree structure while, in the latter, generation is reduced to an attachment problem, i.e., sequentially deciding which pairs of nodes to join with an edge until a tree is formed.\nThe full problem of decoding with structure, i.e., generating a tree-structured object with node labels from a given vector representation, has remained largely unexplored until recently. Recent efforts to adapt RNNs to this context have so far remained relatively close to their sequential counterparts. For example, in order to capture depth and branching in the tree, one can introduce special tokens (Dong & Lapata, 2016) or use alternating RNNs coupled with external classifiers to predict branching (Zhang et al., 2016).\nIn this work, we propose a novel architecture tailored specifically to tree-structured decoding. At the heart of our approach is a doubly-recurrent (breadth and depth-wise recurrent) neural network which separately models the flow of information between parent and children nodes, and between siblings. Each of these relationships is modeled with a recurrent module whose hidden states are updated upon observing node labels. Every node in the tree receives two hidden states, which are then combined and used to predict a label for that node. Besides maintaining separate but simultaneous fraternal and paternal recurrences, the proposed architecture departs from previous methods in that it explicitly models tree topology. Each node in the network has modules that predict, based on the cell state, whether the node is terminal, both in terms of depth and width. Decoupling these decisions from the label prediction allows for a more concise formulation, which does not require artificial tokens to be added to the tree to simulate branching.\nWe test this novel architecture in various encoder-decoder frameworks, coupling it with sequential encoders to predict tree structure from encoded vector representations of sequences. The experimental results show the effectiveness of this approach at recovering latent structure in flattened string representations of trees (Section 4.1) and at mapping from natural language descriptions of simple programs to abstract syntax trees (Section 4.2). In addition, we show that even for sequence-tosequence tasks such as machine translation, the proposed architecture exhibits desirable properties, such as invariance to structural changes and coarse-to-fine generation (Section 4.3).\nTo summarize, the main contributions of this paper are as follows:\n\u2022 We propose a novel neural network architecture specifically tailored to tree-structured decoding, which maintains separate depth and width recurrent states and combines them to obtain hidden states for every node in the tree.\n\u2022 We equip this novel architecture with a mechanism to predict tree topology explicitly (as opposed to implicitly by adding nodes with special tokens).\n\u2022 We show experimentally that the proposed method is capable of recovering trees from encoded representations and that it outperforms state-of-the-art methods in a task consisting of mapping sentences to simple functional programs.\n2 RELATED WORK\nRecursive Neural Networks. Recursive neural networks (Socher & Lin, 2011; Socher et al., 2012) were proposed to model data with hierarchical structures, such as parsed scenes and natural language sentences. Though they have been most successfully applied to encoding objects when their treestructured representation is given (Socher et al., 2013), the original formulation by Socher & Lin (2011) also considered using them to predict the structure (edges), albeit for the case where nodes are given. Thus, besides their limited applicability due to their assumption of binary trees, recursive neural networks are not useful for fully generating trees from scratch.\nTree-structured encoders. The Tree-LSTM of Tai et al. (2015) is a generalization of long shortterm memory networks (Hochreiter & Schmidhuber, 1997) to tree-structured inputs. Their model constructs a sentence representation bottom-up, obtaining at every step the representation of a node in the tree from those of its children. In this sense, this model can be seen as a generalization of recursive neural networks to trees with degree potentially greater than two, with the additional longrange dependency modeling provided by LSTMs. They propose two methods for aggregating the states of the children, depending on the type of underlying tree: N-ary trees or trees with unknown and potentially unbounded branching factor. TreeLSTMs have shown promising results for compositional encoding of structured data, though by construction they cannot be used for decoding, since they operate on a given tree structure.\nTree-structured decoders. Proposed only very recently, most tree-structured decoders rely on stacked on intertwined RNNs, and use heuristic methods for topological decisions during generation. Closest to our method is the Top-down Tree LSTM of Zhang et al. (2016), which generates a tree from an encoded representation. Their method relies on 4 independent LSTMs, which act in alternation\u2014as opposed to simultaneously in our approach\u2014yielding essentially a standard LSTM that changes the weights it uses based on the position of the current node. In addition, their method\nprovides children with asymmetric parent input: \u201cyounger\u201d children receive information from the parent state only through the previous sibling\u2019s state. Though most of their experiments focus on the case where the nodes are given, they mention how to use their method for full prediction by introducing additional binary classifiers which predict which of the four LSTMs is to be used. These classifiers are trained in isolation after the main architecture has been trained. Contrary to this approach, our method can be trained end-to-end in only one pass, has a simpler formulation and explicitly incorporates topological prediction as part of the functioning of each neuron.\nA similar approach is proposed by Dong & Lapata (2016). They propose SEQ2TREE, an encoderdecoder architecture that maps sentences to tree structures. For the decoder, they rely on hierarchical use of an LSTM, similar to Tai et al. (2015), but in the opposite direction: working top-down from the root of the tree. To decide when to change levels in the hierarchy, they augment the training trees with nonterminal nodes labeled with a special token <n>, which when generated during decoding trigger the branching out into a lower level in the tree. Similar to our method, they feed nodes with hidden representations of their parent and sibling, but they do so by concatenating both states and running them through a single recurrent unit, as opposed to our method, where these two sources of information are handled separately. A further difference is that our approach does not require artificial nodes with special tokens to be added to the tree, resulting in smaller trees.\nHierarchical Neural Networks for Parsing. Neural networks have also been recently introduced to the problem of natural language parsing (Chen & Manning, 2014; Kiperwasser & Goldberg, 2016). In this problem, the task is to predict a parse tree over a given sentence. For this, Kiperwasser & Goldberg (2016) use recurrent neural networks as a building block, and compose them recursively to obtain a tree-structured encoder. Starting from the leaves (words) they predict a parse tree with a projective bottom-up strategy, which sequentially updates the encoded vector representation of the tree and uses it to guide edge-attaching decisions. Though conceptually similar to our approach, their method relies on having access to the nodes of the tree (words) and only predicts its topology, so\u2014similar to recursive neural networks\u2014it cannot be used for a fully generative decoding.\n3 DOUBLY RECURRENT NEURAL NETWORKS\nGenerating a tree-structured object from scratch using only an encoded representation poses several design challenges. First, one must decide in which order to generate the tree. If the nodes on the decoder side were given (such as in parsing), it would be possible to generate a tree bottom-up from these nodes (e.g. as Kiperwasser & Goldberg 2016 do). In the setting we are interested in, however, not even the nodes are known when decoding, so the natural choice is a top-down decoder, which starting from an encoded representation generates the root of the tree and then recursively generates the children (if any) of every node.\nThe second challenge arises from the asymmetric hierarchical nature of trees. Unlike the sequenceto-sequence setting where encoding and decoding can be achieved with analogous procedures, when dealing with tree-structured data these two involve significantly different operations. For example, an encoder that processes a tree bottom-up using information of a node\u2019s children to obtain its representation cannot be simply reversed and used as a decoder, since when generating the tree top-down, nodes have to be generated before their children are.\nAn additional design constraint comes from deciding what information to feed to each node. For sequences, the choice is obvious: a node should receive information from the node preceding or succeeding it (or both), i.e. there is a one-dimensional flow of information. In trees, there is an evident flow of information from parent to children (or vice-versa), but when generating nodes in a top-down order it seems unnatural to generate children in isolation: the label of one of them will likely influence what the states of the other children might be. For example, in the case of parse trees, generating a verb will reduce the chances of other verbs occurring in that branch.\nWith these considerations in mind, we propose an architecture tailored to tree decoding from scratch: top-down, recursive and doubly-recurrent, i.e. where both the ancestral (parent-to-children) and fraternal (sibling-to-sibling) flows of information are modeled with recurrent modules. Thus, the building block of a doubly recurrent neural network (DRNN) is a cell with two types of input states, one coming from its parent, updated and passed on to its descendants, and another one received from\nits previous sibling,1 updated and passed on to the next one. We model the flow of information in the two directions with separate recurrent modules.\nFormally, let T = {V, E ,X} be a connected labeled tree, where V is the set of nodes, E the set of edges and X are node labels.2 Let ga and gf be functions which apply one step of the two separate RNNs. For a node i \u2208 V with parent p(i) and previous sibling s(i), the ancestral and fraternal hidden states are updated via\nhai = g a(hap(i),xp(i)) (1)\nhfi = g f (hfs(i),xs(i)) (2)\nwhere xs(j),xp(i) are the vectors representing the previous sibling\u2019s and parent\u2019s values, respectively. Once the hidden depth and width states have been updated with these observed labels, they are combined to obtain a predictive hidden state:\nh (pred) i = tanh ( Ufhfi +U ahai ) (3)\nwhere Uf \u2208 Rn\u00d7Df and Ua \u2208 Rn\u00d7Da are learnable parameters. This state contains combined information of the node\u2019s neighborhood in the tree, and is used to predict a label for it. In its simplest form, the network could compute the output of node i by sampling from distribution\noi = softmax(Wh (pred) i ) (4)\nIn the next section, we propose a slight modification to (4) whereby topological information is included in the computation of cell outputs. After the node\u2019s output symbol xi has been obtained by sampling from oi, the cell passes hai to all its children and h f i to the next sibling (if any), enabling them to apply Eqs (1) and (2) to realize their states. This procedure continues recursively, until termination conditions (explained in the next section) cause it to halt.\n3.1 TOPOLOGICAL PREDICTION\nAs mentioned before, the central issue with free-form tree construction is to predict the topology of the tree. When constructing the tree top-down, for each node we need to decide: (i) whether it is a leaf node (and thus it should not produce offspring) and (ii) whether there should be additional siblings produced after it. Answering these two questions for every node allows us to construct a tree from scratch and eventual stop growing it.\nSequence decoders typically rely on special tokens to terminate generation (Sutskever et al., 2014). The token is added to the vocabulary and treated as a regular word. During training, the examples are padded with this token at the end of the sequence, and during testing, generation of this token signals termination. These ideas has been adopted by most tree decoders (Dong & Lapata, 2016). There are two important downsides of using a padding strategy for topology prediction in trees. First, the size of the tree can grow considerably. While in the sequence framework only one stopping token is needed, a tree with n nodes might need up to O(n) padding nodes to be added. This can have important effects in training speed. The second reason is that a single stopping token selected competitively with other tokens requires one to continually update the associated parameters in response to any changes in the distribution over ordinary tokens so as to maintain topological control.\nBased on these observations, we propose an alternative approach to stopping, in which topological decisions are made explicitly (as opposed to implicitly, with stopping tokens). For this, we use the predictive hidden state of the node h(pred) with a projection and sigmoid activation:\npai = \u03c3(u a \u00b7 h(pred)i ) (5)\nThe value pai \u2208 [0, 1] is interpreted as the probability that node i has children. Analogously, we can obtain a probability of stopping fraternal branch growth after the current node as follows:\npfi = \u03c3(u f \u00b7 h(pred)i ) (6)\n1Unlike the \u201cancestral\u201d line, the order within sibling nodes is ambiguous. While in abstract trees it is assumed that the there is no such ordering, we assume that for the structures were are interested in learning there is always one: either chronological (the temporal order in which the nodes were generated) or latent (e.g. the grammatical order of the words in a parse tree with respect to their sentence representation).\n2We assume throughout that these values are given as class indicators xi \u2208 {1, . . . , N}.\nNote that these stopping strategies depart from the usual padding methods in a fundamental property: the decision to stop is made before instead of in conjunction with the label prediction. The rationale behind this is that the label of a node will likely be influenced not only by its context, but also by the type of node (terminal or non-terminal) where it is to be assigned. This is the case in language, for example, where syntactic constraints restrict the type of words that can be found in terminal nodes. For this purpose, we include the topological information as inputs to the label prediction layer. Thus, (4) takes the form\noi = softmax(Wh (pred) i + \u03b1iv a + \u03d5iv f ) (7)\nwhere \u03b1i, \u03d5i \u2208 {0, 1} are binary variables indicating the topological decisions and va,vf are learnable offset parameters. During training, we use gold-truth values in (7), i.e. \u03b1i = 1 if node i has children and \u03d5i = 1 if it has a succeeding sibling. During testing, these values are obtained from pa, pf by sampling or beam-search. A schematic representation of the internal structure of a DRNN cell and the flow of information in a tree are shown in Figure 1.\n3.2 TRAINING DRNNS\nWe train DRNNs with (reverse) back-propagation through structure (BPTS) (Goller & Kuechler, 1996). In the forward pass, node outputs are computed in a top-down fashion on the structureunrolled version of the network, following the natural3 dependencies of the tree. We obtain error signal at the node level from the two types of prediction: label and topology. For the former, we compute cross-entropy loss of oi with respect to the true label of the node xi. For the topological values pai and p f i we compute binary cross entropy loss with respect to gold topological indicators \u03b1i, \u03d5i \u2208 {0, 1}. In the backward pass, we proceed in the reverse (bottom-up) direction, feeding into every node the gradients received from child and sibling nodes and computing internally gradients with respect to both topology and label prediction. Further details on the backpropagation flow are provided in the Appendix.\nNote that the way BPTS is computed implies and underlying decoupled loss function L(x\u0302) = \u2211 i\u2208V Llabel(xi, x\u0302i) + Ltopo(pi, p\u0302i) (8)\nThe decoupled nature of this loss allows us to weigh these two objectives differently, to emphasize either topology or label prediction accuracy. Investigating the effect of this is left for future work.\n3The traversal is always breadth-first starting from the root, but the order in which sibling nodes are visited might depend on the specific problem. If the nodes of the tree have an underlying order (such as in dependency parse trees), it is usually desirable to preserve this order.\nAs is common with sequence generation, during training we perform teacher forcing: after predicting the label of a node and its corresponding loss, we replace it with its gold value, so that children and siblings receive the correct label for that node. Analogously, we obtain the probabilities pa and pf , compute their loss, and replace them for ground truth variables \u03b1i, \u03d5i for all downstream computations. Addressing this exposure bias by mixing ground truth labels with model predictions during training (Venkatraman et al., 2015) or by incremental hybrid losses (Ranzato et al., 2016) is left as an avenue for future work.\n4 EXPERIMENTS\n4.1 SYNTHETIC TREE RECOVERY\nIn our first set of experiments we evaluate the effectiveness of the proposed architecture to recover trees from flattened string representations. For this, we first generate a toy dataset consisting of simple labeled trees. To isolate the effect of label content from topological prediction, we take a small vocabulary consisting of the 26 letters of the English alphabet. We generate trees in a top-down fashion, conditioning the label and topology of every node on the state of its ancestors and siblings. For simplicity, we use a Markovian assumption on these dependencies, modeling the probability of a node\u2019s label as depending only on the label of its parent and the last sibling generated before it (if any). Conditioned on these two inputs, we model the label of the node as coming from a multinomial distribution over the alphabet with a dirichlet prior. To generate the topology of the tree, we model the probability of a node having children and a next-sibling as depending only on its label and the depth of the tree. For each tree we generate a string representation by traversing it in breadth-first preorder, starting from the root. The labels of the nodes are concatenated into a string in the order in which they were visited, resulting in a string of |T | symbols. We create a dataset of 5,000 trees with this procedure, and split it randomly into train, validation and test sets (with a 80%,10%,10% split). Further details on the construction of this dataset are provided in the Appendix.\nThe task consists of learning a mapping from strings to trees, and using this learned mapping to recover the tree structure of the test set examples, given only their flattened representation. To do so, we use an encoder-decoder framework, where the strings are mapped to a fixed-size vector representation using a recurrent neural network. For the decoder, we use a DRNN with LSTM modules, which given the encoded representation generates a tree. We choose hyper-parameters with cross-validation. Full training details are provided in the Appendix.\nMeasuring performance only in terms of exact recovery would likely yield near-zero accuracies for most trees. Instead, we opt for a finer-grained metric of tree similarity that gives partial credit for correctly predicted subtrees. Treating tree generation as a retrieval problem, we evaluate the quality of the predicted tree in terms of the precision and recall of recovering nodes and edges present in the gold tree. Thus, we penalize both missing and superfluous components. As baseline, we induce a probabilistic context-free grammar (PCFG) on the full training data and use it to parse the test sentences. Note that unlike the DRNN, this parser has direct access to the sentence representation and thus its task is only to infer the tree structure on top of it, so this is indeed a strong baseline.\nFigure 3 shows the results on the test set. Training on the full data yields node and edge retrieval F1-Scores of 75% and 71%, respectively, the latter considerably above the baseline.4 This 4% gap can be explained by correct nodes being generated in the wrong part of the tree, as in the example in\n4Since the PCFG parser has access to the nodes by construction, node accuracy for the baseline method is irrelevant and thus omitted from the analysis.\nFigure 2. The second plot in Figure 3 shows that although small trees are recovered more accurately, precision decays slowly with tree size, with depth accounting for the largest effect (Figure 4).\n4.2 MAPPING SENTENCES TO FUNCTIONAL PROGRAMS\nTree structures arise naturally in the context of programs. A typical compiler takes human-readable source code (expressed as sequences of characters) and transforms it into an executable abstract syntax tree (AST). Source code, however, is already semi-structured. Mapping natural language sentences directly into executable programs is an open problem, which has received considerable interest in the natural language processing community (Kate et al., 2005; Branavan et al., 2009).\nThe IFTTT dataset (Quirk et al., 2015) is a simple testbed for language-to-program mapping. It consists of if-this-then-that programs (called recipes) crawled from the IFTTT website5, paired with natural language descriptions of their purpose. The recipes consist of a trigger and an action, each defined in terms of a channel (e.g. \u201cFacebook\u201d), a function (e.g. \u201cPost a status update\u201d) and potentially arguments and parameters. An example of a recipe and its description are shown in Figure 5. The data is user-generated and extremely noisy, which makes the task significantly challenging.\n5www.ifttt.com\nWe approach this task using an encoder-decoder framework. We use a standard RNN encoder, either an LSTM or a GRU (Cho et al., 2014), to map the sentence to a vector representation, and we use a DRNN decoder to generate the AST representation of the recipe. We use the original data split, which consists of 77,495 training, 5,171 development and 4,294 test examples. For evaluation, we use the same metrics as Quirk et al. (2015), who note that computing exact accuracy on such a noisy dataset is problematic, and instead propose to evaluate the generated AST in terms of F1-score on the set of recovered productions. In addition, they compute accuracy at the channel level (i.e. when both channels are predicted correctly) and at the function level (both channels and both functions predicted correctly).\nWe compare our methods against the various extraction and phrased-based machine translation baselines of Quirk et al. (2015) and the the methods of Dong & Lapata (2016): SEQ2SEQ, a sequenceto-sequence model trained on flattened representations of the AST, and SEQ2TREE, a token-driven hierarchical RNN. Following these two works, we report results on two noise-filtered subsets of the data: one with all non-English and unintelligible recipes removed and the other one with recipes for which at least three humans agreed with the gold AST. The results are shown in Table 1. In both subsets, DRNNs perform on par or above previous approaches, with LSTM-DRNN achieving significantly better results. The improvement is particularly evident in terms of F1-score, which is the only metric used by previous approaches that measures global tree reconstruction accuracy. To better understand the quality of the predicted trees beyond the function level (i.e. (b) in Figure 5), we computed node accuracy on the arguments level. Our best performing model, LSTM-DRNN, achieves a Macro F1 score of 51% (0.71 precision, 0.40 recall) over argument nodes, which shows that the model is reasonably successful at predicting structure even beyond depth three. The best performing alternative model, SEQ2TREE, achieves a corresponding F1 score of 46%.\n4.3 MACHINE TRANSLATION\nIn our last set of experiments, we offer a qualitative evaluation DRNNs in the context of machine translation. Obtaining state-of-the-art results in machine translation requires highly-optimized architectures and large parallel corpora. This is not our goal. Instead, we investigate whether decoding with structure can bring benefits to a task traditionally approached as a sequence-to-sequence problem. For this reason, we consider a setting with limited data: a subset of the WMT14 dataset consisting of about 50K English\u2194 French sentence pairs (see the Appendix for details) along with dependency parses of the target (English) side.\nWe train a sequence-to-tree model using an LSTM encoder and a DRNN decoder as in the previous experiments. A slight modification here is that we distinguish left and right children in the tree, using two symmetric width-modules gfL, g f R that produce children from the parent outwards. With this, children are lexically ordered, and therefore trees can be easily and un-ambiguously projected back into sentences. We compare our model against a sequence-to-sequence architecture of similar complexity (in terms of number of parameters) trained on the same data using the optimized OpenNMT library (Klein et al., 2017). For decoding, we use a simple best-of-k sampling scheme for our model, and beam search for the SEQ2SEQ models.\nSource \u201c produit diffe\u0301rentes re\u0301ponses qui changent avec le temps selon nos expe\u0301riences et nos relations \u201d\n\u201cje ne sais jamais quoi dire dans ces cas la\u0300\u201d\nSEQ2SEQ: l = 1 a I l = 4 with the different actions I do l = 8 with the different actions who change with I do not know what to say\nDRNN: d = 1 answers know d = 2 different answers change but i do not know d = 3 product the different answers change . but i do not know to say\nTable 2: Translations at different resolutions (size constraints imposed during decoding) for two example sentences.\nFirst, we analyze the quality of translations as a function of the maximum allowed target sentence \u201csize\u201d. The notion of size for a sequence decoder is simply the length while for DRNN we use depth instead so as to tap into the inherent granularity at which sentences can be generated from this architecture. Two such examples are shown in Table 2. Since DRNN topology has been trained to mimic dependency parses top-down, the decoder tends to first generate the fundamental aspects of the sentence (verb, nouns), leaving less important refinements for deeper structures down in the tree. The sequence decoder, in contrast, is trained for left-to-right sequential generation, and thus produces less informative translations under max-length restrictions.\nIn our second experiment we investigate the decoders\u2019 ability to entertain natural paraphrases of sentences. If we keep the semantic content of a sentence fixed and only change its grammatical structure, it is desirable that the decoder would assign nearly the same likelihood to the new sentence. One way to assess this invariance is to compare the relative likelihood that the model assigns to the gold sentence in comparison to its paraphrase. To test this, we take 50 examples from the WMT test split and manually generate paraphrases with various types of structural alterations (see details in the Appendix). For each type of decoder, we measure the relative change (in absolute value) of the log-likelihood resulting from the perturbation. All the models we compare have similar standard deviation (40 \u00b1 20) of log-likelihood scores over these examples, so the relative changes in the log-likelihood remain directly comparable. For each architecture we train two versions of different sizes, where the sizes are balanced in terms of the number of parameters across the architectures. The results in Figure 6 show that DRNN\u2019s exhibit significantly lower log-likelihood change, suggesting that, as language models, they are more robust to natural structural variation than their SEQ2SEQ counterparts.\n5 DISCUSSION AND FUTURE WORK\nWe have presented doubly recurrent neural networks, a natural extension of (sequential) recurrent architectures to tree-structured objects. This architecture models the information flow in a tree with two separate recurrent modules: one carrying ancestral information (received from parent and passed on to offspring) and the other carrying fraternal information (passed from sibling to sibling). The topology of the tree is modeled explicitly and separately from the label prediction, with modules that given the state of a node predict whether it has children and siblings.\nThe experimental results show that the proposed method is able to predict reasonable tree structures from encoded vector representations. Despite the simple structure of the IFTTT trees, the results on that task suggest a promising direction of using DRNNs for generating programs or executable queries from natural language. On the other hand, the results on the toy machine translation task show that even when used to generate sequences, DRNN\u2019s exhibit desirable properties, such as invariance over structural modifications and the ability to perform coarse-to-fine decoding. In order to truly use this architecture for machine translation, the approach must be scaled by resorting to batch processing in GPU. This is possible since forward and backward propagation are computed sequentially along tree traversal paths so that inputs and hidden states of parents and siblings can be grouped into tensors and operated in batch. We leave this as an avenue for future work.\nACKNOWLEDGEMENTS\nDA-M acknowledges support from a CONACYT fellowship. The authors would like to thank the anonymous reviewers for their constructive comments.\nB TRAINING DETAILS\nB.1 BACKPROPAGATION WITH DRNN\u2019S\nDuring training, we do the forward pass over the trees in breadth-first preorder, feeding into every node an ancestral and a fraternal state. For computational efficiency, before passing on the ancestral state to the offspring, we update it through the RNN using the current node\u2019s label, so as to avoid repeating this step for every child node. After the forward pass is complete, we compute label (cross-entropy) and topological (binary cross-entropy) loss for every node. In the backward pass, we compute in this order:\n1. Gradient of the current node\u2019s label prediction loss with respect to softmax layer parameters W,va,vf : \u2207\u03b8L(xi, x\u0302i).\n2. Gradients of topological prediction variable loss with respect to sigmoid layer parameters: \u2207\u03b8L(pai , tai ) and \u2207\u03b8L(pfi , tfi ).\n3. Gradient of predictive state layer parameters with respect to h(pred). 4. Gradient of predicted ancestral and fraternal hidden states with respect to gf and ga\u2019s pa-\nrameters.\nThe gradients of the input ancestral and fraternal hidden states are then passed on to the previous sibling and parent. When nodes have more than one child, we combine gradients from multiple children by averaging them. This procedure is repeated until the root note is reached, after which a single (ancestral state) gradient is passed to the encoder.\nB.2 MODEL SPECIFICATION AND TRAINING PARAMETERS\nThe best parameters for all tasks are chosen by performance on the validation sets. We perform early stopping based on the validation loss. For the IFTTT task, we initialize word embeddings with pretrained GloVe vectors (Pennington et al., 2014). For both tasks we clip gradients when the absolute value of any element exceeds 5. We regularize with a small penalty \u03c1 on the l2 norm of the parameters. We train all methods with ADAM (Kingma & Ba, 2014), with initial learning rate chosen by cross-validation. The parameter configurations that yielded the best results and were used for the final models are shown in Table 3. Details about the four models used for the machine translation task are shown in Table 4.\nC DATASET DETAILS\nC.1 SYNTHETIC TREE DATASET GENERATION\nWe generate trees in a top-down fashion, conditioning the label and topology of every node on the state of its ancestors and siblings. For simplicity, we use a Markovian assumption on these dependencies, modeling the probability of a node\u2019s label as depending only on the label of its parent p(i) and the last sibling s(i) generated before it (if any). Conditioned on these two inputs, we model the label of the node as coming from a multinomial distribution over the alphabet:\nP (wi | T ) = P (w | wp(i), ws(i)) \u223c Multi(\u03b8wp(i),ws(i)) (9) where \u03b8wp(i),ws(i) are class probabilities drawn from a Dirichlet prior with parameter \u03b1v . On the other hand, we denote by bai the binary variable indicating whether node i has descendants, and by bfi that indicating whether it has an ensuing sibling. We model these variables as depending only on the label of the current node and its position in the tree:\nP (bai | T ) = P (bai | wi, Di) = Bernoulli(pawi \u00b7 ga(Di)) P (bfi | T ) = P (bfi | wi,Wi) = Bernoulli(pfwi \u00b7 gf (Wi))\nwhereDi is the depth of node i andWi its width, defined as its position among the children of its parent p(i). Intuitively, we want to make P (bai = 1 | T ) decrease as we go deeper and further along the branches of the tree, so as to control its growth. Thus, we model ga and gf as decreasing functions with geometric decay, namely ga(D) = (\u03b3a)D and gf (W ) = (\u03b3f )W , with \u03b3a, \u03b3f \u2208 (0, 1). For the label-conditioned branching probabilities P (bai | wi) and P (bfi | wi), we use Bernoulli distributions with probabilities drawn from beta priors with parameters (\u03b1a, \u03b2a) and (\u03b1f , \u03b2f ), respectively.\nIn summary, we use the following generative procedure to grow the trees:\n1. For each wi \u2208 V , draw pawi \u223c Beta(\u03b1a, \u03b2a) and pfwi \u223c Beta(\u03b1f , \u03b2f ) 2. For each pair (wi, wj) draw \u03b8wi,wj \u223c Dir(\u03b1V ) 3. While there is an unlabeled non-terminal node i do:\n\u2022 Sample a label for i from w\u2217 \u223c P (w|wp(i), ws(i)) = Multi(\u03b8wp(i),ws(i)). \u2022 Draw ba \u223c P (ba|w\u2217, D) = Bernoulli(\u03b3Da \u00b7 paw(i)), where D is the current depth. If ba = 1, generate an node k, set p(k) = i, and add it to the queue.\n\u2022 Draw ba \u223c P (bf |w\u2217, D) = Bernoulli(\u03b3Wf \u00b7 pfw(i)), where W is the current width. If bf = 1, generate an node k, set s(k) = i, and add it to the queue.\nNote that this generative process does create a dependence between the topology and content of the trees (since the variables ba and bf depend on the content of the tree via their dependence on the label of their corresponding node). However, the actual process by which labels and topological decision is generated relies on separate mechanisms. This is natural assumption which is reasonable to expect in practice.\nThe choice of prior parameters is done drawing inspiration from natural language parse trees. We want nodes to have low but diverse probabilities of generating children, so we seek a slow-decaying distribution with most mass allocated in values close to 0. For this, we use (\u03b1a, \u03b2a) = (0.25, 1). For sibling generation, we use (\u03b1f , \u03b2f ) = (7, 2), which yields a distribution concentrated in values close to 1, so that nodes have on average a high and similar probability of producing siblings. Since we seek trees that are wider than they are deep, we use decay parameters \u03b3a = 0.6, \u03b3f = 0.9. Finally, we use a \u03b1v = 10 \u00b7 1 for the parent-sibling probability prior, favoring non-uniform interactions. Using this configuration, we generate 5000 sentence-tree pairs, which we split into training (4000 examples), validation (500) and test (500) sets. The characteristics of the trees in the dataset are summarized in Table 5.\nThe IFTTT dataset comes with a script to generate the data by crawling and parsing the recipes. Unfortunately, by the time we ran the script many recipes had been removed or changed. We therefore resorted to the original dataset used by Quirk et al. (2015). We converted these recipes into our tree format, assigning a node to each element in the first three levels (channels, functions and arguments, see figure 5). For the parameters level, many recipes have sentences instead of single tokens, so we broke these up creating one node per word. The last two layers are therefore the most topologically diverse, whereas the structure of the first two layers is constant (all trees have channels and functions). A very small fraction (< 1%) of trees that could not by parsed into our format was excluded from the dataset.\nTable 6 shows various statistics about the topological characteristics of the recipes in the IFTTT dataset. The middle columns show percentage of trees that contain nonempty arguments and parameters in trigger (IF) and action (THEN) branches. Almost all recipes have none empty arguments and parameters (and thus depth 4, excluding the root), and a lower percentage\u2014but still a majority\u2014has arguments and parameters on the trigger side too. The last two columns show tree statistics pertaining to the complexity of trees after conversion to our format. The distribution of tree sizes is mostly concentrated between 4 and 30 nodes, with a slow-decaying tail of examples above this range (see Figure 8).\nRegarding the content of the trees, the labels of the nodes in the first two levels (channels and functions) come from somewhat reduced vocabularies: 111 and 434 unique symbols for the trigger branch, respectively, and 157 and 85 for the action branch. The lower layers of the tree have a much more diverse vocabulary, with about 60K unique tokens in total. On the source side, the vocabulary over the sentence descriptions is large too, with about 30K unique tokens. The average sentence size is 6.07 tokens, with 80% of the sentences having at most 12 tokens.\nC.3 MACHINE TRANSLATION\nStarting from a preprocessed6 2% sub-selection of the English-French section of the WMT14 dataset, we further prune down the data by keeping only sentences of length between 5 and 20 words, and for which every word is within the 20K most frequent. The reason for this is to simplify the task by keeping only common words and avoiding out-of-vocabulary tokens. After this filtering, we are left with 53,607, 918 and 371 sentences for train, validation and test sets. After tokenizing, we obtain dependency parses for the target (English) sentences using the Stanford CoreNLP toolkit (Manning et al., 2014).\nFor the perturbation experiments, we randomly selected 50 sentences from among those in the test that could be easily restructured without significantly altering their meaning. The type of alterations we perform are: subordinate clause swapping, alternative construction substitution, passive/active voice change. In doing this, we try to keep the number of added/deleted words to a minimum, to minimize vocabulary-induced likelihood variations. When inserting new words, be verify that they are contained in the original vocabulary of 20K words. In Table 7 we show a few examples of the source, original target and perturbed target sentences.\n6http://www-lium.univ-lemans.fr/ schwenk/cslm joint paper/\nD ADDITIONAL EXAMPLE GENERATED TREES\n", "cats": {"READ": 1.0, "DONTREAD": 0.0}, "reviews": [{"IS_META_REVIEW": true, "comments": "Authors' response well answered my questions. Thanks. \nEvaluation not changed.\n\n###\n\nThis paper proposes a neural model for generating tree structure output from scratch. The model does 1) separate the recurrence between depths and siblings; 2) separate the topology and label generation, and outperforms previous methods on a benchmark IFTTT dataset. Compared to previous tree-decoding methods, the model avoids manually annotating subtrees with special tokens, and thus is a very good alternative to such problems. The paper does solid experiments on one synthetic dataset, and outperforms alternative methods on one real-world IFTTT dataset. \n\nThere are couple of interesting results in the paper that I believe is worth further investigation. Firstly, on the synthetic dataset, the precision drops rapidly with the number of nodes. Is it because that the vector representation of the sequential encoder fails to provide sufficient information of long sequences, such that the tree decoder can not do a good job? Or is it because that such tree decoder is not tolerant to the long sequence input, i.e., large tree structure? I believe that it is important to understand this before a better model can be developed. For example, if it is the fault of encoder, maybe an attention layer can be added, as in a seq-to-seq model, to preserve more information of the input sequence. \n\nMoreover, besides only showing how the precision changes with the number of nodes in the tree, it might be interesting to investigate how it goes with 1) number of depths; 2) number of widths; 3) symmetricity; etc. Moreover, as greedy search is used in decoding, it might be interesting to see how it helps, if it does, to use beam-search in tree decoding. \n\nOn the IFTTT dataset, listing more statistics about this dataset might be helpful for better understanding the difficulty of this task. How deep are the trees? How large are the vocabularies on both language and program sides?\n\nThe paper is well written, except for minor typo as mentioned in my pre-review questions. \n\nIn general, I believe this is a solid paper, and more can be explored in this direction. So I tend to accept it."}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "The paper introduces a new model for generating trees decorated with node embeddings. Interestingly the authors do not assume that even leaf nodes in the tree are known a-priori. There has been very little work on this setting, and, the problem is quite important and general. Though the experiments are somewhat limited, reviewers generally believe that they are sufficient to show that the approach holds a promise.\n \n + an important and under-explored setting\n + novel model\n + well written\n \n - experimentation could be stronger (but seems sufficient -- both on real and artificial data)", "OTHER_KEYS": "ICLR 2017 pcs"}, {"DATE": "22 Dec 2016", "TITLE": "Some questions", "IS_META_REVIEW": false, "comments": "It is really a nice work and paper is written quite well. The related work section is comprehensive and the problem is well motivated. And in my view, the experiments are good enough especially the paper contribution is introducing a new model which can be very useful in generating structured outputs using recurrent structure.\n\nQuestions: \nq1) How long did it take to train each of the networks in the paper?\nq2) Wondering any plan to release the code?\n\n\nThanks.\n \n", "OTHER_KEYS": "(anonymous)"}, {"TITLE": "No Title", "MEANINGFUL_COMPARISON": 2, "RECOMMENDATION_UNOFFICIAL": 2, "comments": "This paper proposes a variant of a recurrent neural network that has two orthogonal temporal dimensions that can be used as a decoder to generate tree structures (including the topology) in an encoder-decoder setting. The architecture is well motivated and I can see several applications (in addition to what's presented in the paper) that need to generate tree structures given an unstructured data.\n\nOne weakness of the paper is the limitation of experiments. IFTTT dataset seems to be an interesting appropriate application, and there is also a synthetic dataset, however it would be more interesting to see more natural language applications with syntactic tree structures. Still, I consider the experiments sufficient as a first step to showcase a novel architecture.\n\nA strength is that the authors experiment with different design decisions when building the topology predictor components of the architecture, about when / how to decide to terminate, as opposed to making a single arbitrary choice.\n\nI see future applications of this architecture and it seems to have interesting directions for future work so I suggest its acceptance as a conference contribution.", "ORIGINALITY": 4, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "19 Dec 2016", "CLARITY": 5, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "REVIEWER_CONFIDENCE": 4}, {"IMPACT": 1, "MEANINGFUL_COMPARISON": 1, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "The paper propose DRNN as a neural decoder for tree structures. I like the model architecture since it has two clear improvements over traditional approaches \u2014 (1) the information flows in two directions, both from the parent and from siblings, which is desirable in tree structures (2) the model use a probability distribution to model the tree boundary (i.e. the last sibling or the leaf). This avoids the use of special ending symbols which is larger in size and putting more things to learn for the parameters (shared with other symbols).\n\nThe authors test the DRNN using the tasks of recovering the synthetic trees and recovering functional programs. The model did better than traditional methods like seq2seq models.\n\nI think the recovering synthetic tree task is not very satisfying for two reasons \u2014 (1) the surface form itself already containing some of the topological information which makes the task easier than it should be (2) as we can see from figure 3, when the number of nodes grows (even to a number not very large), the performance of the model drops dramatically, I am not sure if a simple baseline only captures the topological information in the surface string would be much worse than this. And DRNN in this case, seems can\u2019t show its full potentials since the length of the information flow in the model won\u2019t be very long.\n\nI think the experiments are interesting. But I think there are some other tasks which are more difficult and the tree structure information are more important in such tasks. For example, we have the seq2seq parsing model (Vinyals et al, 2014), is it possible to use the DRNN proposed here on the decoder side? I think tasks like this can show more potentials of the DRNN and can be very convincing that model architectures like this are better than traditional alternatives.", "ORIGINALITY": 2, "IS_ANNOTATED": true, "TITLE": "review", "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "17 Dec 2016", "CLARITY": 2, "REVIEWER_CONFIDENCE": 4}, {"TITLE": "Accept", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer4", "comments": "Authors' response well answered my questions. Thanks. \nEvaluation not changed.\n\n###\n\nThis paper proposes a neural model for generating tree structure output from scratch. The model does 1) separate the recurrence between depths and siblings; 2) separate the topology and label generation, and outperforms previous methods on a benchmark IFTTT dataset. Compared to previous tree-decoding methods, the model avoids manually annotating subtrees with special tokens, and thus is a very good alternative to such problems. The paper does solid experiments on one synthetic dataset, and outperforms alternative methods on one real-world IFTTT dataset. \n\nThere are couple of interesting results in the paper that I believe is worth further investigation. Firstly, on the synthetic dataset, the precision drops rapidly with the number of nodes. Is it because that the vector representation of the sequential encoder fails to provide sufficient information of long sequences, such that the tree decoder can not do a good job? Or is it because that such tree decoder is not tolerant to the long sequence input, i.e., large tree structure? I believe that it is important to understand this before a better model can be developed. For example, if it is the fault of encoder, maybe an attention layer can be added, as in a seq-to-seq model, to preserve more information of the input sequence. \n\nMoreover, besides only showing how the precision changes with the number of nodes in the tree, it might be interesting to investigate how it goes with 1) number of depths; 2) number of widths; 3) symmetricity; etc. Moreover, as greedy search is used in decoding, it might be interesting to see how it helps, if it does, to use beam-search in tree decoding. \n\nOn the IFTTT dataset, listing more statistics about this dataset might be helpful for better understanding the difficulty of this task. How deep are the trees? How large are the vocabularies on both language and program sides?\n\nThe paper is well written, except for minor typo as mentioned in my pre-review questions. \n\nIn general, I believe this is a solid paper, and more can be explored in this direction. So I tend to accept it. ", "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "16 Dec 2016 (modified: 20 Jan 2017)", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "Beam size", "MEANINGFUL_COMPARISON": 2, "RECOMMENDATION_UNOFFICIAL": 2, "comments": "", "ORIGINALITY": 4, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "DATE": "02 Dec 2016", "CLARITY": 5, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3"}, {"IMPACT": 1, "MEANINGFUL_COMPARISON": 1, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "", "ORIGINALITY": 2, "IS_ANNOTATED": true, "TITLE": "questions", "IS_META_REVIEW": false, "DATE": "02 Dec 2016", "CLARITY": 2}, {"DATE": "01 Dec 2016", "TITLE": "Reasoning, experiments and typos", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer4"}, {"IS_META_REVIEW": true, "comments": "Authors' response well answered my questions. Thanks. \nEvaluation not changed.\n\n###\n\nThis paper proposes a neural model for generating tree structure output from scratch. The model does 1) separate the recurrence between depths and siblings; 2) separate the topology and label generation, and outperforms previous methods on a benchmark IFTTT dataset. Compared to previous tree-decoding methods, the model avoids manually annotating subtrees with special tokens, and thus is a very good alternative to such problems. The paper does solid experiments on one synthetic dataset, and outperforms alternative methods on one real-world IFTTT dataset. \n\nThere are couple of interesting results in the paper that I believe is worth further investigation. Firstly, on the synthetic dataset, the precision drops rapidly with the number of nodes. Is it because that the vector representation of the sequential encoder fails to provide sufficient information of long sequences, such that the tree decoder can not do a good job? Or is it because that such tree decoder is not tolerant to the long sequence input, i.e., large tree structure? I believe that it is important to understand this before a better model can be developed. For example, if it is the fault of encoder, maybe an attention layer can be added, as in a seq-to-seq model, to preserve more information of the input sequence. \n\nMoreover, besides only showing how the precision changes with the number of nodes in the tree, it might be interesting to investigate how it goes with 1) number of depths; 2) number of widths; 3) symmetricity; etc. Moreover, as greedy search is used in decoding, it might be interesting to see how it helps, if it does, to use beam-search in tree decoding. \n\nOn the IFTTT dataset, listing more statistics about this dataset might be helpful for better understanding the difficulty of this task. How deep are the trees? How large are the vocabularies on both language and program sides?\n\nThe paper is well written, except for minor typo as mentioned in my pre-review questions. \n\nIn general, I believe this is a solid paper, and more can be explored in this direction. So I tend to accept it."}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "The paper introduces a new model for generating trees decorated with node embeddings. Interestingly the authors do not assume that even leaf nodes in the tree are known a-priori. There has been very little work on this setting, and, the problem is quite important and general. Though the experiments are somewhat limited, reviewers generally believe that they are sufficient to show that the approach holds a promise.\n \n + an important and under-explored setting\n + novel model\n + well written\n \n - experimentation could be stronger (but seems sufficient -- both on real and artificial data)", "OTHER_KEYS": "ICLR 2017 pcs"}, {"DATE": "22 Dec 2016", "TITLE": "Some questions", "IS_META_REVIEW": false, "comments": "It is really a nice work and paper is written quite well. The related work section is comprehensive and the problem is well motivated. And in my view, the experiments are good enough especially the paper contribution is introducing a new model which can be very useful in generating structured outputs using recurrent structure.\n\nQuestions: \nq1) How long did it take to train each of the networks in the paper?\nq2) Wondering any plan to release the code?\n\n\nThanks.\n \n", "OTHER_KEYS": "(anonymous)"}, {"TITLE": "No Title", "MEANINGFUL_COMPARISON": 2, "RECOMMENDATION_UNOFFICIAL": 2, "comments": "This paper proposes a variant of a recurrent neural network that has two orthogonal temporal dimensions that can be used as a decoder to generate tree structures (including the topology) in an encoder-decoder setting. The architecture is well motivated and I can see several applications (in addition to what's presented in the paper) that need to generate tree structures given an unstructured data.\n\nOne weakness of the paper is the limitation of experiments. IFTTT dataset seems to be an interesting appropriate application, and there is also a synthetic dataset, however it would be more interesting to see more natural language applications with syntactic tree structures. Still, I consider the experiments sufficient as a first step to showcase a novel architecture.\n\nA strength is that the authors experiment with different design decisions when building the topology predictor components of the architecture, about when / how to decide to terminate, as opposed to making a single arbitrary choice.\n\nI see future applications of this architecture and it seems to have interesting directions for future work so I suggest its acceptance as a conference contribution.", "ORIGINALITY": 4, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "19 Dec 2016", "CLARITY": 5, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "REVIEWER_CONFIDENCE": 4}, {"IMPACT": 1, "MEANINGFUL_COMPARISON": 1, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "The paper propose DRNN as a neural decoder for tree structures. I like the model architecture since it has two clear improvements over traditional approaches \u2014 (1) the information flows in two directions, both from the parent and from siblings, which is desirable in tree structures (2) the model use a probability distribution to model the tree boundary (i.e. the last sibling or the leaf). This avoids the use of special ending symbols which is larger in size and putting more things to learn for the parameters (shared with other symbols).\n\nThe authors test the DRNN using the tasks of recovering the synthetic trees and recovering functional programs. The model did better than traditional methods like seq2seq models.\n\nI think the recovering synthetic tree task is not very satisfying for two reasons \u2014 (1) the surface form itself already containing some of the topological information which makes the task easier than it should be (2) as we can see from figure 3, when the number of nodes grows (even to a number not very large), the performance of the model drops dramatically, I am not sure if a simple baseline only captures the topological information in the surface string would be much worse than this. And DRNN in this case, seems can\u2019t show its full potentials since the length of the information flow in the model won\u2019t be very long.\n\nI think the experiments are interesting. But I think there are some other tasks which are more difficult and the tree structure information are more important in such tasks. For example, we have the seq2seq parsing model (Vinyals et al, 2014), is it possible to use the DRNN proposed here on the decoder side? I think tasks like this can show more potentials of the DRNN and can be very convincing that model architectures like this are better than traditional alternatives.", "ORIGINALITY": 2, "IS_ANNOTATED": true, "TITLE": "review", "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "17 Dec 2016", "CLARITY": 2, "REVIEWER_CONFIDENCE": 4}, {"TITLE": "Accept", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer4", "comments": "Authors' response well answered my questions. Thanks. \nEvaluation not changed.\n\n###\n\nThis paper proposes a neural model for generating tree structure output from scratch. The model does 1) separate the recurrence between depths and siblings; 2) separate the topology and label generation, and outperforms previous methods on a benchmark IFTTT dataset. Compared to previous tree-decoding methods, the model avoids manually annotating subtrees with special tokens, and thus is a very good alternative to such problems. The paper does solid experiments on one synthetic dataset, and outperforms alternative methods on one real-world IFTTT dataset. \n\nThere are couple of interesting results in the paper that I believe is worth further investigation. Firstly, on the synthetic dataset, the precision drops rapidly with the number of nodes. Is it because that the vector representation of the sequential encoder fails to provide sufficient information of long sequences, such that the tree decoder can not do a good job? Or is it because that such tree decoder is not tolerant to the long sequence input, i.e., large tree structure? I believe that it is important to understand this before a better model can be developed. For example, if it is the fault of encoder, maybe an attention layer can be added, as in a seq-to-seq model, to preserve more information of the input sequence. \n\nMoreover, besides only showing how the precision changes with the number of nodes in the tree, it might be interesting to investigate how it goes with 1) number of depths; 2) number of widths; 3) symmetricity; etc. Moreover, as greedy search is used in decoding, it might be interesting to see how it helps, if it does, to use beam-search in tree decoding. \n\nOn the IFTTT dataset, listing more statistics about this dataset might be helpful for better understanding the difficulty of this task. How deep are the trees? How large are the vocabularies on both language and program sides?\n\nThe paper is well written, except for minor typo as mentioned in my pre-review questions. \n\nIn general, I believe this is a solid paper, and more can be explored in this direction. So I tend to accept it. ", "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "16 Dec 2016 (modified: 20 Jan 2017)", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "Beam size", "MEANINGFUL_COMPARISON": 2, "RECOMMENDATION_UNOFFICIAL": 2, "comments": "", "ORIGINALITY": 4, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "DATE": "02 Dec 2016", "CLARITY": 5, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3"}, {"IMPACT": 1, "MEANINGFUL_COMPARISON": 1, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "", "ORIGINALITY": 2, "IS_ANNOTATED": true, "TITLE": "questions", "IS_META_REVIEW": false, "DATE": "02 Dec 2016", "CLARITY": 2}, {"DATE": "01 Dec 2016", "TITLE": "Reasoning, experiments and typos", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer4"}]}
{"text": "A RECURRENT NEURAL NETWORK WITHOUT CHAOS\n1 INTRODUCTION\nGated recurrent neural networks, such as the Long Short Term Memory network (LSTM) introduced by Hochreiter & Schmidhuber (1997) and the Gated Recurrent Unit (GRU) proposed by Cho et al. (2014), prove highly effective for machine learning tasks that involve sequential data. We propose an exceptionally simple variant of these gated architectures. The basic model takes the form\nht = \u03b8t tanh(ht\u22121) + \u03b7t tanh(Wxt), (1)\nwhere stands for the Hadamard product. The horizontal/forget gate (i.e. \u03b8t) and the vertical/input gate (i.e. \u03b7t) take the usual form used in most gated RNN architectures. Specifically\n\u03b8t := \u03c3 (U\u03b8ht\u22121 + V\u03b8xt + b\u03b8) and \u03b7t := \u03c3 (U\u03b7ht\u22121 + V\u03b7xt + b\u03b7) (2)\nwhere \u03c3(x) := (1 + e\u2212x)\u22121 denotes the logistic sigmoid function. The network (1)\u2013(2) has quite intuitive dynamics. Suppose the data xt present the model with a sequence\n(Wxt)(i) = { 10 if t = T 0 otherwise,\n(3)\nwhere (Wxt)(i) stands for the ith component of the vector Wxt. In other words we consider an input sequence xt for which the learned ith feature (Wxt)(i) remains off except at time T . When initialized from h0 = 0, the corresponding response of the network to this \u201cimpulse\u201d in the ith feature is\nht(i) \u2248  0 if t < T \u03b7T if t = T \u03b1t if t > T\n(4)\nwith \u03b1t a sequence that relaxes toward zero. The forget gate \u03b8t control the rate of this relaxation. Thus ht(i) activates when presented with a strong ith feature, and then relaxes toward zero until the data present the network once again with strong ith feature. Overall this leads to a dynamically simple model, in which the activation patterns in the hidden states of the network have a clear cause and predictable subsequent behavior.\nDynamics of this sort do not occur in other RNN models. Instead, the three most popular recurrent neural network architectures, namely the vanilla RNN, the LSTM and the GRU, have complex, irregular, and unpredictable dynamics. Even in the absence of input data, these networks can give rise to chaotic dynamical systems. In other words, when presented with null input data the activation patterns in their hidden states do not necessarily follow a predictable path. The proposed network (1)\u2013(2) has rather dull and minimalist dynamics in comparison; its only attractor is the zero state,\nand so it stands at the polar-opposite end of the spectrum from chaotic systems. Perhaps surprisingly, at least in the light of this comparison, the proposed network (1) performs as well as LSTMs and GRUs on the word level language modeling task. We therefore conclude that the ability of an RNN to form chaotic temporal dynamics, in the sense we describe in Section 2, cannot explain its success on word-level language modeling tasks.\nIn the next section, we review the phenomenon of chaos in RNNs via both synthetic examples and trained models. We also prove a precise, quantified description of the dynamical picture (3)\u2013(4) for the proposed network. In particular, we show that the dynamical system induced by the proposed network is never chaotic, and for this reason we refer to it as a Chaos-Free Network (CFN). The final section provides a series of experiments that demonstrate that CFN achieve results comparable to LSTM on the word-level language modeling task. All together, these observations show that an architecture as simple as (1)\u2013(2) can achieve performance comparable to the more dynamically complex LSTM.\n2 CHAOS IN RECURRENT NEURAL NETWORKS\nThe study of RNNs from a dynamical systems point-of-view has brought fruitful insights into generic features of RNNs (Sussillo & Barak, 2013; Pascanu et al., 2013). We shall pursue a brief investigation of CFN, LSTM and GRU networks using this formalism, as it allows us to identify key distinctions between them. Recall that for a given mapping \u03a6 : Rd 7\u2192 Rd, a given initial time t0 \u2208 N and a given initial state u0 \u2208 Rd, a simple repeated iteration of the mapping \u03a6\nut+1 = \u03a6(ut) t > t0,\nut0 = u0 t = t0,\ndefines a discrete-time dynamical system. The index t \u2208 N represents the current time, while the point ut \u2208 Rd represents the current state of the system. The set of all visited states O+(u0) := {ut0 , ut0+1, . . . , ut0+n, . . .} defines the forward trajectory or forward orbit through u0. An attractor for the dynamical system is a set that is invariant (any trajectory that starts in the set remains in the set) and that attracts all trajectories that start sufficiently close to it. The attractors of chaotic dynamical systems are often fractal sets, and for this reason they are referred to as strange attractors.\nMost RNNs generically take the functional form\nut = \u03a8(ut\u22121,W1xt,W2xt, . . . ,Wkxt), (5)\nwhere xt denotes the tth input data point. For example, in the case of the CFN (1)\u2013(2), we have W1 = W , W2 = V\u03b8 and W3 = V\u03b7 . To gain insight into the underlying design of the architecture of an RNN, it proves usefull to consider how trajectories behave when they are not influenced by any external input. This lead us to consider the dynamical system\nut = \u03a6(ut\u22121) \u03a6(u) := \u03a8(u, 0, 0, . . . , 0), (6)\nwhich we refer to as the dynamical system induced by the recurrent neural network. The timeinvariant system (6) is much more tractable than (5), and it offers a mean to investigate the inner working of a given architecture; it separates the influence of input data xt, which can produce essentially any possible response, from the model itself. Studying trajectories that are not influenced by external data will give us an indication on the ability of a given RNN to generate complex and sophisticated trajectories by its own. As we shall see shortly, the dynamical system induced by a CFN has excessively simple and predictable trajectories: all of them converge to the zero state. In other words, its only attractor is the zero state. This is in sharp contrast with the dynamical systems induced by LSTM or GRU, who can exhibit chaotic behaviors and have strange attractors.\nThe learned parameters Wj in (5) describe how data influence the evolution of hidden states at each time step. From a modeling perspective, (6) would occur in the scenario where a trained RNN has learned a weak coupling between a specific data point xt0 and the hidden state at that time, in the sense that the data influence is small and so all Wjxt0 \u2248 0 nearly vanish. The hidden state then transitions according to ut0 \u2248 \u03a8(ut0\u22121, 0, 0, . . . , 0) = \u03a6(ut0\u22121). We refer to Bertschinger & Natschla\u0308ger (2004) for a study of the chaotic behavior of a simplified vanilla RNN with a specific statistical model, namely an i.i.d. Bernoulli process, for the input data as well as a specific statistical model, namely i.i.d. Gaussian, for the weights of the recurrence matrix.\n2.1 CHAOTIC BEHAVIOR OF LSTM AND GRU IN THE ABSENCE OF INPUT DATA\nIn this subsection we briefly show that LSTM and GRU, in the absence of input data, can lead to dynamical systems ut = \u03a6(ut\u22121) that are chaotic in the classical sense of the term (Strogatz, 2014). Figure 1 depicts the strange attractor of the dynamical system:\nut = [ ht ct ] u 7\u2192 \u03a6(u) = [ o tanh (f c+ i g) f c+ i g ] (7)\ni := \u03c3(Wih+ bi) f := \u03c3(Wfh+ bf ) o := \u03c3(Woh+ bo) g := tanh(Wgh+ bg) (8)\ninduced by a two-unit LSTM with weight matrices\nWi = [ \u22121 \u22124 \u22123 \u22122 ] Wo = [ 4 1 \u22129 \u22127 ] Wf = [ \u22122 6 0 \u22126 ] Wg = [ \u22121 \u22126 6 \u22129 ] (9)\nand zero bias for the model parameters. These weights were randomly generated from a normal distribution with standard deviation 5 and then rounded to the nearest integer. Figure 1(a) was obtained by choosing an initial state u0 = (h0, c0) uniformly at random in [0, 1]2 \u00d7 [0, 1]2 and plotting the h-component of the iterates ut = (ht, ct) for t between 103 and 105 (so this figure should be regarded as a two dimensional projection of a four dimensional attractor, which explain its tangled appearance). Most trajectories starting in [0, 1]2 \u00d7 [0, 1]2 converge toward the depicted attractor. The resemblance between this attractor and classical strange attractors such as the He\u0301non attractor is striking (see Figure 5 in the appendix for a depiction of the He\u0301non attractor). Successive zooms on the branch of the LSTM attractor from Figure 1(a) reveal its fractal nature. Figure 1(b) is an enlargement of the red box in Figure 1(a), and Figure 1(c) is an enlargement of the magenta box in Figure 1(b). We see that the structure repeats itself as we zoom in.\nThe most practical consequence of chaos is that the long-term behavior of their forward orbits can exhibit a high degree of sensitivity to the initial states u0. Figure 2 provides an example of such behavior for the dynamical system (7)\u2013(9). An initial condition u0 was drawn uniformly at random in [0, 1]2 \u00d7 [0, 1]2. We then computed 100, 000 small amplitude perturbations u\u03020 of u0 by adding a small random number drawn uniformly from [\u221210\u22127, 10\u22127] to each component. We then iterated (7)\u2013(9) for 200 steps and plotted the h-component of the final state u\u0302200 for each of the 100, 000 trials on Figure 2(a). The collection of these 100, 000 final states essentially fills out the entire attractor, despite the fact that their initial conditions are highly localized (i.e. at distance of no more than 10\u22127) around a fixed point. In other words, the time t = 200 map of the dynamical system will map a small neighborhood around a fixed initial condition u0 to the entire attractor. Figure 2(b) additionally illustrates this sensitivity to initial conditions for points on the attractor itself. We take an initial condition u0 on the attractor and perturb it by 10\u22127 to a nearby initial condition u\u03020. We then plot the distance \u2016u\u0302t \u2212 ut\u2016 between the two corresponding trajectories for the first 200 time steps. After an initial phase of agreement, the trajectories strongly diverge.\nThe synthetic example (7)\u2013(9) illustrates the potentially chaotic nature of the LSTM architecture. We now show that chaotic behavior occurs for trained models as well, and not just for synthetically generated instances. We take the parameter values of an LSTM with 228 hidden units trained on the\nPenn Treebank corpus without dropout (c.f. the experimental section for the precise procedure). We then set all data inputs xt to zero and run the corresponding induced dynamical system. Two trajectories starting from nearby initial conditions u0 and u\u03020 were computed (as before u\u03020 was obtained by adding to each components of u0 a small random number drawn uniformly from [\u221210\u22127, 10\u22127]). Figure 3(a) plots the first component h(1) of the hidden state for both trajectories over the first 1600 time steps. After an initial phase of agreement, the forward trajectories O+(u0) and O+(u\u03020) strongly diverge. We also see that both trajectories exhibit the typical aperiodic behavior that characterizes chaotic systems. If the inputs xt do not vanish, but come from actual word-level data, then the behavior is very different. The LSTM is now no longer an autonomous system whose dynamics are driven by its hidden states, but a time dependent system whose dynamics are mostly driven by the external inputs. Figure 3(b) shows the first component h(1) of the hidden states of two trajectories that start with initial conditions u0 and u\u03020 that are far apart. The sensitivity to initial condition disappears, and instead the trajectories converge toward each other after about 70 steps. The memory of this initial difference is lost. Overall these experiments indicate that a trained LSTM, when it is not driven by external inputs, can be chaotic. In the presence of input data, the LSTM becomes a forced system whose dynamics are dominated by external forcing.\nLike LSTM networks, GRU can also lead to dynamical systems that are chaotic and they can also have strange attractors. The depiction of such an attractor, in the case of a two-unit GRU, is provided in Figure 6 of the appendix.\n2.2 CHAOS-FREE BEHAVIOR OF THE CFN\nThe dynamical behavior of the CFN is dramatically different from that of the LSTM. In this subsection we start by showing that the hidden states of the CFN activate and relax toward zero in a predictable fashion in response to input data. On one hand, this shows that the CFN cannot produce non-trivial dynamics without some influence from data. On the other, this leads to an interpretable model; any non-trivial activations in the hidden states of a CFN have a clear cause emanating from\ndata-driven activation. This follows from a precise, quantified description of the intuitive picture (3)\u2013(4) sketched in the introduction.\nWe begin with the following simple estimate that sheds light on how the hidden states of the CFN activate and then relax toward the origin. Lemma 1. For any T, k > 0 we have\n|hT+k(i)| \u2264 \u0398k |hT (i)|+ H\n1\u2212\u0398\n( max\nT\u2264t\u2264T+k |(Wxt)(i)| ) where \u0398 and H are the maximum values of the ith components of the \u03b8 and \u03b7 gate in the time interval [T, T + k], that is:\n\u0398 = max T\u2264t\u2264T+k \u03b8t(i) and H = max T\u2264t\u2264T+k \u03b7t(i).\nThis estimate shows that if during a time interval [T1, T2] one of\n(i) the embedded inputs Wxt have weak ith feature (i.e. maxT\u2264t\u2264T+k |(Wxt)(i)| is small), (ii) or the input gates \u03b7t have their ith component close to zero (i.e. H is small),\noccurs then the ith component of the hidden state ht will relaxes toward zero at a rate that depends on the value of the ith component the the forget gate. Overall this leads to the following simple picture: ht(i) activates when presented with an embedded input Wxt with strong ith feature, and then relaxes toward zero until the data present the network once again with strong ith feature. The strength of the activation and the decay rate are controlled by the ith component of the input and forget gates. The proof of Lemma 1 is elementary \u2014\nProof of Lemma 1. Using the non-expansivity of the hyperbolic tangent, i.e. | tanh(x)| \u2264 |x|, and the triangle inequality, we obtain from (1)\n|ht(i)| \u2264 \u0398 |ht\u22121(i)|+H max T\u2264t\u2264T+k |(Wxt)(i)|\nwhenever t is in the interval [T, T + k]. Iterating this inequality and summing the geometric series then gives |hT+k(i)| \u2264 \u0398k|hT (i)|+ ( 1\u2212\u0398k\n1\u2212\u0398\n) H max\nT\u2264t\u2264T+k |(Wxt)(i)|\nfrom which we easily conclude.\nWe now turn toward the analysis of the long-term behavior of the the dynamical system\nut = ht, u 7\u2192 \u03a6(u) := \u03c3 (U\u03b8u + b\u03b8) tanh(u). (10) induced by a CFN. The following lemma shows that the only attractor of this dynamical system is the zero state. Lemma 2. Starting from any initial state u0, the trajectory O+(u0) will eventually converge to the zero state. That is, limt\u2192+\u221e ut = 0 regardless of the the initial state u0.\nProof. From the definition of \u03a6 we clearly have that the sequence defined by ut+1 = \u03a6(ut) satisfies \u22121 < ut(i) < 1 for all t and all i. Since the sequence ut is bounded, so is the sequence vt := U\u03b8ut + b\u03b8. That is there exists a finite C > 0 such that (U\u03b8ut)(i) + b\u03b8(i) < C for all t and i. Using the non-expansivity of the hyperbolic tangent, we then obtain that |ut(i)| \u2264 \u03c3(C)|ut\u22121(i)|, for all t and all i. We conclude by noting that 0 < \u03c3(C) < 1.\nLemma 2 remains true for a multi-layer CFN, that is, a CFN in which the first layer is defined by (1) and the subsequent layers 2 \u2264 ` \u2264 L are defined by:\nh (`) t = \u03b8 (`) t tanh(h (`) t\u22121) + \u03b7 (`) t tanh(W (`)h (`\u22121) t ).\nAssume that Wxt = 0 for all t > T , then an extension of the arguments contained in the proof of the two previous lemmas shows that\n|h(`)T+k| \u2264 C(1 + k) (`\u22121)\u0398k (11)\nwhere 0 < \u0398 < 1 is the maximal values for the input gates involved in layer 1 to ` of the network, and C > 0 is some constant depending only on the norms \u2016W (j)\u2016\u221e of the matrices and the sizes |h(j)T | of the initial conditions at all previous 1 \u2264 j \u2264 ` levels. Estimate (11) shows that Lemma 2 remains true for multi-layer architectures.\nInequality (11) shows that higher levels (i.e. larger `) decay more slowly, and remain non-trivial, while earlier levels (i.e. smaller `) decay more quickly. We illustrate this behavior computationally with a simple experiment. We take a 2-layer, 224-unit CFN network trained on Penn Treebank and feed it the following input data: The first 1000 inputs xt are the first 1000 words of the test set of Penn Treebank; All subsequent inputs are zero. In other words, xt = 0 if t > 1000. For each of the two layers we then select the 10 units that decay the slowest after t > 1000 and plot them on Figure 4. The figure illustrates that the second layer retains information for much longer than the first layer. To quantify this observation we define the relaxation time (or half-life) of the ith unit as the smallest T such that |h1000+T (i)| < 0.5|h1000(i)|. Using this definition yields average relaxation times of 2.2 time steps for the first layer and 23.2 time steps for the second layer. The first layer has a standard deviations of approximately 5 steps while the second layer has a standard deviation of approximately 75 time steps. A more fine-grained analysis reveals that some units in the second layer have relaxation times of several hundred steps. For instance, if instead of averaging the relaxation times over the whole layer we average them over the top quartile (i.e. the 25% units that decay the most slowly) we get 4.8 time steps and 85.6 time steps for the first and second layers, respectively. In other words, by restricting attention to long-term units the difference between the first and second layers becomes much more striking.\nOverall, this experiment conforms with the analysis (11), and indicates that adding a third or fourth layer would potentially allow a multi-layer CFN architecture to retain information for even longer.\n3 EXPERIMENTS\nIn this section we show that despite its simplicity, the CFN network achieves performance comparable to the much more complex LSTM network on the word level language modeling task. We use two datasets for these experiments, namely the Penn Treebank corpus (Marcus et al., 1993) and the Text8 corpus (Mikolov et al., 2014). We consider both one-layer and two-layer CFNs and LSTMs for our experiments. We train both CFN and LSTM networks in a similar fashion and always compare models that use the same number of parameters. We compare their performance with and without dropout, and show that in both cases they obtain similar results. We also provide results published in Mikolov et al. (2014), Jozefowicz et al. (2015) and Sukhbaatar et al. (2015) for the sake of comparison.\nFor concreteness, the exact implementation for the two-layer architecture of our model is\nh (0) t = W (0)xt\nh\u0302 (0) t = Drop(h (0) t , p)\nh (1) t = \u03b8 (1) t tanh(h (1) t\u22121) + \u03b7 (1) t tanh(W (1)h\u0302 (0) t )\nh\u0302 (1) t = Drop(h (1) t , p)\nh (2) t = \u03b8 (2) t tanh(h (2) t\u22121) + \u03b7 (2) t tanh(W (2)h\u0302 (1) t )\nh\u0302 (2) t = Drop(h (2) t , p)\nyt = LogSoftmax(W (3)h\u0302 (2) t + b)\nwhere Drop(z, p) denotes the dropout operator with a probability p of setting components in z to zero. We compute the gates according to\n\u03b8 (`) t := \u03c3\n( U\n(`) \u03b8 h\u0303 (`) t\u22121 + V (`) \u03b8 h\u0303 (`\u22121) t + b\u03b8 ) and \u03b7(`)t := \u03c3 ( U (`)\u03b7 h\u0303 (`) t\u22121 + V (`) \u03b7 h\u0303 (`\u22121) t + b\u03b7 ) where h\u0303(`)t\u22121 = Drop(h (`) t\u22121, q) and h\u0303 (`\u22121) t = Drop(h (`\u22121) t , q),\nand thus the model has two dropout hyperparameters. The parameter p controls the amount of dropout between layers; the parameter q controls the amount of dropout inside each gate. We use a similar dropout strategy for the LSTM, in that all sigmoid gates f, o and i receive the same amount q of dropout.\nTo train the CFN and LSTM networks, we use a simple online steepest descent algorithm. We update the weights w via w(k+1) = w(k) \u2212 lr \u00b7 ~p where ~p = \u2207wL\u2016\u2207wL\u20162 , (12) where lr is the learning rate and \u2207wL denotes the approximate gradient of the loss with respect to the weights as estimated from a certain number of presented examples. We use the usual backpropagation through time approximation when estimating the gradient: we unroll the net T steps in the past and neglect longer dependencies. In all experiments, the CFN and LSTM networks are unrolled for T = 35 steps and we take minibatches of size 20. As all search directions ~p have Euclidean norm \u2016~p\u20162 = 1, we perform no gradient clipping during training. We initialize all the weights in the CFN, except for the bias of the gates, uniformly at random in [\u22120.07, 0.07]. We initialize the bias b\u03b8 and b\u03b7 of the gates to 1 and \u22121, respectively, so that at the beginning of the training \u03b8t \u2248 \u03c3(1) \u2248 0.73 and \u03b7t \u2248 \u03c3(\u22121) \u2248 0.23. We initialize the weights of the LSTM in exactly the same way; the bias for the forget and input gate are initialized to 1 and \u22121, and all the other weights are initialized uniformly in [\u22120.07, 0.07]. This initialization scheme favors\nthe flow of information in the horizontal direction. The importance of a careful initialization of the forget gate was pointed out in Gers et al. (2000) and Jozefowicz et al. (2015). Finally, we initialize all hidden states to zero for both models.\nDataset Construction. The Penn Treebank Corpus has 1 million words and a vocabulary size of 10,000. We used the code from Zaremba et al. (2014) to construct and split the dataset into a training set (929K words), a validation set (73K words) and a test set (82K words). The Text8 corpus has 100 million characters and a vocabulary size of 44,000. We used the script from Mikolov et al. (2014) to construct and split the dataset into a training set (first 99M characters) and a development set (last 1M characters).\nExperiments without Dropout. Tables 1 and 2 provide a comparison of various recurrent network architectures without dropout evaluated on the Penn Treebank corpus and the Text8 corpus. The last two rows of each table provide results for LSTM and CFN networks trained and initialized in the manner described above. We have tried both one and two layer architectures, and reported only the best result. The learning rate schedules used for each network are described in the appendix.\nWe also report results published in Jozefowicz et al. (2015) were a vanilla RNN, a GRU and an LSTM network were trained on Penn Treebank, each of them having 5 million parameters (only the test perplexity was reported). Finally we report results published in Mikolov et al. (2014) and Sukhbaatar et al. (2015) where various networks are trained on Text8. Of these four networks, only the LSTM network from Mikolov et al. (2014) has the same number of parameters than the CFN and LSTM networks we trained (46.4M parameters). The vanilla RNN, Structurally Constrained Recurrent Network (SCRN) and End-To-End Memory Network (MemN2N) all have 500 units, but less than 46.4M parameters. We nonetheless indicate their performance in Table 2 to provide some context.\nExperiments with Dropout. Table 3 provides a comparison of various recurrent network architectures with dropout evaluated on the Penn Treebank corpus. The first three rows report results published in (Jozefowicz et al., 2015) and the last four rows provide results for LSTM and CFN networks trained and initialized with the strategy previously described. The dropout rate p and q are chosen as follows: For the experiments with 20M parameters, we set p = 55% and q = 45% for the CFN and p = 60% and q = 40% for the LSTM; For the experiments with 50M parameters, we set p = 65% and q = 55% for the CFN and p = 70% and q = 50% for the LSTM.\n4 CONCLUSION\nDespite its simple dynamics, the CFN obtains results that compare well against LSTM networks and GRUs on word-level language modeling. This indicates that it might be possible, in general, to build RNNs that perform well while avoiding the intricate, uninterpretable and potentially chaotic dynamics that can occur in LSTMs and GRUs. Of course, it remains to be seen if dynamically simple RNNs such as the proposed CFN can perform well on a wide variety of tasks, potentially requiring longer term dependencies than the one needed for word level language modeling. The experiments presented in Section 2 indicate a plausible path forward \u2014 activations in the higher layers of a multi-layer CFN decay at a slower rate than the activations in the lower layers. In theory, complexity and long-term dependencies can therefore be captured using a more \u201cfeed-forward\u201d approach (i.e. stacking layers) rather than relying on the intricate and hard to interpret dynamics of an LSTM or a GRU.\nOverall, the CFN is a simple model and it therefore has the potential of being mathematically wellunderstood. In particular, Section 2 reveals that the dynamics of its hidden states are inherently more interpretable than those of an LSTM. The mathematical analysis here provides a few key insights into the network, in both the presence and absence of input data, but obviously more work is needed before a complete picture can emerge. We hope that this investigation opens up new avenues of inquiry, and that such an understanding will drive subsequent improvements.\n", "cats": {"READ": 1.0, "DONTREAD": 0.0}, "reviews": [{"IS_META_REVIEW": true, "comments": "This paper poses an interesting idea: removing chaotic behavior or RNNs.\nWhile many other papers on new RNN architecture usually focus too much on the performance improvement and leave the analysis part on their success as a black-box, this paper does a good job on presenting why its method may work well.\n\nAlthough, the paper shows lots of comparison between the chaotic systems (GRUs & LSTMs) and the stable system (proposed CFN model), the reviewer is not fully convinced by the main claim of this paper, the nuance that chaotic behaviour makes dynamic system to have rich representation power but makes the system too unstable. In the paper, the LSTM shows a very sensitive behaviour, even when a very small amount of noise is added to the input. However, it still performs surprisingly well with this chaotic behaviour. \n\nMeasuring the model complexity is a very difficult task, therefore, many papers manage to use either same number of hidden units or choose approximately close model sizes. In this paper, the experiments were carried by using the same amount of parameters for both the LSTM and CFN. However, I think the CFN may have much more simpler computational graph. Taking the idea of this work, can we develop a stable dynamic system, but which does not only have one attractor?\n\nIt is also interesting to see that the layers of CFNs are updated in different timescales in a sense that the decaying speed decreases when the layer gets higher. Could you provide more statistics on this? For example, what is the average relaxation time of the whole hidden units at each layer?\n\nBatch normalization and layer normalization can be helpful to make the training of RNNs become more stable. How would the behaviour of batch normalized or perhaps layer normalized LSTM look like? Also, it is often not trivial to make batch normalization or layer normalization to work on a new architecture. I think it may be useful to compare batch normalized or layer normalized versions of the LSTM and CFN.\n\nThe quality of the work is good, explanation is clear enough along with nice analyses and proofs. Overall, the performance is not any better than LSTMs, but it is still interesting when thinking of simplicity of this model. I am a bit concerned if this model might not work that well in more harder task, e.g., translation. Figure 4 of this paper is very interesting, where the proposed architecture shows that the hidden units at the second layer tends to keep its information longer than the first layer ones."}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "The reviewers all enjoyed this paper and the analysis.\n \n pros:\n - novel new model\n - interesting insights into the design of model, through analysis of trajectories of hidden states of RNNs.\n \n cons:\n - results are worse than LSTMs.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"DATE": "31 Dec 2016", "TITLE": "Questions", "IS_META_REVIEW": false, "comments": "Thanks for a very interesting read.\n\nWhat happens if instead of driving the LSTMs with x_t = 0, you drive it with a fixed input, like the word \"What\"? Would that behave the same as in fig 3?\n\nIf you drive the LSTMs with some input and then fix x_t = 0 for t > T (as in fig 4), do you still see chaos? If there is gradual decay in the hidden units' activations, do you also see that the second layer forgets more slowly than the first?\n\nHave you tried training on the copy task as in the algorithmic learning literature (like NTM), to see whether there is a actual difference in how long memory is retained in CFN vs LSTM?", "OTHER_KEYS": "Greg Yang"}, {"OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "TITLE": "Nice investigation", "comments": "The authors of the paper set out to answer the question whether chaotic behaviour is a necessary ingredient for RNNs to perform well on some tasks. For that question's sake, they propose an architecture which is designed to not have chaos. The subsequent experiments validate the claim that chaos is not necessary.\n\nThis paper is refreshing. Instead of proposing another incremental improvement, the authors start out with a clear hypothesis and test it. This might set the base for future design principles of RNNs.\n\nThe only downside is that the experiments are only conducted on tasks which are known to be not that demanding from a dynamical systems perspective; it would have been nice if the authors had traversed the set of data sets more to find data where chaos is actually necessary.", "ORIGINALITY": 5, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "RECOMMENDATION": 8, "DATE": "20 Dec 2016", "REVIEWER_CONFIDENCE": 3}, {"IMPACT": 4, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "TITLE": "interesting starting point", "comments": "I think the authors provide an interesting direction for understanding and maybe constructing recurrent models that are easier to interpret. Is not clear where such direction will lead but I think it could be an interesting starting point for future work, one that worth exploring. ", "SOUNDNESS_CORRECTNESS": 4, "ORIGINALITY": 3, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "18 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "TITLE": "Cool paper", "comments": "This paper poses an interesting idea: removing chaotic behavior or RNNs.\nWhile many other papers on new RNN architecture usually focus too much on the performance improvement and leave the analysis part on their success as a black-box, this paper does a good job on presenting why its method may work well.\n\nAlthough, the paper shows lots of comparison between the chaotic systems (GRUs & LSTMs) and the stable system (proposed CFN model), the reviewer is not fully convinced by the main claim of this paper, the nuance that chaotic behaviour makes dynamic system to have rich representation power but makes the system too unstable. In the paper, the LSTM shows a very sensitive behaviour, even when a very small amount of noise is added to the input. However, it still performs surprisingly well with this chaotic behaviour. \n\nMeasuring the model complexity is a very difficult task, therefore, many papers manage to use either same number of hidden units or choose approximately close model sizes. In this paper, the experiments were carried by using the same amount of parameters for both the LSTM and CFN. However, I think the CFN may have much more simpler computational graph. Taking the idea of this work, can we develop a stable dynamic system, but which does not only have one attractor?\n\nIt is also interesting to see that the layers of CFNs are updated in different timescales in a sense that the decaying speed decreases when the layer gets higher. Could you provide more statistics on this? For example, what is the average relaxation time of the whole hidden units at each layer?\n\nBatch normalization and layer normalization can be helpful to make the training of RNNs become more stable. How would the behaviour of batch normalized or perhaps layer normalized LSTM look like? Also, it is often not trivial to make batch normalization or layer normalization to work on a new architecture. I think it may be useful to compare batch normalized or layer normalized versions of the LSTM and CFN.\n\nThe quality of the work is good, explanation is clear enough along with nice analyses and proofs. Overall, the performance is not any better than LSTMs, but it is still interesting when thinking of simplicity of this model. I am a bit concerned if this model might not work that well in more harder task, e.g., translation. Figure 4 of this paper is very interesting, where the proposed architecture shows that the hidden units at the second layer tends to keep its information longer than the first layer ones.", "SOUNDNESS_CORRECTNESS": 5, "ORIGINALITY": 4, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "17 Dec 2016 (modified: 18 Dec 2016)", "REVIEWER_CONFIDENCE": 4}, {"DATE": "16 Dec 2016", "TITLE": "Authors\u2019 comment: Conclusion added", "IS_META_REVIEW": false, "comments": "We added a short conclusion reflecting some of the discussions with the reviewers. ", "OTHER_KEYS": "Thomas Laurent"}, {"DATE": "12 Dec 2016", "TITLE": "Authors\u2019 comment: Added experiments on long term dependencies.", "IS_META_REVIEW": false, "comments": "Several reviewers have posted comments asking about the capability of the proposed model to capture long-term dependencies. This is a natural question since the model was designed so that units get activated when presented the correct feature, then relax to zero at a rate controlled by the forget gate. At a first glance it is unclear that such a simple mechanism could capture long term dependencies (the relaxing rates might be too fast).\n\nWe added a simple experiment in the paper showing that long term dependencies can be obtained by stacking multiple layers of the basic architecture (see Figure 4). We took a 2-layer, 224-unit CFN network trained on Penn Treebank and ran it with the following input data: The first 1000 inputs x_t are the first 1000 words of the test set of PTB; All subsequent inputs are set to zero, so that x_t=0 if t>1000. For each layer we then select the 10 units that decay the slowest after t>1000 and plotted them on Figure 4. The first layer retains information for about 10 time steps, whereas the second layer retains information for about 100 steps. Adding a third or fourth layer would then allow the architecture to retain information for even longer periods. We have not yet implemented a multi-layer network to handle tasks (other than language modeling) where such longer-term dependencies are needed, but we believe the main obstacle here is one of proper initialization and training rather than a shortcoming of the architecture itself.\n\nImportantly, this behavior (i.e. higher layers decay more slowly) can be explained analytically, see equation (11).\n\nOverall, we find it interesting that complexity and long-term dependencies can plausibly be obtained in a classical way (i.e. stacking layers) rather than relying on the intricate and hard to interpret dynamics of an LSTM.", "OTHER_KEYS": "Thomas Laurent"}, {"TITLE": "Is chaos bad?", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "", "ORIGINALITY": 5, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "DATE": "10 Dec 2016"}, {"OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "TITLE": "Long-term dependencies", "comments": "", "SOUNDNESS_CORRECTNESS": 5, "ORIGINALITY": 4, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "DATE": "08 Dec 2016"}, {"IMPACT": 4, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "TITLE": "Pre-review questions", "comments": "", "SOUNDNESS_CORRECTNESS": 4, "ORIGINALITY": 3, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "DATE": "06 Dec 2016"}, {"DATE": "07 Nov 2016", "TITLE": "Edge of chaos?", "IS_META_REVIEW": false, "comments": "I think it would be useful to discuss the concept of *edge* of chaos here (see e.g. Bertschinger, Nachschlager - Real-Time Computation at the Edge of Chaos in Recurrent Neural Networks), i.e. the hypothesis that RNNs are optimal (in some sense) at the boundary between chaotic and deterministic regimes. Specifically, it would be nice to see if your network gets closer to this edge during training (I think it will).\n\nIt wasn't clear to me if you studied the chaoticity in the case *with* input... the \"epsilon-activation\" thing seems very nonstandard. Why didn't you just compute the mean Lyapunov exponent? You can do that with or without input. I think you might find that the RNN with input will approach the edge of chaos during training (Lyapunov exp gets closer to zero, probably starting from negative values in your case).\n\nThe LSTM phase space diagram in Fig. 2 looks pretty bad... I think that particular unit is not behaving well at all. What you should get in properly trained models is something like in Fig. 1 (a), but more noisy because there's effects from the input.\n\nAnyway, overall a very interesting paper! I'm glad to see RNNs studied from a chaotic dynamics perspective.", "OTHER_KEYS": "Heikki Arponen"}, {"IS_META_REVIEW": true, "comments": "This paper poses an interesting idea: removing chaotic behavior or RNNs.\nWhile many other papers on new RNN architecture usually focus too much on the performance improvement and leave the analysis part on their success as a black-box, this paper does a good job on presenting why its method may work well.\n\nAlthough, the paper shows lots of comparison between the chaotic systems (GRUs & LSTMs) and the stable system (proposed CFN model), the reviewer is not fully convinced by the main claim of this paper, the nuance that chaotic behaviour makes dynamic system to have rich representation power but makes the system too unstable. In the paper, the LSTM shows a very sensitive behaviour, even when a very small amount of noise is added to the input. However, it still performs surprisingly well with this chaotic behaviour. \n\nMeasuring the model complexity is a very difficult task, therefore, many papers manage to use either same number of hidden units or choose approximately close model sizes. In this paper, the experiments were carried by using the same amount of parameters for both the LSTM and CFN. However, I think the CFN may have much more simpler computational graph. Taking the idea of this work, can we develop a stable dynamic system, but which does not only have one attractor?\n\nIt is also interesting to see that the layers of CFNs are updated in different timescales in a sense that the decaying speed decreases when the layer gets higher. Could you provide more statistics on this? For example, what is the average relaxation time of the whole hidden units at each layer?\n\nBatch normalization and layer normalization can be helpful to make the training of RNNs become more stable. How would the behaviour of batch normalized or perhaps layer normalized LSTM look like? Also, it is often not trivial to make batch normalization or layer normalization to work on a new architecture. I think it may be useful to compare batch normalized or layer normalized versions of the LSTM and CFN.\n\nThe quality of the work is good, explanation is clear enough along with nice analyses and proofs. Overall, the performance is not any better than LSTMs, but it is still interesting when thinking of simplicity of this model. I am a bit concerned if this model might not work that well in more harder task, e.g., translation. Figure 4 of this paper is very interesting, where the proposed architecture shows that the hidden units at the second layer tends to keep its information longer than the first layer ones."}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "The reviewers all enjoyed this paper and the analysis.\n \n pros:\n - novel new model\n - interesting insights into the design of model, through analysis of trajectories of hidden states of RNNs.\n \n cons:\n - results are worse than LSTMs.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"DATE": "31 Dec 2016", "TITLE": "Questions", "IS_META_REVIEW": false, "comments": "Thanks for a very interesting read.\n\nWhat happens if instead of driving the LSTMs with x_t = 0, you drive it with a fixed input, like the word \"What\"? Would that behave the same as in fig 3?\n\nIf you drive the LSTMs with some input and then fix x_t = 0 for t > T (as in fig 4), do you still see chaos? If there is gradual decay in the hidden units' activations, do you also see that the second layer forgets more slowly than the first?\n\nHave you tried training on the copy task as in the algorithmic learning literature (like NTM), to see whether there is a actual difference in how long memory is retained in CFN vs LSTM?", "OTHER_KEYS": "Greg Yang"}, {"OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "TITLE": "Nice investigation", "comments": "The authors of the paper set out to answer the question whether chaotic behaviour is a necessary ingredient for RNNs to perform well on some tasks. For that question's sake, they propose an architecture which is designed to not have chaos. The subsequent experiments validate the claim that chaos is not necessary.\n\nThis paper is refreshing. Instead of proposing another incremental improvement, the authors start out with a clear hypothesis and test it. This might set the base for future design principles of RNNs.\n\nThe only downside is that the experiments are only conducted on tasks which are known to be not that demanding from a dynamical systems perspective; it would have been nice if the authors had traversed the set of data sets more to find data where chaos is actually necessary.", "ORIGINALITY": 5, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "RECOMMENDATION": 8, "DATE": "20 Dec 2016", "REVIEWER_CONFIDENCE": 3}, {"IMPACT": 4, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "TITLE": "interesting starting point", "comments": "I think the authors provide an interesting direction for understanding and maybe constructing recurrent models that are easier to interpret. Is not clear where such direction will lead but I think it could be an interesting starting point for future work, one that worth exploring. ", "SOUNDNESS_CORRECTNESS": 4, "ORIGINALITY": 3, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "18 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "TITLE": "Cool paper", "comments": "This paper poses an interesting idea: removing chaotic behavior or RNNs.\nWhile many other papers on new RNN architecture usually focus too much on the performance improvement and leave the analysis part on their success as a black-box, this paper does a good job on presenting why its method may work well.\n\nAlthough, the paper shows lots of comparison between the chaotic systems (GRUs & LSTMs) and the stable system (proposed CFN model), the reviewer is not fully convinced by the main claim of this paper, the nuance that chaotic behaviour makes dynamic system to have rich representation power but makes the system too unstable. In the paper, the LSTM shows a very sensitive behaviour, even when a very small amount of noise is added to the input. However, it still performs surprisingly well with this chaotic behaviour. \n\nMeasuring the model complexity is a very difficult task, therefore, many papers manage to use either same number of hidden units or choose approximately close model sizes. In this paper, the experiments were carried by using the same amount of parameters for both the LSTM and CFN. However, I think the CFN may have much more simpler computational graph. Taking the idea of this work, can we develop a stable dynamic system, but which does not only have one attractor?\n\nIt is also interesting to see that the layers of CFNs are updated in different timescales in a sense that the decaying speed decreases when the layer gets higher. Could you provide more statistics on this? For example, what is the average relaxation time of the whole hidden units at each layer?\n\nBatch normalization and layer normalization can be helpful to make the training of RNNs become more stable. How would the behaviour of batch normalized or perhaps layer normalized LSTM look like? Also, it is often not trivial to make batch normalization or layer normalization to work on a new architecture. I think it may be useful to compare batch normalized or layer normalized versions of the LSTM and CFN.\n\nThe quality of the work is good, explanation is clear enough along with nice analyses and proofs. Overall, the performance is not any better than LSTMs, but it is still interesting when thinking of simplicity of this model. I am a bit concerned if this model might not work that well in more harder task, e.g., translation. Figure 4 of this paper is very interesting, where the proposed architecture shows that the hidden units at the second layer tends to keep its information longer than the first layer ones.", "SOUNDNESS_CORRECTNESS": 5, "ORIGINALITY": 4, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "17 Dec 2016 (modified: 18 Dec 2016)", "REVIEWER_CONFIDENCE": 4}, {"DATE": "16 Dec 2016", "TITLE": "Authors\u2019 comment: Conclusion added", "IS_META_REVIEW": false, "comments": "We added a short conclusion reflecting some of the discussions with the reviewers. ", "OTHER_KEYS": "Thomas Laurent"}, {"DATE": "12 Dec 2016", "TITLE": "Authors\u2019 comment: Added experiments on long term dependencies.", "IS_META_REVIEW": false, "comments": "Several reviewers have posted comments asking about the capability of the proposed model to capture long-term dependencies. This is a natural question since the model was designed so that units get activated when presented the correct feature, then relax to zero at a rate controlled by the forget gate. At a first glance it is unclear that such a simple mechanism could capture long term dependencies (the relaxing rates might be too fast).\n\nWe added a simple experiment in the paper showing that long term dependencies can be obtained by stacking multiple layers of the basic architecture (see Figure 4). We took a 2-layer, 224-unit CFN network trained on Penn Treebank and ran it with the following input data: The first 1000 inputs x_t are the first 1000 words of the test set of PTB; All subsequent inputs are set to zero, so that x_t=0 if t>1000. For each layer we then select the 10 units that decay the slowest after t>1000 and plotted them on Figure 4. The first layer retains information for about 10 time steps, whereas the second layer retains information for about 100 steps. Adding a third or fourth layer would then allow the architecture to retain information for even longer periods. We have not yet implemented a multi-layer network to handle tasks (other than language modeling) where such longer-term dependencies are needed, but we believe the main obstacle here is one of proper initialization and training rather than a shortcoming of the architecture itself.\n\nImportantly, this behavior (i.e. higher layers decay more slowly) can be explained analytically, see equation (11).\n\nOverall, we find it interesting that complexity and long-term dependencies can plausibly be obtained in a classical way (i.e. stacking layers) rather than relying on the intricate and hard to interpret dynamics of an LSTM.", "OTHER_KEYS": "Thomas Laurent"}, {"TITLE": "Is chaos bad?", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "", "ORIGINALITY": 5, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "DATE": "10 Dec 2016"}, {"OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "TITLE": "Long-term dependencies", "comments": "", "SOUNDNESS_CORRECTNESS": 5, "ORIGINALITY": 4, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "DATE": "08 Dec 2016"}, {"IMPACT": 4, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "TITLE": "Pre-review questions", "comments": "", "SOUNDNESS_CORRECTNESS": 4, "ORIGINALITY": 3, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "DATE": "06 Dec 2016"}, {"DATE": "07 Nov 2016", "TITLE": "Edge of chaos?", "IS_META_REVIEW": false, "comments": "I think it would be useful to discuss the concept of *edge* of chaos here (see e.g. Bertschinger, Nachschlager - Real-Time Computation at the Edge of Chaos in Recurrent Neural Networks), i.e. the hypothesis that RNNs are optimal (in some sense) at the boundary between chaotic and deterministic regimes. Specifically, it would be nice to see if your network gets closer to this edge during training (I think it will).\n\nIt wasn't clear to me if you studied the chaoticity in the case *with* input... the \"epsilon-activation\" thing seems very nonstandard. Why didn't you just compute the mean Lyapunov exponent? You can do that with or without input. I think you might find that the RNN with input will approach the edge of chaos during training (Lyapunov exp gets closer to zero, probably starting from negative values in your case).\n\nThe LSTM phase space diagram in Fig. 2 looks pretty bad... I think that particular unit is not behaving well at all. What you should get in properly trained models is something like in Fig. 1 (a), but more noisy because there's effects from the input.\n\nAnyway, overall a very interesting paper! I'm glad to see RNNs studied from a chaotic dynamics perspective.", "OTHER_KEYS": "Heikki Arponen"}]}
{"text": "Trusting SVM for Piecewise Linear CNNs\n1 Introduction\nThe backpropagation algorithm is commonly employed to estimate the parameters of a convolutional neural network (CNN) using a supervised training data set (Rumelhart et al., 1986). Part of the appeal of backpropagation comes from the fact that it is applicable to a wide variety of networks, namely those that have (sub-)differentiable non-linearities and employ a (sub-)differentiable learning objective. However, the generality of backpropagation comes at the cost of a high sensitivity to its hyperparameters such as the learning rate and momentum. Standard line-search algorithms cannot be used on the primal objective function in this setting, as (i) there may not exist a step-size guaranteeing a monotonic decrease because of the use of sub-gradients, and (ii) even in the smooth case, each function evaluation requires a forward pass over the entire data set without any update, making the approach computationally unfeasible. Choosing the learning rate thus remains an open issue, with the state-of-the-art algorithms suggesting adaptive learning rates (Duchi et al., 2011; Zeiler, 2012; Kingma & Ba, 2015). In addition, techniques such as batch normalization (Ioffe & Szegedy, 2015) and dropout (Srivastava et al., 2014) have been introduced to respectively reduce the sensitivity to the learning rate and to prevent from overfitting.\nWith this work, we open a different line of inquiry, namely, is it possible to design more robust optimization algorithms for special but useful classes of CNNs? To this end, we focus on the networks that are commonly used in computer vision. Specifically, we consider CNNs with convolutional and dense layers that apply a set of piecewise linear (PL) non-linear operations to obtain a discriminative representation of an input image. While this assumption may sound restrictive at first, we show that commonly used non-linear operations such as ReLU and max-pool fall under the category of PL functions. The representation obtained in this way is used to classify the image via a multi-class SVM, which forms the final layer of the network. We refer to this class of networks as PL-CNN.\nWe design a novel, principled algorithm to optimize the learning objective of a PL-CNN. Our algorithm is a layerwise method, that is, it iteratively updates the parameters of one layer while keeping the other layers fixed. For this work, we use a simple schedule over the\nlayers, namely, repeated passes from the output layer to the input one. However, it may be possible to further improve the accuracy and efficiency of our algorithm by designing more sophisticated scheduling strategies. The key observation of our approach is that the parameter estimation of one layer of PL-CNN can be formulated as a difference-of-convex (DC) program that can be viewed as a latent structured SVM problem (Yu & Joachims, 2009). This allows us to solve the DC program using the concave-convex procedure (CCCP) (Yuille & Rangarajan, 2002). Each iteration of CCCP requires us to solve a convex structured SVM problem. To this end, we use the powerful block-coordinate Frank-Wolfe (BCFW) algorithm (Lacoste-Julien et al., 2013), which solves the dual of the convex program iteratively by computing the conditional gradients corresponding to a subset of training samples. In order to further improve BCFW for PL-CNNs, we extend it in three important ways. First, we introduce a trust-region term that allows us to initialize the BCFW algorithm using the current estimate of the layer parameters. Second, we reduce the memory requirement of BCFW by an order of magnitude, via an efficient representation of the feature vectors corresponding to the dense layers. Third, we show that, empirically, the number of constraints of the structural SVM problem can be reduced substantially without any loss in accuracy, which allows us to significantly reduce its time complexity.\nCompared to backpropagation (Rumelhart et al., 1986) or its variants (Duchi et al., 2011; Zeiler, 2012; Kingma & Ba, 2015), our algorithm offers three advantages. First, the CCCP algorithm provides a monotonic decrease in the learning objective at each layer. Since layerwise optimization itself can be viewed as a block-coordinate method, our algorithm guarantees a monotonic decrease of the overall objective function after each layer\u2019s parameters have been updated. Second, since the dual of the SVM problem is a smooth convex quadratic program, each step of the BCFW algorithm (in the inner iteration of the CCCP) provides a monotonic increase in its dual objective. Third, since the only step-size required in our approach comes while solving the SVM dual, we can use the optimal step-size that is computed analytically during each iteration of BCFW (Lacoste-Julien et al., 2013). In other words, our algorithm has no learning rate, initial or not, that requires tuning.\nUsing standard network architectures and publicly available data sets, we show that our algorithm provides a boost over the state of the art variants of backpropagation for learning PL-CNNs and we demonstrate scalability of the method.\n2 Related Work\nWhile some of the early successful approaches for the optimization of deep neural networks relied on greedy layer-wise training (Hinton et al., 2006; Bengio et al., 2007), most currently used methods are variants of backpropagation (Rumelhart et al., 1986) with adaptive learning rates, as discussed in the introduction.\nAt every iteration, backpropagation performs a forward pass and a backward pass on the network, and updates the parameters of each layer by stochastic or mini-batch gradient descent. This makes the choice of the learning rate critical for efficient optimization. Duchi et al. (2011) have proposed the Adagrad convex solver, which adapts the learning rate for every direction and takes into account past updates. Adagrad changes the learning rate to favor steps in gradient directions that have not been observed frequently in past updates. When applied to the non-convex CNN optimization problem, Adagrad may converge prematurely due to a rapid decrease in the learning rate (Goodfellow et al., 2016). In order to prevent this behavior, the Adadelta algorithm (Zeiler, 2012) makes the decay of the learning rate slower. It is worth noting that this fix is empirical, and to the best of our knowledge, provides no theoretical guarantees. Kingma & Ba (2015) propose a different scheme for the learning rate, called Adam, which uses an online estimation of the first and second moments of the gradients to provide centered and normalized updates. However all these methods still require the tuning of the initial learning rate to perform well.\nSecond-order and natural gradient optimization methods have also been a subject of attention. The focus in this line of work has been to come up with appropriate approximations to make the updates cheaper. Martens & Sutskever (2012) suggested a Hessian-free second order optimization using finite differences to approximate the Hessian and conjugate gradient to\ncompute the update. Martens & Grosse (2015) derive an approximation of the Fisher matrix inverse, which provides a more efficient method for natural gradient descent. Ollivier (2013) explore a set of Riemannian methods based on natural gradient descent and quasi-Newton methods to guarantee reparametrization invariance of the problem. Desjardins et al. (2015) demonstrate a scaled up natural gradient descent method by training on the ImageNet data set (Russakovsky et al., 2015). Though providing more informative updates and solid theoretical support than SGD-based approaches, these methods do not take into account the structure of the problem offered by the commonly used non-linear operations.\nOur work is also related to some of the recent developments in optimization for deep learning. For example, Taylor et al. (2016) use ADMM for massive distribution of computation in a layer-wise fashion, and in particular their method will yield closed-form updates for any PLCNN. Lee et al. (2015) propose to use targets instead of gradients to propagate information through the network, which could help to extend our algorithm. Zhang et al. (2016) derive a convex relaxation for the learning objective for a restricted class of CNNs, which also relies on solving an approximate convex problem. In (Amos et al., 2016), the authors identify convex problems for the inference task, when the neural network is a convex function of some of its inputs.\nWith a more theoretical approach, Goel et al. (2016) propose an algorithm to learn shallow ReLU nets with guarantees of time convergence and generalization error. Heinemann et al. (2016) show that a subclass of neural networks can be modeled as an improper kernel, which then reduces the learning problem to a simple SVM with the constructed kernel.\nMore generally, we believe that our hitherto unknown observation regarding the relationship between PL-CNNs and latent SVMs can (i) allow the progress made in one field to be transferred to the other and (ii) help design a new generation of principled algorithms for deep learning optimization.\n3 Piecewise Linear Convolutional Neural Networks\nA piecewise linear convolutional neural network (PL-CNN) consists of a series of convolutional layers, followed by a series of dense layers, which provides a concise representation of an input image. Each layer of the network performs two operations: a linear transformation (that is, a convolution or a matrix multiplication), followed by a piecewise linear non-linear operation such as ReLU or max-pool. The resulting representation of the image is used for classification via an SVM. In the remainder of this section, we provide a formal description of PL-CNN.\nPiecewise Linear Functions. A piecewise linear (PL) function f(u) is a function of the following form (Melzer, 1986):\nf(u) = max i\u2208[m] {a>i u} \u2212max j\u2208[n] {b>j u}, (1)\nwhere [m] = {1, \u00b7 \u00b7 \u00b7 ,m}, and [n] = {1, \u00b7 \u00b7 \u00b7 , n}. Each of the two maxima above is a convex function, therefore such a function f is not generally convex, but it is rather a difference of two convex functions. Importantly, many commonly used non-linear operations such as ReLU or max-pool are PL functions of their input. For example, ReLU corresponds to the function R(v) = max{v, 0} where v is a scalar. Similarly, max-pool for a D-dimensional vector u corresponds to M(u) = maxi\u2208[D]{e>i u}, where ei is a vector whose i-th element is 1 and all other elements are 0. Given a value of u, we say that (i\u2217, j\u2217) is the activation of the PL function at u if i\u2217 = argmaxi\u2208[m]{a>i u} and j\u2217 = argmaxj\u2208[n]{b>j u}.\nPL-CNN Parameters. We denote the parameters of an L layer PL-CNN by W = {W l; l \u2208 [L]}. In other words, the parameters of the l-th layer is defined as W l. The CNN defines a composite function, that is, the output zl\u22121 of layer l\u2212 1 is the input to the layer l. Given the input zl\u22121 to layer l, the output is computed as zl = \u03c3l(W l \u00b7 zl\u22121), where \u201c\u00b7\u201d is either a convolution or a matrix multiplication, and \u03c3l is a PL non-linear function, such as ReLU or max-pool. The input to the first layer is an image x, that is, z0 = x. We denote\nthe input to the final layer by zL = \u03a6(x;W) \u2208 RD. In other words, given an image x, the convolutional and dense layers of a PL-CNN provide a D-dimensional representation of x to the final classification layer. The final layer of a PL-CNN is a C class SVM W svm, which specifies one parameter W svmy \u2208 RD for each class y \u2208 Y.\nPrediction. Given an image x, a PL-CNN predicts its class using the following rule:\ny\u2217 = argmax y\u2208Y W svmy \u03a6(x;W). (2)\nIn other words, the dot product of the D-dimensional representation of x with the SVM parameter for a class y provides the score for the class. The desired prediction is obtained by maximizing the score over all possible classes.\nLearning Objective. Given a training data set D = {(xi, yi), i \u2208 [N ]}, where xi is the input image and yi is its ground-truth class, we wish to estimate the parameters W \u222aW svm of the PL-CNN. To this end, we minimize a regularized upper bound on the empirical risk. The risk of a prediction y\u2217i given the ground-truth yi is measured with a user-specified loss function \u2206(y\u2217i , yi). For example, the standard 0\u2212 1 loss has a value of 0 for a correct prediction and 1 for an incorrect prediction. Formally, the parameters of a PL-CNN are estimated using the following learning objective:\nmin W,W svm\n\u03bb\n2 \u2211 l\u2208[L]\u222a{svm} \u2016W l\u20162F + 1 N N\u2211 i=1 max y\u0304i\u2208Y ( \u2206(y\u0304i, yi) + ( W svmy\u0304i \u2212W svm yi )T \u03a6(xi;W) ) . (3)\nThe hyperparameter \u03bb denotes the relative weight of the regularization compared to the upper bound of the empirical risk. Note that, due to the presence of piecewise linear non-linearities, the representation \u03a6(\u00b7;W) (and hence, the above objective) is highly non-convex in the PL-CNN parameters.\n4 Parameter Estimation for PL-CNN\nIn order to enable layerwise optimization of PL-CNNs, we show that parameter estimation of a layer can be formulated as a difference-of-convex (DC) program (subsection 4.1). This allows us to use the concave-convex procedure, which solves a series of convex optimization problems (subsection 4.2). We show that each convex problem closely resembles a structured SVM objective, which can be addressed by the powerful block-coordinate Frank-Wolfe (BCFW) algorithm. We extend BCFW to improve its initialization, time complexity and memory requirements, thereby enabling its use in learning PL-CNNs (subsection 4.3). For the sake of clarity, we only provide sketches of the proofs for those propositions that are necessary for understanding the paper. The detailed proofs of the remaining propositions are provided in the Appendix.\n4.1 Layerwise Optimization as a DC Program\nGiven the values of the parameters for the convolutional and the dense layers (that is, W), the learning objective (3) is the standard SVM problem in parameters W svm. In other words, it is a convex optimization problem with several efficient solvers (Tsochantaridis et al., 2004; Joachims et al., 2009; Shalev-Shwartz et al., 2009), including the BCFW algorithm (LacosteJulien et al., 2013). Hence, the optimization of the final layer is a computationally easy problem. In contrast, the optimization of the parameters of a convolutional or a dense layer l does not result in a convex program. In general, this problem can be arbitrarily hard to solve. However, in the case of PL-CNN, we show that the problem can be formulated as a specific type of DC program, which enables efficient optimization via the iterative use of BCFW. The key property that enables our approach is the following proposition that shows that the composition of PL functions is also a PL function.\nProposition 1. Consider PL functions g : Rm \u2192 R and gi : Rn \u2192 R, for all i \u2208 [m]. Define a function f : Rn \u2192 R as f(u) = g([g1(u), g2(u), \u00b7 \u00b7 \u00b7 , gm(u)]>). Then f is also a PL function (proof in Appendix A).\nUsing the above proposition, we can reformulate the problem of optimizing the parameters of one layer of the network as a DC program. Specifically, the following proposition shows that the problem can be formulated as a latent structured SVM objective (Yu & Joachims, 2009).\nProposition 2. The learning objective of a PL-CNN with respect to the parameters of the l-th layer can be specified as follows:\nmin W l\n\u03bb 2 \u2016W l\u20162F + 1 N N\u2211 i=1\nmax hi\u2208H y\u0304i\u2208Y\n( \u2206(y\u0304i, yi) + (W l)>\u03a8(xi, y\u0304i,hi) ) \u2212 max\nhi\u2208H\n( (W l)>\u03a8(xi, yi,hi) ) ,\n(4)\nfor an appropriate choice of the latent space H and joint feature vectors \u03a8(x, y,h) of the input x, the output y and the latent variables h. In other words, parameter estimation for the l-th layer corresponds to minimizing the sum of its Frobenius norm plus a PL function for each training sample.\nSketch of the Proof. For a given image x with the ground-truth class y, consider the input to the layer l, which we denote by zl\u22121. Since all the layers except the l-th one are fixed, the input zl\u22121 is a constant vector, which only depends on the image x (that is, its value does not depend on the variables W l). In other words, we can write zl\u22121 = \u03d5(x).\nGiven the input zl\u22121, all the elements of the output of the l-th layer, denoted by zl, are a PL function of W l since the layer performs a linear transformation of zl\u22121 according to the parameters W l, followed by an application of PL operations such as ReLU or max-pool. The vector zl is then fed to the (l + 1)-th layer. The output zl+1 of the (l + 1)-th layer is a vector whose elements are PL functions of zl. Therefore, by proposition (1), the elements of zl+1 are a PL function of W l. By applying the same argument until we reach the layer L, we can conclude that the representation \u03a6(x;W) is a PL function of W l. Next, consider the upper bound of the empirical risk, which is specified as follows:\nmax y\u0304\u2208Y\n( \u2206(y\u0304, y) + ( W svmy\u0304 \u2212W svmy )T \u03a6(x;W) ) . (5)\nOnce again, since W svm is fixed, the above upper bound can be interpreted as a PL function of \u03a6(x;W), and thus, by proposition (1), the upper bound is a PL function of W l. It only remains to observe that the learning objective (3) also contains the Frobenius norm of W l. Thus, it follows that the estimation of the parameters of layer l can be reformulated as minimizing the sum of its Frobenius norm and the PL upper bound of the empirical risk over all training samples, as shown in problem (4). Note that we have ignored the constants corresponding to the Frobenius norm of the parameters of all the fixed layers. This constitutes an existential proof of Proposition 2. In the next paragraph, we give an intuition about the feature vectors \u03a8(xi, y\u0304i,hi) and the latent space H.\nFeature Vectors & Latent Space. The exact form of the joint feature vectors depends on the explicit DC decomposition of the objective function. In Appendix B, we detail the practical computations and give an example: we construct two interleaved neural networks whose outputs define the convex and concave parts of the DC objective function. Given the explicit DC objective function, the feature vectors are given by a subgradient and can therefore be obtained by automatic differentiation.\nWe now give an intuition of what the latent space H represents. Consider an input image x and a corresponding latent variable h \u2208 H. The latent variable can be viewed as a set of variables hk, k \u2208 {l + 1, \u00b7 \u00b7 \u00b7 , L}. In other words, each subset hk of the latent variable corresponds to one of the layers of the network that follow the layer l. Intuitively, hk represents the choice of activation at layer k when going through the PL activation: for each neuron j of layer k, hkj takes value i if and only if the i-th piece of the piecewise linear activation is selected. For instance, i is the index of the selected input in the case of a max-pooling unit.\nNote that the latent space only depends on the layers that follow the current layer being optimized. This is due to the fact that the input zl\u22121 to the l-th layer is a constant vector\nthat does not depend on the value of W l. However, the activations of all subsequent layers following the l-th one depend on the value of the parameters W l. As a consequence, the greater the number of following layers, the greater the size of the latent space, and this growth happens to be exponential. However, as will be seen shortly, it is still possible to efficiently optimize problem (4) for all the layers of the network despite this exponential increase.\n4.2 Concave-Convex Procedure\nThe optimization problem (4) is a DC program in the parameters W l. This follows from the fact that the upper bound of the empirical risk is a PL function, and can therefore be expressed as the difference of two convex PL functions (Melzer, 1986). Furthermore, the Frobenius norm of W l is also a convex function of W l. This observation allows us to obtain an approximate solution of problem (4) using the iterative concave-convex procedure (CCCP) (Yuille & Rangarajan, 2002).\nAlgorithm 1 describes the main steps of CCCP. In step 3, we impute the best value of the latent variable corresponding to the ground-truth class yi for each training sample. This imputation corresponds to the linearization step of the CCCP. The selected latent variable corresponds to a choice of activations at each non-linear layer of the network, and therefore defines a path of activations to the ground truth. Next, in step 4, we update the parameters by solving a convex optimization problem. This convex problem amounts to finding the path of activations which minimizes the maximum margin violations given the path to the ground truth defined in step 3.\nThe CCCP algorithm has the desirable property of providing a monotonic decrease in the objective function at each iteration. In other words, the objective function value of problem (4) at W lt is greater than or equal to its value at W l t+1. Since layerwise optimization itself can be viewed as a block-coordinate algorithm for minimizing the learning objective (3), our overall algorithm provides guarantees of monotonic decrease until convergence. This is one of the main advantages of our approach compared to backpropagation and its variants, which fail to provide similar guarantees on the value of the objective function from one iteration to the next.\nAlgorithm 1 CCCP for parameter estimation of the l-th layer of the PL-CNN.\nRequire: Data set D = {(xi, yi), i \u2208 [N ]}, fixed parameters {W \u222a W svm}\\W l, initial estimate W l0.\n1: t = 0 2: repeat 3: For each sample (xi, yi), find the best latent variable value by solving the following\nproblem: h\u2217i = argmax\nh\u2208H (W lt ) >\u03a8(xi, yi,h). (6)\n4: Update the parameters by solving the following convex optimization problem:\nW lt+1 = argmin W l\n\u03bb 2 \u2016W l\u20162F + 1 N N\u2211 i=1\nmax y\u0304i\u2208Y hi\u2208H\n( \u2206(y\u0304i, yi) + (W l)>\u03a8(xi, y\u0304i,hi) ) \u2212\n( (W l)>\u03a8(xi, yi,h \u2217 i ) ) . (7)\n5: t = t+1 6: until Objective function of problem (4) cannot be improved beyond a specified tolerance.\nIn order to solve the convex program (7), which corresponds to a structured SVM problem, we make use of the powerful BCFW algorithm (Lacoste-Julien et al., 2013) that solves its dual via conditional gradients. This has two main advantages: (i) as the dual is a smooth quadratic program, each iteration of BCFW provides a monotonic increase in its\nobjective; and (ii) the optimal step-size at each iteration can be computed analytically. This is once again in stark contrast to backpropagation, where the estimation of the step-size is still an active area of research (Duchi et al., 2011; Zeiler, 2012; Kingma & Ba, 2015). As shown by Lacoste-Julien et al. (2013), given the current estimate of the parameters W l, the conditional gradient of the dual of program (7) with respect to a training sample (xi, yi) can be obtained by solving the following problem:\n(y\u0302i, h\u0302i) = argmax y\u0304\u2208Y,h\u2208H\n(W l)>\u03a8(xi, y\u0304,h) + \u2206(y\u0304, yi). (8)\nWe refer the interested reader to (Lacoste-Julien et al., 2013) for further details.\nThe overall efficiency of the CCCP algorithm relies on our ability to solve problems (6) and (8). At first glance, these problems may appear to be computationally intractable as the latent space H can be very large, especially for layers close to the input (of the order of millions of dimensions for a typical network). However, the following proposition shows that both the problems can be solved efficiently using the forward and backward passes that are employed in backpropagation.\nProposition 3. Given the current estimate W l of the parameters for the l-th layer, as well as the parameter values of all the other fixed layers, problems (6) and (8) can be solved\nusing a forward pass on the network. Furthermore, the joint feature vectors \u03a8(xi, y\u0302i, h\u0302i) and \u03a8(xi, yi,h \u2217 i ) can be computed using a backward pass on the network.\nSketch of the Proof. Recall that the latent space consists of the putative activations for each PL operation in the layers following the current one. Thus, intuitively, the maximization over the latent variables corresponds to finding the exact activations of all such PL operations. In other words, we need to identify the indices of the linear pieces that are used to compute the value of the PL function in the current state of the network. For a ReLU operation, this corresponds to estimating max{0, v}, where the input to the ReLU is a scalar v. Similarly, for a max-pool operation, this corresponds to estimating maxi{e>i u}, where u is the input vector to the max-pool. This is precisely the computation that the forward pass of backpropagation performs. Given the activations, the joint feature vector is the subgradient of the sample with respect to the current layer. Once again, this is precisely what is computed during the backward pass of the backpropagation algorithm.\nAn example is constructed in Appendix B to illustrate how to compute the feature vectors in practice.\n4.3 Improving the BCFW Algorithm\nAs the BCFW algorithm was originally designed to solve a structured SVM problem, it requires further extensions to be suitable for training a PL-CNN. In what follows, we present three such extensions that improve the initialization, memory requirements and time complexity of the BCFW algorithm respectively.\nTrust-Region for Initialization. The original BCFW algorithm starts with an initial parameter W l = 0 (that is, all the parameters are set to 0). The reason for this initialization is that it is possible to compute the dual variables that correspond to the 0 primal variable. However, since our algorithm visits each layer of the network several times, it would be desirable to initialize its parameters using its current value W tl . To this end, we introduce a trust-region in the constraints of problem (7), or equivalently, an `2 norm based proximal term in its objective function (Parikh & Boyd, 2014). The following proposition shows that this has the desired effect of initializing the BCFW algorithm close to the current parameter values.\nProposition 4. By adding a proximal term \u00b52 \u2016W l \u2212W lt\u20162F to the objective function in (7), we can compute a feasible dual solution whose corresponding primal solution is equal to \u00b5 \u03bb+\u00b5W l t . Furthermore, the addition of the proximal term still allows us to efficiently compute the conditional gradient using a forward-backward pass (proof in Appendix D).\nIn practice, we always choose a value of \u00b5 = 10\u03bb: this yields an initialization of ' 0.9W lt which does not significantly change the value of the objective function.\nEfficient Representation of Joint Feature Vectors. The BCFW algorithm requires us to store a linear combination of the feature vectors for each mini-batch. While this requirement is not too stringent for convolutional and multi-class SVM layers, where the dimensionality of the feature vectors is small, it becomes prohibitively expensive for dense layers. The following proposition prevents a blow-up in the memory requirements of BCFW.\nProposition 5. When optimizing dense layer l, if W l \u2208 Rp\u00d7q, we can store a representation of the joint feature vectors \u03a8(x, y,h) with vectors of size p in problems (6) and (7). This is in contrast to the na\u0308\u0131ve approach that requires them to be of size p\u00d7 q.\nSketch of the Proof. By Proposition (3), the feature vectors are subgradients of the hinge loss function, which we loosely denote by \u03b7 for this proof. Then by the chain rule: \u2202\u03b7 \u2202W l\n= \u2202\u03b7 \u2202zl \u2202zl \u2202W l = \u2202\u03b7 \u2202zl \u00b7 ( zl\u22121 )T . Noting that zl\u22121 \u2208 Rq is a forward pass up until layer l (independent of W l), we can store only \u2202\u03b7 \u2202zl \u2208 Rp and still reconstruct the full feature vector \u2202\u03b7 \u2202W l by a forward pass and an outer product.\nReducing the Number of Constraints. In order to reduce the amount of time required for the BCFW algorithm to converge, we use the structure of H to simplify problem (7) to a much simpler problem. Specifically, since H represents the activations of the network for a given sample, it has a natural decomposition over the layers: H = H1 \u00d7 ...\u00d7HL. We use this structure in the following observation.\nObservation 1. Problem (7) can be approximately solved by optimizing the dual problem on increasingly large search spaces. In other words, we start with constraints of Y, followed by Y \u00d7HL, then Y \u00d7HL \u00d7HL\u22121 and so on. The algorithm converges when the primal-dual gap is below tolerance.\nThe latent variables which are not optimized over are set to be the same as the ones selected for the ground truth. Experimentally, we observe that for convolutional layers (architectures in section 5), restricting the search space to Y yields a dual gap low enough to consider the problem has converged. This means that in practice for these layers, problem (7) can be solved by searching directions over the search space Y instead of the much larger Y \u00d7 H. The intuition is that the norm of the difference-of-convex decomposition grows with the number of activations selected differently in the convex and concave parts (see Appendix A for the decomposition of piecewise linear functions). This compels the path of activations to be the same in the convex and the concave part to avoid large margin violations, especially for convolutional layers which are followed by numerous non-linearities at the max-pooling layers.\n5 Experiments\nOur experiments are designed to assess the ability of LW-SVM (Layer-Wise SVM, our method) and the SGD baselines to optimize problem (3). To compare LW-SVM with the state-of-the-art variants of backpropagation, we look at the training and testing accuracies as well as the training objective value. Unlike dropout, which effectively learns an ensemble model, we learn a single model using each baseline optimization algorithm. All experiments are conducted on a GPU (Nvidia Titan X) and use Theano (Bergstra et al., 2010; Bastien et al., 2012). We compare LW-SVM with Adagrad, Adadelta and Adam. For all data sets, we start at a good solution provided by these solvers and fine-tune it with LW-SVM. We then check whether a longer run of the SGD solver reaches the same level of performance.\nThe practical use of the LW-SVM algorithm needs choices at the three following levels: how to select the layer to optimize (i), when to stop the CCCP on each layer (ii) and when to stop the convex optimization at each inner iteration of the CCCP (iii). These choices are detailed in the next paragraph.\nThe layer-wise schedule of LW-SVM is as follows: as long as the validation accuracy increases, we perform passes from the end of the network (SVM) to the first layer (i). At each pass, each layer is optimized with one outer iteration of the CCCP (ii). The inner iterations are stopped when the dual objective function does not increase by more than 1% over an epoch (iii). We point out that the dual objective function is cheap to compute since we are maintaining its value at all time. By contrast, to compute the exact primal objective function requires a forward pass over the data set without any update.\n5.1 MNIST Data Set\nData set & Architecture The training data set consists in 60,000 gray scale images of size 28\u00d7 28 with 10 classes, which we split into 50,000 samples for training and 10,000 for validating. The images are normalized, and we do not use any data augmentation. The architecture used for this experiment is shown in Figure 1.\nMethod The number of epochs is set to 200, 100 and 100 for Adagrad, Adadelta and Adam - Adagrad is given more epochs as we observed it took a longer time to converge. We then use LW-SVM and compare the results on training objective, training accuracy and testing accuracy. We also let the solvers run to up to 500 epochs to verify that we have not stopped the optimization prematurely. The regularization hyperparameter \u03bb and the initial learning rate are chosen by cross-validation. \u03bb is set to 0.001 for all solvers, and the initial learning rates can be found in Appendix C. For LW-SVM, \u03bb is set to the same value as the baseline, and the proximal term \u00b5 to \u00b5 = 10\u03bb = 0.01.\nResults As Table 1 shows, LW-SVM systematically improves on all training objective, training accuracy and testing accuracy. In particular, it obtains the best testing accuracy when combined with Adadelta. Because each convex sub-problem is run up to sufficient convergence, the objective function of LW-SVM features of monotonic decrease at each iteration of the CCCP (blue curves in first row of Figure 2).\n5.2 CIFAR Data Sets\nData sets & Architectures The CIFAR-10/100 data sets are comprised of 60,000 RGB natural images of size 32\u00d7 32 with 10/100 classes (Krizhevsky, 2009)). We split the training set into 45,000 training samples and 5,000 validation samples in both cases. The images are centered and normalized, and we do not use any data augmentation. To obtain a strong enough baseline, we employ (i) a pre-training with a softmax and cross-entropy loss and (ii) Batch-Normalization (BN) layers before each non-linearity.\nWe have experimentally found out that pre-training with a softmax layer followed by a cross-entropy loss led to better behavior and results than using an SVM loss alone. The baselines are trained with batch normalization. Once they have converged, the estimated mean and standard deviation are fixed like they would be at test time. Then batch normalization becomes a linear transformation, which can be handled by the LW-SVM algorithm. This allows us to compare LW-SVM with a baseline benefiting from batch normalization. Specifically, we use the architecture shown in Figure 3:\nMethod Again, the initial learning rates and regularization weight \u03bb are obtained by cross-validation, and a value of 0.001 is obtained for \u03bb for all solvers on both datasets. As before, \u00b5 is set to 10\u03bb. The initial learning rates are reported in Appendix C. The layer schedule and convergence criteria are as described at the beginning of the section. For each SGD optimizer, we train the network for 10 epochs with a cross-entropy loss (preceded by a softmax layer). Then it is trained with an SVM loss (without softmax) for respectively 1000, 100 and 100 epochs for Adagrad, Adadelta and Adam. This amount is doubled to verify that the baselines are not harmed by a premature stopping. Results are presented in Tables 2 and 3.\nResults It can be seen from this set of results that LW-SVM always improves over the solution of the SGD algorithm, for example on CIFAR-100, decreasing the objective value of Adam from 0.22 to 0.06, or improving the test accuracy of Adadelta from 84.4% to 86.6% on\nCIFAR-10. The automatic step-size allows for a precise fine-tuning to optimize the training objective, while the regularization of the proximal term helps for better generalization.\n5.3 ImageNet Data Set\nNetwork Top-1 Accuracy Top-5 Accuracy VGG-16 (PT) 73.30% 91.33% VGG-16 (PT + LW-SVM) 73.81% 91.61%\nSince the objective function penalizes the top-1 error, it is logical to observe that the improvement is most important on the top-1 accuracy. Importantly, having an efficient representation of feature vectors proves to be essential for such large networks: for instance, in the optimization of the first fully connected layer with a batch-size of 100, the use of our representation lowers the memory requirements of the BCFW algorithm from 7,600GB to 20GB, which can then fit in the memory of a powerful computer.\n6 Discussion\nWe presented a novel layerwise optimization algorithm for a large and useful class of convolutional neural networks, which we term PL-CNNs. Our key observation is that the optimization of the parameters of one layer of a PL-CNN is equivalent to solving a latent structured SVM problem. As the problem is a DC program, it naturally lends itself to the iterative CCCP approach, which optimizes a convex structured SVM objective at each iteration. This allows us to leverage the advancements made in structured SVM optimization over the past decade to design a computationally feasible approach for learning PL-CNNs. Specifically, we use the BCFW algorithm and extend it to improve its initialization, memory requirements and time complexity. In particular, this allows our method to not require the tuning of any learning rate. Using the publicly available MNIST, CIFAR-10 and CIFAR-100 data sets, we show that our approach provides a boost for learning PL-CNNs over the state of the art backpropagation algorithms. Furthermore, we demonstrate scalability of the method with results on the ImageNet data set with a large network.\nWhen the mean and standard deviation estimations of batch normalization are not fixed (unlike in our experiments with LW-SVM), batch normalization is not a piecewise linear transformation, and therefore cannot be used in conjunction with the BCFW algorithm for SVMs. However, it is difference-of-convex as it is a C2 function (Horst & Thoai, 1999). Incorporating a normalization scheme into our framework will be the object of future work. With our current methodology, LW-SVM algorithm can already be used on most standard architectures like VGG, Inception and ResNet-type architectures.\nIt is worth noting that other approaches for solving structured SVM problems, such as cutting-plane algorithms (Tsochantaridis et al., 2004; Joachims et al., 2009) and stochastic subgradient descent (Shalev-Shwartz et al., 2009), also rely on the efficiency of estimating the conditional gradient of the dual. Hence, all these methods are equally applicable to our setting. Indeed, the main strength of our approach is the establishment of a hitherto unknown connection between CNNs and latent structured SVMs. We believe that our observation will allow researchers to transfer the substantial existing knowledge of DC programs in general, and latent SVMs specifically, to produce the next generation of principled optimization algorithms for deep learning. In fact, there are already several such improvements that can be readily applied in our setting, which were not explored only due to a lack of time. This includes multi-plane variants of BCFW (Shah et al., 2015; Osokin et al., 2016), as well as generalizations of Frank-Wolfe such as partial linearization (Mohapatra et al., 2016).\nAcknowledgments\nThis work was supported by the EPSRC AIMS CDT grant EP/L015987/1, the EPSRC Programme Grant Seebibyte EP/M013774/1 and Yougov. Many thanks to A. Desmaison, R. Bunel and D. Bouchacourt for the helpful discussions.\nA Piecewise Linear Functions\nProof of Proposition (1) By the definition from (Melzer, 1986), we can write each function as the difference of two point-wise maxima of linear functions:\ng(v) = max j\u2208[m+] {a>i v} \u2212 max k\u2208[m\u2212] {b>j v} And \u2200i \u2208 [n], gi(u) = g+i (u)\u2212 g \u2212 i (u)\nWhere all the g+i , g \u2212 i are linear point-wise maxima of linear functions. Then:\nf(u) = g([g1(u), \u00b7 \u00b7 \u00b7 , gn(u)]>) = max j\u2208[m+] {a>j [g1(u), \u00b7 \u00b7 \u00b7 , gn(u)]>} \u2212 max k\u2208[m\u2212] {b>k [g1(u), \u00b7 \u00b7 \u00b7 , gn(u)]>}\n= max j\u2208[m+] { n\u2211 i=1 aj,igi(u) } \u2212 max k\u2208[m\u2212] { n\u2211 i=1 bk,igi(u) }\n= max j\u2208[m+] { n\u2211 i=1 aj,ig + i (u)\u2212 n\u2211 i=1 aj,ig \u2212 i (u) } \u2212 max k\u2208[m\u2212] { n\u2211 i=1 bk,ig + i (u)\u2212 n\u2211 i=1 bk,ig \u2212 i (u) }\n= max j\u2208[m+]  n\u2211 i=1 aj,ig + i (u) + \u2211 j\u2032\u2208[m+]\\{j} n\u2211 i=1 aj,ig \u2212 i (u) \u2212 \u2211 j\u2032\u2208[m+] n\u2211 i=1 aj,ig \u2212 i (u)\n\u2212 max k\u2208[m\u2212]  n\u2211 i=1 bk,ig + i (u) + \u2211 k\u2032\u2208[m\u2212]\\{k} n\u2211 i=1 bk,ig \u2212 i (u) + \u2211 k\u2032\u2208[m\u2212] n\u2211 i=1 bk,ig \u2212 i (u)\n= max j\u2208[m+]  n\u2211 i=1 aj,ig + i (u) + \u2211 j\u2032\u2208[m+]\\{j} n\u2211 i=1 aj,ig \u2212 i (u) + \u2211 k\u2032\u2208[m\u2212] n\u2211 i=1 bk,ig \u2212 i (u)\n\u2212  max k\u2208[m\u2212]  n\u2211 i=1 bk,ig + i (u) + \u2211 k\u2032\u2208[m\u2212]\\{k} n\u2211 i=1 bk,ig \u2212 i (u) + \u2211 j\u2032\u2208[m+] n\u2211 i=1 aj,ig \u2212 i (u)  = max j\u2208[m+]  n\u2211 i=1 aj,ig + i (u) + \u2211 j\u2032\u2208[m+]\\{j} n\u2211 i=1 aj,ig \u2212 i (u) + \u2211 k\u2032\u2208[m\u2212] n\u2211 i=1 bk,ig \u2212 i (u)\n \u2212 max k\u2208[m\u2212]  n\u2211 i=1 bk,ig + i (u) + \u2211 k\u2032\u2208[m\u2212]\\{k} n\u2211 i=1 bk,ig \u2212 i (u) + \u2211 j\u2032\u2208[m+] n\u2211 i=1 aj,ig \u2212 i (u)\n In each line of the last equality, we recognize a pointwise maximum of a linear combination of pointwise maxima of linear functions. This constitutes a pointwise maximum of linear functions.\nThis derivation also extends equation (10) to the multi-dimensional case by showing an explicit DC decomposition of the output.\nB Computing the Feature Vectors\nWe describe here how to compute the feature vectors in practice. To this end, we show how to construct two (intertwined) neural networks that decompose the objective function into a convex and a concave part. We call these Difference of Convex (DC) networks. Once the DC networks are defined, a standard forward and backward pass in the two networks yields the feature vectors for the convex and concave contribution to the objective function. First, we derive how to perform a DC decomposition in linear and non-linear layers, and then we construct an example of DC networks.\nDC Decomposition in a Linear Layer Let W be the weights of a fixed linear layer. We introduce W+ = 12 (|W |+W ) and W \u2212 = 12 (|W | \u2212W ). We can note that W + and W\u2212 have exclusively non-negative weights, and that W = W+ \u2212W\u2212. Say we have an input u with the DC decomposition (ucvx, uccv), that is: u = ucvx \u2212 uccv, where both ucvx and uccv are convex. Then we can decompose the output of the layer as:\nW \u00b7 u = (W+ \u00b7 ucvx +W\u2212 \u00b7 uccv)\ufe38 \ufe37\ufe37 \ufe38 convex \u2212 (W\u2212 \u00b7 ucvx +W+ \u00b7 uccv)\ufe38 \ufe37\ufe37 \ufe38 convex\n(9)\nDC Decomposition in a Piecewise Linear Activation Layer For simplicity purposes, we consider that the non-linear layer is a point-wise maximum across [K] scalar inputs, that is, for an input (uk)k\u2208[K] \u2208 RK , the output is maxk\u2208[K] uk (the general multi-dimensional case can be found in Appendix A). We suppose that we have a DC decomposition (ucvxk , u ccv k ) for each input k. Then we can write the following decomposition for the output of the layer:\nmax k\u2208[K] uk = max k\u2208[K]\n(ucvxk \u2212 uccvk )\n= max k\u2208[K] ucvxk + \u2211 i\u2208[K],i6=k uccvi  \ufe38 \ufe37\ufe37 \ufe38\nconvex\n\u2212 \u2211 k\u2208[K]\nuccvk\ufe38 \ufe37\ufe37 \ufe38 convex\n(10)\nIn particular, for a ReLU, we can write:\nmax(ucvx \u2212 uccv, 0) = max(ucvx, uccv)\ufe38 \ufe37\ufe37 \ufe38 convex \u2212 uccv\ufe38\ufe37\ufe37\ufe38 convex\n(11)\nAnd for a Max-Pooling layer, one can easily verify that equation (10) is equivalent to:\nMaxPool(ucvx \u2212 uccv) = MaxPool(ucvx \u2212 uccv) + SumPool(uccv)\ufe38 \ufe37\ufe37 \ufe38 convex \u2212SumPool(uccv)\ufe38 \ufe37\ufe37 \ufe38 convex (12)\nAn Example of DC Networks We use the previous observations to obtain a DC decomposition in any layer. We now take the example of the neural network used for the experiments on the MNIST data set, and we show how to construct the two neural networks when optimizing W 1, the weights of the first convolutional layer. First let us recall the architecture without decomposition:\nWe want to optimize the first convolutional layer, therefore we fix all other parameters. Then we apply all operations as described in the previous paragraphs, which yields the DC networks in Figure 7.\nThe network graph in Figure 7 illustrates Proposition 3 for the optimization of W 1: suppose we are interested in f cvx(x,W 1), the convex part of the objective function for a given sample x, and we wish to obtain the feature vector needed to perform an update of BCFW. With a\nforward pass, the oracle for the latent and label variables (h\u0302, y\u0302) is efficiently computed; and\nwith a backward pass, we obtain the corresponding feature vector \u03a8(x, y\u0302, h\u0302). Indeed, we recall from problem (8) that (h\u0302, y\u0302) are the latent and label variables maximizing f cvx(x,W 1). Then given x, the forward pass in the DC networks sequentially solves the nested maximization: it maximizes the activation of the ReLU and MaxPooling units at each layer, thereby selecting\nthe best latent variable h\u0302 at each non-linear layer, and maximizes the output of the SVM\nConcave Network Convex Network\nNon-Decomposed Corresponding\nNetwork\nlayer, thereby selecting the best label y\u0302. At the end of the forward pass, f cvx(x,W 1) is\ntherefore available as the output of the convex network, and the feature vector \u03a8(x, y\u0302, h\u0302) can be computed as a subgradient of f cvx(x,W 1) with respect to W 1.\nLinearizing the concave part is equivalent to fixing the activations of the DC networks, which can be done by using a fixed copy of W 1 at the linearization point (all other weights being fixed anyway). Then one can re-use the above reasoning to obtain the feature vectors for the linearized concave part. Altogether, this methodology allows our algorithm to be implemented in any standard deep learning library (our implementation is available at http://github.com/oval-group/pl-cnn).\nC Experimental Details\nHyper-parameters The hyper-parameters are obtained by cross-validation with a search on powers of 10. In this section, \u03b7 will denote the initial learning rate. We denote the Softmax + Cross-Entropy loss by SCE, while SVM stands for the usual Support Vector Machines loss.\nOne may note that the hyper-parameters are the same for both CIFAR-10 and CIFAR-100 for each combination of solver and loss. This makes sense since the initial learning rate mainly depends on the architecture of the network (and not so much on which particular images are fed to this network), which is very similar for the experiments on the CIFAR-10 and CIFAR-100 data sets.\nD SVM Formulation & Dual Derivation\nMulti-Class SVM Suppose we are given a data set of N samples, for which every sample i has a feature vector \u03c6i \u2208 Rd and a ground truth label yi \u2208 Y. For every possible label y\u0304i \u2208 Y, we introduce the augmented feature vector \u03c8i(y\u0304i) \u2208 R|Y|\u00d7d containing \u03c6i at index y\u0304i, \u2212\u03c6i at index yi, and zeros everywhere else (then \u03c8i(yi) is just a vector of zeros). We also define \u2206(y\u0304i, yi) as the loss by choosing the output y\u0304i instead of the ground truth yi in our task. For classification, this is the zero-one loss for example.\nThe SVM optimization problem is formulated as:\nmin w,\u03bei\n\u03bb 2 \u2016w\u20162 + 1 N N\u2211 i=1 \u03bei\nsubject to: \u2200i \u2208 [N ], \u2200y\u0304i \u2208 Y, \u03bei \u2265 wT\u03c8i(y\u0304i) + \u2206(yi, y\u0304i)\nWhere \u03bb is the regularization hyperparameter. We now add a proximal term to a given starting point w0:\nmin w,\u03bei\n\u03bb 2 \u2016w\u20162 + \u00b5 2 \u2016w \u2212 w0\u20162 + 1 N N\u2211 i=1 \u03bei\nsubject to: \u2200i \u2208 [N ], \u2200y\u0304i \u2208 Y, \u03bei \u2265 wT\u03c8i(y\u0304i) + \u2206(yi, y\u0304i)\nFactorizing the second-order polynomial in w, we obtain the equivalent problem (changed by a constant):\nmin w,\u03bei\n\u03bb+ \u00b5 2 \u2016w \u2212 \u00b5 \u03bb+ \u00b5 w0\u20162 + 1 N N\u2211 i=1 \u03bei\nsubject to: \u2200i \u2208 [N ], \u2200y\u0304i \u2208 Y, \u03bei \u2265 wT\u03c8i(y\u0304i) + \u2206(yi, y\u0304i)\nFor simplicity, we introduce the ratio \u03c1 = \u00b5\n\u03bb+ \u00b5 .\nDual Objective function The primal problem is:\nmin w,\u03bei\n\u03bb+ \u00b5\n2 \u2016w \u2212 \u03c1w0\u20162 +\n1\nN N\u2211 i=1 \u03bei\nsubject to: \u2200i \u2208 [N ], \u2200y\u0304i \u2208 Y, \u03bei \u2265 wT\u03c8i(y\u0304i) + \u2206(yi, y\u0304i)\nThe dual problem can be written as:\nmax \u03b1\u22650 min w,\u03bei\n\u03bb+ \u00b5\n2 \u2016w \u2212 \u03c1w0\u20162 +\n1\nN N\u2211 i=1 \u03bei + 1 N N\u2211 i=1 \u2211 y\u0304i\u2208Y \u03b1i(y\u0304i) ( \u2206(yi, y\u0304i) + w T\u03c8i(y\u0304i)\u2212 \u03bei )\nThen we obtain the following KKT conditions:\n\u2200i \u2208 [N ], \u2202\u00b7 \u2202\u03bei = 0 \u2212\u2192 \u2211 y\u0304i\u2208Y \u03b1i(y\u0304i) = 1\n\u2202\u00b7 \u2202w = 0 \u2212\u2192 w = \u03c1w0 \u2212 1 N 1 \u03bb+ \u00b5 N\u2211 i=1 \u2211 y\u0304i\u2208Y\n\u03b1i(y\u0304i)\u03c8i(y\u0304i)\ufe38 \ufe37\ufe37 \ufe38 A\u03b1\nWe also introduce b = 1N (\u2206(yi, y\u0304i))i,y\u0304i . We define Pn(Y) as the sample-wise probability simplex:\nu \u2208 Pn(Y) if: \u2200i \u2208 [N ], \u2200y\u0304i \u2208 Y, ui(y\u0304i) \u2265 0 \u2200i \u2208 [N ], \u2211 y\u0304i\u2208Y ui(y\u0304i) = 1\nWe inject back and simplify to:\nmax \u03b1\u2208Pn(Y) \u2212(\u03bb+ \u00b5) 2 \u2016A\u03b1\u20162 + \u00b5wT0 (A\u03b1) + \u03b1T b\nFinally:\nmin \u03b1\u2208Pn(Y) f(\u03b1)\nWhere:\nf(\u03b1) , \u03bb+ \u00b5\n2 \u2016A\u03b1\u20162 \u2212 \u00b5wT0 (A\u03b1)\u2212 \u03b1T b\nBCFW derivation We write \u2207(i)f the gradient of f w.r.t. the block (i) of variables in \u03b1, padded with zeros on blocks (j) for j 6= i. Similarly, A(i) and b(i) contain the rows of A and the elements of b for the block of coordinates (i) and zeros elsewhere. We can write:\n\u2207(i)f(\u03b1) = (\u03bb+ \u00b5)AT(i)A\u03b1\u2212 \u00b5A(i)w0 \u2212 b(i)\nThen the search corner for the block of coordinates (i) is given by:\nsi = argmin s\u2032i\n( < s\u2032i,\u2207(i)f(\u03b1) > ) = argmin\ns\u2032i\n( (\u03bb+ \u00b5)\u03b1TATA(i)s \u2032 i \u2212 \u00b5wT0 A(i)s\u2032i \u2212 bT(i)s \u2032 i ) We replace:\nA\u03b1 = \u03c1w0 \u2212 w\nA(i)s \u2032 i =\n1\nN\n1\n\u03bb+ \u00b5 \u2211 y\u0304i\u2208Y s\u2032i(y\u0304i)\u03c8i(y\u0304i)\nbT(i)s \u2032 i =\n1\nN \u2211 y\u0304i\u2208Y s\u2032i(y\u0304i)\u2206(y\u0304i, yi)\nWe then obtain:\nsi = argmin s\u2032i \u2212(w \u2212 \u03c1w0)T \u2211 y\u0304i\u2208Y s\u2032i(y\u0304i)\u03c8i(y\u0304i)\u2212 wT0 \u03c1 \u2211 y\u0304i\u2208Y s\u2032i(y\u0304i)\u03c8i(y\u0304i)\u2212 \u2211 y\u0304i\u2208Y s\u2032i(y\u0304i)\u2206(y\u0304i, yi)  = argmax\ns\u2032i wT \u2211 y\u0304i\u2208Y s\u2032i(y\u0304i)\u03c8i(y\u0304i) + \u2211 y\u0304i\u2208Y s\u2032i(y\u0304i)\u2206(y\u0304i, yi)  As expected, this maximum is obtained by setting si to one at y \u2217 i = argmax y\u0304i\u2208Y ( wT\u03c8i(y\u0304i) + \u2206(y\u0304i, yi) ) and zeros elsewhere. We introduce the notation:\nwi = \u2212A(i)\u03b1(i) li = b T (i)\u03b1(i)\nws = \u2212A(i)si ls = b T (i)si\nThen we have:\nws = \u2212 1\nN\n1\n\u03bb+ \u00b5 \u03c8(y\u2217i ) = \u2212\n1\nN\n1\n\u03bb+ \u00b5\n\u2202Hi(y \u2217 i )\n\u2202w\nls = 1\nN \u2206(yi, y\n\u2217 i )\nThe optimal step size in the direction of the block of coordinates (i) is given by :\n\u03b3\u2217 = argmin \u03b3 f(\u03b1+ \u03b3(si \u2212 \u03b1i))\nThe optimal step-size is given by:\n\u03b3\u2217 = < \u2207(i)f(\u03b1), si \u2212 \u03b1i > (\u03bb+ \u00b5)\u2016A(si \u2212 \u03b1i)\u20162\nWe introduce wd = \u2212A\u03b1 = w \u2212 \u03c1w0. Then we obtain:\n\u03b3\u2217 = (wi \u2212 ws)T (w \u2212 \u03c1w0) + \u03c1wT0 (wi \u2212 ws)\u2212 1\u03bb+\u00b5 (li \u2212 ls)\n\u2016wi \u2212 ws\u20162\n= (wi \u2212 ws)Tw \u2212 1\u03bb+\u00b5 (li \u2212 ls)\n\u2016wi \u2212 ws\u20162\nAnd the updates are the same as in standard BCFW:\nAlgorithm 2 BCFW with warm start\n1: Let w(0) = w0, \u2200i \u2208 [N ], w(0)i = 0 2: Let l(0) = 0, \u2200i \u2208 [N ], l(0)i = 0 3: for k=0...K do 4: Pick i randomly in {1, .., n}\n5: Get y\u2217i = argmax y\u0304i\u2208Y Hi(y\u0304i, w (k)) and ws = \u2212\n1\nN\n1\n\u03bb+ \u00b5\n\u2202Hi(y \u2217 i , w (k))\n\u2202w(k)\n6: ls = 1 N\u2206(y \u2217 i , yi) 7: \u03b3 = (wi \u2212 ws)Tw \u2212 1\u03bb+\u00b5 (li \u2212 ls)\n\u2016wi \u2212 ws\u20162 clipped to [0, 1]\n8: w (k+1) i = (1\u2212 \u03b3)w (k) i + \u03b3ws 9: l (k+1) i = (1\u2212 \u03b3)l (k) i + \u03b3ls 10: w(k+1) = w(k) + w (k+1) i \u2212 w (k) i = w (k) + \u03b3(w (k) s \u2212 w(k)i ) 11: l(k+1) = l(k) + l (k+1) i \u2212 l (k) i 12: end for\nIn particular, we have proved Proposition (4) in this section: w is initialized to \u03c1w0 (KKT conditions), and the direction of the conditional gradient, ws, is given by \u2202Hi(y \u2217 i )\n\u2202w , which is\nindependent of w0.\nNote that the derivation of the Lagrangian dual has introduced a dual variable \u03b1i(y\u0304i) for each linear constraint of the SVM problem (this can be replaced by \u03b1i(hi, (y\u0304i)) if we consider latent variables). These dual variables indicate the complementary slackness not only for the output class y\u0304i, but also for each of the activation which defines a piece of the piecewise linear hinge loss. Therefore a choice of \u03b1 defines a path of activations.\nE Sensitivity of SGD Algorithms\nHere we discuss some weaknesses of the SGD-based algorithms that we have encountered in practice for our learning objective function. These behaviors have been observed in the case of PL-CNNs, and generally may not appear in different architectures (in particular the failure to learn with high regularization goes away with the use of batch normalization layers).\nE.1 Initial Learning Rate\nAs mentioned in the experiments section, the choice of the initial learning rate is critical for good performance of all Adagrad, Adadelta and Adam. When the learning rate is too high, the network does not learn anything and the training and validating accuracies are stuck at random level. When it is too low, the network may take a considerably greater number of epochs to converge.\nE.2 Failures to Learn\nRegularization When the regularization hyper-parameter \u03bb is set to a value of 0.01 or higher on CIFAR-10, SGD solvers get trapped in a local minimum and fail to learn. The SGD solvers indeed fall in the local minimum of shutting down all activations on ReLUs, which provide zero-valued feature vector to the SVM loss layer (and a hinge loss of one). As a consequence, no information can be back-propagated. We plot this behavior below:\nIn this situation, the network is at a bad saddle point (note that the training and validation accuracies are stuck at random levels). Our algorithm does not fall into such bad situations, however it is not able to get out of it either: each layer is at a pathological critical point of its own objective function, which makes our algorithm unable to escape from it.\nWith a lower initial learning rate, the evolution is slower, but eventually the solver goes back to the bad situation presented above.\nBiases The same failing behavior as above has been observed when not using the biases in the network. Again our algorithm is robust to this change.\n", "cats": {"READ": 1.0, "DONTREAD": 0.0}, "reviews": [{"IS_META_REVIEW": true, "comments": "A layer wise optimization for CNNs with ReLU activations and max-pooling is proposed and shown to correspond to a series of latent structured SVM problems. Using CCCP style optimization a monotonic decrease of the overall objective function can be guaranteed.\n\nSummary:\n\u2014\u2014\u2014\nI think the discussed insights are very interesting but not presented convincingly. Firstly, claims are emphasized which are often violated in practice (e.g., no convergence guarantees due to mini-batches), statements could be validated more convincingly (e.g., is monotone convergence a curse or a blessing), the experimental evaluation should be extended. In summary, I think the paper requires some more attention to form a compelling story.\n\nQuality: I think some of the techniques could be described more carefully to better convey the intuition. At times apples are compared to oranges, e.g., back propagation is contrasted with CCCP.\nClarity: Some of the derivations and intuitions could be explained in more detail.\nOriginality: The suggested idea is reasonable albeit heuristics are required.\nSignificance: Since the experimental setup is somewhat limited according to my opinion, significance is hard to judge at this point in time.\n\nDetails:\n\u2014\u2014\u2014\u2014\n1. I think the provided guarantees for the optimization procedure are certainly convenient theoretically but their practical relevance still needs to be demonstrated more convincingly, e.g., mini-batch optimization alleviates any form of monotonic decrease. Hence the emphasize in the paper is somewhat misguided according to my opinion and given he current experimental evaluation.\n\n2. In spirit similar is work by B. Amos and J. Kolter, Input-Convex Deep Networks ("}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "The authors present a novel layer-wise optimization approach for learning convolutional neural networks with piecewise linear nonlinearities. The proposed approach trains piecewise linear ConvNets layer by layer, reduces the sub-problem into latent structured SVM. \n \n Reviewers mainly expressed concerns about the experimental results, which the authors have diligently addressed in their revised versions. While the reviewers haven't updated explicitly their reviews, I believe the changes made should have been sufficient for them to do so.\n \n Thus, I recommend this paper be accepted.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"DATE": "14 Jan 2017 (modified: 20 Jan 2017)", "TITLE": "Submission Revision", "IS_META_REVIEW": false, "comments": "tldr: New results on ImageNet, CIFAR-100, improved results on CIFAR-10.\n\nWe thank the reviewers for their helpful feedbacks. We list here the changes made in the revisions of the paper (version 1 being the original submission read by the reviewers).\n\nList of changes in version 2:\n\n1) New results with batch-normalization on CIFAR-10 (new subsection 5.2)\n2) Clarification of the objective of the paper and experiments (Methods paragraph in subsection 5.1)\n\nList of changes in version 3:\n\n1) New results on CIFAR-10: deeper architecture for a stronger baseline (subsection 5.2)\n2) New results on CIFAR-100 (subsection 5.2)\n3) Re-wording of the experiments section and removal of previous experiments on CIFAR-10 (with and without batch normalization) (section 5)\n4) New Appendix about the computation of the feature vectors and detailed example (Appendix B).\n5) Infeasibility of standard line-search in Introduction\n6) New references, including suggestions from the reviewers (section 2)\n7) More compact abstract\n8) Inclusion of batch normalization in Discussion (section 6)\n9) Minor rewording and typo fixes throughout the paper.\n\nList of changes in version 4:\n1) Added ImageNet results (subsection 5.3)\n\n", "OTHER_KEYS": "Leonard Berrada"}, {"TITLE": "No Title", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "This paper presents a novel layer-wise optimization approach for learning CNN with piecewise linear nonlinearities.  The proposed approach trains piecewise linear CNNs layer by layer and reduces the sub-problem into latent structured SVM, which has been well-studied in the literature. In addition, the paper presents improvements of the BCFW algorithm used in the inner procedure. Overall, this paper is interesting. However, unfortunately, the experiment is not convincing. \n\nPros:\n\n- To my best knowledge, the proposed approach is novel, and the authors provide nice theoretical analysis.\n- The paper is well-written and easy to follow. \n\nCons:\n\n- Although the proposed approach can be applied in general structured prediction problem, the experiments only conduct on a simple multi-class classification task. This makes this work less compelling. \n\t\n- The test accuracy performance on CIFAR-10 reported in the paper doesn't look right. The accuracy of the best model reported in this paper is 70.2% while existing work often reports 90+%. For example, ", "IS_META_REVIEW": false, "RECOMMENDATION": 5, "DATE": "20 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "My thoughts", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "A layer wise optimization for CNNs with ReLU activations and max-pooling is proposed and shown to correspond to a series of latent structured SVM problems. Using CCCP style optimization a monotonic decrease of the overall objective function can be guaranteed.\n\nSummary:\n\u2014\u2014\u2014\nI think the discussed insights are very interesting but not presented convincingly. Firstly, claims are emphasized which are often violated in practice (e.g., no convergence guarantees due to mini-batches), statements could be validated more convincingly (e.g., is monotone convergence a curse or a blessing), the experimental evaluation should be extended. In summary, I think the paper requires some more attention to form a compelling story.\n\nQuality: I think some of the techniques could be described more carefully to better convey the intuition. At times apples are compared to oranges, e.g., back propagation is contrasted with CCCP.\nClarity: Some of the derivations and intuitions could be explained in more detail.\nOriginality: The suggested idea is reasonable albeit heuristics are required.\nSignificance: Since the experimental setup is somewhat limited according to my opinion, significance is hard to judge at this point in time.\n\nDetails:\n\u2014\u2014\u2014\u2014\n1. I think the provided guarantees for the optimization procedure are certainly convenient theoretically but their practical relevance still needs to be demonstrated more convincingly, e.g., mini-batch optimization alleviates any form of monotonic decrease. Hence the emphasize in the paper is somewhat misguided according to my opinion and given he current experimental evaluation.\n\n2. In spirit similar is work by B. Amos and J. Kolter, Input-Convex Deep Networks (", "IS_META_REVIEW": false, "RECOMMENDATION": 4, "DATE": "17 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "TITLE": "No Title", "comments": "This paper proposes a new approaches for optimizing the objective of CNNs. The proposed method uses a lay-wise optimization, i.e. at each step, it optimizes the parameters in one layer of CNN while fixing the parameters in other layers. The key insight of this paper is that, for a large class of CNNs, the optimization problem at a particular can be formulated as optimizing a piecewise linear (PL) function. This PL function optimization happens to be the optimization problem commonly encountered in latent structural SVM. This connection allows this paper to borrows ideas from the latent structural SVM literature, in particular concave-convex procedure, to learn the parameters of CNNs.\n\nOverall, the paper is well-written. Traditional, CNNs and structural SVMs are almost two separate research communties. The connection of CNNs to latent structural SVM is interesting, and might bridge the gap and facilitate the transferring of ideas between these two camps.\n\nOf course, the proposed method also has some limitations. 1) It is limited to layer-wise optimization. Nowadays layer-wise optimization is essentially a coordinate descent algorithm and is not really a competitive strategy in learning CNNs. When you choose layer-wise optimization, you already lose something in terms of optimizing the objective (since you are using coordinate descent, instead of gradient descent). Of course, you also gain something since now you can guarantee that each coordinate descent step always improve the objective. It is not clear to me how the loss/gain balances each other. 2) This paper focues on improving the optimization of CNN objective. However, we all know that a better objective does not necessarily correspond to a good model (e.g. due to overfitting). Although the SGD with backprop in standard CNN learning does not always improve the solution of the objective (unlike the proposed method in this paper), but to me, this might be a good thing since it can prevent overfitting (the goal of learning is not to get better solution for the optimization problem in the first place -- the optimization problem is merely a proxy to learn a model with good generalization ability).\n\nThe experiment is a bit weak.\n1) Only CIFAR10 is used. This is a very small dataset by today's standard, while CNNs are typically used in large-scale datasets, such as ImageNet. It is not clear whether the conclusions of this paper still hold when applied on ImageNet.\n\n2) This paper only compares with a crippled variant of SGD (without batch normalization, dropout, etc). Although this paper mentions that the reason is that it wants to focus on optimization. But I mentioned earlier, SGD is not designed to purely obtain the best solution that optimizes the objective, the goal of SGD is to reasonably optimize the objective, while preventing overfitting. So the comparison to SGD purely in terms of the optimization is that meaningful in the first place. \n", "IS_ANNOTATED": true, "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "15 Dec 2016", "CLARITY": 3, "REVIEWER_CONFIDENCE": 4}, {"TITLE": "comparison with backprop", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "", "IS_ANNOTATED": true, "IS_META_REVIEW": false, "DATE": "29 Nov 2016", "CLARITY": 3}, {"IS_META_REVIEW": true, "comments": "A layer wise optimization for CNNs with ReLU activations and max-pooling is proposed and shown to correspond to a series of latent structured SVM problems. Using CCCP style optimization a monotonic decrease of the overall objective function can be guaranteed.\n\nSummary:\n\u2014\u2014\u2014\nI think the discussed insights are very interesting but not presented convincingly. Firstly, claims are emphasized which are often violated in practice (e.g., no convergence guarantees due to mini-batches), statements could be validated more convincingly (e.g., is monotone convergence a curse or a blessing), the experimental evaluation should be extended. In summary, I think the paper requires some more attention to form a compelling story.\n\nQuality: I think some of the techniques could be described more carefully to better convey the intuition. At times apples are compared to oranges, e.g., back propagation is contrasted with CCCP.\nClarity: Some of the derivations and intuitions could be explained in more detail.\nOriginality: The suggested idea is reasonable albeit heuristics are required.\nSignificance: Since the experimental setup is somewhat limited according to my opinion, significance is hard to judge at this point in time.\n\nDetails:\n\u2014\u2014\u2014\u2014\n1. I think the provided guarantees for the optimization procedure are certainly convenient theoretically but their practical relevance still needs to be demonstrated more convincingly, e.g., mini-batch optimization alleviates any form of monotonic decrease. Hence the emphasize in the paper is somewhat misguided according to my opinion and given he current experimental evaluation.\n\n2. In spirit similar is work by B. Amos and J. Kolter, Input-Convex Deep Networks ("}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "The authors present a novel layer-wise optimization approach for learning convolutional neural networks with piecewise linear nonlinearities. The proposed approach trains piecewise linear ConvNets layer by layer, reduces the sub-problem into latent structured SVM. \n \n Reviewers mainly expressed concerns about the experimental results, which the authors have diligently addressed in their revised versions. While the reviewers haven't updated explicitly their reviews, I believe the changes made should have been sufficient for them to do so.\n \n Thus, I recommend this paper be accepted.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"DATE": "14 Jan 2017 (modified: 20 Jan 2017)", "TITLE": "Submission Revision", "IS_META_REVIEW": false, "comments": "tldr: New results on ImageNet, CIFAR-100, improved results on CIFAR-10.\n\nWe thank the reviewers for their helpful feedbacks. We list here the changes made in the revisions of the paper (version 1 being the original submission read by the reviewers).\n\nList of changes in version 2:\n\n1) New results with batch-normalization on CIFAR-10 (new subsection 5.2)\n2) Clarification of the objective of the paper and experiments (Methods paragraph in subsection 5.1)\n\nList of changes in version 3:\n\n1) New results on CIFAR-10: deeper architecture for a stronger baseline (subsection 5.2)\n2) New results on CIFAR-100 (subsection 5.2)\n3) Re-wording of the experiments section and removal of previous experiments on CIFAR-10 (with and without batch normalization) (section 5)\n4) New Appendix about the computation of the feature vectors and detailed example (Appendix B).\n5) Infeasibility of standard line-search in Introduction\n6) New references, including suggestions from the reviewers (section 2)\n7) More compact abstract\n8) Inclusion of batch normalization in Discussion (section 6)\n9) Minor rewording and typo fixes throughout the paper.\n\nList of changes in version 4:\n1) Added ImageNet results (subsection 5.3)\n\n", "OTHER_KEYS": "Leonard Berrada"}, {"TITLE": "No Title", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "This paper presents a novel layer-wise optimization approach for learning CNN with piecewise linear nonlinearities.  The proposed approach trains piecewise linear CNNs layer by layer and reduces the sub-problem into latent structured SVM, which has been well-studied in the literature. In addition, the paper presents improvements of the BCFW algorithm used in the inner procedure. Overall, this paper is interesting. However, unfortunately, the experiment is not convincing. \n\nPros:\n\n- To my best knowledge, the proposed approach is novel, and the authors provide nice theoretical analysis.\n- The paper is well-written and easy to follow. \n\nCons:\n\n- Although the proposed approach can be applied in general structured prediction problem, the experiments only conduct on a simple multi-class classification task. This makes this work less compelling. \n\t\n- The test accuracy performance on CIFAR-10 reported in the paper doesn't look right. The accuracy of the best model reported in this paper is 70.2% while existing work often reports 90+%. For example, ", "IS_META_REVIEW": false, "RECOMMENDATION": 5, "DATE": "20 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "My thoughts", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "A layer wise optimization for CNNs with ReLU activations and max-pooling is proposed and shown to correspond to a series of latent structured SVM problems. Using CCCP style optimization a monotonic decrease of the overall objective function can be guaranteed.\n\nSummary:\n\u2014\u2014\u2014\nI think the discussed insights are very interesting but not presented convincingly. Firstly, claims are emphasized which are often violated in practice (e.g., no convergence guarantees due to mini-batches), statements could be validated more convincingly (e.g., is monotone convergence a curse or a blessing), the experimental evaluation should be extended. In summary, I think the paper requires some more attention to form a compelling story.\n\nQuality: I think some of the techniques could be described more carefully to better convey the intuition. At times apples are compared to oranges, e.g., back propagation is contrasted with CCCP.\nClarity: Some of the derivations and intuitions could be explained in more detail.\nOriginality: The suggested idea is reasonable albeit heuristics are required.\nSignificance: Since the experimental setup is somewhat limited according to my opinion, significance is hard to judge at this point in time.\n\nDetails:\n\u2014\u2014\u2014\u2014\n1. I think the provided guarantees for the optimization procedure are certainly convenient theoretically but their practical relevance still needs to be demonstrated more convincingly, e.g., mini-batch optimization alleviates any form of monotonic decrease. Hence the emphasize in the paper is somewhat misguided according to my opinion and given he current experimental evaluation.\n\n2. In spirit similar is work by B. Amos and J. Kolter, Input-Convex Deep Networks (", "IS_META_REVIEW": false, "RECOMMENDATION": 4, "DATE": "17 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "TITLE": "No Title", "comments": "This paper proposes a new approaches for optimizing the objective of CNNs. The proposed method uses a lay-wise optimization, i.e. at each step, it optimizes the parameters in one layer of CNN while fixing the parameters in other layers. The key insight of this paper is that, for a large class of CNNs, the optimization problem at a particular can be formulated as optimizing a piecewise linear (PL) function. This PL function optimization happens to be the optimization problem commonly encountered in latent structural SVM. This connection allows this paper to borrows ideas from the latent structural SVM literature, in particular concave-convex procedure, to learn the parameters of CNNs.\n\nOverall, the paper is well-written. Traditional, CNNs and structural SVMs are almost two separate research communties. The connection of CNNs to latent structural SVM is interesting, and might bridge the gap and facilitate the transferring of ideas between these two camps.\n\nOf course, the proposed method also has some limitations. 1) It is limited to layer-wise optimization. Nowadays layer-wise optimization is essentially a coordinate descent algorithm and is not really a competitive strategy in learning CNNs. When you choose layer-wise optimization, you already lose something in terms of optimizing the objective (since you are using coordinate descent, instead of gradient descent). Of course, you also gain something since now you can guarantee that each coordinate descent step always improve the objective. It is not clear to me how the loss/gain balances each other. 2) This paper focues on improving the optimization of CNN objective. However, we all know that a better objective does not necessarily correspond to a good model (e.g. due to overfitting). Although the SGD with backprop in standard CNN learning does not always improve the solution of the objective (unlike the proposed method in this paper), but to me, this might be a good thing since it can prevent overfitting (the goal of learning is not to get better solution for the optimization problem in the first place -- the optimization problem is merely a proxy to learn a model with good generalization ability).\n\nThe experiment is a bit weak.\n1) Only CIFAR10 is used. This is a very small dataset by today's standard, while CNNs are typically used in large-scale datasets, such as ImageNet. It is not clear whether the conclusions of this paper still hold when applied on ImageNet.\n\n2) This paper only compares with a crippled variant of SGD (without batch normalization, dropout, etc). Although this paper mentions that the reason is that it wants to focus on optimization. But I mentioned earlier, SGD is not designed to purely obtain the best solution that optimizes the objective, the goal of SGD is to reasonably optimize the objective, while preventing overfitting. So the comparison to SGD purely in terms of the optimization is that meaningful in the first place. \n", "IS_ANNOTATED": true, "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "15 Dec 2016", "CLARITY": 3, "REVIEWER_CONFIDENCE": 4}, {"TITLE": "comparison with backprop", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "", "IS_ANNOTATED": true, "IS_META_REVIEW": false, "DATE": "29 Nov 2016", "CLARITY": 3}]}
{"text": "1 INTRODUCTION\nThere has been an increased interest in unsupervised learning of representations from video sequences (Mathieu et al., 2016; Srivastava et al., 2015; Vondrick et al., 2016). A popular formulation of the task is to learn to predict a small number of future frames given the previous K frames; the motivation being that predicting future frames requires understanding how objects interact and what plausible sequences of motion are. These methods directly aim to predict pixel values, with either MSE loss or adversarial loss.\nIn this paper, we take a different approach to the problem of next frame prediction. In particular, our model operates in the space of transformations between frames, directly modeling the source of variability. We exploit the assumption that the transformations of objects from frame to frame should be smooth, even when the pixel values are not. Instead of predicting pixel values, we directly predict how objects transform. The key insight is that while there are many possible outputs, predicting one such transformation will yield motion that may not correspond to ground truth, yet will be realistic; see fig. 1. We therefore propose a transformation-based model that operates in the space of affine transforms. Given the affine transforms of a few previous frames, the model learns to predict the local affine transforms that can be deterministically applied on the image patches of the previous frame to generate the next frame. The intuition is that estimation errors will lead to a slightly different yet plausible motion. Note that this allows us to keep using the MSE criterion, which is easy to optimize, as long as it is in transformation space. No blur in the pixel space will be introduced since the output of the transformation model is directly applied to the pixels, keeping sharp edges intact. Refer to fig. 5 and our online material 1 for examples.\nThe other contribution of this work is the evaluation protocol. Typically, generative models of video sequences are evaluated in terms of MSE in pixel space (Srivastava et al., 2015), which is not a good choice since this metric favors blurry predictions over other more realistic looking options that just happen to differ from the ground truth. Instead, we propose to feed the generated frames to a video\n\u2217Work done as part of internship with FAIR 1see: http://joo.st/ICLR/GenerationBenchmark\nclassifier trained on ground truth sequences. The idea is that the less the classifier\u2019s performance is affected by the generates frames the more the model has preserved distinctive features and the more the generated sequences are plausible. Regardless of whether they resemble the actual ground truth or not. This protocol treats the classifier as a black box to measure how well the generated sequences can serve as surrogate for the truth sequence for the classification task. In this paper we will validate our assumption that motion can be modelled by local affine transforms, after which we will compare our method with networks trained using adversarial training and simple regression on the output frame, using both this new evaluation protocol and by providing samples for qualitative inspection.\nOur experiments show that our simple and efficient model outperforms other baselines, including much more sophisticated models, on benchmarks on the UCF-101 data set (Soomro et al., 2012). We also provide qualitative comparisons to the moving MNIST digit data set (Srivastava et al., 2015).\n1.1 RELATED WORK\nEarly work on video modeling focused on predicting small patches (Michalski et al., 2014; Srivastava et al., 2015); unfortunately, these models have not shown to scale to the complexity of highresolution videos. Also these models require a significant amount of parameters and computational power for even relatively simple data.\nIn Ranzato et al. (2014), the authors circumvented this problem by quantizing the space of image patches. While they were able to predict a few high-resolution frames in the future, it seems dissatisfying to impose such a drastic assumption to simplify the prediction task.\nMathieu et al. (2016) recently proposed to replace MSE in pixel space with a MSE on image gradients, leveraging prior domain knowledge, and further improved using a multi-scale architecture with adversarial training (Goodfellow et al., 2014). While producing better results than earlier methods, the models used require a very large amount of computational power. We make an explicit comparison to this paper in the experiments section 3.\nIn Oh et al. (2015), frames of a video game are predicted given an action (transformation) taken by the player. While the paper shows great results, the movement in a natural video cannot be described by a simple action and is therefore not widely applicable. Finally, our work is also related to optical flow estimation (Brox et al., 2004). Instead of estimating the flow of pixels, here we estimate the flow of patches and separately predict how these patches transform in future frames.\nPrior work relating to the evaluation protocol can be found in Yan et al. (2015). The authors generate images using a set of predefined attributes and later show that they can recover these using a pretrained neural network. Our proposal extends this to videos, which is more complicated since both appearance and motion are needed for correct classification.\n2 MODEL\nThe model we propose is based on three key assumptions: 1) just estimating object motion yields sequences that are plausible and relatively sharp, 2) global motion can be estimated by tiling highresolution video frames into patches and estimating motion \u201cconvolutionally\u201d at the patch level, and 3) patches at the same spatial location over two consecutive time steps undergo a deformation which can be well described by an affine transformation.\nThe first assumption is at the core of the proposed method: by considering uncertainty in the space of transformations we produce sequences that may still look plausible. The other two assumptions state that a video sequence can be composed by patches undergoing affine transformations. We agree that these are simplistic assumptions, which ignore how object identity affects motion and do not account for out of plane rotations and more general forms of deformation. However, our qualitative and quantitative evaluation shows the efficacy of these assumptions to real video sequence as can be seen in section 3 and from visualizations in the supplementary material2.\nOur approach consists of three steps. First, we estimate affine transforms of every video sequence to build a training set for our model. Second, we train a model that takes the past N affine transforms and predicts the next M affine transforms. Finally, at test time, the model uses the predicted affine transforms to reconstruct pixel values of the generated sequence. We describe the details of each phase in the following sections.\n2.1 AFFINE TRANSFORM EXTRACTOR\nGiven a frame x and the subsequent frame y, the goal of the affine transform extractor is to learn mappings that can warp x into y. Since different parts of the scene may undergo different transforms, we tile x into overlapping patches and infer a transformation for each patch. The estimation process couples the transformations at different spatial locations because we minimize the reconstruction error of the entire frame y, as opposed to treating each patch independently.\n2see: http://joo.st/ICLR/ReconstructionsFromGroundTruth and http://joo.st/ ICLR/GenerationBenchmark\nLet x and y have size Dr \u00d7Dc. Let image x be decomposed into a set of overlapping patches, each containing pixels from patches of size dr\u00d7dc with dr \u2264 Dr and dc \u2264 Dc. These patches are laid out on a regular grid with stride sr and sc pixels over rows and columns, respectively. Therefore, every pixel participates in drsr dc sc\noverlapping patches, not taking into account for the sake of simplicity border effects and non-integer divisions. We denote the whole set of overlapping patches by {Xk}, where index k runs over the whole set of patches. Similarly and using the same coordinate system, we denote by {Yk} the set of overlapping patches of y. We assume that there is an affine mapping Ak that maps Xk to Yk, for all values of k. Ak is a 2 \u00d7 3 matrix of free parameters representing a generic affine transform (translation, rotation and scaling) between the coordinates of output and input frame. Let Y\u0303k be the transformed patches obtained when Ak is applied to Xk. Since coordinates overlap between patches, we reconstruct y by averaging all predictions at the same location, yielding the estimate y\u0303. The joint set of Ak is then jointly determined by minimizing the mean squared reconstruction error between y and y\u0303.\nNotice that our approach and aim differs from spatial transformer networks (Jaderberg et al., 2015) since we perform this estimation off-line only for the input frames, computing one transform per patch.\nIn our experiments, we extracted 16 \u00d7 16 pixel patches from the input and we used stride 4 over rows and columns. The input patches are then matched at the output against smaller patches of size 8\u00d7 8 pixels, to account for objects moving in and out of the patch region.\n2.2 AFFINE TRANSFORM PREDICTOR\nThe affine transform predictor is used to predict the affine transforms between the last input frame and the next frame in the sequence. A schematic illustration of the system is shown in fig. 2. It receives as input the affine transforms between pairs of adjacent frames, as produced by the affine transform extractor described in the previous section. Each transform is arranged in a grid of size 6 \u00d7 n \u00d7 n, where n is the number of patches in a row/column and 6 is the number of parameters of each affine transform. Therefore, if four frames are used to initialize the model, the actual input consists of 18 maps of size n \u00d7 n, which are the concatenation of At\u22122, At\u22121, At, where At is the collection of patch affine transforms between frame at time t\u2212 1 and t. The model consists of a multi-layer convolutional network without any pooling. The network is the composition of convolutional layers with ReLU non-linearity, computing a component-wise thresholding as in v = max(0, u). We learn the parameters in the filters of the convolutional layers by minimizing the mean squared error between the output of the network and the target transforms.\nNotice that we do not add any regularization to the model. In particular, we rely on the convolutional structure of the model to smooth out predictions at nearby spatial locations.\n2.3 MULTI-STEP PREDICTION\nIn the previous section, we described how to predict the set of affine transforms at the next time step. In practice, we would like to predict several time steps in the future.\nA greedy approach would: a) train as described above to minimize the prediction error for the affine transforms at the next time step, and b) at test time, predict one step ahead and then re-circulate the model prediction back to the input to predict the affine transform two steps ahead, etc. Unfortunately, errors may accumulate throughout this process because the model was never exposed to its own predictions at training time.\nThe approach we propose replicates the model over time, also during training as shown in fig. 3. If we wish to predict M steps in the future, we replicate the CNN M times and pass the output of the CNN at time step t as input to the same CNN at time step t + 1, as we do at test time. Since predictions live in a continuous space, the whole system is differentiable and amenable to standard back-propagation of the error. Since parameters of the CNN are shared across time, the overall system is equivalent to a peculiar recurrent neural network, where affine transforms play the role of recurrent states. The experiments in section 3 demonstrate that this method is more accurate and robust than the greedy approach.\n2.4 TESTING\nAt test time, we wish to predict M frames in the future given the past N frames. After extracting the N \u2212 1 affine transforms from the frames we condition upon, we replicate the model M times and feed its own prediction back to the input, as explained in the previous section.\nOnce the affine transforms are predicted, we can reconstruct the actual pixel values. We use the last frame of the sequence and apply the first set of affine transforms to each patch in that frame. Each pixel in the output frame is predicted multiple times, depending on the stride used. We average these predictions and reconstruct the whole frame. As required, we can repeat this process for as many frames as necessary, using the last reconstructed frame and the next affine transform.\nIn order to evaluate the generation, we propose to feed the generated frames to a trained classifier for a task of interest. For instance, we can condition the generation using frames taken from video clips which have been labeled with the corresponding action. The classifier has been trained on ground truth data but it is evaluated using frames fantasized by the generative model. The performance of the classifier on ground truth data is an upper bound on the performance of any generative model. This evaluation protocol does not penalize any generation that deviates from the ground truth, as standard MSE would. It instead check that discriminative features and the overall semantics of the generated sequence is correct, which is ultimately what we are interested in.\n3 EXPERIMENTS\nIn this section, we validate the key assumptions made by our model and compare against state-ofthe-art generative models on two data sets. We strongly encourage the reader to watch the short video clips in the Supplementary Material to better understand the quality of our generations.\nIn section 2, we discussed the three key assumptions at the foundations of our model: 1) errors in the transformation space look still plausible, 2) a frame can be decomposed into patches, and 3) each patch motion is well modeled by an affine transform. The results in the Supplementary Material 3 validate assumption 2 and 3 qualitatively. Every row shows a sequence from the UCF101 dataset (Soomro et al., 2012). The column on the left shows the original video frames and the one on the right the reconstructions from the estimated affine transforms, as described in section 2.1. As you can see there is barely any noticeable difference between these video sequences, suggesting that video sequences can be very well represented as tiled affine transforms. For a quantitative comparison and for an assessment of how well the first assumption holds, please refer to section 3.2.\n3see: http://joo.st/ICLR/ReconstructionsFromGroundTruth\nIn the next section, we will first report some results using the toy data set of \u201cmoving MNIST digits\u201d (Srivastava et al., 2015). We then discuss generations of natural high-resolution videos using the UCF-101 dataset and compare to current state-of-the-art methods.\n3.1 MOVING MNIST\nFor our first experiment, we used the dataset of moving MNIST digits (Srivastava et al., 2015) and perform qualitative analysis4. It consists of one or two MNIST digits, placed at random locations and moving at constant speed inside a 64 \u00d7 64 frame. When a digit hits a boundary, it bounces, meaning that velocity in that direction is reversed. Digits can occlude each other and bounce off walls, making the data set challenging.\nUsing scripts provided by Srivastava et al. (2015), we generated a fixed dataset of 128,000 sequences and used 80% for training, 10% for validation and 10% for testing. Next, we estimated the affine transforms between every pair of adjacent frames to a total of 4 frames, and trained a small CNN in the space of affine transforms. The CNN has 3 convolutional layers and the following number of feature maps: 18, 32, 32, 6. All filters have size 3\u00d7 3. Fig. 4 shows some representative test sequences and the model outputs. Each subfigure corresponds to a sequence from the test set; the top row corresponds to the ground truth sequence while the bottom row shows the generations. The input to the CNN are three sets of affine transforms corresponding to the first four consecutive frames. The network predicts the next six sets of affine transforms from which we reconstruct the corresponding frames. These results should be compared to fig. 5 in Srivastava et al. (2015). The generations in fig. 4 show that the model has potential to represent and generate video sequences, it learns to move digits in the right direction, to bounce them, and it handles multiple digits well except when occluion makes inputs too ambiguous. The model\u2019s performance is analyzed quantitatively in the next section using high resolution natural videos.\n3.2 UCF 101 DATA SET\nThe UCF-101 dataset (Soomro et al., 2012) is a collection of 13320 videos of 101 action categories. Frames have size 240\u00d7 320 pixels. We train a CNN on patches of size 64\u00d7 64 pixels; the CNN has 6 convolutional layers and the following number of feature maps: 18, 128, 128, 128, 64, 32, 16, 6. All filters have size 3 \u00d7 3. The optimal number of filters has been found using cross-validation in order to minimize the estimation error of the affine transform parameters. Unless otherwise stated, we condition generation on 4 ground truth frames and we predict the following 8 frames.\nWe evaluate several models5: a) a baseline which merely copies the last frame used for conditioning, b) a baseline method which estimates optical flow (Brox et al., 2004) from two consecutive frames\n4A quantitative analysis would be difficult for this data set because metrics reported in the literature like MSE (Srivastava et al., 2015) are not appropriate for measuring generation quality, and it would be difficult to use the metric we propose because we do not have labels at the sequence level and the design of a classifier is not trivial.\n5Unfortunately, we could not compare against the LSTM-based method in Srivastava et al. (2015) because it does not scale to high-resolution videos, but only to small patches.\nand extrapolates flow in subsequent frames under the assumption of constant flow speed, c) an adversarially trained multi-scale CNN (Mathieu et al., 2016) and several variants of our proposed approach.\nQualitative comparisons can be seen in the fig. 5 and in the supplementary material6. The first column on the page shows the input, the second the ground truth, followed by results from our model, Mathieu et al. (2016) and optical flow (Brox et al., 2004). Note especially the severe deformations in the last two columns, while our model keeps the frame recognizable. It produces fairly sharp reconstructions validating our first hypothesis that errors in the space of transformations still yield plausible reconstructions (see section 2). However it is also apparent that our approach underestimates movement, which follows directly from using the MSE criterion. As discussed before, MSE in pixel space leads to blurry results, however using MSE in transformation space also has some drawbacks. In practice, the model will predict the average of several likely transformations, which could lead to an understimation of the true movement.\n6see: http://joo.st/ICLR/GenerationBenchmark\nIn order to quantify the generation quality we use the metric described in section 2.4. We use C3D network (Tran et al., 2015) as the video action classifier: C3D uses both appearance and temporal information jointly, and is pre-trained with Sports1M (Karpathy et al., 2014) and fine tuned on UCF 101. Due to the model constraints, we trained only two models, that takes 4 and 8 frames as input, respectively.\nWe evaluate the quality of generation using 4 (the first four predicted frames) and the whole set of 8 predicted frames, for the task of action classification. At test time, we generate frames from each model under consideration, and then use them as input to the corresponding C3D network.\nTable 1 shows the accuracy of our approach and several baselines. The best performance is achieved by using ground truth frames, a result comparable to methods recently appeared in the literature (Karpathy et al., 2014; Tran et al., 2015). We see that for ground truth frames, the number of frames (4 or 8) doesn\u2019t make a difference. There is not much additional temporal or spatial signal provided by having greater than four frames. Next, we evaluate how much we lose by representing frames as tiled affine transforms. As the second row shows there is negligible if any loss of accuracy when using frames reconstructed from the estimated affine transforms (using the method described in section 2.1), validating our assumptions at the beginning of section 2 on how video sequences can be represented. The next question is then whether these affine transforms are predictable at all. The last two rows of Table 1 show that this is indeed the case, to some extent. The longer the sequence of generated frames the poorer the performance, since the generation task gets more and more difficult.\nCompared to other methods, our approach performs better than optical flow and even the more sophisticated multi-scale CNN proposed in Mathieu et al. (2016) while being computationally cheaper. For instance, our method has less than half a million parameters and requires about 2G floating point operations to generate a frame at test time, while the multi-scale CNN of Mathieu et al. (2016) has 25 times more parameters (not counting the discriminator used at training time) and it requires more than 100 times more floating point operations to generate a single frame.\nFinally, we investigate the robustness of the system to its hyper-parameters: a) choice of patch size, b) number of input frames, and c) number of predicted frames. The results reported in Table 2 demonstrate that the model is overall pretty robust to these choices. Using patch sizes that are too big makes reconstructions blocky but within each block motion is coherent. Smaller patch sizes give more flexibility but make the prediction task harder as well. Mapping into patches of size smaller than 16 \u00d7 16 seems a good choice. Using only 2 input frames does not seem to provide enough context to the predictor, but anything above 3 works equally well. Training for prediction of the next frame works well, but better results can be achieved by training to predict several frames in the future, overall when evaluating longer sequences.\n4 CONCLUSIONS\nIn this work, we proposed a new approach to generative modeling of video sequences. This model does not make any assumption about the spatio-temporal resolution of video sequences nor about object categories. The key insight of our approach is to model in the space of transformations as opposed to raw pixel space. A priori we lack a good metric to measure how well a frame is reconstructed under uncertainty due to objects motion in natural scenes. Uncertainty about object motion and occlusions causes blurry generations when using MSE in pixel space. Instead, by operating in the space of transformations we aim at predicting how objects move, and estimation errors only yield a different, and possibly still plausible, motion. With this motivation we proposed a simple CNN operating in the space of affine transforms and we showed that it can generate sensible sequences up to about 4 frames. This model produces sequences that are both visually and quantitatively better than previously proposed approaches.\nThe second contribution of this work is the metric to compare generative models of video sequences. A good metric should not penalize a generative model for producing a sequence which is plausible but different from the ground truth. With this goal in mind and assuming we have at our disposal labeled sequences, we can first train a classifier using ground truth sequences. Next, the classifier is fed with sequences produced by our generative model for evaluation. A good generative model should produce sequences that still retain discriminative features. In other words, plausibility of generation is assessed in terms of how well inherent information is preserved during generation as opposed to necessarily and merely reproducing the ground truth sequences.\nThe proposed model is relatively simple; straightforward extensions that could improve its prediction accuracy are the use of a multi-scale architecture and the addition of recurrent units. These would enable a better modeling of objects of different sizes moving at varying speeds and to better capture complex temporal dynamics (e.g., cyclical movements like walking). A larger extension would be the addition of an appearance model, which together with our explicit transformation model could lead to learning better feature representations for classification.\nIn our view, the proposed approach should be considered as a stronger baseline for future research into next frame prediction. Even though our analysis shows improved performance and better looking generations, there are also obvious limitations. The first such limitation is the underestimation of transformations due to usage of the MSE as a criterion. We consider two main avenues worth pursuing in this space. First, we consider modelling a distribution of transformations and sampling one from it. The challenge of this approach is to sample a consistent trajectory. One could model the distribution of an entire trajectory, but that is a complex optimization problem. A second option is to use adversarial training to force the model to pick a plausible action. This option does not guarantee that underestimation of movement will be avoided. This will depend on the discriminator model accepting this as a plausible option.\nAnother limitation is that the current model does not factor out the \u201cwhat\u201d from the \u201cwhere\u201d, appearance from motion. The representation of two distinct objects subject to the same motion, as well as the representation of the same object subject to two different motion patterns are intrinsically different. Instead, it would be more powerful to learn models that can discover such factorization and leverage it to produce more efficient and compact representations.\nACKNOWLEDGMENTS\nAuthors thank Camille Couprie and Michael Mathieu for discussions and helping with evaluation of their models.\n", "cats": {"READ": 0.0, "DONTREAD": 1.0}, "reviews": [{"IS_META_REVIEW": true, "comments": "This paper describes an approach to predict (unseen) future frames of a video given a set of known past frames. The approach is based on a CNN that, in contrast to most related papers, work in the space of affine transformations (instead of pixels or flow). Said another way, the network takes as input a set of affine transforms that describe the motion of patches in the past frames, and likewise, outputs a set of affine transforms that predict future patch motion.\n\nTo that aim, the authors make a few simplifying hypotheses, namely, that a sequence of frames can be modeled accurately enough in their patch-affine framework. This is not unreasonable. A lot of papers in the optical flow community are based on similar hypotheses, i.e. model the flow as a smoothly varying affine field (for instance see \"Locally affine sparse-to-dense matching for motion and occlusion estimation\" by Leordeanu et al., \"EpicFlow: Edge-Preserving Interpolation of Correspondences for Optical Flow\" by Revaud et al., \"Optical Flow With Semantic Segmentation and Localized Layers\" by Sevilla-Lara et al.). These methods are state of the art, which gives a hint about the validity of this kind of approach. \nIn addition, it also seems very reasonable to reformulate the prediction task as predicting motion rather than predicting raw pixels. Indeed, the (patch-affine) motion space is considerably smaller than the image space, making the problem much more tractable and amenable to high-resolution videos.\n\nWhile I agree with the authors on these points, I also find that the paper suffer from important flaws. Specifically:\n\n  - the choice of not comparing with previous approaches in term of pixel prediction error seems very \"convenient\", to say the least. While it is clear that the evaluation metric is imperfect, it is not a reason to completely dismiss all quantitative comparisons with previous work. The frames output by the network on, e.g. the moving digits datasets (Figure 4), looks ok and can definitely be compared with other papers. Yet, the authors chose not to, which is suspicious.\n  \n  - The newly proposed metric poses several problems. First, action classification is evaluated with C3D, which is not a state-of-the-art approach at all for this task. Second, this metric actually *does not* evaluate what the network is claimed to do, that is, next frame prediction. Instead, it evaluates if another network, which was never trained to distinguish between real or synthetic frames by the way, can accurately classify an action from the predicted frames. I find that this proxy metric is only weakly related to what is supposed to be measured. In adition, it does not really make sense to train a network for something else that the final task it is evaluated for.\n  \n  - how is the affine motion of patches estimated? It is only explained that the problem is solved globally (not treating each patch independently) in a pretty vague manner. Estimating the motion of all patches is akin to solving the optical flow, which is still an active subject of research. Therefore, an important flaw of the paper lies in the potentially erroneous etimation of the motion input to the network. In the videos made available, it is clear that the motion is wrongly estimated sometimes. Since the entire approach depends on this input, I find it important to discuss this aspect. How do motion estimation failures impact the network? Also, the patch-affine hypothesis does not hold when patches are large enough that they cover several objects with contradictory motion. Which appears to be the case on UCF101 videos.\n  \n  - Even ignoring the weird proxy-evaluation part, the network is still not trained end-to-end. That is, the network is trained to minimize the difference between (noisy) ground-truth and output affine transforms, instead of minimizing a loss in the actual output space (frame pixels) for which an (exact) ground-truth is available. It is true that the MSE loss on raw pixels leads to blurry results, but other types of losses do exist, for instance the gradient loss introduced by Mathieu et al. was shown to solve this issue. As noted by the authors themselves, minimizing a loss in the transformation space, where affine parameters are harder to intepret, introduces unexpected artifacts. The motion is often largely underestimated, as is obvious in Figure 5 where it is hard to tell the difference between the input and output frames. \n  \n  - The proposed approach is not sufficiently compared to previous work. In particular, the approach is closely related to \"SPATIO-TEMPORAL VIDEO AUTOENCODER WITH DIFFERENTIABLE MEMORY\" of Taraucean et al, ICLR'15. This paper also output prediction in the motion space. Experimental results should compare against it.\n\n  - The comparison with optical flow is unfair. First, the approach of Brox et al. is more than 10 years old. Second, it is not really fair to assume a constant flow for all frames. At least some basic extrapolation could be done to take into account the flow of all pairs of input frames and not just the last one. Overall, the approach is not compared to very challenging baselines.\n\n  - I disagree with the answer that the authors gave to a reviewer's question. Denote ground-truth frames as {X_0, X_1 ...} and predicted frames as {Y_1, Y_2, ...}. When asked if the videos at"}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "There were serious concerns raised about the originality of the work in regard to prior methods. The authors' responses to numerous reviewer questions on this matter were also unsatisfactory. In the end, it is difficult to tell what the contribution in regard to the video prediction model is. The proposed evaluation metric is interesting, but also raised serious concerns. Finally, several reviewers raised concerns about the quality of the results.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"TITLE": "reasonable paper but the contribution right now is very incremental compared to previous works", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "The paper proposes a method for future frame prediction based on transformation of previous frame rather than direct pixel prediction.\n\nMany previous works have proposed similar methods. The authors in their responses state that previous work is deterministic, yet the proposed model also does not handle multimodality.\n\nFurther, i asked if they could test their method using 2 RGB frames as input and predicting the transformation as output, to be able to quantify the importance of using transformations both as input and output, since this is the first work that uses transformations as input also. The authors dismissed the suggestion by saying \"if we were to use RGB frames as input and ask the model to output future frames it would produce very blurry results\", that is, misunderstanding what the suggestion was. So, currently, it does not seem to be a valid novel contribution in this work compared to previous works.", "IS_META_REVIEW": false, "RECOMMENDATION": 5, "DATE": "19 Dec 2016", "REVIEWER_CONFIDENCE": 3}, {"TITLE": "Reasonable paper, can be improved.", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "Paper Summary\nThis paper makes two contributions -\n(1) A model for next step prediction, where the inputs and outputs are in the\nspace of affine transforms between adjacent frames.\n(2) An evaluation method in which the quality of the generated data is assessed\nby measuring the reduction in performance of another model (such as a\nclassifier) when tested on the generated data.\n\nThe authors show that according to this metric, the proposed model works better\nthan other baseline models (including the recent work of Mathieu et al. which\nuses adversarial training).\n\nStrengths\n- This paper attempts to solve a major problem in unsupervised learning\n  with videos, which is evaluating them.\n- The results show that using MSE in transform space does prevent the blurring\n  problem to a large extent (which is one of the main aims of this paper).\n- The results show that the generated data reduces the performance of the C3D\n  model on UCF-101 to a much less extent than other baselines.\n- The paper validates the assumption that videos can be approximated to quite a\n  few time steps by a sequence of affine transforms starting from an initial\nframe.\n\nWeaknesses\n- The proposed metric makes sense only if we truly just care about the performance\n  of a particular classifier on a given task. This significantly narrows the\nscope of applicability of this metric because arguably, one the important\nreasons for doing unsupervised learning is to come up a representation that is\nwidely applicable across a variety of tasks. The proposed metric would not help\nevaluate generative models designed to achieve this objective.\n\n- It is possible that one of the generative models being compared will interact\n  with the idiosyncrasies of the chosen classifier in unintended ways.\nTherefore, it would be hard to draw strong conclusions about the relative\nmerits of generative models from the results of such experiments. One way to\nameliorate this would be to use several different classifiers (C3D,\ndual-stream network, other state-of-the-art methods) and show that the ranking\nof different generative models is consistent across the choice of classifier.\nAdding such experiments would help increase certainty in the conclusions drawn\nin this paper.\n\n- Using only 4 or 8 input frames sampled at 25fps seems like very little context\n  if we really expect the model to extrapolate the kind of motion seen in\nUCF-101. The idea of working in the space of affine transforms would be much\nmore appealing if the model can be shown to really generated non-trivial motion\npatterns. Currently, the motion patterns seem to be almost linear\nextrapolations.\n\n- The model that predicts motion does not have access to content at all. It only\n  gets access to previous motion. It seems that this might be a disadvantage\nbecause the motion predictor cannot use any cues like object boundaries, or\ndecide what to do when two motion fields collide (it is probably easier to argue\nabout occlusions in content space).\n\nQuality/Clarity\nThe paper is clearly written and easy to follow. The assumptions are clearly\nspecified and validated. Experimental details seem adequate.\n\nOriginality\nThe idea of generating videos by predicting motion has been used previously.\nSeveral recent papers also use this idea. However the exact implementation in\nthis paper is new. The proposed evaluation protocol is novel.\n\nSignificance\nThe proposed evaluation method is an interesting alternative, especially if it\nis extended to include multiple classifiers representative of different\nstate-of-the-art approaches. Given how hard it is to evaluate generative models\nof videos, this paper could help start an effort to standardize on a benchmark\nset.\n\nMinor comments and suggestions\n\n(1) In the caption for Table 1: ``Each column shows the accuracy on the test set\nwhen taking a different number of input frames as input\" - ``input\" here refers\nto the input to the classifier (Output of the next step prediction model). However\nin the next sentence ``Our approach maps 16 \\times 16 patches into 8 \\times 8\nwith stride 4, and it takes 4 frames at the input\" - here ``input\" refers to\nthe input to the next step prediction model. It might be a good idea to rephrase\nthese sentences to make the distinction clear.\n\n(2) In order to better understand the space of affine transform\nparameters, it might help to include a histogram of these parameters in the\npaper. This can help us see at a glance, what is the typical range of these\n6 parameters, should we expect a lot of outliers, etc.\n\n(3) In order to compare transforms A and B, instead of ||A - B||^2, one\ncould consider A^{-1}B being close to identity as the metric. Did the authors\ntry this ?\n\n(4) \"The performance of the classifier on ground truth data is an upper bound on\nthe performance of any generative model.\" This is not *strictly* true. It is\npossible (though highly unlikely) that a generative model might make the data\nlook cleaner, sharper, or highlight some aspect of it which could improve the\nperformance of the classifier (even compared to ground truth). This is\nespecially true if the the generative model had access to the classifier, it\ncould then see what makes the classifier fire and highlight those discriminative\nfeatures in the generated output.\n\nOverall\nThis paper proposes future prediction in affine transform space. This does\nreduce blurriness and makes the videos look relatively realistic (at least to the\nC3D classifier). However, the paper can be improved by showing that the model can\npredict more non-trivial motion flows and the experiments can be strengthened by\nadding more classifiers besides than C3D.", "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "18 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "An interesting direction, but has many flaws", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "This paper describes an approach to predict (unseen) future frames of a video given a set of known past frames. The approach is based on a CNN that, in contrast to most related papers, work in the space of affine transformations (instead of pixels or flow). Said another way, the network takes as input a set of affine transforms that describe the motion of patches in the past frames, and likewise, outputs a set of affine transforms that predict future patch motion.\n\nTo that aim, the authors make a few simplifying hypotheses, namely, that a sequence of frames can be modeled accurately enough in their patch-affine framework. This is not unreasonable. A lot of papers in the optical flow community are based on similar hypotheses, i.e. model the flow as a smoothly varying affine field (for instance see \"Locally affine sparse-to-dense matching for motion and occlusion estimation\" by Leordeanu et al., \"EpicFlow: Edge-Preserving Interpolation of Correspondences for Optical Flow\" by Revaud et al., \"Optical Flow With Semantic Segmentation and Localized Layers\" by Sevilla-Lara et al.). These methods are state of the art, which gives a hint about the validity of this kind of approach. \nIn addition, it also seems very reasonable to reformulate the prediction task as predicting motion rather than predicting raw pixels. Indeed, the (patch-affine) motion space is considerably smaller than the image space, making the problem much more tractable and amenable to high-resolution videos.\n\nWhile I agree with the authors on these points, I also find that the paper suffer from important flaws. Specifically:\n\n  - the choice of not comparing with previous approaches in term of pixel prediction error seems very \"convenient\", to say the least. While it is clear that the evaluation metric is imperfect, it is not a reason to completely dismiss all quantitative comparisons with previous work. The frames output by the network on, e.g. the moving digits datasets (Figure 4), looks ok and can definitely be compared with other papers. Yet, the authors chose not to, which is suspicious.\n  \n  - The newly proposed metric poses several problems. First, action classification is evaluated with C3D, which is not a state-of-the-art approach at all for this task. Second, this metric actually *does not* evaluate what the network is claimed to do, that is, next frame prediction. Instead, it evaluates if another network, which was never trained to distinguish between real or synthetic frames by the way, can accurately classify an action from the predicted frames. I find that this proxy metric is only weakly related to what is supposed to be measured. In adition, it does not really make sense to train a network for something else that the final task it is evaluated for.\n  \n  - how is the affine motion of patches estimated? It is only explained that the problem is solved globally (not treating each patch independently) in a pretty vague manner. Estimating the motion of all patches is akin to solving the optical flow, which is still an active subject of research. Therefore, an important flaw of the paper lies in the potentially erroneous etimation of the motion input to the network. In the videos made available, it is clear that the motion is wrongly estimated sometimes. Since the entire approach depends on this input, I find it important to discuss this aspect. How do motion estimation failures impact the network? Also, the patch-affine hypothesis does not hold when patches are large enough that they cover several objects with contradictory motion. Which appears to be the case on UCF101 videos.\n  \n  - Even ignoring the weird proxy-evaluation part, the network is still not trained end-to-end. That is, the network is trained to minimize the difference between (noisy) ground-truth and output affine transforms, instead of minimizing a loss in the actual output space (frame pixels) for which an (exact) ground-truth is available. It is true that the MSE loss on raw pixels leads to blurry results, but other types of losses do exist, for instance the gradient loss introduced by Mathieu et al. was shown to solve this issue. As noted by the authors themselves, minimizing a loss in the transformation space, where affine parameters are harder to intepret, introduces unexpected artifacts. The motion is often largely underestimated, as is obvious in Figure 5 where it is hard to tell the difference between the input and output frames. \n  \n  - The proposed approach is not sufficiently compared to previous work. In particular, the approach is closely related to \"SPATIO-TEMPORAL VIDEO AUTOENCODER WITH DIFFERENTIABLE MEMORY\" of Taraucean et al, ICLR'15. This paper also output prediction in the motion space. Experimental results should compare against it.\n\n  - The comparison with optical flow is unfair. First, the approach of Brox et al. is more than 10 years old. Second, it is not really fair to assume a constant flow for all frames. At least some basic extrapolation could be done to take into account the flow of all pairs of input frames and not just the last one. Overall, the approach is not compared to very challenging baselines.\n\n  - I disagree with the answer that the authors gave to a reviewer's question. Denote ground-truth frames as {X_0, X_1 ...} and predicted frames as {Y_1, Y_2, ...}. When asked if the videos at ", "IS_META_REVIEW": false, "RECOMMENDATION": 3, "DATE": "14 Dec 2016 (modified: 16 Dec 2016)", "REVIEWER_CONFIDENCE": 3}, {"DATE": "08 Dec 2016", "TITLE": "Qualitative results", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1"}, {"DATE": "08 Dec 2016", "TITLE": "novelty-finetuning-RGB input", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2"}, {"DATE": "05 Dec 2016", "TITLE": "Question regarding related work", "IS_META_REVIEW": false, "comments": "Could the authors comment on the relationship between their work and these previous works that appear to use a similar transformation-based video prediction technique?\n\nDynamic Filter Networks (NIPS 2016)\nUnsupervised Learning for Physical Interaction through Video Prediction (NIPS 2016)\nVisual Dynamics: Probabilistic Future Frame Synthesis via Cross Convolutional Networks (NIPS 2016)", "OTHER_KEYS": "ICLR 2017 conference"}, {"DATE": "02 Dec 2016", "TITLE": "Reconstructions using ground truth affine transforms", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3"}, {"DATE": "13 Nov 2016", "TITLE": "Missing relevant reference and comparison? ", "IS_META_REVIEW": false, "comments": "The transformation-based approach for generating the next frame in a sequence was used in ICLRw2016 ", "OTHER_KEYS": "(anonymous)"}, {"IS_META_REVIEW": true, "comments": "This paper describes an approach to predict (unseen) future frames of a video given a set of known past frames. The approach is based on a CNN that, in contrast to most related papers, work in the space of affine transformations (instead of pixels or flow). Said another way, the network takes as input a set of affine transforms that describe the motion of patches in the past frames, and likewise, outputs a set of affine transforms that predict future patch motion.\n\nTo that aim, the authors make a few simplifying hypotheses, namely, that a sequence of frames can be modeled accurately enough in their patch-affine framework. This is not unreasonable. A lot of papers in the optical flow community are based on similar hypotheses, i.e. model the flow as a smoothly varying affine field (for instance see \"Locally affine sparse-to-dense matching for motion and occlusion estimation\" by Leordeanu et al., \"EpicFlow: Edge-Preserving Interpolation of Correspondences for Optical Flow\" by Revaud et al., \"Optical Flow With Semantic Segmentation and Localized Layers\" by Sevilla-Lara et al.). These methods are state of the art, which gives a hint about the validity of this kind of approach. \nIn addition, it also seems very reasonable to reformulate the prediction task as predicting motion rather than predicting raw pixels. Indeed, the (patch-affine) motion space is considerably smaller than the image space, making the problem much more tractable and amenable to high-resolution videos.\n\nWhile I agree with the authors on these points, I also find that the paper suffer from important flaws. Specifically:\n\n  - the choice of not comparing with previous approaches in term of pixel prediction error seems very \"convenient\", to say the least. While it is clear that the evaluation metric is imperfect, it is not a reason to completely dismiss all quantitative comparisons with previous work. The frames output by the network on, e.g. the moving digits datasets (Figure 4), looks ok and can definitely be compared with other papers. Yet, the authors chose not to, which is suspicious.\n  \n  - The newly proposed metric poses several problems. First, action classification is evaluated with C3D, which is not a state-of-the-art approach at all for this task. Second, this metric actually *does not* evaluate what the network is claimed to do, that is, next frame prediction. Instead, it evaluates if another network, which was never trained to distinguish between real or synthetic frames by the way, can accurately classify an action from the predicted frames. I find that this proxy metric is only weakly related to what is supposed to be measured. In adition, it does not really make sense to train a network for something else that the final task it is evaluated for.\n  \n  - how is the affine motion of patches estimated? It is only explained that the problem is solved globally (not treating each patch independently) in a pretty vague manner. Estimating the motion of all patches is akin to solving the optical flow, which is still an active subject of research. Therefore, an important flaw of the paper lies in the potentially erroneous etimation of the motion input to the network. In the videos made available, it is clear that the motion is wrongly estimated sometimes. Since the entire approach depends on this input, I find it important to discuss this aspect. How do motion estimation failures impact the network? Also, the patch-affine hypothesis does not hold when patches are large enough that they cover several objects with contradictory motion. Which appears to be the case on UCF101 videos.\n  \n  - Even ignoring the weird proxy-evaluation part, the network is still not trained end-to-end. That is, the network is trained to minimize the difference between (noisy) ground-truth and output affine transforms, instead of minimizing a loss in the actual output space (frame pixels) for which an (exact) ground-truth is available. It is true that the MSE loss on raw pixels leads to blurry results, but other types of losses do exist, for instance the gradient loss introduced by Mathieu et al. was shown to solve this issue. As noted by the authors themselves, minimizing a loss in the transformation space, where affine parameters are harder to intepret, introduces unexpected artifacts. The motion is often largely underestimated, as is obvious in Figure 5 where it is hard to tell the difference between the input and output frames. \n  \n  - The proposed approach is not sufficiently compared to previous work. In particular, the approach is closely related to \"SPATIO-TEMPORAL VIDEO AUTOENCODER WITH DIFFERENTIABLE MEMORY\" of Taraucean et al, ICLR'15. This paper also output prediction in the motion space. Experimental results should compare against it.\n\n  - The comparison with optical flow is unfair. First, the approach of Brox et al. is more than 10 years old. Second, it is not really fair to assume a constant flow for all frames. At least some basic extrapolation could be done to take into account the flow of all pairs of input frames and not just the last one. Overall, the approach is not compared to very challenging baselines.\n\n  - I disagree with the answer that the authors gave to a reviewer's question. Denote ground-truth frames as {X_0, X_1 ...} and predicted frames as {Y_1, Y_2, ...}. When asked if the videos at"}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "There were serious concerns raised about the originality of the work in regard to prior methods. The authors' responses to numerous reviewer questions on this matter were also unsatisfactory. In the end, it is difficult to tell what the contribution in regard to the video prediction model is. The proposed evaluation metric is interesting, but also raised serious concerns. Finally, several reviewers raised concerns about the quality of the results.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"TITLE": "reasonable paper but the contribution right now is very incremental compared to previous works", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "The paper proposes a method for future frame prediction based on transformation of previous frame rather than direct pixel prediction.\n\nMany previous works have proposed similar methods. The authors in their responses state that previous work is deterministic, yet the proposed model also does not handle multimodality.\n\nFurther, i asked if they could test their method using 2 RGB frames as input and predicting the transformation as output, to be able to quantify the importance of using transformations both as input and output, since this is the first work that uses transformations as input also. The authors dismissed the suggestion by saying \"if we were to use RGB frames as input and ask the model to output future frames it would produce very blurry results\", that is, misunderstanding what the suggestion was. So, currently, it does not seem to be a valid novel contribution in this work compared to previous works.", "IS_META_REVIEW": false, "RECOMMENDATION": 5, "DATE": "19 Dec 2016", "REVIEWER_CONFIDENCE": 3}, {"TITLE": "Reasonable paper, can be improved.", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "Paper Summary\nThis paper makes two contributions -\n(1) A model for next step prediction, where the inputs and outputs are in the\nspace of affine transforms between adjacent frames.\n(2) An evaluation method in which the quality of the generated data is assessed\nby measuring the reduction in performance of another model (such as a\nclassifier) when tested on the generated data.\n\nThe authors show that according to this metric, the proposed model works better\nthan other baseline models (including the recent work of Mathieu et al. which\nuses adversarial training).\n\nStrengths\n- This paper attempts to solve a major problem in unsupervised learning\n  with videos, which is evaluating them.\n- The results show that using MSE in transform space does prevent the blurring\n  problem to a large extent (which is one of the main aims of this paper).\n- The results show that the generated data reduces the performance of the C3D\n  model on UCF-101 to a much less extent than other baselines.\n- The paper validates the assumption that videos can be approximated to quite a\n  few time steps by a sequence of affine transforms starting from an initial\nframe.\n\nWeaknesses\n- The proposed metric makes sense only if we truly just care about the performance\n  of a particular classifier on a given task. This significantly narrows the\nscope of applicability of this metric because arguably, one the important\nreasons for doing unsupervised learning is to come up a representation that is\nwidely applicable across a variety of tasks. The proposed metric would not help\nevaluate generative models designed to achieve this objective.\n\n- It is possible that one of the generative models being compared will interact\n  with the idiosyncrasies of the chosen classifier in unintended ways.\nTherefore, it would be hard to draw strong conclusions about the relative\nmerits of generative models from the results of such experiments. One way to\nameliorate this would be to use several different classifiers (C3D,\ndual-stream network, other state-of-the-art methods) and show that the ranking\nof different generative models is consistent across the choice of classifier.\nAdding such experiments would help increase certainty in the conclusions drawn\nin this paper.\n\n- Using only 4 or 8 input frames sampled at 25fps seems like very little context\n  if we really expect the model to extrapolate the kind of motion seen in\nUCF-101. The idea of working in the space of affine transforms would be much\nmore appealing if the model can be shown to really generated non-trivial motion\npatterns. Currently, the motion patterns seem to be almost linear\nextrapolations.\n\n- The model that predicts motion does not have access to content at all. It only\n  gets access to previous motion. It seems that this might be a disadvantage\nbecause the motion predictor cannot use any cues like object boundaries, or\ndecide what to do when two motion fields collide (it is probably easier to argue\nabout occlusions in content space).\n\nQuality/Clarity\nThe paper is clearly written and easy to follow. The assumptions are clearly\nspecified and validated. Experimental details seem adequate.\n\nOriginality\nThe idea of generating videos by predicting motion has been used previously.\nSeveral recent papers also use this idea. However the exact implementation in\nthis paper is new. The proposed evaluation protocol is novel.\n\nSignificance\nThe proposed evaluation method is an interesting alternative, especially if it\nis extended to include multiple classifiers representative of different\nstate-of-the-art approaches. Given how hard it is to evaluate generative models\nof videos, this paper could help start an effort to standardize on a benchmark\nset.\n\nMinor comments and suggestions\n\n(1) In the caption for Table 1: ``Each column shows the accuracy on the test set\nwhen taking a different number of input frames as input\" - ``input\" here refers\nto the input to the classifier (Output of the next step prediction model). However\nin the next sentence ``Our approach maps 16 \\times 16 patches into 8 \\times 8\nwith stride 4, and it takes 4 frames at the input\" - here ``input\" refers to\nthe input to the next step prediction model. It might be a good idea to rephrase\nthese sentences to make the distinction clear.\n\n(2) In order to better understand the space of affine transform\nparameters, it might help to include a histogram of these parameters in the\npaper. This can help us see at a glance, what is the typical range of these\n6 parameters, should we expect a lot of outliers, etc.\n\n(3) In order to compare transforms A and B, instead of ||A - B||^2, one\ncould consider A^{-1}B being close to identity as the metric. Did the authors\ntry this ?\n\n(4) \"The performance of the classifier on ground truth data is an upper bound on\nthe performance of any generative model.\" This is not *strictly* true. It is\npossible (though highly unlikely) that a generative model might make the data\nlook cleaner, sharper, or highlight some aspect of it which could improve the\nperformance of the classifier (even compared to ground truth). This is\nespecially true if the the generative model had access to the classifier, it\ncould then see what makes the classifier fire and highlight those discriminative\nfeatures in the generated output.\n\nOverall\nThis paper proposes future prediction in affine transform space. This does\nreduce blurriness and makes the videos look relatively realistic (at least to the\nC3D classifier). However, the paper can be improved by showing that the model can\npredict more non-trivial motion flows and the experiments can be strengthened by\nadding more classifiers besides than C3D.", "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "18 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "An interesting direction, but has many flaws", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "This paper describes an approach to predict (unseen) future frames of a video given a set of known past frames. The approach is based on a CNN that, in contrast to most related papers, work in the space of affine transformations (instead of pixels or flow). Said another way, the network takes as input a set of affine transforms that describe the motion of patches in the past frames, and likewise, outputs a set of affine transforms that predict future patch motion.\n\nTo that aim, the authors make a few simplifying hypotheses, namely, that a sequence of frames can be modeled accurately enough in their patch-affine framework. This is not unreasonable. A lot of papers in the optical flow community are based on similar hypotheses, i.e. model the flow as a smoothly varying affine field (for instance see \"Locally affine sparse-to-dense matching for motion and occlusion estimation\" by Leordeanu et al., \"EpicFlow: Edge-Preserving Interpolation of Correspondences for Optical Flow\" by Revaud et al., \"Optical Flow With Semantic Segmentation and Localized Layers\" by Sevilla-Lara et al.). These methods are state of the art, which gives a hint about the validity of this kind of approach. \nIn addition, it also seems very reasonable to reformulate the prediction task as predicting motion rather than predicting raw pixels. Indeed, the (patch-affine) motion space is considerably smaller than the image space, making the problem much more tractable and amenable to high-resolution videos.\n\nWhile I agree with the authors on these points, I also find that the paper suffer from important flaws. Specifically:\n\n  - the choice of not comparing with previous approaches in term of pixel prediction error seems very \"convenient\", to say the least. While it is clear that the evaluation metric is imperfect, it is not a reason to completely dismiss all quantitative comparisons with previous work. The frames output by the network on, e.g. the moving digits datasets (Figure 4), looks ok and can definitely be compared with other papers. Yet, the authors chose not to, which is suspicious.\n  \n  - The newly proposed metric poses several problems. First, action classification is evaluated with C3D, which is not a state-of-the-art approach at all for this task. Second, this metric actually *does not* evaluate what the network is claimed to do, that is, next frame prediction. Instead, it evaluates if another network, which was never trained to distinguish between real or synthetic frames by the way, can accurately classify an action from the predicted frames. I find that this proxy metric is only weakly related to what is supposed to be measured. In adition, it does not really make sense to train a network for something else that the final task it is evaluated for.\n  \n  - how is the affine motion of patches estimated? It is only explained that the problem is solved globally (not treating each patch independently) in a pretty vague manner. Estimating the motion of all patches is akin to solving the optical flow, which is still an active subject of research. Therefore, an important flaw of the paper lies in the potentially erroneous etimation of the motion input to the network. In the videos made available, it is clear that the motion is wrongly estimated sometimes. Since the entire approach depends on this input, I find it important to discuss this aspect. How do motion estimation failures impact the network? Also, the patch-affine hypothesis does not hold when patches are large enough that they cover several objects with contradictory motion. Which appears to be the case on UCF101 videos.\n  \n  - Even ignoring the weird proxy-evaluation part, the network is still not trained end-to-end. That is, the network is trained to minimize the difference between (noisy) ground-truth and output affine transforms, instead of minimizing a loss in the actual output space (frame pixels) for which an (exact) ground-truth is available. It is true that the MSE loss on raw pixels leads to blurry results, but other types of losses do exist, for instance the gradient loss introduced by Mathieu et al. was shown to solve this issue. As noted by the authors themselves, minimizing a loss in the transformation space, where affine parameters are harder to intepret, introduces unexpected artifacts. The motion is often largely underestimated, as is obvious in Figure 5 where it is hard to tell the difference between the input and output frames. \n  \n  - The proposed approach is not sufficiently compared to previous work. In particular, the approach is closely related to \"SPATIO-TEMPORAL VIDEO AUTOENCODER WITH DIFFERENTIABLE MEMORY\" of Taraucean et al, ICLR'15. This paper also output prediction in the motion space. Experimental results should compare against it.\n\n  - The comparison with optical flow is unfair. First, the approach of Brox et al. is more than 10 years old. Second, it is not really fair to assume a constant flow for all frames. At least some basic extrapolation could be done to take into account the flow of all pairs of input frames and not just the last one. Overall, the approach is not compared to very challenging baselines.\n\n  - I disagree with the answer that the authors gave to a reviewer's question. Denote ground-truth frames as {X_0, X_1 ...} and predicted frames as {Y_1, Y_2, ...}. When asked if the videos at ", "IS_META_REVIEW": false, "RECOMMENDATION": 3, "DATE": "14 Dec 2016 (modified: 16 Dec 2016)", "REVIEWER_CONFIDENCE": 3}, {"DATE": "08 Dec 2016", "TITLE": "Qualitative results", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1"}, {"DATE": "08 Dec 2016", "TITLE": "novelty-finetuning-RGB input", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2"}, {"DATE": "05 Dec 2016", "TITLE": "Question regarding related work", "IS_META_REVIEW": false, "comments": "Could the authors comment on the relationship between their work and these previous works that appear to use a similar transformation-based video prediction technique?\n\nDynamic Filter Networks (NIPS 2016)\nUnsupervised Learning for Physical Interaction through Video Prediction (NIPS 2016)\nVisual Dynamics: Probabilistic Future Frame Synthesis via Cross Convolutional Networks (NIPS 2016)", "OTHER_KEYS": "ICLR 2017 conference"}, {"DATE": "02 Dec 2016", "TITLE": "Reconstructions using ground truth affine transforms", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3"}, {"DATE": "13 Nov 2016", "TITLE": "Missing relevant reference and comparison? ", "IS_META_REVIEW": false, "comments": "The transformation-based approach for generating the next frame in a sequence was used in ICLRw2016 ", "OTHER_KEYS": "(anonymous)"}]}
{"text": "1 INTRODUCTION\nIt is widely believed that deeper networks tend to achieve better performance than shallow ones in various computer vision tasks. As a trade-off of such impressive improvements, deeper networks impose heavy computational load both in terms of processing time and memory consumption due to an enormous amount of network parameters. For example, VGG-16 model (Simonyan & Zisserman, 2015) requires about 528 MBytes to store the network weights where fully connected layers account for 89% of them. A large number of multiplications and additions must also be processed at each layer which prevent real-time processing, consume vast amounts of electricity, and require a large number of logic gates when implementing a deep network on a FPGA or ASIC.\nThis article addresses the above issues. Specifically, we aimed to reduce the test-time computational load of a pre-trained network. Since our approach does not depend on a network configuration (e.g. a choice of an activation function, layer structures, and a number of neurons) and acts as a post-processing of network training, pre-trained networks shared in a download site of MatConvNet (Vedaldi & Lenc, 2015) and Model Zoo (BVLC) can be compressed and accelerated. Our method is outlined in Figure 1. The main idea is to factorize both weights and activations into integer and non-integer components. Our method is composed of two building blocks, as shown below.\nTernary weight decomposition for memory compression: We introduce a factored representation where the real-valued weight matrix is approximated by a multiplication of a ternary basis matrix and a real-valued co-efficient matrix. While the ternary basis matrix is sufficiently informative to reconstruct the original weights, it only consumes 2 bits per element. The number of rows of the coefficient matrix is also smaller than that of the original weight matrix. These compact representations result in efficient memory compression.\nBinary activation encoding for fast feed-forward propagation: It has been reported that an inner product between a ternary and binary vector can be computed extremely fast by using three logical operations: AND, XOR, and bit count (Ambai & Sato, 2014). To use this technique, we approximate the activation vector by a weighted sum of binary vectors. This binary encoding must be processed as fast as possible at test-time. To overcome this issue, we use a fast binary encoding method based on a small lookup table.\n1.1 RELATED WORK\nThere have been extensive studies on accelerating and compressing deep neural networks, e.g., on an FFT-based method (Mathieu et al., 2014), re-parameterization of a weight matrix (Yang et al., 2015), pruning network connection (Han et al., 2015; 2016), and hardware-specific optimization (Vanhoucke et al., 2011). In the following paragraphs, we only review previous studies that are intimately connected to ours.\nIt was pointed out by Denil et al. (2013) that network weights have a significant redundancy. Motivated by this fact, researchers have been involved in a series of studies on matrix/tensor factorization (Jaderberg et al., 2014; Zhang et al., 2015). In these studies, a weight matrix (or tensor) was factorized by minimizing an approximation error of original weights or activations. Jaderberg et al. (2014) exploited 1-D separable filter decomposition to accelerate feed-forward propagation. Zhang et al. (2015) proposed low-rank approximation based on generalized SVD to compress an entire deep network. Taking into account the lessons learned from these best practices, we also exploit the redundancy of the weights.\nThere is an another series of studies, integer decomposition (Hare et al., 2012; Yuji et al., 2014; Ambai & Sato, 2014), which involved accelerating test-time speed of a classifier by using fast logical operations. Although their contributions are limited to a shallow architecture such as a linear SVM, they achieved a noticeable acceleration. In these approaches, a real-valued weight vector is approximated by a weighted sum of a few binary or ternary basis vectors. To use fast logical operations, they extracted binary features from an image. Hare et al. (2012) and Yuji et al. (2014) exploited binary basis vectors, and Ambai & Sato (2014) investigated a case of ternary basis to improve approximation quality.\nIn a manner of speaking, our method is a unified framework of matrix/tensor factorization and integer decomposition reviewed in the above and inherits both their advantages. While the weight matrix is factorized to exploit low-rank characteristics, the basis matrix is restricted to take only three integer values, {\u22121, 0,+1}. In contrast to recent binary weighted networks such as XNOR-Net (Rastegari et al., 2016) which quantizes both activations and weights during backpropagation, it is not necessary for our method to change training algorithms at all. We can benefit from recent sophisticated training techniques, e.g. batch normalization (Ioffe & Szegedy, 2015), in combination with our method. Furthermore, our method does not need (iterative) end-to-end retraining which is needed for several previous studies such as network pruning (Han et al., 2015; 2016) and distillation (Hinton et al., 2014).\n2 NETWORK COMPRESSION MODEL\nIn this section, we introduce our compression model and discuss time and space complexity. We consider a convolutional layer with a filter size of wx \u00d7 wy \u00d7 c, where wx and wy are the spacial size and c is a number of input channels. If wx = wy = 1, we can regard this layer as a fully connected layer. This three dimensional volume is reshaped to form a DI dimensional vector where DI = wx\u00d7wy\u00d7c. The filter weights and biases can be formulated by W \u2208 RDI\u00d7DO and b \u2208 RDO , where DO is a number of output channels. Let x \u2208 RDI denote an activation vector obtained by\nvectorizing the corresponding three dimensional volume. In test-time, we need to compute W>x+b followed by a non-linear activation function.\nIn our compressed network, W is decomposed into two matrices before test-time as follows:\nW \u2248MwCw, (1)\nwhere Mw \u2208 {\u22121, 0,+1}DI\u00d7kw is a ternary basis matrix, Cw \u2208 Rkw\u00d7DO is a co-efficient matrix, and kw is the number of basis vectors, respectively. Since Mw only takes the three values, it consumes only 2 bits per element. Setting a sufficiently small value to kw further reduces total memory consumption. From the viewpoint of approximation quality, it should be noted that a large number of elements in W takes close to zero values. To fit them well enough, a zero value must be included in the basis. The ternary basis satisfies this characteristic. In practice, the ternary basis gives better approximation than the binary basis, as we discuss in Section 3.\nThe activation vector x is also factored to the following form:\nx \u2248Mxcx + bx1, (2)\nwhere Mx \u2208 {\u22121,+1}DI\u00d7kx is a binary basis matrix, cx \u2208 Rkx is a real-valued co-efficient vector, bx \u2208 R is a bias, and kx is the number of basis vectors, respectively. Since elements of x are often biased, e.g., activations from ReLU take non-negative values and have a non-zero mean, bx is added to this decomposition model. While cx and bx reflect a range of activation values, Mx determines approximated activation values within the defined range. This factorization must be computed at test-time because the intermediate activations depend on an input to the first layer. However, in practice, factorizing x into Mx, cx, and bx requires an iterative optimization, which is very slow. Since a scale of activation values within a layer is almost similar regardless of x, we pre-computed canonical cx and bx in advance and only optimized Mx at test-time. As we discuss in Section 4, an optimal Mx under fixed cx and bx can be selected using a lookup table resulting in fast factorization.\nSubstituting Eqs.(1) and (2) into W>x+ b, approximated response values are obtained as follows:\nW>x+ b \u2248 (MwCw)>(Mxcx + bx1) + b = C>wM > wMxcx + bxC > wM > w1+ b. (3)\nA new bias bxC>wM > w1+ b in Eq.(3) is pre-computable in advance, because Cw,Mw, and bx are fixed at test-time. It should be noted that M>wMx is a multiplication of the ternary and binary matrix, which is efficiently computable using three logical operations: XOR, AND, and bit count, as previously investigated (Ambai & Sato, 2014). After computing M>wMx, the two co-efficient components, cx and Cw, are multiplied from the right and left in this order. Since cx and Cw are much smaller than W, the total number of floating point computations is drastically reduced.\nThe time and space complexity are summarized in Tables 1 and 2. As can be seen from Table 1, most of the floating operations are replaced with logical operations. In this table, B means the bit width of a variable used in the logical operations, e.g., B = 64 if a type of unsigned long long is used in C/C++ language. Table 2 suggests that if kw is sufficiently smaller than DI and DO, the total size of Mw and Cw is reduced compared to the original parameterization.\nAlgorithm 1 Decompose W into Mw and Cw Require: W, kw Ensure: factorized components Mw and Cw.\n1: R\u2190W 2: for i\u2190 1 to kw do 3: Initialize m(i)w by three random values {\u22121, 0,+1}. 4: Minimize ||R\u2212m(i)w c(i)w ||2F by repeating the following two steps until convergence. 5: [Step 1] c(i)w \u2190m(i)>w R/m(i)>w m(i)w 6: [Step 2] mij \u2190 arg min\n\u03b1\u2208{\u22121,0,+1} ||rj \u2212 \u03b1c(i)w ||22, for j = 1, \u00b7 \u00b7 \u00b7 , DI\n7: R\u2190 R\u2212m(i)w c(i)w 8: end for\n3 TERNARY WEIGHT DECOMPOSITION\nTo factorize W, we need to solve the following optimization problem.\nJw = min Mw,Cw\n||W \u2212MwCw||2F . (4)\nHowever, the ternary constraint makes this optimization very difficult. Therefore, we take an iterative approach that repeats rank-one approximation one by one, as shown in Algorithm 1. Let m (i) w \u2208 {\u22121, 0,+1}DI\u00d71 denote an i-th column vector of Mw and c(i)w \u2208 R1\u00d7DO denote an i-th row vector of Cw. Instead of directly minimizing Eq. (4), we iteratively solve the following rank-one approximation,\nJ (i)w = min m(i)w ,c(i)w ||R\u2212m(i)w c(i)w ||2F , (5)\nwhere R is a residual matrix initialized by W. We applied alternating optimization to obtain m(i)w and c(i)w . If m (i) w is fixed, c (i) w can be updated using a least squares method, as shown in line 5 of Algorithm 1. If c(i)w is fixed, mij , the j-th element of m (i) w , can be independently updated by exhaustively verifying three choices {\u22121, 0,+1} for each j = 1, \u00b7 \u00b7 \u00b7 , DI , as shown in line 6 of Algorithm 1, where rj is a j-th row vector of R. After the alternating optimization is converged, R is updated by subtracting m(i)w c (i) w and passed to the next (i+ 1)-th iteration. Comparison of binary constraints with ternary constraints can be seen in Appendix A.\n4 BINARY ACTIVATION ENCODING\nBinary decomposition for a given activation vector x can be performed by minimizing\nJx(Mx, cx, bx;x) = ||x\u2212 (Mxcx + bx1)||22. (6)\nIn contrast to the case of decomposing W, a number of basis vectors kx can be set to a very small value (from 2 to 4 in practice) because x is not a matrix but a vector. This characteristic enables an exhaustive search for updating Mx. Algorithm 2 is an alternating optimization with respect to Mx, cx, and bx. By fixing Mx, we can apply a least squares method to update cx and bx (in lines 3-4 of Algorithm 2). If cx and bx are fixed, m (j) x , the j-th row vector of Mx, is independent of any other m (j\u2032) x , j\u2032 6= j. We separately solve DI sub-problems formulated as follows:\nm(j)x = arg min \u03b2\u2208{\u22121,+1}1\u00d7kx (xj \u2212 (\u03b2cx + bx))2, j = 1, \u00b7 \u00b7 \u00b7 , DI , (7)\nwhere xj is a j-th element of x. Since kx is sufficiently small, 2kx possible solutions can be exhaustively verified (in line 5 of Algorithm 2).\nOur method makes this decomposition faster by pre-computing canonical cx and bx from training data and only optimizing Mx at test-time using lookup table. This compromise is reasonable because of the following two reasons: (1) scale of activation values is similar regardless of vector elements\nAlgorithm 2 Decompose x into Mx, cx, and bx Require: x, kx Ensure: factorized components Mx, cx, and bx.\n1: Initialize Mx by three random values {\u22121,+1}. 2: Minimize ||x\u2212 (Mxcx + bx1)||22 by repeating the following two steps until convergence. 3: [Step 1] Update cx and bx using a least squares method. 4: cx \u2190 (M>xMx)\u22121M > x (x\u2212 bx1), bx \u2190 1>(x\u2212Mxcx)/DI 5: [Step 2] Update m(j)x for each j = 1, \u00b7 \u00b7 \u00b7DI by an exhaustive search that minimizes Eq.(7).\nwithin a layer, and (2) cx and bx reflect a scale of approximated activation values. Knowing these properties, cx and bx are obtained by minimizing Jx(M\u0302x, cx, bx; x\u0302) ,where x\u0302 is constructed as follows. First, NT different activation vectors T \u2208 {xi}NTi=1 are collected from randomly chosen NT training data. Second, n elements are randomly sampled from xi. The sampled nNT elements are concatenated to form a vector x\u0302 \u2208 RnNT . We use cx and bx as constants at test-time, and discard M\u0302x.\nAt test-time, we only need to solve the optimization of Eq. (7) for each xj . This can be regarded as the nearest neighbour search in one-dimensional space. We call \u03b2cx + bx a prototype. There are 2kx possible prototypes because \u03b2 takes 2kx possible combinations. The nearest prototype to xj and an optimal solution m(j)x can be efficiently found using a lookup table as follows.\nPreparing lookup table: We define L bins that evenly divide one-dimensional space in a range from the smallest to largest prototype. Let x\u0302l denote a representative value of the l-th bin. This is located at the center of the bin. For each x\u0302l, we solve Eq. (7) and assign the solution to the bin.\nActivation encoding: At test-time, xj is quantized into L-levels. In other words, xj is transformed to an index of the lookup table. Let pmax and pmin denote the largest and smallest prototype, respectively. We transform xj as follows:\nq = (L\u2212 1)(xj \u2212 pmin)/(pmax \u2212 pmin) + 1, (8) l\u0302 = min(max(bq + 1/2c, 1), L). (9)\nThe range from pmin to pmax is linearly mapped to the range from 1 to L by Eq. (8). The term q is rounded and truncated from 1 to L by the max and min function in Eq. (9). If L is sufficiently large, the solution assigned to the l\u0302-th bin can be regarded as a nearly optimal solution because the difference between xj and the center of the bin x\u0302l\u0302 becomes very small. We found that L = 4096 is sufficient. The time complexity of this encoding is O(DI).\n5 EXPERIMENTS\nWe tested our method on three different convolutional neural networks: CNN for handwritten digits (LeCun et al., 1998), VGG-16 for ImageNet classification (Simonyan & Zisserman, 2015), and VGGFace for large-scale face recognition (Parkhi et al., 2015). To compute memory compression rate, a size of W and a total size of Mw and Cw were compared. To obtain a fair evaluation of computation time, a test-time code of forward propagation was implemented without using any parallelization scheme, e.g., multi-threading or SIMD, and was used for both compressed and uncompressed networks. The computation time includes both binary activation encoding and calculation of Eq. (3). We used an Intel Core i7-5500U 2.40-GHz processor.\n5.1 CNN FOR HANDWRITTEN DIGITS\nMNIST is a database of handwritten digits which consists of 60000 training and 10000 test sets of 28\u00d7 28 gray-scale images with ground-truth labels from 0 to 9. We trained our CNN by using an example code in MatConvNet 1.0-beta18 (Vedaldi & Lenc, 2015). Our architecture is similar to LeNet-5 (LeCun et al., 1998) but has a different number of input and output channels. Each layer\u2019s configuration is shown below:\n(conv5-20)(maxpool)(conv5-64)(maxpool)(fc1024-640)(relu)(fc640-10)(softmax), (10)\nwhere the parameters of a convolutional layer are denoted as (conv<receptive field size>-<number of output channels>), and parameters of a fully connected layer are denoted as (fc<number of input channels>-<number of output channels>). The (maxpool) is 2\u00d72 subsampling without overlapping. The error rate of this network is 0.86%.\nWe applied our method to the first fully connected layer (fc1024-640) and set n = 10 andNT = 1000 to learn cx and bx from randomly chosen nNT activations. The cases of kx = 1, 2, 3, 4 and kw = DO, DO/2, DO/5 were tested. This means that kw was set to 640, 320, and 128.\nFigures 2(a) and (b) show the relationships among the increases in error rates, memory compression rates, and acceleration rates. It was observed that error rates basically improved along with increasing kx and saturated at kx = 4. It is interesting that kx = 2, only 2 bits per element for encoding an activation x, still achieved good performance. While the smaller kw achieved better compression and acceleration rate, error rates rapidly increased when kw = DO/5. One of the well balanced parameters was (kx, kw) = (4, DO/2) which resulted in 1.95\u00d7 faster processing and a 34.4% memory compression rate in exchange of a 0.19% increase in the error rate.\n5.2 VGG-16 FOR IMAGENET CLASSIFICATION TASK\nA dataset of ILSVRC2012 (Russakovsky et al., 2015) consists of 1.2 million training, 50,000 validation, and 100,000 test sets. Each image represents one of 1000 object categories. In this experiment, we used a network model of VGG-16 (model D in (Simonyan & Zisserman, 2015)) that consists of 13 convolutional layers and 3 fully connected layers followed by a softmax layer. The architecture is shown below:\n(input) \u00b7 \u00b7 \u00b7 (fc25088-4096)(relu)(fc4096-4096)(relu)(fc4096-1000)(softmax), (11) where layers before the first fully connected layer are omitted.\nFirst, all three fully connected layers were compressed with our algorithm. We set n = 10 and NT = 1000 to learn cx and bx from randomly chosen nNT activations. The cases of kx = 2, 3, 4 and kw = DO/2, DO/4, DO/8, DO/16 were tested. The case of kx = 1 was omitted because this setting resulted in a very high error rate. Note that each of the fully connected layers has different DO. The kw was independently set for each layer according to its DO. The top-5 error rates were evaluated on the validation dataset. The top-5 error rate of the original network is 13.4%.\nThe three lines with circles in Figure 3 show these results. It should be noted that much higher acceleration rates and smaller compression rates with small loss of accuracies were achieved than the case of the network for MNIST. Interestingly, the case of kw = DO/4 still performed well due to the low-rank characteristics of weights in the VGG-16 network.\nAlthough the error rates rapidly increased when kw took much smaller values, we found that this could be improved by tuning kw of the third layer. More specifically, we additionally tested the\nfollowing cases. While kw was set to DO/2, DO/4, DO/8, and DO/16 for the first and second layers, kw was fixed to DO for the third layer. The kx was set to 4. This is plotted with a red line in Figure 3. In this way, the memory compression rate and acceleration rate noticeably improved. Setting appropriate parameters for each layer is important to improve the total performance. Table 3 shows the details of the best balanced case in which 15\u00d7 faster processing and 5.2% compression rate were achieved in exchange of a 1.43% increase in error rate.\nNext, we also tested to compress convolutional layers. In this experiment, kw and kx were set to DO and 4. This setting accelerates each of the layers averagely 2.5 times faster. Table 4 shows positions of compressed layers, top-5 errors, and acceleration rates of the entire network. Although kw and kx must be larger than those of fully connected layers to avoid error propagation, it is still beneficial for entire acceleration. In summary, while compressing fully connected layers is beneficial for reducing memory, compressing convolutional layers is beneficial for reducing entire computation time.\n5.3 VGG-FACE FOR FACE RECOGNITION TASK\nThe VGG-Face (Parkhi et al., 2015) is a model for extracting a face descriptor. It consists of a similar structure to VGG-16. The difference is that VGG-Face has only two fully connected layers, as shown below. (input) \u00b7 \u00b7 \u00b7 (fc25088-4096)(relu)(fc4096-4096). (12) This network outputs a 4096-dimensional descriptor. We can verify whether two face images are identical, by evaluating the Euclidean distance of two l2-normalized descriptors extracted from\nthem. In our experiment, we did not apply a descriptor embedding technique based on triplet loss minimization (Parkhi et al., 2015). Following the evaluation protocol introduced in a previous paper (Parkhi et al., 2015), we used Labeled Faces in the Wild dataset (LFW) (Huang et al., 2007), which includes 13,233 face images with 5,749 identities. The LFW defines 1200 positive and 1200 negative pairs for testing. We used the 2400 test pairs to compute ROC curve and equal error rate (EER). The EER is defined as an error rate at the ROC operating point where the false positive and false negative rates are equal. The EER of the original network is 3.8%.\nFirst, the two fully connected layers were compressed using our algorithm. We set n = 10 and NT = 1000 to learn cx and bx from randomly chosen nNT activations. We tested the cases of kx = 1, 2, 3, 4, and kw = DO/2, DO/4, DO/8, DO/16. Figure 4 reveals an interesting fact that even the fastest and smallest network configuration, kx = 1 and kw = DO/16, had less impact on the EER, in contrast to the previous ImageNet classification task in which the recognition results were corrupted when kx = 1. This indicates that the 4096-dimensional feature space is well preserved regardless of such coarse discretization of both weights and activations.\nNext, we also tested to compress convolutional layers. In this experiment, kw and kx were set to DO and 4 which are the the same setting used in Table 4. Table 5 shows positions of compressed layers and EERs. The acceleration rates were almost the same as the results shown in Table 4. This is because architecture of VGG-face is the same as VGG-16 and we used the same parameter for kw and kx. Interestingly, compressing multiple layers from 2nd to 10th still preserves the original EER. As can be seen from this table, our method works very well depending on a certain kind of machine learning task.\n6 CONCLUSION\nWe proposed a network compression model that consists of two components: ternary matrix decomposition and binary activation encoding. Our experiments revealed that the proposed compression model is available not only for multi-class recognition but also for feature embedding. Since our approach is post-processing for a pre-trained model, it is promising that recent networks designed for semantic segmentation, describing images, stereo matching, depth estimation, and much more can also be compressed with our method. For future work, we plan to improve approximation error further by investigating the discrete optimization algorithm.\nA BINARY VS. TERNARY\nFigure 5 illustrates the reconstruction errors of a 4096\u00d71000 weight matrix of the last fully connected layer in VGG-16 model (Simonyan & Zisserman, 2015). We tested both the binary and ternary constraints on Mw for comparison. The reconstruction error Jw monotonically decreased along with an increase in kw. It was clear that the ternary basis provided better reconstruction than the binary basis.\n", "cats": {"READ": 1.0, "DONTREAD": 0.0}, "reviews": [{"IS_META_REVIEW": true, "comments": "This paper explores a new quantization method for both the weights and the activations that does not need re-training. In VGG-16 the method reaches compression ratios of 20x and experiences a speed-up of 15x. The paper is very well written and clearly exposes the details of the methodology and the results.\n\nMy major criticisms are three-fold: for one, the results are not compared to one of the many other pruning methods that are described in section 1.1, and as such the performance of the method is difficult to judge from the paper alone. Second, there have been several other compression schemes involving pruning, re-training and vector-quantization [e.g. 1, 2, 3] that seem to achieve much higher accuracies, compression ratios and speed-ups. Hence, for the practical application of running such networks on low-power, low-memory devices, other methods seem to be much more suited. The advantage of the given method - other then possibly reducing the time it takes to compress the network - is thus unclear. In particular, taking a pre-trained network as a starting point for a quantized model that is subsequently fine-tuned might not take much longer to process then the method given here (but maybe the authors can quantify this?). Finally, much of the speed-up and memory reduction in the VGG-model seems to arise from the three fully-connected layers, in particular the last one. The speed-up in the convolutional layers is comparably small, making me wonder how well the method would work in all-convolutional networks such as the Inception architecture.\n\n[1] Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding,"}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "The paper presents a method for quantizing neural network weights and activations. The method is not compared to related state-of-the-art quantization techniques, so in the current form the paper is not ready for acceptance.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"TITLE": "Review", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "This paper addresses to reduce test-time computational load of DNNs. Another factorization approach is proposed and shows good results. The comparison to the other methods is not comprehensive, the paper provides good insights.", "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "19 Dec 2016", "REVIEWER_CONFIDENCE": 3}, {"TITLE": "Clarify my comments", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "I do need to see the results in a clear table. Original results and results when compression is applied for all the tasks. In any case, i would like to see the results when the compression is applied to state of the art nets where the float representation is important. For instance a network with 0.5% - 0.8% in MNIST. A Imagenet lower that 5% - 10%. Some of this results are feasible with float representation but probably imposible for restricted representations.", "IS_META_REVIEW": false, "RECOMMENDATION": 5, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 3}, {"TITLE": "Novel quantization method to reduce memory and complexity of pre-trained networks, but benefit over other methods is unclear", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "This paper explores a new quantization method for both the weights and the activations that does not need re-training. In VGG-16 the method reaches compression ratios of 20x and experiences a speed-up of 15x. The paper is very well written and clearly exposes the details of the methodology and the results.\n\nMy major criticisms are three-fold: for one, the results are not compared to one of the many other pruning methods that are described in section 1.1, and as such the performance of the method is difficult to judge from the paper alone. Second, there have been several other compression schemes involving pruning, re-training and vector-quantization [e.g. 1, 2, 3] that seem to achieve much higher accuracies, compression ratios and speed-ups. Hence, for the practical application of running such networks on low-power, low-memory devices, other methods seem to be much more suited. The advantage of the given method - other then possibly reducing the time it takes to compress the network - is thus unclear. In particular, taking a pre-trained network as a starting point for a quantized model that is subsequently fine-tuned might not take much longer to process then the method given here (but maybe the authors can quantify this?). Finally, much of the speed-up and memory reduction in the VGG-model seems to arise from the three fully-connected layers, in particular the last one. The speed-up in the convolutional layers is comparably small, making me wonder how well the method would work in all-convolutional networks such as the Inception architecture.\n\n[1] Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding, ", "IS_META_REVIEW": false, "RECOMMENDATION": 4, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"DATE": "15 Dec 2016", "TITLE": "Suggestion for missing reference", "IS_META_REVIEW": false, "comments": "I suggest to refer the following two papers.\n\n- Kyuyeon Hwang and Wonyong Sung. \"Fixed-point feedforward deep neural network design using weights +1, 0, and \u22121.\" 2014 IEEE Workshop on Signal Processing Systems (SiPS). IEEE, 2014.\n\n- Jonghong Kim, Kyuyeon Hwang, and Wonyong Sung. \"X1000 real-time phoneme recognition VLSI using feed-forward deep neural networks.\" 2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2014.\n\nThe retrain-based neural network quantization algorithm was first published in these two papers.\n\nThanks.", "OTHER_KEYS": "Sungho Shin"}, {"DATE": "09 Dec 2016", "TITLE": "Comparison", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1"}, {"DATE": "02 Dec 2016", "TITLE": "Improve results ", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2"}, {"IS_META_REVIEW": true, "comments": "This paper explores a new quantization method for both the weights and the activations that does not need re-training. In VGG-16 the method reaches compression ratios of 20x and experiences a speed-up of 15x. The paper is very well written and clearly exposes the details of the methodology and the results.\n\nMy major criticisms are three-fold: for one, the results are not compared to one of the many other pruning methods that are described in section 1.1, and as such the performance of the method is difficult to judge from the paper alone. Second, there have been several other compression schemes involving pruning, re-training and vector-quantization [e.g. 1, 2, 3] that seem to achieve much higher accuracies, compression ratios and speed-ups. Hence, for the practical application of running such networks on low-power, low-memory devices, other methods seem to be much more suited. The advantage of the given method - other then possibly reducing the time it takes to compress the network - is thus unclear. In particular, taking a pre-trained network as a starting point for a quantized model that is subsequently fine-tuned might not take much longer to process then the method given here (but maybe the authors can quantify this?). Finally, much of the speed-up and memory reduction in the VGG-model seems to arise from the three fully-connected layers, in particular the last one. The speed-up in the convolutional layers is comparably small, making me wonder how well the method would work in all-convolutional networks such as the Inception architecture.\n\n[1] Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding,"}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "The paper presents a method for quantizing neural network weights and activations. The method is not compared to related state-of-the-art quantization techniques, so in the current form the paper is not ready for acceptance.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"TITLE": "Review", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "This paper addresses to reduce test-time computational load of DNNs. Another factorization approach is proposed and shows good results. The comparison to the other methods is not comprehensive, the paper provides good insights.", "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "19 Dec 2016", "REVIEWER_CONFIDENCE": 3}, {"TITLE": "Clarify my comments", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "I do need to see the results in a clear table. Original results and results when compression is applied for all the tasks. In any case, i would like to see the results when the compression is applied to state of the art nets where the float representation is important. For instance a network with 0.5% - 0.8% in MNIST. A Imagenet lower that 5% - 10%. Some of this results are feasible with float representation but probably imposible for restricted representations.", "IS_META_REVIEW": false, "RECOMMENDATION": 5, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 3}, {"TITLE": "Novel quantization method to reduce memory and complexity of pre-trained networks, but benefit over other methods is unclear", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "This paper explores a new quantization method for both the weights and the activations that does not need re-training. In VGG-16 the method reaches compression ratios of 20x and experiences a speed-up of 15x. The paper is very well written and clearly exposes the details of the methodology and the results.\n\nMy major criticisms are three-fold: for one, the results are not compared to one of the many other pruning methods that are described in section 1.1, and as such the performance of the method is difficult to judge from the paper alone. Second, there have been several other compression schemes involving pruning, re-training and vector-quantization [e.g. 1, 2, 3] that seem to achieve much higher accuracies, compression ratios and speed-ups. Hence, for the practical application of running such networks on low-power, low-memory devices, other methods seem to be much more suited. The advantage of the given method - other then possibly reducing the time it takes to compress the network - is thus unclear. In particular, taking a pre-trained network as a starting point for a quantized model that is subsequently fine-tuned might not take much longer to process then the method given here (but maybe the authors can quantify this?). Finally, much of the speed-up and memory reduction in the VGG-model seems to arise from the three fully-connected layers, in particular the last one. The speed-up in the convolutional layers is comparably small, making me wonder how well the method would work in all-convolutional networks such as the Inception architecture.\n\n[1] Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding, ", "IS_META_REVIEW": false, "RECOMMENDATION": 4, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"DATE": "15 Dec 2016", "TITLE": "Suggestion for missing reference", "IS_META_REVIEW": false, "comments": "I suggest to refer the following two papers.\n\n- Kyuyeon Hwang and Wonyong Sung. \"Fixed-point feedforward deep neural network design using weights +1, 0, and \u22121.\" 2014 IEEE Workshop on Signal Processing Systems (SiPS). IEEE, 2014.\n\n- Jonghong Kim, Kyuyeon Hwang, and Wonyong Sung. \"X1000 real-time phoneme recognition VLSI using feed-forward deep neural networks.\" 2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2014.\n\nThe retrain-based neural network quantization algorithm was first published in these two papers.\n\nThanks.", "OTHER_KEYS": "Sungho Shin"}, {"DATE": "09 Dec 2016", "TITLE": "Comparison", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1"}, {"DATE": "02 Dec 2016", "TITLE": "Improve results ", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2"}]}
{"text": "Despite the long history of research on recommender systems, current approaches still face a number of challenges in practice, e.g. the difficulties in handling new items, the high diversity of user interests, and the noisiness and sparsity of observations. Many of such difficulties stem from the lack of expressive power to capture the complex relations between items and users. This paper presents a new method to tackle this problem, called Collaborative Deep Embedding. In this method, a pair of dual networks, one for encoding items and the other for users, are jointly trained in a collaborative fashion. Particularly, both networks produce embeddings at multiple aligned levels, which, when combined together, can accurately predict the matching between items and users. Compared to existing methods, the proposed one not only provides greater expressive power to capture complex matching relations, but also generalizes better to unseen items or users. On multiple real-world datasets, this method outperforms the state of the art.\n1 INTRODUCTION\nWhat do consumers really want? \u2013 this is a question to which everyone wishes to have an answer. Over the past decade, the unprecedented growth of web services and online commercial platforms such as Amazon, Netflix, and Spotify, gives rise to a vast amount of business data, which contain valuable information about the customers. However, \u201cdata don\u2019t speak for themselves\u201d. To accurately predict what the customers want, one needs not only the data, but also an effective means to extract useful messages therefrom.\nThere has been extensive study on recommender systems. Existing methods roughly fall into two categories, namely content-based filtering (Pazzani & Billsus, 2007) and collaborative filtering (Mnih & Salakhutdinov, 2008; Hu et al., 2008; Yu et al., 2009). The former focuses on extracting relevant features from the content, while the latter attempts to exploit the common interest among groups of users. In recent efforts, hybrid methods (Agarwal & Chen, 2009; Van den Oord et al., 2013) that combine both aspects have also been developed.\nWhereas remarkable progress has been made on this topic, the state of the art remains far from satisfactory. The key challenges lie in several aspects. First, there is a large semantic gap between the true cause of a matching and what we observe from the data. For example, what usually attracts a book consumer is the implied emotion that one has to feel between the lines instead of the occurrences of certain words. It is difficult for classical techniques to extract such deep meanings from the observations. Second, the cold-start issue, namely making predictions for unseen items or users, has not been well addressed. Many collaborative filtering methods rely on the factorization of the matching matrix. Such methods implicitly assume that all the users and items are known in advance, and thus are difficult to be applied in real-world applications, especially online services.\nThe success of deep learning brings new inspiration to this task. In a number of areas, including image classification (Krizhevsky et al., 2012), speech recognition (Hinton et al., 2012), and natural language understanding (Socher et al., 2011), deep learning techniques have substantially pushed forward the state of the art. The power of deep networks in capturing complex variations and bridging semantic gaps has been repeatedly shown in previous study. However, deep models were primarily used for classification or regression, e.g. translating images to sentences. How deep networks can be used to model cross-domain relations remains an open question.\nIn this work, we aim to explore deep neural networks for learning the matching relations across two domains, with our focus placed on the matching between items and users. Specifically, we propose a new framework called Collaborative Deep Embedding, which comprises a pair of dual networks, one for encoding items and the other for users. Each network contains multiple embedding layers that are aligned with their dual counterparts of the other network. Predictions can then be made by coupling these embeddings. Note that unlike a conventional network, the dual networks are trained on two streams of data. In this paper, we devise an algorithm that can jointly train both networks using dual mini-batches. Compared to previous methods, this method not only narrows the semantic gap through a deep modeling architecture, but also provides a natural way to generalize \u2013 new items and new users can be encoded by the trained networks, just like those present in the training stage.\nOn a number of real world tasks, the proposed method yields significant improvement over the current state-of-the-art. It is worth stressing that whereas our focus is on the matching between items and users, Collaborative Deep Embedding is a generic methodology, which can be readily extended to model other kinds of cross-domain relations.\n2 RELATED WORK\nExisting methods for recommendation roughly fall into two categories: content-based methods (Pazzani & Billsus, 2007) and collaborative filtering (CF) (Mnih & Salakhutdinov, 2008; Hu et al., 2008; Yu et al., 2009). Specifically, content-based methods rely primarily on feature representation of the content, in which recommendations are often made based on feature similarity (Slaney et al., 2008). Following this, there are also attempts to incorporate additional information, such as meta-data of users, to further improve the performance (McFee et al., 2012). Instead, collaborative filtering exploits the interaction between users and items. A common approach to CF is to derive latent factors of both users and items through matrix factorization, and measure the degree of matching by their inner products. Previous study (Ricci et al., 2011) showed that CF methods tend to have higher recommendation accuracy than content-based methods, as they directly target the recommendation task. However, practical use of CF is often limited by the cold start problem. It is difficult to recommend items without a sufficient amount of use history. Issues like this motivated hybrid methods (Agarwal & Chen, 2009; Van den Oord et al., 2013) that combine both aspects of information, which have showed encouraging improvement. Our exploration is also along this line.\nDespite the progress on both family of methods, the practical performance of state-of-the-art still leaves a lot to be desired. This, to a large extent, is due to the lack of capability of capturing complex variations in interaction patterns. Recently, deep learning (Bengio, 2009) emerges as an important technique in machine learning. In a number of successful stories (Krizhevsky et al., 2012; Hinton et al., 2012; Socher et al., 2011), deep models have demonstrated remarkable representation power in capturing complex patterns. This power has been exploited by some recent work for recommendation. Van den Oord et al. (2013) applies deep learning for music recommendation. It uses the latent item vector learned by CF as ground truth to train a deep network for extracting content features, obtaining considerable performance gain. However the latent vectors for known users and items are not improved. Wang & Wang (2014) proposed an extension to this method, which concatenates both the CF features and the deep features, resulting in slight improvement.\nWang & Blei (2011) showed that CF and topic modeling, when combined, can benefit each other. Inspired by this, Wang et al. (2015) proposed Collaborative Deep Learning (CDL), which incorporates CF and deep feature learning with a combined objective function. This work represents the latest advances in recommendation methods. Yet, its performance is still limited by several issues, e.g. the difficulties in balancing diversified objectives and the lack of effective methods for user encoding. An important aspect that distinguishes our work from CDL and other previous methods is that it encodes both items and users through a pair of deep networks that are jointly trained, which substantially\nenhance the representation power on both sides. Moreover, the objective function of our learning framework directly targets the recommendation accuracy, which also leads to better performance.\n3 COLLABORATIVE DEEP EMBEDDING\nAt the heart of a recommender system is matching model, namely, a model that can predict whether a given item matches the interest of a given user. Generally, this can be formalized as below. Suppose there are m users and n items, respectively indexed by i and j. Items are usually associated with inherent features, e.g. the descriptions or contents. Here, we use xj to denote the observed features of the j-th item. However, inherent information for users is generally very limited and often irrelevant. Hence, in most cases, users are primarily characterized by their history, i.e. the items they have purchased or rated. Specifically, the user history can be partly captured by a matching matrix R \u2208 {0, 1}m\u00d7n, where R(i, j) = 1 indicates that the i-th user purchased the j-th item and gave a positive rating. Note that R is often an incomplete reflection of the user interest \u2013 it is not uncommon that a user does not purchase or rate an item that he/she likes.\n3.1 DUAL EMBEDDING\nTo motivate our approach, we begin with a brief revisit of collaborative filtering (CF), which is widely adopted in practical recommender systems. The basic idea of CF is to derive vector representations for both users and items by factorizing the matching matrix R. A representative formulation in this family is the Weighted Matrix Factorization (WMF) (Hu et al., 2008), which adopts an objective function as below: \u2211\ni \u2211 j cij(Rij \u2212 uTi vj)2 + \u03bbu \u2211 i \u2016ui\u201622 + \u03bbv \u2211 j \u2016vj\u201622. (1)\nHere, ui and vj denote the vector representations of the i-th user and the j-th item, cij the confidence coefficient of an observed entry, and \u03bbu, \u03bbv the regularization coefficients. Underlying such methods lies a common assumption, namely, all users and items must be known a priori. As a result, they will face fundamental difficulties when handling new items and new users.\nEncoding Networks. In this work, we aim to move beyond this limitation by exploring an alternative approach. Instead of pursuing the embeddings of a given set of items and users, our approach jointly learns a pair of encoding networks, respectively for items and users. Compared to CF, the key advantage of this approach is that it is generalizable by nature. When new items or new users come, their vector embeddings can be readily derived using the learned encoders.\nGenerally, the items can be encoded based on their own inherent features, using, for example, an auto-encoder. The key question here, however, is how to encode users, which, as mentioned, have no inherent features. Again, we revisit conventional CF methods such as WMF and find that in these methods, the user representations can be expressed as:\nui = argmin u \u2211 j cij\u2016Rij \u2212 uTi vj\u20162 + \u03bbu \u2211 i \u2016ui\u20162 = ( VCiV T + \u03bbuI )\u22121 Vri. (2)\nHere, V = [v1, . . . ,vn] is a matrix comprised of all item embeddings, each column for one; ri is the i-th row of R treated as a column vector, which represents the history of the i-th user; and Ci = diag(ci1, . . . , cin) captures the confidence weights.\nThe analysis above reveals that ui is a linear transform of ri as ui = Wuri, where the transform matrix Wu depends on the item embeddings V. This motivates our idea of user encoding, that is, to use a deep neural network instead the linear transform above, as\nui = g(ri;Wu), (3)\nwhere g denotes a nonlinear transform based on a deep network with parameters Wu. As we will show in our experiments, by drawing on the expressive power of deep neural networks, the proposed way of user encoding can substantially improve the prediction accuracy.\nOverall Formulation. By coupling an item-network denoted by f(xj ;Wv) and a user-network g as introduced above, we can predict the matching of any given pair of user and item based on the inner product of their embeddings, as \u3008f(x;Wv), g(r;Wu)\u3009. The inputs to these networks include x, the inherent feature of the given item, and r, the history of the given user on a set of reference items. With both encoding networks, we formulate the learning objective as follows:\nmin Wu,Wv \u2211 i \u2211 j cij\u2016Rij \u2212 \u3008f(xj ;Wv), g(ri;Wu)\u3009\u20162. (4)\nHere, X = [x1, . . . ,xn] denotes the input features of all reference items. This formulation differs from previous ones in two key aspects: (1) Both users and items are encoded using deep neural networks. The learning objective above encourages the cooperation of both networks such that the coupling of both sides yield the highest accuracy. Hence, the user-network parameters Wu depends on the item embeddings V, and likewise for the item-network. (2) The learning task is to estimate the parameters of the encoding networks. Once the encoding networks are learned, they encode users and items in a uniform way, no matter whether they are seen during training. In other words, new users and new items are no longer second-class citizens \u2013 they are encoded in exactly the same way as those in the training set.\nComparison with CDL. The Collaborative Deep Learning (CDL) recently proposed by Wang et al. (2015) was another attempt to tackle the cold-start issue. This method leverages the item features by aligning the item encoder with the embeddings resulted from matrix factorization. In particular, the objective function is given as follows:\u2211 ij cij(Rij\u2212uTi vj)2+\u03bbv \u2211 j \u2016vj\u2212fe(x\u0303j ,\u03b8)\u20162+\u03bbn \u2211 j \u2016x\u0303j\u2212fr(x\u0303j ,\u03b8)\u20162+\u03bbu \u2211 i \u2016ui\u20162+r(\u03b8). (5) Here, a Stacked Denoising Autoencoder (SDAE) (Vincent et al., 2010) with parameter \u03b8 is used to encode the items, based on {x\u0303j}, noisy versions of their features. Compared to our formulation, CDL has several limitations: (1) The objective is to balance the SDAE reconstruction error and the matching accuracy, which does not necessarily lead to improved recommendation. Tuning this balance also turns out to be tricky. (2) Only items are encoded, while the representations of the users are still obtained by matrix factorization. As a result, its expressive power in capturing user interest remains limited. (3) There are inconsistencies between known items and new ones \u2013 the embedding of known items is resulted from a tradeoff between the matching accuracy and the fidelity to SDAE features, while the embedding of new items are purely based on SDAE encoding.\n3.2 NETWORK ARCHITECTURE DESIGNS\nOur model consists of two networks, namely the item-network f and the user-network g. We went through a progressive procedure in designing their architectures, obtaining three different designs, from basic design, multi-level design, to multi-level branching design. Each new design was motivated by the observation of certain limitations in the previous version.\nThe basic design, as shown in Figure 1 (a) adopts the multilayer perceptron as the basic architecture, using tanh as the nonlinear activation function between layers1. The top layer of the item-network produces a vector f(xj ;Wv) for each item; while that of the user-network produces a dual vector g(ri;Wu) for each user. During training, the loss layer takes their inner products and compares them with the ground-truth R(i, j).\nEach layer in these networks generates a vector representation. We observe that representations from different layers are complementary. Representations from lower layers tend to be closer to the inputs and preserve more information; while those from higher layers focus on deeper semantics. The representations from these levels have their respective values, as different users tend to focus on different aspects of an item. Following this intuition, we reach a multi-level design, as shown in Figure 1 (b). In this design, dot products between dual embeddings at corresponding levels are aggregated to produce the final prediction.\nThere is an issue of the multi-level design \u2013 the output of each intermediate layer actually plays two roles. On one hand, it is the input to the next layer for further abstraction; on the other hand, it also serves as a facet to be matched with the other side. These two roles require different properties of the representations. Particularly, for the former role, the representation needs to preserve more information for higher-level abstraction; while for the latter, those parts related to the current level of matching need to be emphasized. To address this issue, we design a multi-level branching architecture, as shown in Figure 1 (c). In this design, a matching branch is introduced to transform the representation at each level to a form that is more suitable for matching. This can also be considered as learning an alternative metric to measure the matchness between the embeddings. As we will show in our experiments, this design can considerably improve the prediction accuracy.\n4 TRAINING WITH DUAL MINI-BATCHES\nA distinctive aspect of our training algorithm is the use of dual mini-batches. Specifically, in each iteration, Bv items and Bu users are selected. In addition to the item features and user histories, the corresponding part of the matching matrix R will also be loaded and fed to the network. Here, the two batch sizes Bv and Bu can be different, and they should be chosen according to the sparsity of the matching matrix R, such that each dual mini-batch can cover both positive and zero ratings.\nDuring the backward pass, the loss layer that compares the predictions with the ground-truth matchings will produce two sets of gradients, respectively for items and users. These gradients are then back-propagated along respective networks. Note that when the multi-level designs (both with and without branching) are used, each intermediate layer will receive gradients from two sources \u2013 those from the upper layers and those from the dual network (via the dot-product layer). Hence, the training of one network would impact that of the other.\nThe entire training procedure consists of two stages: pre-training and optimization. In the pre-training stage, we initialize the item-network with unsupervised training (Vincent et al., 2010) and the usernetwork randomly. The unsupervised training of the item-network allows it to capture the feature statistics. Then both networks will be jointly refined in a layer-by-layer fashion. Particularly, we first tune the one-level networks, taking the dot products of their outputs as the predictions. Subsequently, we stack the second layers on top and refine them in a similar way. Empirically, we found that this layer-wise refinement scheme provides better initialization. In the optimization stage, we adopt the SGD algorithm with momentum and use the dual mini-batch scheme presented above. In this stage, the training is conducted in epochs. Each epoch, through multiple iterations, traverses the whole matching matrix R without repetition. The order of choosing mini-batches is arbitrary and will be shuffled at the beginning of each epoch. Additional tricks such as dropout and batch normalization are employed to further improve the performance.\n1The choice of tanh as the activation function is based on empirical comparison.\n5 EXPERIMENTS\nWe tested our method on three real-world datasets with different kinds of items and matching relations:\n1. CiteULike, constructed by Wang & Blei (2011), provides a list of researchers and the papers that they interested. Each paper comes with a text document that comprises both the title and the abstract. In total, it contains 5, 551 researchers (as users) and 16, 980 papers (as items) with 0.22% density. The task is to predict the papers that a researcher would like.\n2. MovieLens+Posters is constructed based on the MovieLens 20M Dataset (Harper & Konstan, 2016), which provides about 20M user ratings on movies. For each movie, we collect a movie poster from TMDb and extract a visual feature therefrom using a convolutional neural network (Szegedy et al., 2016) as the item feature. Removing all those movies without posters and the users with fewer than 10 ratings, we obtain a dataset that contains 76, 531 users and 14, 101 items with 0.24% density. In this dataset, all 5 ratings are considered as positive matchings.\n3. Ciao is organized by Tang et al. (2012) from a product review site, where each product comes with a series of reviews. The reviews for each product are concatenated to serve as the item content. We removed those items with less than 5 rated users and the users with less than 10 ratings. This results in a dataset with 4, 663 users and 12, 083 items with 0.25% density. All ratings with 40 or above (the rating ranges from 0 to 50) are regarded as positive matchings.\n5.1 EVALUATION\nThe performance of a recommender system can be assessed from different perspective. In this paper, we follow Wang & Blei (2011) and perform the evaluation from the retrieval perspective. Specifically, a fraction of rating entries are omitted in the training phase, and the algorithms being tested will be used to predict those entries. As pointed out by Wang & Blei (2011), as the ratings are implicit feedback (Hu et al., 2008) \u2013 some positive matchings are not reflected in the ratings, recall is more suitable than precision in measuring the performance. In particular, we use Recall@M averaged over all users as the performance metric. Here, for a certain user, Recall@M is defined as follows:\nrecall@M = the number of items a user likes in top M recommendations\nthe total number of items the user likes .\nIn our experiments, the value of M varies from 50 to 200.\nFollowing Wang & Blei (2011), we consider two tasks, in-matrix prediction and out-matrix prediction. Specifically, we divide all users into two disjoint parts, known and unknown, by the ratio of 9 to 1. The in-matrix prediction task only considers known items. For this task, all rating entries are split into three disjoint sets: training, validation and testing, by the ratio 3 : 1 : 1. It is ensured that all items in the validation and testing sets have appeared in the training stage (just that part of their ratings were omitted). The out-matrix prediction task is to make predictions for the items that are completely unseen in the training phase. This task is to test the performance of generalization and the capability of handling the cold-start issue.\n5.2 COMPARISON WITH OTHER METHODS\nWe compared our method, which we refer to as DualNet with two representative methods in previous work: (1) Weighted Matrix Factorization (WMF) (Hu et al., 2008), a representative method for for collaborative filtering (CF), and (2) Collaborative deep learning (CDL) (Wang et al., 2015), a hybrid method that combines deep encoding of the items and CF, which represents the latest advances in recommendation techniques.\nOn each dataset, we chose the design parameters for each method via grid search. The parameter combinations that attain best performance on the validation set are used. For our DualNet method, we adopt a three-level branching configuration, where the embedding dimensions of each network, from bottom to top, are set to 200, 200, 50. For WMF, the latent dimension is set to 300 on CDL and 450 on other datasets. For CDL, the best performance is attained when the structure of SDAE is configured to be (2000, 1000, 300), with drop out ratio 0.1. Other design parameters of CDL are set as a = 1.0, b = 0.01, lu = 1, lv = 10, ln = 1000, lw = 0.0005.\nNote that on CiteULike, there are two ways to split the data. One is the scheme in (Wang et al., 2015), and the other is the scheme in (Wang & Blei, 2011), which is the one presented in the previous section. Note that in the former scheme, a fixed number of ratings from each user are selected for training. This may result in some testing items being missed in the training set. To provide a complete comparison with prior work, we use both schemes in our experiments, which are respectively denoted as CiteULike1 and CiteULike2.\nTable 1 compares the performance of WML, CDL, and DualNet on all three datasets (four data splitting settings). From the results, we observed: (1) Our proposed DualNet method outperforms both WML and CDL on all datasets. On certain data sets, the performance gains are substantial. For example, on MovieLens, we obtained average recalls at 44.95%, 59.15%, and 72.56% respectively when M = 50, 100, 200. Comparing what CDL achieves (38.11%, 49, 73%, and 61.00%), the relative gains are around 18%. On other data sets, the gains are also considerable. (2) The performance gains vary significantly across different datasets, as they are closely related to the relevance of the item features. Particularly, when the item features are pertinent to the user interest, we may see remarkable improvement when those features are incorporated; otherwise, the performance gains would be relatively smaller.\n5.3 DETAILED STUDY\nWe conducted additional experiments on CiteULike to further study the proposed algorithm. In this study, we investigate the performance of out-matrix prediction, the impact of various modeling choices, e.g. multi-level branching, as well as the influence of training tactics.\nOut-matrix prediction. As mentioned, the out-matrix prediction task is to examine an algorithm\u2019s capability of handling new items, i.e. those unseen in the training stage. For this task, we compared CDL and DualNet on the CiteULike dataset. WML is not included here as it is not able to handle new items. Table 2 shows the results. It can be clearly seen that DualNet outperforms CDL by a notable margin. For example, Recall@50 increases from 32.18% to 47.51% \u2013 the relative gain is 47.6%, a very remarkable improvement. The strong generalization performance as demonstrated here is, to a large extent, ascribed to our basic formulation, where the encoding networks uniformly encode both known and new items.\nMulti-level branching. We compared three different designs presented in Section 3: basic design, multi-level design, and multi-level branching design. From the results shown in Table 3, we can observe limited improvement of the multi-level design over the basic one. More significant performance\ngains are observed when the branching design is introduced. This shows that the branches contribute a lot to the overall performance.\nNoise injection. Sometimes we noticed overfitting during training i.e. the validation performance gets worse while the training loss is decreasing. To tackle this issue, we inject noises to the inputs, i.e. setting a fraction of input entries to zeros. Generally, we observed that noise injection has little effect for Recall@M on in-matrix predictions when M < 30. However, it can considerably increase the recall for largeM value or out-matrix predictions. Particularly, on CiteULike, it increases in-matrix Recall@300 from 67.3% to 71.2%, and out-matrix Recall@50 from 38.6% to 47.5%.\nUnsuccessful Tactics. Finally, we show some tactics that we have tried and found to be not working. (1) Replacing the weighted Euclidean loss with logistic loss would lead to substantial degradation of the performance (sometimes by up to 20%). Also, when using logistic loss, we observed severe overfitting. Rendle et al. (2009) proposed Bayesian Personalized Recommendation (BPR) which directly targets on ranking. We tested this on CiteULike with parameters tuned to obtain the optimal performance. Our experimental results showed that its performance is similar to WMF. Particularly, the Recall@50, 100, 200 for BPR are respectively 39.11%, 49.16%, 59.96%, while those for WMF are 40.45%, 50.25%, 59.95%.\n(2) Motivated by the observation that positive ratings are sparse, we tried a scheme that ignores a fraction of dual mini-batches that correspond to all zero ratings, with an aim to speed up the training. Whereas this can reduces the time needed to run an epoch, it takes significantly more epochs to reach the same level of performance. As a result, the overall runtime is even longer.\n6 CONCLUSIONS AND FUTURE WORK\nThis paper presented a new method for predicting the interactions between users and items, called Collaborative Deep Embedding. This method uses dual networks to encode users and items respectively. The user-network and item-network are trained jointly, in a collaborative manner, based on two streams of data. We obtained considerable performance gains over the state-of-the-art consistently on three large datasets. The proposed method also demonstrated superior generalization performance (on out-matrix predictions). This improvement, from our perspective, is ascribed to three important reasons: (1) the expressive power of deep models for capturing the rich variations in user interests, (2) the collaborative training process that encourages closely coupled embeddings, and (3) an objective function that directly targets the prediction accuracy.\nWe consider this work as a significant step that brings the power of deep models to relational modeling. However, the space of deep relational modeling remains wide open \u2013 lots of questions remain yet to be answered. In future, we plan to investigate more sophisticated network architectures, and extend the proposed methodology to applications that involve more than two domains.\n", "cats": {"READ": 1.0, "DONTREAD": 0.0}, "reviews": [{"IS_META_REVIEW": true, "comments": "The responses to the pre-review questions are not strong; especially w.r.t. the question about dataset density and why the dataset had to be subsampled, the authors responded that subsampling is common in recommender systems work, including the papers cited. This isn't a particularly strong justification of why subsampling is a good idea, and in particular doesn't answer the question of \"how would the results look without subsampling,\" which I think is a question that could easily have been answered directly.\n\nEspecially given that the goal of dealing with the cold-start issue is so heavily emphasized in the paper, in seems odd to sample the data to reduce sparsity.\n\nOther than that, the pre-review questions seem to have been answered satisfactorily.\n\nThe contribution of the paper is to propose user and item embedding methods, as a means of learning complex non-linear interactions between users and items. This is fairly similar to recent work on deep RS, though the network formulation has some differences.\n\nOverall this is an reasonably put together paper that makes a contribution in an important area, though there are still some shortcomings that should be addressed, namely:\n1) The evaluation is unusual. Recall@M is the only result reported, though this is not usually an evaluation seen in recommender systems research. At the very least other performance measures (rmse or AUC) should be reported for completeness, even if the results are not strong\n2) Given that the contribution is fairly simple (i.e., the \"standard\" recommender systems task, but with a new model) it's a shame that unusual data samples have to be taken. This should be a case where it's possible to report results against competing methods using *exactly* the same data they used, and exactly the same error measure, for the fairest comparison possible.\n\nWithout the above it's hard to tell how much the performance improvements are really due to the method being better, versus the choice of datasets and the choice of loss functions."}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "The paper presents a collaborative filtering method, using dual deep nets for users and items. The nets can take advantage of content in addition to ratings. This contribution is just below the bar, in that its novelty relative to existing methods is limited and the results are good but not sufficiently impressive, especially since they focus exclusively on Recall@N. In the response, the authors do present results on other metrics but the results are mixed.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"DATE": "28 Dec 2016", "TITLE": "Please see the review", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1"}, {"TITLE": "No Title", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "The authors proposed to learn embeddings of users and items by using deep neural network for a recommendation task. The resulting method has only minor differences from the previous CDL, in which neural networks were also used for recommendation tasks. In the experiments, since the proposed method, DualNets have use more item features than WMF and CDL, the comparisons are unfair. ", "IS_META_REVIEW": false, "RECOMMENDATION": 5, "DATE": "28 Dec 2016", "REVIEWER_CONFIDENCE": 3}, {"TITLE": "review for Collaborative Deep Embedding via Dual Networks", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "This paper provides a minor improvement paper of DeepRS. The major improvement comes from the coupling of user-item factors in prediction. While the motivation is clear, the improvement of the model architecture is minor. \nI think the author should improve the paper to discuss more on the impact of introduction of coupling, which might make this paper stronger. Specifically, conduct isolate experiment to change loss, architecture gradually, from a non-coupled network to a final proposed coupled network to demonstrate the importance of coupling.\nAnother important missing part of the paper seems to be time complexity. Since coupled net would be much more costly to generate recommendations, a discussion on how it would impact real world usages should be added.\nOverall, I think this is a paper that should be improved before accepted.\n", "IS_META_REVIEW": false, "RECOMMENDATION": 5, "DATE": "17 Dec 2016 (modified: 21 Dec 2016)", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "official review", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "The responses to the pre-review questions are not strong; especially w.r.t. the question about dataset density and why the dataset had to be subsampled, the authors responded that subsampling is common in recommender systems work, including the papers cited. This isn't a particularly strong justification of why subsampling is a good idea, and in particular doesn't answer the question of \"how would the results look without subsampling,\" which I think is a question that could easily have been answered directly.\n\nEspecially given that the goal of dealing with the cold-start issue is so heavily emphasized in the paper, in seems odd to sample the data to reduce sparsity.\n\nOther than that, the pre-review questions seem to have been answered satisfactorily.\n\nThe contribution of the paper is to propose user and item embedding methods, as a means of learning complex non-linear interactions between users and items. This is fairly similar to recent work on deep RS, though the network formulation has some differences.\n\nOverall this is an reasonably put together paper that makes a contribution in an important area, though there are still some shortcomings that should be addressed, namely:\n1) The evaluation is unusual. Recall@M is the only result reported, though this is not usually an evaluation seen in recommender systems research. At the very least other performance measures (rmse or AUC) should be reported for completeness, even if the results are not strong\n2) Given that the contribution is fairly simple (i.e., the \"standard\" recommender systems task, but with a new model) it's a shame that unusual data samples have to be taken. This should be a case where it's possible to report results against competing methods using *exactly* the same data they used, and exactly the same error measure, for the fairest comparison possible.\n\nWithout the above it's hard to tell how much the performance improvements are really due to the method being better, versus the choice of datasets and the choice of loss functions.\n", "IS_META_REVIEW": false, "RECOMMENDATION": 4, "DATE": "15 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"DATE": "03 Dec 2016", "TITLE": "pre-review questions", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2"}, {"DATE": "01 Dec 2016", "TITLE": "baseline with negative sampling based approach", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3"}, {"IS_META_REVIEW": true, "comments": "The responses to the pre-review questions are not strong; especially w.r.t. the question about dataset density and why the dataset had to be subsampled, the authors responded that subsampling is common in recommender systems work, including the papers cited. This isn't a particularly strong justification of why subsampling is a good idea, and in particular doesn't answer the question of \"how would the results look without subsampling,\" which I think is a question that could easily have been answered directly.\n\nEspecially given that the goal of dealing with the cold-start issue is so heavily emphasized in the paper, in seems odd to sample the data to reduce sparsity.\n\nOther than that, the pre-review questions seem to have been answered satisfactorily.\n\nThe contribution of the paper is to propose user and item embedding methods, as a means of learning complex non-linear interactions between users and items. This is fairly similar to recent work on deep RS, though the network formulation has some differences.\n\nOverall this is an reasonably put together paper that makes a contribution in an important area, though there are still some shortcomings that should be addressed, namely:\n1) The evaluation is unusual. Recall@M is the only result reported, though this is not usually an evaluation seen in recommender systems research. At the very least other performance measures (rmse or AUC) should be reported for completeness, even if the results are not strong\n2) Given that the contribution is fairly simple (i.e., the \"standard\" recommender systems task, but with a new model) it's a shame that unusual data samples have to be taken. This should be a case where it's possible to report results against competing methods using *exactly* the same data they used, and exactly the same error measure, for the fairest comparison possible.\n\nWithout the above it's hard to tell how much the performance improvements are really due to the method being better, versus the choice of datasets and the choice of loss functions."}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "The paper presents a collaborative filtering method, using dual deep nets for users and items. The nets can take advantage of content in addition to ratings. This contribution is just below the bar, in that its novelty relative to existing methods is limited and the results are good but not sufficiently impressive, especially since they focus exclusively on Recall@N. In the response, the authors do present results on other metrics but the results are mixed.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"DATE": "28 Dec 2016", "TITLE": "Please see the review", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1"}, {"TITLE": "No Title", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "The authors proposed to learn embeddings of users and items by using deep neural network for a recommendation task. The resulting method has only minor differences from the previous CDL, in which neural networks were also used for recommendation tasks. In the experiments, since the proposed method, DualNets have use more item features than WMF and CDL, the comparisons are unfair. ", "IS_META_REVIEW": false, "RECOMMENDATION": 5, "DATE": "28 Dec 2016", "REVIEWER_CONFIDENCE": 3}, {"TITLE": "review for Collaborative Deep Embedding via Dual Networks", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "This paper provides a minor improvement paper of DeepRS. The major improvement comes from the coupling of user-item factors in prediction. While the motivation is clear, the improvement of the model architecture is minor. \nI think the author should improve the paper to discuss more on the impact of introduction of coupling, which might make this paper stronger. Specifically, conduct isolate experiment to change loss, architecture gradually, from a non-coupled network to a final proposed coupled network to demonstrate the importance of coupling.\nAnother important missing part of the paper seems to be time complexity. Since coupled net would be much more costly to generate recommendations, a discussion on how it would impact real world usages should be added.\nOverall, I think this is a paper that should be improved before accepted.\n", "IS_META_REVIEW": false, "RECOMMENDATION": 5, "DATE": "17 Dec 2016 (modified: 21 Dec 2016)", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "official review", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "The responses to the pre-review questions are not strong; especially w.r.t. the question about dataset density and why the dataset had to be subsampled, the authors responded that subsampling is common in recommender systems work, including the papers cited. This isn't a particularly strong justification of why subsampling is a good idea, and in particular doesn't answer the question of \"how would the results look without subsampling,\" which I think is a question that could easily have been answered directly.\n\nEspecially given that the goal of dealing with the cold-start issue is so heavily emphasized in the paper, in seems odd to sample the data to reduce sparsity.\n\nOther than that, the pre-review questions seem to have been answered satisfactorily.\n\nThe contribution of the paper is to propose user and item embedding methods, as a means of learning complex non-linear interactions between users and items. This is fairly similar to recent work on deep RS, though the network formulation has some differences.\n\nOverall this is an reasonably put together paper that makes a contribution in an important area, though there are still some shortcomings that should be addressed, namely:\n1) The evaluation is unusual. Recall@M is the only result reported, though this is not usually an evaluation seen in recommender systems research. At the very least other performance measures (rmse or AUC) should be reported for completeness, even if the results are not strong\n2) Given that the contribution is fairly simple (i.e., the \"standard\" recommender systems task, but with a new model) it's a shame that unusual data samples have to be taken. This should be a case where it's possible to report results against competing methods using *exactly* the same data they used, and exactly the same error measure, for the fairest comparison possible.\n\nWithout the above it's hard to tell how much the performance improvements are really due to the method being better, versus the choice of datasets and the choice of loss functions.\n", "IS_META_REVIEW": false, "RECOMMENDATION": 4, "DATE": "15 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"DATE": "03 Dec 2016", "TITLE": "pre-review questions", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2"}, {"DATE": "01 Dec 2016", "TITLE": "baseline with negative sampling based approach", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3"}]}
{"text": "THE INCREDIBLE SHRINKING NEURAL NETWORK: NEW PERSPECTIVES\n1 INTRODUCTION\nIn this work we propose and evaluate a novel algorithm for pruning whole neurons from a trained neural network without any re-training and examine its performance compared to two simpler methods. We then analyze the kinds of errors made by our algorithm and use this as a stepping off point to launch an investigation into the fundamental nature of learning representations in neural networks. Our results corroborate an insightful though largely forgotten observation by Mozer & Smolensky (1989a) concerning the nature of neural network learning. This observation is best summarized in a quotation from Segee & Carter (1991) on the notion of fault-tolerance in multilayer perceptron networks:\nContrary to the belief widely held, multilayer networks are not inherently fault tolerant. In fact, the loss of a single weight is frequently sufficient to completely\ndisrupt a learned function approximation. Furthermore, having a large number of weights does not seem to improve fault tolerance. [Emphasis added]\nEssentially, Mozer & Smolensky (1989b) observed that during training neural networks do not distribute the learning representation evenly or equitably across hidden units. What actually happens is that a few, elite neurons learn an approximation of the input-output function, and the remaining units must learn a complex interdependence function which cancels out their respective influence on the network output. Furthermore, assuming enough units exist to learn the function in question, increasing the number of parameters does not increase the richness or robustness of the learned approximation, but rather simply increases the likelihood of overfitting and the number of noisy parameters to be canceled during training. This is evinced by the fact that in many cases, multiple neurons can be removed from a network with no re-training and with negligible impact on the quality of the output approximation. In other words, there are few bipartisan units in a trained network. A unit is typically either part of the (possibly overfit) input-output function approximation, or it is part of an elaborate noise cancellation task force. Assuming this is the case, most of the compute-time spent training a neural network is likely occupied by this arguably wasteful procedure of silencing superfluous parameters, and pruning can be viewed as a necessary procedure to \u201ctrim the fat.\u201d\nWe observed copious evidence of this phenomenon in our experiments, and this is the motivation behind our decision to evaluate the pruning algorithms in this study on the simple criteria of their ability to trim neurons without any re-training. If we were to employ re-training as part of our evaluation criteria, we would arguably not be evaluating the quality of our algorithm\u2019s pruning decisions per se but rather the ability of back-propagation trained networks to recover from faults caused by non-ideal pruning decisions, as suggested by the conclusions of Segee & Carter (1991) and Mozer & Smolensky (1989a). Moreover, as Fahlman & Lebiere (1989) discuss, due to the \u201cherd effect\u201d and \u201cmoving target\u201d phenomena in back-propagation learning, the remaining units in a network will simply shift course to account for whatever error signal is re-introduced as a result of a bad pruning decision or network fault. So long as there are enough critical parameters to learn the function in question, a network can typically recover faults with additional training. This limits the conclusions we can draw about the quality of our pruning criteria when we employ re-training.\nIn terms of removing units without re-training, what we discovered is that predicting the behavior of a network when a unit is to be pruned is very difficult, and most of the approximation techniques put forth in existing pruning algorithms do not fare well at all when compared to a brute-force search. To begin our discussion of how we arrived at our algorithm and set up our experiments, we review of the existing literature.\n2 LITERATURE REVIEW\nPruning algorithms, as comprehensively surveyed by Reed (1993), are a useful set of heuristics designed to identify and remove elements from a neural network which are either redundant or do not significantly contribute to the output of the network. This is motivated by the observed tendency of neural networks to overfit to the idiosyncrasies of their training data given too many trainable parameters or too few input patterns from which to generalize, as stated by Chauvin (1990).\nNetwork architecture design and hyperparameter selection are inherently difficult tasks typically approached using a few well-known rules of thumb, e.g. various weight initialization procedures, choosing the width and number of layers, different activation functions, learning rates, momentum, etc. Some of this \u201cblack art\u201d appears unavoidable. For problems which cannot be solved using linear threshold units alone, Baum & Haussler (1989) demonstrate that there is no way to precisely determine the appropriate size of a neural network a priori given any random set of training instances. Using too few neurons seems to inhibit learning, and so in practice it is common to attempt to overparameterize networks initially using a large number of hidden units and weights, and then prune or compress them afterwards if necessary. Of course, as the old saying goes, there\u2019s more than one way to skin a neural network.\n2.1 NON-PRUNING BASED GENERALIZATION & COMPRESSION TECHNIQUES\nThe generalization behavior of neural networks has been well studied, and apart from pruning algorithms many heuristics have been used to avoid overfitting, such as dropout (Srivastava et al.\n(2014)), maxout (Goodfellow et al. (2013)), and cascade correlation (Fahlman & Lebiere (1989)), among others. Of course, while cascade correlation specifically tries to construct of minimal networks, many techniques to improve network generalization do not explicitly attempt to reduce the total number of parameters or the memory footprint of a trained network per se.\nModel compression often has benefits with respect to generalization performance and the portability of neural networks to operate in memory-constrained or embedded environments. Without explicitly removing parameters from the network, weight quantization allows for a reduction in the number of bytes used to represent each weight parameter, as investigated by Balzer et al. (1991), Dundar & Rose (1994), and Hoehfeld & Fahlman (1992).\nA recently proposed method for compressing recurrent neural networks (Prabhavalkar et al. (2016)) uses the singular values of a trained weight matrix as basis vectors from which to derive a compressed hidden layer. \u00d8land & Raj (2015) successfully implemented network compression through weight quantization with an encoding step while others such as Han et al. (2016) have tried to expand on this by adding weight-pruning as a preceding step to quantization and encoding.\nIn summary, we can say that there are many different ways to improve network generalization by altering the training procedure, the objective error function, or by using compressed representations of the network parameters. But these are not, strictly speaking, examples of techniques to reduce the number of parameters in a network. For this we must employ some form of pruning criteria.\n2.2 PRUNING TECHNIQUES\nIf we wanted to continually shrink a neural network down to minimum size, the most straightforward brute-force way to do it is to individually switch each element off and measure the increase in total error on the training set. We then pick the element which has the least impact on the total error, and remove it. Rinse and repeat. This is extremely computationally expensive, given a reasonably large neural network and training set. Alternatively, we might accomplish this using any number of much faster off-the-shelf pruning algorithms, such as Skeletonization (Mozer & Smolensky (1989a)), Optimal Brain Damage (LeCun et al. (1989)), or later variants such as Optimal Brain Surgeon (Hassibi & Stork (1993)). In fact, we borrow much of our inspiration from these algorithms, with one major variation: Instead of pruning individual weights, we prune entire neurons, thereby eliminating all of their incoming and outgoing weight parameters in one go, resulting in more memory saved, faster.\nThe algorithm developed for this paper is targeted at reducing the total number of neurons in a trained network, which is one way of reducing its computational memory footprint. This is often a desirable criteria to minimize in the case of resource-constrained or embedded devices, and also allows us to probe the limitations of pruning down to the very last essential network elements. In terms of generalization as well, we can measure the error of the network on the test set as each element is sequentially removed from the network. With an oracle pruning algorithm, what we expect to observe is that the output of the network remains stable as the first few superfluous neurons are removed, and as we start to bite into the more crucial members of the function approximation, the error should start to rise dramatically. In this paper, the brute-force approach described at the beginning of this section serves as a proxy for an oracle pruning algorithm.\nOne reason to choose to rank and prune individual neurons as opposed to weights is that there are far fewer elements to consider. Furthermore, the removal of a single weight from a large network is a drop in the bucket in terms of reducing a network\u2019s core memory footprint. If we want to reduce the size of a network as efficiently as possible, we argue that pruning neurons instead of weights is more efficient computationally as well as practically in terms of quickly reaching a hypothetical target reduction in memory consumption. This approach also offers downstream applications a realistic expectation of the minimal increase in error resulting from the removal of a specified percentage of neurons. Such trade-offs are unavoidable, but performance impacts can be limited if a principled approach is used to find the best candidate neurons for removal.\nIt is well known that too many free parameters in a neural network can lead to overfitting. Regardless of the number of weights used in a given network, as Segee & Carter (1991) assert, the representation of a learned function approximation is almost never evenly distributed over the hidden units, and thus the removal of any single hidden unit at random can actually result in a network fault. Mozer & Smolensky (1989b) argue that only a subset of the hidden units in a neural network actually\nlatch on to the invariant or generalizing properties of the training inputs, and the rest learn to either mutually cancel each other\u2019s influence or begin overfitting to the noise in the data. We leverage this idea in the current work to rank all neurons in pre-trained networks based on their effective contributions to the overall performance. We then remove the unnecessary neurons to reduce the network\u2019s footprint. Through our experiments we not only concretely validate the theory put forth by Mozer & Smolensky (1989b) but we also successfully build on it to prune networks to 40 to 60 % of their original size without any major loss in performance.\n3 PRUNING NEURONS TO SHRINK NEURAL NETWORKS\nAs discussed in Section 1 our aim is to leverage the highly non-uniform distribution of the learning representation in pre-trained neural networks to eliminate redundant neurons, without focusing on individual weight parameters. Taking this approach enables us to remove all the weights (incoming and outgoing) associated with a non-contributing neuron at once. We would like to note here that in an ideal scenario, based on the neuron interdependency theory put forward by Mozer & Smolensky (1989a), one would evaluate all possible combinations of neurons to remove (one at a time, two at a time, three at a time and so forth) to find the optimal subset of neurons to keep. This is computationally unacceptable, and so we will only focus on removing one neuron at a time and explore more \u201cgreedy\u201d algorithms to do this in a more efficient manner.\nThe general approach taken to prune an optimally trained neural network here is to create a ranked list of all the neurons in the network based off of one of the 3 proposed ranking criteria: a brute force approximation, a linear approximation and a quadratic approximation of the neuron\u2019s impact on the output of the network. We then test the effects of removing neurons on the accuracy and error of the network. All the algorithms and methods presented here are easily parallelizable as well.\nOne last thing to note here before moving forward is that the methods discussed in this section involve some non-trivial derivations which are beyond the scope of this paper. We are more focused on analyzing the implications of these methods on our understanding of neural network learning representations. However, a complete step-by-step derivation and proof of all the results presented is provided in the Supplementary Material as an Appendix.\n3.1 BRUTE FORCE REMOVAL APPROACH\nThis is perhaps the most naive yet the most accurate method for pruning the network. It is also the slowest and hence possibly unusable on large-scale neural networks with thousands of neurons. This method explicitly evaluates each neuron in the network. The idea is to manually check the effect of every single neuron on the output. This is done by running a forward propagation on the validation set K times (where K is the total number of neurons in the network), turning off exactly one neuron each time (keeping all other neurons active) and noting down the change in error. Turning a neuron off can be achieved by simply setting its output to 0. This results in all the outgoing weights from that neuron being turned off. This change in error is then used to generate the ranked list.\n3.2 TAYLOR SERIES REPRESENTATION OF ERROR\nLet us denote the total error from the optimally trained neural network for any given validation dataset by E. E can be seen as a function of O, where O is the output of any general neuron in the network. This error can be approximated at a particular neuron\u2019s output (say Ok) by using the 2nd order Taylor Series as,\nE\u0302(O) \u2248 E(Ok) + (O \u2212Ok) \u00b7 \u2202E\n\u2202O \u2223\u2223\u2223\u2223 Ok + 0.5 \u00b7 (O \u2212Ok)2 \u00b7 \u22022E \u2202O2 \u2223\u2223\u2223\u2223 Ok , (1)\nWhen a neuron is pruned, its output O becomes 0.\nReplacing O by Ok in equation 1 shows us that the error is approximated perfectly by equation 1 at Ok. So:\n\u2206Ek = E\u0302(0)\u2212 E\u0302(Ok) = \u2212Ok \u00b7 \u2202E\n\u2202O \u2223\u2223\u2223\u2223 Ok + 0.5 \u00b7O2k \u00b7 \u22022E \u2202O2 \u2223\u2223\u2223\u2223 Ok , (2)\nwhere \u2206Ek is the change in the total error of the network when exactly one neuron (k) is turned off. Most of the terms in this equation are fairly easy to compute, as we have Ok already from the activations of the hidden units and we already compute \u2202E\u2202O |Ok for each training instance during backpropagation. The \u2202\n2E \u2202O2 |Ok terms are a little more difficult to compute. This is derived in the\nappendix and summarized in the sections below.\n3.2.1 LINEAR APPROXIMATION APPROACH\nWe can use equation 2 to get the linear error approximation of the change in error due to the kth neuron being turned off and represent it as \u2206E1k as follows:\n\u2206E1k = \u2212Ok \u00b7 \u2202E\n\u2202O \u2223\u2223\u2223\u2223 Ok\n(3)\nThe derivative term above is the first-order gradient which represents the change in error with respect to the output a given neuron. This term can be collected during back-propagation. As we shall see further in this section, linear approximations are not reliable indicators of change in error but they provide us with an interesting basis for comparison with the other methods discussed in this paper.\n3.2.2 QUADRATIC APPROXIMATION APPROACH\nAs above, we can use equation 2 to get the quadratic error approximation of the change in error due to the kth neuron being turned off and represent it as \u2206E2k as follows:\n\u2206E2k = \u2212Ok \u00b7 \u2202E\n\u2202O \u2223\u2223\u2223\u2223 Ok + 0.5 \u00b7O2k \u00b7 \u22022E \u2202O2 \u2223\u2223\u2223\u2223 Ok\n(4)\nThe additional second-order gradient term appearing above represents the quadratic change in error with respect to the output of a given neuron. This term can be generated by performing backpropagation using second order derivatives. Collecting these quadratic gradients involves some non-trivial mathematics, the entire step-by-step derivation procedure of which is provided in the Supplementary Material as an Appendix.\n3.3 PROPOSED PRUNING ALGORITHM\nFigure 1 shows a random error function plotted against the output of any given neuron. Note that this figure is for illustration purposes only. The error function is minimized at a particular value of the neuron output as can be seen in the figure. The process of training a neural network is essentially the process of finding these minimizing output values for all the neurons in the network. Pruning this particular neuron (which translates to getting a zero output from it will result in a change in the total overall error. This change in error is represented by distance between the original minimum error (shown by the dashed line) and the top red arrow. This neuron is clearly a bad candidate for removal since removing it will result in a huge error increase.\nThe straight red line in the figure represents the first-order approximation of the error using Taylor Series as described before while the parabola represents a second-order approximation. It can be clearly seen that the second-order approximation is a much better estimate of the change in error.\nOne thing to note here is that it is possible in some cases that there is some thresholding required when trying to approximate the error using the 2nd order Taylor Series expansion. These cases might arise when the parabolic approximation undergoes a steep slope change. To take into account such cases, mean and median thresholding were employed, where any change above a certain threshold was assigned a mean or median value respectively.\nTwo pruning algorithms are proposed here. They are different in the way the neurons are ranked but both of them use \u2206Ek, the approximation of the change in error as the basis for the ranking. \u2206Ek can be calculated using the Brute Force method, or one of the two Taylor Series approximations discussed previously.\nThe first step in both the algorithms is to decide a stopping criterion. This can vary depending on the application but some intuitive stopping criteria can be: maximum number of neurons to remove, percentage scaling needed, maximum allowable accuracy drop etc.\n3.3.1 ALGORITHM I: SINGLE OVERALL RANKING\nThe complete algorithm is shown in Algorithm 1. The idea here is to generate a single ranked list based on the values of \u2206Ek. This involves a single pass of second-order back-propagation (without weight updates) to collect the gradients for each neuron. The neurons from this rank-list (with the lowest values of \u2206Ek) are then pruned according to the stopping criterion decided. We note here that this algorithm is intentionally naive and is used for comparison only.\nData: optimally trained network, training set Result: A pruned network initialize and define stopping criterion ; perform forward propagation over the training set ; perform second-order back-propagation without updating weights and collect linear and quadratic\ngradients ; rank the remaining neurons based on \u2206Ek; while stopping criterion is not met do\nremove the last ranked neuron ; end\nAlgorithm 1: Single Overall Ranking\n3.3.2 ALGORITHM II: ITERATIVE RE-RANKING\nIn this greedy variation of the algorithm (Algorithm 2), after each neuron removal, the remaining network undergoes a single forward and backward pass of second-order back-propagation (without weight updates) and the rank list is formed again. Hence, each removal involves a new pass through\nthe network. This method is computationally more expensive but takes into account the dependencies the neurons might have on one another which would lead to a change in error contribution every time a dependent neuron is removed.\nData: optimally trained network, training set Result: A pruned network initialize and define stopping criterion ; while stopping criterion is not met do\nperform forward propagation over the training set ; perform second-order back-propagation without updating weights and collect linear and\nquadratic gradients ; rank the remaining neurons based on \u2206Ek ; remove the worst neuron based on the ranking ;\nend Algorithm 2: Iterative Re-Ranking\n4 EXPERIMENTAL RESULTS\n4.1 EXAMPLE REGRESSION PROBLEM\nThis problem serves as a quick example to demonstrate many of the phenomena described in previous sections. We trained two networks to learn the cosine function, with one input and one output. This is a task which requires no more than 11 sigmoid neurons to solve entirely, and in this case we don\u2019t care about overfitting because the cosine function has a precise definition. Furthermore, the cosine function is a good toy example because it is a smooth continuous function and, as demonstrated by Nielsen (2015), if we were to tinker directly with the weights and bias parameters of the network, we could allocate individual units within the network to be responsible for constrained ranges of inputs, similar to a basis spline function with many control points. This would distribute the learned function approximation evenly across all hidden units, and thus we have presented the network with a problem in which it could productively use as many hidden units as we give it. In this case, a pruning algorithm would observe a fairly consistent increase in error after the removal of each successive unit. In practice however, regardless of the number of experimental trials, this is not what happens. The network will always use 10-11 hidden units and leave the rest to cancel each other\u2019s influence.\nFigure 2 shows two graphs. Both graphs demonstrate the use of the iterative re-ranking algorithm and the comparative performance of the brute-force pruning method (in blue), the first order method (in green), and the second order method (in red). The graph on the left shows the performance of these algorithms starting from a network with two layers of 10 neurons (20 total), and the graph on the right shows a network with two layers of 50 neurons (100 total).\nIn the left graph, we see that the brute-force method shows a graceful degradation, and the error only begins to rise sharply after 50% of the total neurons have been removed. The error is basically constant up to that point. In the first and second order methods, we see evidence of poor decision making in the sense that both made mistakes early on, which disrupted the output function approximation. The first order method made a large error early on, though we see after a few more neurons were removed this error was corrected somewhat (though it only got worse from there). This is direct evidence of the lack of fault tolerance in a trained neural network. This phenomenon is even more starkly demonstrated in the second order method. After making a few poor neuron removal decisions in a row, the error signal rose sharply, and then went back to zero after the 6th neuron was removed. This is due to the fact that the neurons it chose to remove were trained to cancel each others\u2019 influence within a localized part of the network. After the entire group was eliminated, the approximation returned to normal. This can only happen if the output function approximation is not evenly distributed over the hidden units in a trained network.\nThis phenomenon is even more starkly demonstrated in the graph on the right. Here we see the first order method got \u201clucky\u201d in the beginning and made decent decisions up to about the 40th removed neuron. The second order method had a small error in the beginning which it recovered from gracefully and proceeded to pass the 50 neuron point before finally beginning to unravel. The brute force method, in sharp contrast, shows little to no increase in error at all until 90% of the neurons in the network have been obliterated. Clearly first and second order methods have some value in that they do not make completely arbitrary choices, but the brute force method is far better at this task.\nThis also demonstrates the sharp dualism in neuron roles within a trained network. These networks were trained to near-perfect precision and each pruning method was applied without any re-training of any kind. Clearly, in the case of the brute force or oracle method, up to 90% of the network can be completely extirpated before the output approximation even begins to show any signs of degradation. This would be impossible if the learning representation were evenly or equitably distributed. Note, for example, that the degradation point in both cases is approximately the same. This example is not a real-world application of course, but it brings into very clear focus the kind of phenomena we will discuss in the following sections.\n4.2 RESULTS ON MNIST DATASET\nFor all the results presented in this section, the MNIST database of Handwritten Digits by LeCun & Cortes (2010) was used. It is worth noting that due to the time taken by the brute force algorithm we rather used a 5000 image subset of the MNIST database in which we have normalized the pixel values between 0 and 1.0, and compressed the image sizes to 20x20 images rather than 28x28, so the starting test accuracy reported here appears higher than those reported by LeCun et al. We do not believe that this affects the interpretation of the presented results because the basic learning problem does not change with a larger dataset or input dimension.\n4.3 PRUNING A 1-LAYER NETWORK\nThe network architecture in this case consisted of 1 layer, 100 neurons, 10 outputs, logistic sigmoid activations, and a starting test accuracy of 0.998.\n4.3.1 SINGLE OVERALL RANKING ALGORITHM\nWe first present the results for a single-layer neural network in Figure 3, using the Single Overall algorithm (Algorithm 1) as proposed in Section 3. (We again note that this algorithm is intentionally naive and is used for comparison only. Its performance should be expected to be poor.) After training, each neuron is assigned its permanent ranking based on the three criteria discussed previously: A brute force \u201cground truth\u201d ranking, and two approximations of this ranking using first and second order Taylor estimations of the change in network output error resulting from the removal of each neuron.\nAn interesting observation here is that with only a single layer, no criteria for ranking the neurons in the network (brute force or the two Taylor Series variants) using Algorithm 1 emerges superior, indicating that the 1st and 2nd order Taylor Series methods are actually reasonable approximations\nof the brute force method under certain conditions. Of course, this method is still quite bad in terms of the rate of degradation of the classification accuracy and in practice we would likely follow Algorithm 2 which is takes into account Mozer & Smolensky (1989a)\u2019s observations stated in the Related Work section. The purpose of the present investigation, however, is to demonstrate how much of a trained network can be theoretically removed without altering the network\u2019s learned parameters in any way.\n4.3.2 ITERATIVE RE-RANKING ALGORITHM\nIn Figure 4 we present our results using Algorithm 2 (The iterative re-ranking Algorithm) in which all remaining neurons are re-ranked after each successive neuron is switched off. We compute the same brute force rankings and Taylor series approximations of error deltas over the remaining active neurons in the network after each pruning decision. This is intended to account for the effects of cancelling interactions between neurons.\nThere are 2 key observations here. Using the brute force ranking criteria, almost 60% of the neurons in the network can be pruned away without any major loss in performance. The other noteworthy observation here is that the 2nd order Taylor Series approximation of the error performs consistently better than its 1st order version, in most situations, though Figure 21 is a poignant counter-example.\n4.3.3 VISUALIZATION OF ERROR SURFACE & PRUNING DECISIONS\nAs explained in Section 3, these graphs are a visualization of the error surface of the network output with respect to the neurons chosen for removal using each of the 3 ranking criteria, represented in\nintervals of 10 neurons. In each graph, the error surface of the network output is displayed in log space (left) and in real space (right) with respect to each candidate neuron chosen for removal. We create these plots during the pruning exercise by picking a neuron to switch off, and then multiplying its output by a scalar gain value \u03b1 which is adjusted from 0.0 to 10.0 with a step size of 0.001. When the value of \u03b1 is 1.0, this represents the unperturbed neuron output learned during training. Between 0.0 and 1.0, we are graphing the literal effect of turning the neuron off (\u03b1 = 0), and when \u03b1 > 1.0 we are simulating a boosting of the neuron\u2019s influence in the network, i.e. inflating the value of its outgoing weight parameters.\nWe graph the effect of boosting the neuron\u2019s output to demonstrate that for certain neurons in the network, even doubling, tripling, or quadrupling the scalar output of the neuron has no effect on the overall error of the network, indicating the remarkable degree to which the network has learned to ignore the value of certain parameters. In other cases, we can get a sense of the sensitivity of the network\u2019s output to the value of a given neuron when the curve rises steeply after the red 1.0 line. This indicates that the learned value of the parameters emanating from a given neuron are relatively important, and this is why we should ideally see sharper upticks in the curves for the later-removed neurons in the network, that is, when the neurons crucial to the learning representation start to be picked off. Some very interesting observations can be made in each of these graphs.\nRemember that lower is better in terms of the height of the curve and minimal (or negative) horizontal change between the vertical red line at 1.0 (neuron on, \u03b1 = 1.0) and 0.0 (neuron off, \u03b1 = 0.0) is indicative of a good candidate neuron to prune, i.e. there will be minimal effect on the network output when the neuron is removed.\n4.3.4 VISUALIZATION OF BRUTE FORCE PRUNING DECISIONS\nIn Figure ??, we notice how low to the floor and flat most of the curves are. It\u2019s only until the 90th removed neuron that we see a higher curve with a more convex shape (clearly a more sensitive, influential piece of the network).\n4.3.5 VISUALIZATION OF 1ST ORDER APPROXIMATION PRUNING DECISIONS\nIt can be seen in Figure 6 that most choices seem to have flat or negatively sloped curves, indicating that the first order approximation seems to be pretty good, but examining the brute force choices shows they could be better.\n4.3.6 VISUALIZATION OF 2ND ORDER APPROXIMATION PRUNING DECISIONS\nThe method in Figure 7 looks similar to the brute force method choices, though clearly not as good (they\u2019re more spread out). Notice the difference in convexity between the 2nd and 1st order method\nchoices. It\u2019s clear that the first order method is fitting a line and the 2nd order method is fitting a parabola in their approximation.\n4.4 PRUNING A 2-LAYER NETWORK\nThe network architecture in this case consisted of 2 layers, 50 neurons per layer, 10 outputs, logistic sigmoid activations, and a starting test accuracy of 1.000.\n4.4.1 SINGLE OVERALL RANKING ALGORITHM\nFigure 8 shows the pruning results for Algorithm 1 on a 2-layer network. The ranking procedure is identical to the one used to generate Figure 3. (We again note that this algorithm is intentionally naive and is used for comparison only. Its performance should be expected to be poor.)\nUnsurprisingly, a 2-layer network is harder to prune because a single overall ranking will never capture the interdependencies between neurons in different layers. It makes sense that this is worse\nthan the performance on the 1-layer network, even if this method is already known to be bad, and we\u2019d likely never use it in practice.\n4.4.2 ITERATIVE RE-RANKING ALGORITHM\nFigure 9 shows the results from using Algorithm 2 on a 2-layer network. We compute the same brute force rankings and Taylor series approximations of error deltas over the remaining active neurons in the network after each pruning decision used to generate Figure 4. Again, this is intended to account for the effects of cancelling interactions between neurons.\nIt is clear that it becomes harder to remove neurons 1-by-1 with a deeper network (which makes sense because the neurons have more interdependencies in a deeper network), but we see an overall better performance with 2nd order method vs. 1st order, except for the first 20% of the neurons (but this doesn\u2019t seem to make much difference for classification accuracy.)\nPerhaps a more important observation here is that even with a more complex network, it is possible to remove up to 40% of the neurons with no major loss in performance which is clearly illustrated by the brute force curve. This shows the clear potential of an ideal pruning technique and also shows how inconsistent 1st and 2nd order Taylor Series approximations of the error can be as ranking criteria.\n4.4.3 VISUALIZATION OF ERROR SURFACE & PRUNING DECISIONS\nAs seen in the case of a single layered network, these graphs are a visualization the error surface of the network output with respect to the neurons chosen for removal using each algorithm, represented in intervals of 10 neurons.\n4.4.4 VISUALIZATION OF BRUTE FORCE PRUNING DECISIONS\nIn Figure 10, it is clear why these neurons got chosen, their graphs clearly show little change when neuron is removed, are mostly near the floor, and show convex behaviour of error surface, which argues for the rationalization of using 2nd order methods to estimate difference in error when they are turned off.\n4.4.5 VISUALIZATION OF 1ST ORDER APPROXIMATION PRUNING DECISIONS\nDrawing a flat line at the point of each neurons intersection with the red vertical line (no change in gain) shows that the 1st derivative method is actually accurate for estimation of change in error in these cases, but still ultimately leads to poor decisions.\n4.4.6 VISUALIZATION OF 2ND ORDER APPROXIMATION PRUNING DECISIONS\nClearly these neurons are not overtly poor candidates for removal (error doesn\u2019t change much between 1.0 & zero-crossing left-hand-side), but could be better (as described above in the brute force Criterion discussion).\n4.5 INVESTIGATION OF PRUNING PERFORMANCE WITH IMPERFECT STARTING CONDITIONS\nIn our experiments thus far we have tacitly assumed that we start with a network which has learned an \u201coptimal\u201d representation of the training objective, i.e. it has been trained to the point where we accept its performance on the test set. Here we explore what happens when we prune with a sub-optimal starting network.\nIf the assumptions of this paper regarding the nature of neural network learning are correct, we expect that two processes are essentially at work during back-propagation training. First, we expect that the neurons which directly participate in the fundamental learning representation (even if redundantly) work together to reduce error on the training data. Second, we expect that neurons which do not directly participate in the learning representation work to cancel each other\u2019s negative influence. Furthermore, we expect that these two groups are essentially distinct, as evinced by the fact that multiple neurons can often be removed as a group with little to no effect on the network output. Some non-trivial portion of the training time, then, is spent doing work which has nothing intrinsically to do with the learning representation and essentially functions as noise cancellation.\nIf this is the case, when we attempt to prune a network which has not fully canceled the noisy influence of extraneous or redundant units, we might expect to see the error actually improve after removing a few bad apples. This is in fact what we observe, as demonstrated in the following experiments.\nFor each experiment in this section we trained with the full MNIST training set (LeCun & Cortes (2010)), uncompressed and without any data normalization. We trained three different networks to learn to distinguish a single handwritten digit from the rest of the data. The network architectures were each composed of 784 inputs, 1 hidden layer with 100 neurons, and 2 soft-max outputs; one to say yes, and the other to say no. These networks were trained to distinguish the digits 0, 1, and 2, and their respective starting accuracies were a sub-optimal 0.9757, 0.9881, and 0.9513. Finally, we only consider the iterative re-ranking algorithm, as the single overall ranking algorithm is clearly nonviable.\n4.5.1 MNIST SINGLE DIGIT CLASSIFICATION: DIGIT 0\nFigure 13 shows the degradation in squared error after removing neurons from a network trained to distinguish the digit 0. What we observe is that the first and second order methods both fail in different ways, though clearly the second order method makes better decisions overall. The first order method explodes spectacularly in the first few iterations. The brute force method, in stark contrast, actually improves in the first few iterations, and remains essentially flat until around the 60% mark, at which point it begins to gradually increase and meet the other curves.\nThe behavior of the brute force method here demonstrates that the network was essentially working to cancel the effect of a few bad neurons when the training convergence criteria were met, i.e. the network was no longer able to make progress on the training set. After removing these neurons during pruning, the output improved. We can investigate this by looking at the error surface with respect to the neurons chosen for removal by each method in turn. Below in Figure 14 is the graph of the brute force method.\nFigure 14 shows an interesting phenomenon, which we will see in later experiments as well. The high blue curve corresponding to neuron 0 is negatively sloped in the beginning and clearly after removing this neuron, the output will improve. The rest of the curves, in correspondence with the squared error degradation curve above, are mostly flat and tightly layered together, indicating that they are good neurons to remove.\nIn Figure 15 below, we observe a stark contrast to this. The curves corresponding to neurons 0 and 10 are mostly flat, and fairly lower than the rest, though clearly a mistake was made early on and the rest of the curves are clearly bad choices. In all of these cases however, we see that the curves are\neasily approximated with a straight line and so the first order method may have been fairly accurate in its predictions, even though it still made poor decisions.\nFigure 15 is an example of how things can go south once a few bad mistakes are made at the outset. Figure 16 shows a much better set of choices made by the second order method, though clearly not as good as the brute force method. The log-space plots make it a bit easier to see the difference between the brute force and second order methods in Figures 14 and 16, respectively.\n4.5.2 MNIST SINGLE DIGIT CLASSIFICATION: DIGIT 1\nExamining Figure 17, we see a much starker example of the previous phenomenon, in which the brute force method continues to improve the performance of the network after removing 80% of the neurons in the network. The first and second order methods fail early and proceed in fits and starts (clearly demonstrating evidence of interrelated groups of noise-canceling neurons), and never fully recover. It should be noted that it would be impossible to see curves like this if neural networks evenly distributed the learning representation evenly or equitably over their hidden units.\nOne of the most striking things about the blue curve in Figure 17 is the fact that the network never drops below its starting error until it crosses the 80% mark, indicating that only 20% of the neurons in this network are actually essential to the learning the training objective. In this sense, we can only\nwonder how much of the training time was spent winnowing the error out of the remaining 80% of the network.\nIn Figures 18, 19 and 20 we can examine the choices made by the respective methods. The brute force method serves as our example of a near-optimal pruning regimen, and the rest are first and second order approximations of this. Small differences, clearly, can lead to large effects on the network output as shown in Figure 17.\n4.5.3 MNIST SINGLE DIGIT CLASSIFICATION: DIGIT 2\nFigure 21 is an interesting case because it shatters our confidence in the reliability of the second order method to make good pruning decisions, and further demonstrates the phenomenon of how much the error can improve if the right neurons are removed after training gets stuck. In this case, though still a poor performance overall, the first order method vastly outperforms the second order method.\nFigure 22 shows us a clear example of the first element to remove having a negative error slope, and improving the output as a result. The rest of the pruning decisions are reasonable. Comparing with the blue curve in Figure 21, we see the correspondence between the first pruning decision improving the output, and the remaining pruning decisions keeping the output fairly flat. Clearly, however, there isn\u2019t much room to get worse given our starting point with a sub-optimal network, and we see that the ending sum of squared errors is not much higher than the starting point. At the same time, we can still see the contrast in performance if we make optimal pruning decisions, and most of the neurons in this network were clearly doing nothing.\nIn Figure 23, we see a mixed bag in which the decisions are clearly sub-optimal, though much better than Figure 24, in which we can observe how a bad first decision essentially ruined the network for good. The jagged edges of the red curve in Figure 21 correspond with the positive and negative\nslopes of the cluster of bad pruning decisions in 24. Once again, these are not necessarily bad decisions, but the starting point is already bad and this cannot be recovered without re-training the network.\n4.5.4 ASIDE: IMPLICATIONS OF THIS EXPERIMENT\nFrom the three examples above, we see that in each case, starting from a sub-optimal network, a brute force removal technique consistently improves performance for the first few pruning iterations, and the sum of squared errors does not degrade beyond the starting point until around 60-80% of the neurons have been removed. This is only possible if we have an essentially strict dichotomy between the roles of different neurons during training. If the network needs only 20-40% of the neurons it began with, the training process is essentially dominated by the task of canceling the residual noise of redundant neurons. Furthermore, the network can get stuck in training with redundant units and distort the final output. This is strong evidence of our thesis that the learning representation is neither equitably nor evenly distributed and that most of the neurons which do not directly participate in the learning representation can be removed without any retraining.\n4.6 EXPERIMENTS ON TOY DATASETS\nAs can be seen from the experiments on MNIST, even though the 2nd-order approximation criterion is consistently better than 1st-order, its performance is not nearly as good as brute force based rank-\ning, especially beyond the first layer. What is interesting to note is that from some other experiments conducted on toy datasets (predicting whether a given point would lie inside a given shape on the Cartesian plane), the performance of the 2nd-order method was found to be exceptionally good and produced results very close to the brute force method. The 1st-order method, as expected, performed poorly here as well. Some of these results are illustrated in Figure 25.\n5 CONCLUSIONS & FUTURE WORK\nIn conclusion, we must first re-assert that we do not present this work as a bench-marking study of the algorithm we derived and tested. We have merely used this algorithm as a jumping off point to investigate the nature of learning representations in neural networks. What we discovered is that first and second order methods do not make particularly good pruning decisions, and can get hopelessly lost after making a bad pruning decision resulting in a network fault. Furthermore, the brute-force algorithm does surprisingly well, despite being computationally expensive. This method does so well in fact, we argue that further investigation is warranted to make this algorithm computationally tractable, though we do not speculate on how that should be done here.\nWe also observed strong evidence for the hypotheses of Mozer & Smolensky (1989a) regarding the \u201cdualist\u201d nature of hidden units, i.e. that learning representations are divided between units which either participate in the output approximation or learn to cancel each others influence. This suggests that neural networks may in fact learn a minimal network implicitly, though we cannot say for sure that this is the case without further investigation. A necessary experiment to this end would be to compare the size of network constructed using cascade correlation (Fahlman & Lebiere (1989)) and compare it to the results described herein.\nWe have presented a novel algorithm for pruning whole neurons from a trained neural network using a second-order Taylor series approximation of the change in error resulting from the removal a given neuron as a pruning criteria. We compared this method to a first order method and a bruteforce serial removal method which exhaustively found the next best single neuron to remove at each stage. Our algorithm relies on a combination of assumptions similar to the ones made by Mozer & Smolensky (1989a) and LeCun et al. (1989) in the formulation of the Skeletonization and Optimal Brain Damage algorithms.\nFirst, we assumed that the error function with respect to each individual neuron can be approximated with a straight line or more precisely with a parabola. Second, for second derivative terms we consider only the diagonal elements of the Hessian matrix, i.e. we assume that each neuron-weight connection can be treated independently of the other elements in the network. Third, we assumed that pruning could be done in a serial fashion in which we find the single least productive element in the network, remove it, and move on. We found that all of these assumptions are deeply flawed in the sense that the true relevance of a neuron can only be partially approximated by a first or second order method, and only at certain stages of the pruning process.\nFor most problems, these methods can usually remove between 10-30% of the neurons in a trained network, but beyond this point their reliability breaks down. For certain problems, none of the described methods seem to perform very well, though for obvious reasons the brute-force method always exhibits the best results. The reason for this is that the error function with respect to each hidden unit is more complex than a simple second-order Taylor series can approximate. Furthermore, we have not directly taken into account the interdependence of elements within a network, though the work of Hassibi & Stork (1993) could provide some guidance in this regard. This is another critical issue to investigate in the future.\nRe-training may help in this regard. We freely admit that our algorithm does not use re-training to recover from errors made in pruning decisions. We argue that evaluating a network pruning algorithm using re-training does not allow us to make fair comparisons between the kinds of decisions made by these algorithms. Neural networks are very good at recovering from the removal of individual elements with re-training and so this compensates for sub-optimal pruning criteria.\nWe have observed that pruning whole neurons from an optimally trained network without major loss in performance is not only possible but also enables compressing networks to 40-70% of their original size, which is of great importance in constrained memory environments like embedded devices. We cite the results of our experiments using the brute force criterion as evidence of this conclusion. However expensive, it would be extremely easy to parallelize this method, or potentially approximate it using a subset of the training data to decide which neurons to prune. This avoids the problem of trying to approximate the importance of a unit and potentially making a mistake.\nIt would also be interesting to see how these methods perform on deeper networks and on some other popular and real world datasets. In our case, on the MNIST dataset, we observed that it was more difficult to prune neurons from a deeper network than from one with a single layer. We should expect\nthis trend to continue as networks get deeper and deeper, which also calls into further question the reliability of the described first and second order methods. We did investigate the order in which neurons were plucked from each layer of the networks and we found that the brute force method primarily removes neurons from the deepest layer of the network first, but there was no obvious pattern in layer preference for the other two methods.\nOur experiments using the visualization of error surfaces and pruning decisions concretely establish the fact that not all neurons in a network contribute to its performance in the same way, and the observed complexity of these functions demonstrates limitations of the approximations we used.\nFinally, we encourage the readers of this work to take these results into consideration when making decisions as to which methods to use to improve network generalization or compress their models. It should be remembered that various heuristics may perform well in practice for reasons which are in fact orthogonal to the accepted justifications given by their proponents.\n", "cats": {"READ": 0.0, "DONTREAD": 1.0}, "reviews": [{"IS_META_REVIEW": true, "comments": "The paper introduces a new pruning method for neural networks based on the second-order Taylor expansion and compares the results against a first-order method and brute-force pruning. It performs experiments of the three methods on several toy examples - including a two-layer network on MNIST - and shows that the second-order method behaves much worse then the brute-force baseline. In addition, from the success of the brute-force pruning the authors conclude that the hypothesis of Mozer et al - that neurons either contribute to performance or cancel out the effect of other neurons - is probably correct.\n\nThe authors put in considerable effort to explain all details of the paper clearly and at length, so the content of the paper is accessible even to people novel to pruning methods. Additionally, the authors have very carefully answered all questions that were coming up through the pre-review and have been very responsive.\n\nMy major criticism is that the paper lacks focus, does not have a concrete conclusion and does not explain what it adds to the literature. To make this apparent, I here summarise each paragraph of the conclusion section:\n\nParagraph 1: We do not benchmark / Pruning methods do not fare well against brute-force baseline / Some evidence for hypothesis of Mozer & Smolensky, but further investigation needed\n\nParagraph 2: Introduced 2nd order Taylor method / Does not fare well against baseline\n\nParagraph 3: Re-training may help but is not fair\n\nParagraph 4: Brute-force can prune 40-70% in shallow networks\n\nParagraph 5: Brute-force less effective in deep networks\n\nParagraph 6: Not all neurons contribute equally to performance of network\n\nThe title of the paper and answers of the authors to the pre-review questions seemed to strongly suggest that the paper is not about the new second-order method, is not about benchmarking pruning algorithms but is instead about the learnt representations. But only two or three sentences in the conclusion, and no sentence in the part on results in the abstract, even refers to neural representations. In an answer to the pre-review questions the authors stated:\n\n> Furthermore, we do not have to accept the conclusion that re-training is a necessary part of pruning because a brute force search reveals that neurons can in fact be \n> pruned from trained networks in a piecemeal fashion with no retraining and minimal adverse effect on the overall performance of the network. This would be \n> impossible if neurons did not belong to the distinct classes we describe.\"\n\nBut this can already be concluded from the 2nd order method, which has a similar characteristic and is based on other 2nd order methods (not shown here). What is the motivation to introduce a new 2nd order method here?\n\nIn addition, some other minor conclusions about representations - in particular the cancellation effect - might be based on side-effects of the greedy serial pruning method. Optimally, one would need to consider all the different ways of pruning (which, of course, scales exponentially with the number of neurons and is computationally infeasible). Notably, the authors do consider this limitation in the context of conventional pruning methods in the conclusions: \"Third, we assumed that pruning could be done in a serial fashion [...]. We found that all of these assumptions are deeply flawed in the sense that the true relevance of a neuron can only be partially approximated [...] at certain stages of the pruning process\". But the brute-force pruning process is also serial - why is that not a problem?\n\nAll in all it is unclear to me what the paper adds: there are little conclusions regarding the learnt representations nor is there sufficient benchmarking against state-of-the-art pruning methods. I would suggest to focus the paper in the following way: first, use a state-of-the-art pruning method from the literature (that works without re-training) or do not use any other pruning methods besides brute-force (depending on whether you want to compare pruning methods against brute-force, or want to learn something about the learnt representations). In this way you need to write little about this second-order tuning methods, and readers are not so easily confused about the purpose of this paper (plus it will be considerably shorter!). Then concentrate on 2-layer MNIST and a deeper CIFAR10 network. Further focus the paper by adding an itemised list of the exact contributions that you make, and streamline the paper accordingly. These measures could strongly boost the impact of your work but will require a major revision.\n\nPS: I think the confusion starts with the following sentence in the abstract: \"In this work we set out to test several long-held hypothesis about neural network learning representations and numerical approaches to pruning.\" Both aspects are pretty orthogonal, but are completely mixed up in the paper."}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "The paper does not seem to have enough novelty, and the contribution is not clear enough due to presentation issues.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"DATE": "02 Feb 2017", "TITLE": "Some related works", "IS_META_REVIEW": false, "comments": "1) Wen, Wei, et al. \"Learning structured sparsity in deep neural networks.\" Advances in Neural Information Processing Systems. 2016.\n2) Lebedev, Vadim, and Victor Lempitsky. \"Fast convnets using group-wise brain damage.\" Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2016.\n3) Alvarez, Jose M., and Mathieu Salzmann. \"Learning the Number of Neurons in Deep Networks.\" Advances in Neural Information Processing Systems. 2016.", "OTHER_KEYS": "(anonymous)"}, {"TITLE": "a good effort at understanding representations via pruning, but unclear overall contribution, with less than necessary results and methodological novelty ", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "The authors have put forward a sincere effort to investigate the \"fundamental nature of learning representations in neural networks\", a topic of great interest and importance to our field.  They propose to do this via a few simplistic pruning algorithms, to essentially monitor performance decay as a function of unit pruning.  This is an interesting idea and one that could potentially be instructive, though in total I don't think that has been achieved here.  \n\nFirst, I find the introduction of pruning lengthy and not particularly novel or surprising.  For example, Fig 1 is not necessary, nor is most of the preamble section 3.3.0.  The pruning algorithms themselves are sensible (though overly simplistic) approaches, which of course would not matter if they were effective in addressing the question.  However, in looking for contributions this paper makes, an interesting, pithy, or novel take on pruning is not one of them, in my opinion.\n\nSecond, and most relevant to my overall rating, Section 4 does not get deeper than scratching the surface.  The figures do not offer much beyond the expected decay in performance as a percentage of neurons removed or gain value.  The experiments themselves are not particularly deep, covering a toy problem and MNIST, which does not convince me that I can draw lessons to the broader story of neural networks more generally.  \n\nThird, there is no essential algorithmic, architectural, or mathematical insight, which I expect out of all but the most heavily experimental papers.", "IS_META_REVIEW": false, "RECOMMENDATION": 3, "DATE": "17 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "No Title", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "I did enjoy reading some of the introductions and background, in particular that of reminding readers of popular papers from the late 1980s and early 1990s. The idea of the proposal is straight forward: remove neurons based on the estimated change in the loss function from the packpropagation estimate with either first or second order backpropagation. The results are as expected that the first order method is worse then the second order method which in turn is worse than the brute force method.\n\nHowever, there are many reasons why I think that this work is not appropriate for ICLR. For one, there is now a much stronger comprehension of weight decay algorithms and their relation to Bayesian priors which has not been mentioned at all. I would think that any work in this regime would require at least some comments about this. Furthermore, there are many statements in the text that are not necessarily true, in particular in light of deep networks with modern regularization methods. For example, the authors state that the most accurate method is what they call brute-force. However, this assumes that the effects of each neurons are independent which might not be the case. So the serial order of removal is not necessarily the best. \n\nI also still think that this paper is unnecessarily long and the idea and the results could have been delivered in a much compressed way. I also don\u2019t think just writing a Q&A section is not enough, and the points should be included in the paper.\n", "IS_META_REVIEW": false, "RECOMMENDATION": 3, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "Looking at feature representations from the point of pruning is an interesting topic, but the conclusions nor the focus of this paper are clear", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "The paper introduces a new pruning method for neural networks based on the second-order Taylor expansion and compares the results against a first-order method and brute-force pruning. It performs experiments of the three methods on several toy examples - including a two-layer network on MNIST - and shows that the second-order method behaves much worse then the brute-force baseline. In addition, from the success of the brute-force pruning the authors conclude that the hypothesis of Mozer et al - that neurons either contribute to performance or cancel out the effect of other neurons - is probably correct.\n\nThe authors put in considerable effort to explain all details of the paper clearly and at length, so the content of the paper is accessible even to people novel to pruning methods. Additionally, the authors have very carefully answered all questions that were coming up through the pre-review and have been very responsive.\n\nMy major criticism is that the paper lacks focus, does not have a concrete conclusion and does not explain what it adds to the literature. To make this apparent, I here summarise each paragraph of the conclusion section:\n\nParagraph 1: We do not benchmark / Pruning methods do not fare well against brute-force baseline / Some evidence for hypothesis of Mozer & Smolensky, but further investigation needed\n\nParagraph 2: Introduced 2nd order Taylor method / Does not fare well against baseline\n\nParagraph 3: Re-training may help but is not fair\n\nParagraph 4: Brute-force can prune 40-70% in shallow networks\n\nParagraph 5: Brute-force less effective in deep networks\n\nParagraph 6: Not all neurons contribute equally to performance of network\n\nThe title of the paper and answers of the authors to the pre-review questions seemed to strongly suggest that the paper is not about the new second-order method, is not about benchmarking pruning algorithms but is instead about the learnt representations. But only two or three sentences in the conclusion, and no sentence in the part on results in the abstract, even refers to neural representations. In an answer to the pre-review questions the authors stated:\n\n> Furthermore, we do not have to accept the conclusion that re-training is a necessary part of pruning because a brute force search reveals that neurons can in fact be \n> pruned from trained networks in a piecemeal fashion with no retraining and minimal adverse effect on the overall performance of the network. This would be \n> impossible if neurons did not belong to the distinct classes we describe.\"\n\nBut this can already be concluded from the 2nd order method, which has a similar characteristic and is based on other 2nd order methods (not shown here). What is the motivation to introduce a new 2nd order method here?\n\nIn addition, some other minor conclusions about representations - in particular the cancellation effect - might be based on side-effects of the greedy serial pruning method. Optimally, one would need to consider all the different ways of pruning (which, of course, scales exponentially with the number of neurons and is computationally infeasible). Notably, the authors do consider this limitation in the context of conventional pruning methods in the conclusions: \"Third, we assumed that pruning could be done in a serial fashion [...]. We found that all of these assumptions are deeply flawed in the sense that the true relevance of a neuron can only be partially approximated [...] at certain stages of the pruning process\". But the brute-force pruning process is also serial - why is that not a problem?\n\nAll in all it is unclear to me what the paper adds: there are little conclusions regarding the learnt representations nor is there sufficient benchmarking against state-of-the-art pruning methods. I would suggest to focus the paper in the following way: first, use a state-of-the-art pruning method from the literature (that works without re-training) or do not use any other pruning methods besides brute-force (depending on whether you want to compare pruning methods against brute-force, or want to learn something about the learnt representations). In this way you need to write little about this second-order tuning methods, and readers are not so easily confused about the purpose of this paper (plus it will be considerably shorter!). Then concentrate on 2-layer MNIST and a deeper CIFAR10 network. Further focus the paper by adding an itemised list of the exact contributions that you make, and streamline the paper accordingly. These measures could strongly boost the impact of your work but will require a major revision.\n\nPS: I think the confusion starts with the following sentence in the abstract: \"In this work we set out to test several long-held hypothesis about neural network learning representations and numerical approaches to pruning.\" Both aspects are pretty orthogonal, but are completely mixed up in the paper.", "IS_META_REVIEW": false, "RECOMMENDATION": 3, "DATE": "15 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"DATE": "10 Dec 2016 (modified: 15 Dec 2016)", "TITLE": "Frequently Asked Questions", "IS_META_REVIEW": false, "comments": "Here are answers to some common questions the authors have been asked about the current work in the past by readers of the manuscript. We hope these will help clarify any other questions our reviewers/readers might have.\n\nQ: Why doesn't the paper present numerical comparision to state-of-the-art/recent pruning techniques?\n\nA: Under certain motivational assumptions, it is understandable to demand benchmarking comparisons against state-of-the-art methods, but this may be missing the fundamental purpose of the present research. Our investigation is intended less to propose a competing alternative to existing pruning techniques and more to shed light on the limitations of generally accepted approaches to pruning and the degree to which increased numbers of parameters affect learning representations in neural networks. The paper does talk about most, if not all popoular pruning techniques out there. In fact, we examined the literature for numerical methods to approximate the importance of network elements, and the widely-cited 1st & 2nd order techniques proposed by Mozer, LeCun, Hassibi, Stork, et al. provided our initial inspiration. This is the jumping off point for our research in terms of key insights.\n\nQ: The idea of using Taylor series approximations seems interesting but not really effective.\n\nA: It is not effective when used as a pruning technique but it is VERY effective to test out the effectiveness of existing pruning techniques, which is what we do here. We have mentioned it multiple times in the paper that the motivation behind this work is NOT to propose a new pruning technique that will outperform all other techniques out there but to tap into learning representations to see how effective our established techniques are when seen from the perspective of representations. The Taylor series approximations play an important role here. A lot of pruning techniques out there use 2nd Order error gradients and assume that using them is the most effective way to prune networks. We have conclusively proved using the Taylor series that this is very much not the case. Our results with the brute-force method show us that there is a much larger extent to which networks can be pruned. This makes for a great starting-off point for future research to find methods that can produce similar results.\n\nQ: Why did you decide in favor of sigmoid activation functions instead of something more recent and more popular like ReLUs? \n\nA: As mentioned above, the main contribution of this work is to demonstrate the feasibility of pruning entire neurons from trained networks, and offer novel insight on learning representations. We use Taylor methods to approximate the results achieved by the brute-force method but this is not an ideal solution to the problem, as we discuss. The 2nd order approximation technique will not work for ReLU networks because ReLUs do not have a 2nd derivative, unless we use the soft-plus function as a continuous approximation. Furthermore, due to the fact that we are approximating the error surface of a network element with respect to the output using a parabola, if there is no useful parabola to approximate this relationship, then the method breaks down. The derivatives of the activation function are simply parameters of the Taylor series. It doesn\u2019t cease to be a parabolic approximation or become more effective if we use a different doubly-differentiable activation function. \n\nQ: Why carry out your experiments on the MNIST dataset and not go for a larger and more practical image dataset?\n\nA: All experiments were necessarily carried out on optimally trained networks (not counting Section 4.5, which specifically examines non-optimally trained networks), so there is no way to improve them. We derived the algorithm assuming the well-studied sigmoid activation function. Furthermore, the MNIST dataset is a de-facto standard for demonstrating the potential of new techniques. A different dataset, task, activation function, or network architecture will not change the trends we see in the results but could make the results less interpretable. \n\nQ: The best setting is Iterative Re-ranking with Brute Force removal which is too expensive.\n\nA: The brute-force method is highly parallelizable, so time complexity is not necessarily a deal-breaker. Our focus is the proof of concept, and we intend to investigate potential speedups in future work. Also, since pruning is anyways a single step carried out after the training process is over (which usually takes orders of magnitude more time), this is potentially acceptable. \n\n\n", "OTHER_KEYS": "Aditya Sharma"}, {"DATE": "03 Dec 2016", "TITLE": "Revision History", "IS_META_REVIEW": false, "comments": "First Revision: 27 November, 2016. Added Section 4.5: Investigation of Pruning Performance with Imperfect Starting Conditions. We discuss the impact of pruning sub-optimally trained networks in this section by specifically analyzing performance of networks classifying the digits 0, 1 and 2 of the MNIST database.", "OTHER_KEYS": "Aditya Sharma"}, {"DATE": "02 Dec 2016", "TITLE": "appendix a", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1"}, {"DATE": "29 Nov 2016", "TITLE": "What are the new insights regarding representations?", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3"}, {"DATE": "28 Nov 2016", "TITLE": "Page limit", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2"}, {"IS_META_REVIEW": true, "comments": "The paper introduces a new pruning method for neural networks based on the second-order Taylor expansion and compares the results against a first-order method and brute-force pruning. It performs experiments of the three methods on several toy examples - including a two-layer network on MNIST - and shows that the second-order method behaves much worse then the brute-force baseline. In addition, from the success of the brute-force pruning the authors conclude that the hypothesis of Mozer et al - that neurons either contribute to performance or cancel out the effect of other neurons - is probably correct.\n\nThe authors put in considerable effort to explain all details of the paper clearly and at length, so the content of the paper is accessible even to people novel to pruning methods. Additionally, the authors have very carefully answered all questions that were coming up through the pre-review and have been very responsive.\n\nMy major criticism is that the paper lacks focus, does not have a concrete conclusion and does not explain what it adds to the literature. To make this apparent, I here summarise each paragraph of the conclusion section:\n\nParagraph 1: We do not benchmark / Pruning methods do not fare well against brute-force baseline / Some evidence for hypothesis of Mozer & Smolensky, but further investigation needed\n\nParagraph 2: Introduced 2nd order Taylor method / Does not fare well against baseline\n\nParagraph 3: Re-training may help but is not fair\n\nParagraph 4: Brute-force can prune 40-70% in shallow networks\n\nParagraph 5: Brute-force less effective in deep networks\n\nParagraph 6: Not all neurons contribute equally to performance of network\n\nThe title of the paper and answers of the authors to the pre-review questions seemed to strongly suggest that the paper is not about the new second-order method, is not about benchmarking pruning algorithms but is instead about the learnt representations. But only two or three sentences in the conclusion, and no sentence in the part on results in the abstract, even refers to neural representations. In an answer to the pre-review questions the authors stated:\n\n> Furthermore, we do not have to accept the conclusion that re-training is a necessary part of pruning because a brute force search reveals that neurons can in fact be \n> pruned from trained networks in a piecemeal fashion with no retraining and minimal adverse effect on the overall performance of the network. This would be \n> impossible if neurons did not belong to the distinct classes we describe.\"\n\nBut this can already be concluded from the 2nd order method, which has a similar characteristic and is based on other 2nd order methods (not shown here). What is the motivation to introduce a new 2nd order method here?\n\nIn addition, some other minor conclusions about representations - in particular the cancellation effect - might be based on side-effects of the greedy serial pruning method. Optimally, one would need to consider all the different ways of pruning (which, of course, scales exponentially with the number of neurons and is computationally infeasible). Notably, the authors do consider this limitation in the context of conventional pruning methods in the conclusions: \"Third, we assumed that pruning could be done in a serial fashion [...]. We found that all of these assumptions are deeply flawed in the sense that the true relevance of a neuron can only be partially approximated [...] at certain stages of the pruning process\". But the brute-force pruning process is also serial - why is that not a problem?\n\nAll in all it is unclear to me what the paper adds: there are little conclusions regarding the learnt representations nor is there sufficient benchmarking against state-of-the-art pruning methods. I would suggest to focus the paper in the following way: first, use a state-of-the-art pruning method from the literature (that works without re-training) or do not use any other pruning methods besides brute-force (depending on whether you want to compare pruning methods against brute-force, or want to learn something about the learnt representations). In this way you need to write little about this second-order tuning methods, and readers are not so easily confused about the purpose of this paper (plus it will be considerably shorter!). Then concentrate on 2-layer MNIST and a deeper CIFAR10 network. Further focus the paper by adding an itemised list of the exact contributions that you make, and streamline the paper accordingly. These measures could strongly boost the impact of your work but will require a major revision.\n\nPS: I think the confusion starts with the following sentence in the abstract: \"In this work we set out to test several long-held hypothesis about neural network learning representations and numerical approaches to pruning.\" Both aspects are pretty orthogonal, but are completely mixed up in the paper."}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "The paper does not seem to have enough novelty, and the contribution is not clear enough due to presentation issues.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"DATE": "02 Feb 2017", "TITLE": "Some related works", "IS_META_REVIEW": false, "comments": "1) Wen, Wei, et al. \"Learning structured sparsity in deep neural networks.\" Advances in Neural Information Processing Systems. 2016.\n2) Lebedev, Vadim, and Victor Lempitsky. \"Fast convnets using group-wise brain damage.\" Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2016.\n3) Alvarez, Jose M., and Mathieu Salzmann. \"Learning the Number of Neurons in Deep Networks.\" Advances in Neural Information Processing Systems. 2016.", "OTHER_KEYS": "(anonymous)"}, {"TITLE": "a good effort at understanding representations via pruning, but unclear overall contribution, with less than necessary results and methodological novelty ", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "The authors have put forward a sincere effort to investigate the \"fundamental nature of learning representations in neural networks\", a topic of great interest and importance to our field.  They propose to do this via a few simplistic pruning algorithms, to essentially monitor performance decay as a function of unit pruning.  This is an interesting idea and one that could potentially be instructive, though in total I don't think that has been achieved here.  \n\nFirst, I find the introduction of pruning lengthy and not particularly novel or surprising.  For example, Fig 1 is not necessary, nor is most of the preamble section 3.3.0.  The pruning algorithms themselves are sensible (though overly simplistic) approaches, which of course would not matter if they were effective in addressing the question.  However, in looking for contributions this paper makes, an interesting, pithy, or novel take on pruning is not one of them, in my opinion.\n\nSecond, and most relevant to my overall rating, Section 4 does not get deeper than scratching the surface.  The figures do not offer much beyond the expected decay in performance as a percentage of neurons removed or gain value.  The experiments themselves are not particularly deep, covering a toy problem and MNIST, which does not convince me that I can draw lessons to the broader story of neural networks more generally.  \n\nThird, there is no essential algorithmic, architectural, or mathematical insight, which I expect out of all but the most heavily experimental papers.", "IS_META_REVIEW": false, "RECOMMENDATION": 3, "DATE": "17 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "No Title", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "I did enjoy reading some of the introductions and background, in particular that of reminding readers of popular papers from the late 1980s and early 1990s. The idea of the proposal is straight forward: remove neurons based on the estimated change in the loss function from the packpropagation estimate with either first or second order backpropagation. The results are as expected that the first order method is worse then the second order method which in turn is worse than the brute force method.\n\nHowever, there are many reasons why I think that this work is not appropriate for ICLR. For one, there is now a much stronger comprehension of weight decay algorithms and their relation to Bayesian priors which has not been mentioned at all. I would think that any work in this regime would require at least some comments about this. Furthermore, there are many statements in the text that are not necessarily true, in particular in light of deep networks with modern regularization methods. For example, the authors state that the most accurate method is what they call brute-force. However, this assumes that the effects of each neurons are independent which might not be the case. So the serial order of removal is not necessarily the best. \n\nI also still think that this paper is unnecessarily long and the idea and the results could have been delivered in a much compressed way. I also don\u2019t think just writing a Q&A section is not enough, and the points should be included in the paper.\n", "IS_META_REVIEW": false, "RECOMMENDATION": 3, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "Looking at feature representations from the point of pruning is an interesting topic, but the conclusions nor the focus of this paper are clear", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "The paper introduces a new pruning method for neural networks based on the second-order Taylor expansion and compares the results against a first-order method and brute-force pruning. It performs experiments of the three methods on several toy examples - including a two-layer network on MNIST - and shows that the second-order method behaves much worse then the brute-force baseline. In addition, from the success of the brute-force pruning the authors conclude that the hypothesis of Mozer et al - that neurons either contribute to performance or cancel out the effect of other neurons - is probably correct.\n\nThe authors put in considerable effort to explain all details of the paper clearly and at length, so the content of the paper is accessible even to people novel to pruning methods. Additionally, the authors have very carefully answered all questions that were coming up through the pre-review and have been very responsive.\n\nMy major criticism is that the paper lacks focus, does not have a concrete conclusion and does not explain what it adds to the literature. To make this apparent, I here summarise each paragraph of the conclusion section:\n\nParagraph 1: We do not benchmark / Pruning methods do not fare well against brute-force baseline / Some evidence for hypothesis of Mozer & Smolensky, but further investigation needed\n\nParagraph 2: Introduced 2nd order Taylor method / Does not fare well against baseline\n\nParagraph 3: Re-training may help but is not fair\n\nParagraph 4: Brute-force can prune 40-70% in shallow networks\n\nParagraph 5: Brute-force less effective in deep networks\n\nParagraph 6: Not all neurons contribute equally to performance of network\n\nThe title of the paper and answers of the authors to the pre-review questions seemed to strongly suggest that the paper is not about the new second-order method, is not about benchmarking pruning algorithms but is instead about the learnt representations. But only two or three sentences in the conclusion, and no sentence in the part on results in the abstract, even refers to neural representations. In an answer to the pre-review questions the authors stated:\n\n> Furthermore, we do not have to accept the conclusion that re-training is a necessary part of pruning because a brute force search reveals that neurons can in fact be \n> pruned from trained networks in a piecemeal fashion with no retraining and minimal adverse effect on the overall performance of the network. This would be \n> impossible if neurons did not belong to the distinct classes we describe.\"\n\nBut this can already be concluded from the 2nd order method, which has a similar characteristic and is based on other 2nd order methods (not shown here). What is the motivation to introduce a new 2nd order method here?\n\nIn addition, some other minor conclusions about representations - in particular the cancellation effect - might be based on side-effects of the greedy serial pruning method. Optimally, one would need to consider all the different ways of pruning (which, of course, scales exponentially with the number of neurons and is computationally infeasible). Notably, the authors do consider this limitation in the context of conventional pruning methods in the conclusions: \"Third, we assumed that pruning could be done in a serial fashion [...]. We found that all of these assumptions are deeply flawed in the sense that the true relevance of a neuron can only be partially approximated [...] at certain stages of the pruning process\". But the brute-force pruning process is also serial - why is that not a problem?\n\nAll in all it is unclear to me what the paper adds: there are little conclusions regarding the learnt representations nor is there sufficient benchmarking against state-of-the-art pruning methods. I would suggest to focus the paper in the following way: first, use a state-of-the-art pruning method from the literature (that works without re-training) or do not use any other pruning methods besides brute-force (depending on whether you want to compare pruning methods against brute-force, or want to learn something about the learnt representations). In this way you need to write little about this second-order tuning methods, and readers are not so easily confused about the purpose of this paper (plus it will be considerably shorter!). Then concentrate on 2-layer MNIST and a deeper CIFAR10 network. Further focus the paper by adding an itemised list of the exact contributions that you make, and streamline the paper accordingly. These measures could strongly boost the impact of your work but will require a major revision.\n\nPS: I think the confusion starts with the following sentence in the abstract: \"In this work we set out to test several long-held hypothesis about neural network learning representations and numerical approaches to pruning.\" Both aspects are pretty orthogonal, but are completely mixed up in the paper.", "IS_META_REVIEW": false, "RECOMMENDATION": 3, "DATE": "15 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"DATE": "10 Dec 2016 (modified: 15 Dec 2016)", "TITLE": "Frequently Asked Questions", "IS_META_REVIEW": false, "comments": "Here are answers to some common questions the authors have been asked about the current work in the past by readers of the manuscript. We hope these will help clarify any other questions our reviewers/readers might have.\n\nQ: Why doesn't the paper present numerical comparision to state-of-the-art/recent pruning techniques?\n\nA: Under certain motivational assumptions, it is understandable to demand benchmarking comparisons against state-of-the-art methods, but this may be missing the fundamental purpose of the present research. Our investigation is intended less to propose a competing alternative to existing pruning techniques and more to shed light on the limitations of generally accepted approaches to pruning and the degree to which increased numbers of parameters affect learning representations in neural networks. The paper does talk about most, if not all popoular pruning techniques out there. In fact, we examined the literature for numerical methods to approximate the importance of network elements, and the widely-cited 1st & 2nd order techniques proposed by Mozer, LeCun, Hassibi, Stork, et al. provided our initial inspiration. This is the jumping off point for our research in terms of key insights.\n\nQ: The idea of using Taylor series approximations seems interesting but not really effective.\n\nA: It is not effective when used as a pruning technique but it is VERY effective to test out the effectiveness of existing pruning techniques, which is what we do here. We have mentioned it multiple times in the paper that the motivation behind this work is NOT to propose a new pruning technique that will outperform all other techniques out there but to tap into learning representations to see how effective our established techniques are when seen from the perspective of representations. The Taylor series approximations play an important role here. A lot of pruning techniques out there use 2nd Order error gradients and assume that using them is the most effective way to prune networks. We have conclusively proved using the Taylor series that this is very much not the case. Our results with the brute-force method show us that there is a much larger extent to which networks can be pruned. This makes for a great starting-off point for future research to find methods that can produce similar results.\n\nQ: Why did you decide in favor of sigmoid activation functions instead of something more recent and more popular like ReLUs? \n\nA: As mentioned above, the main contribution of this work is to demonstrate the feasibility of pruning entire neurons from trained networks, and offer novel insight on learning representations. We use Taylor methods to approximate the results achieved by the brute-force method but this is not an ideal solution to the problem, as we discuss. The 2nd order approximation technique will not work for ReLU networks because ReLUs do not have a 2nd derivative, unless we use the soft-plus function as a continuous approximation. Furthermore, due to the fact that we are approximating the error surface of a network element with respect to the output using a parabola, if there is no useful parabola to approximate this relationship, then the method breaks down. The derivatives of the activation function are simply parameters of the Taylor series. It doesn\u2019t cease to be a parabolic approximation or become more effective if we use a different doubly-differentiable activation function. \n\nQ: Why carry out your experiments on the MNIST dataset and not go for a larger and more practical image dataset?\n\nA: All experiments were necessarily carried out on optimally trained networks (not counting Section 4.5, which specifically examines non-optimally trained networks), so there is no way to improve them. We derived the algorithm assuming the well-studied sigmoid activation function. Furthermore, the MNIST dataset is a de-facto standard for demonstrating the potential of new techniques. A different dataset, task, activation function, or network architecture will not change the trends we see in the results but could make the results less interpretable. \n\nQ: The best setting is Iterative Re-ranking with Brute Force removal which is too expensive.\n\nA: The brute-force method is highly parallelizable, so time complexity is not necessarily a deal-breaker. Our focus is the proof of concept, and we intend to investigate potential speedups in future work. Also, since pruning is anyways a single step carried out after the training process is over (which usually takes orders of magnitude more time), this is potentially acceptable. \n\n\n", "OTHER_KEYS": "Aditya Sharma"}, {"DATE": "03 Dec 2016", "TITLE": "Revision History", "IS_META_REVIEW": false, "comments": "First Revision: 27 November, 2016. Added Section 4.5: Investigation of Pruning Performance with Imperfect Starting Conditions. We discuss the impact of pruning sub-optimally trained networks in this section by specifically analyzing performance of networks classifying the digits 0, 1 and 2 of the MNIST database.", "OTHER_KEYS": "Aditya Sharma"}, {"DATE": "02 Dec 2016", "TITLE": "appendix a", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1"}, {"DATE": "29 Nov 2016", "TITLE": "What are the new insights regarding representations?", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3"}, {"DATE": "28 Nov 2016", "TITLE": "Page limit", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2"}]}
{"text": "1 INTRODUCTION\nCombinatorial optimization is a fundamental problem in computer science. A canonical example is the traveling salesman problem (TSP), where given a graph, one needs to search the space of permutations to find an optimal sequence of nodes with minimal total edge weights (tour length). The TSP and its variants have myriad applications in planning, manufacturing, genetics, etc. (see (Applegate et al., 2011) for an overview).\nFinding the optimal TSP solution is NP-hard, even in the two-dimensional Euclidean case (Papadimitriou, 1977), where the nodes are 2D points and edge weights are Euclidean distances between pairs of points. In practice, TSP solvers rely on handcrafted heuristics that guide their search procedures to find competitive (and in many cases optimal) tours efficiently. Even though these heuristics work well on TSP, once the problem statement changes slightly, they need to be revised. In contrast, machine learning methods have the potential to be applicable across many optimization tasks by automatically discovering their own heuristics based on the training data, thus requiring less handengineering than solvers that are optimized for one task only.\nWhile most successful machine learning techniques fall into the family of supervised learning, where a mapping from training inputs to outputs is learned, supervised learning is not applicable to most combinatorial optimization problems because one does not have access to optimal labels. However, one can compare the quality of a set of solutions using a verifier, and provide some reward feedbacks to a learning algorithm. Hence, we follow the reinforcement learning (RL) paradigm to tackle combinatorial optimization. We empirically demonstrate that, even when using optimal solutions as labeled data to optimize a supervised mapping, the generalization is rather poor compared to an RL agent that explores different tours and observes their corresponding rewards.\nWe propose Neural Combinatorial Optimization, a framework to tackle combinatorial optimization problems using reinforcement learning and neural networks. We consider two approaches based on policy gradients (Williams, 1992). The first approach, called RL pretraining, uses a training set to optimize a recurrent neural network (RNN) that parameterizes a stochastic policy over solutions, using the expected reward as objective. At test time, the policy is fixed, and one performs inference \u2217Equal contributions. Members of the Google Brain Residency program (g.co/brainresidency).\nby greedy decoding or sampling. The second approach, called active search, involves no pretraining. It starts from a random policy and iteratively optimizes the RNN parameters on a single test instance, again using the expected reward objective, while keeping track of the best solution sampled during the search. We find that combining RL pretraining and active search works best in practice.\nOn 2D Euclidean graphs with up to 100 nodes, Neural Combinatorial Optimization significantly outperforms the supervised learning approach to the TSP (Vinyals et al., 2015b) and obtains close to optimal results when allowed more computation time (see Figure 1). We illustrate the flexibility of the method by also applying it to the KnapSack problem, for which we get optimal results for instances with up to 200 items. Our results, while still inferior to the state-of-the-art in many dimensions (such as speed, scale and performance), give insights into how neural networks can be used as a general tool for tackling combinatorial optimization problems, especially those that are difficult to design heuristics for.\n2 PREVIOUS WORK\nThe Traveling Salesman Problem is a well studied combinatorial optimization problem and many exact or approximate algorithms have been proposed for both Euclidean and non-Euclidean graphs. Christofides (1976) proposes a heuristic algorithm that involves computing a minimum-spanning tree and a minimum-weight perfect matching. The algorithm has polynomial running time and returns solutions that are guaranteed to be within a factor of 1.5\u00d7 to optimality in the metric instance of the TSP.\nThe best known exact dynamic programming algorithm for TSP has a complexity of \u0398(2nn2), making it infeasible to scale up to large instances, say with 40 points. Nevertheless, state of the art TSP solvers, thanks to carefully handcrafted heuristics that describe how to navigate the space of feasible solutions in an efficient manner, can solve symmetric TSP instances with thousands of nodes. Concorde (Applegate et al., 2006), widely accepted as one of the best exact TSP solvers, makes use of cutting plane algorithms (Dantzig et al., 1954; Padberg & Rinaldi, 1990; Applegate et al., 2003), iteratively solving linear programming relaxations of the TSP, in conjunction with a branch-and-bound approach that prunes parts of the search space that provably will not contain an optimal solution. Similarly, the Lin-Kernighan-Helsgaun heuristic (Helsgaun, 2000), inspired from the Lin-Kernighan heuristic (Lin & Kernighan, 1973), is a state of the art approximate search heuristic for the symmetric TSP and has been shown to solve instances with hundreds of nodes to optimality.\nMore generic solvers, such as Google\u2019s vehicle routing problem solver (Google, 2016) that tackles a superset of the TSP, typically rely on a combination of local search algorithms and metaheuristics. Local search algorithms apply a specified set of local move operators on candidate solutions, based\non hand-engineered heuristics such as 2-opt (Johnson, 1990), to navigate from solution to solution in the search space. A metaheuristic is then applied to propose uphill moves and escape local optima. A popular choice of metaheuristic for the TSP and its variants is guided local search (Voudouris & Tsang, 1999), which moves out of a local minimum by penalizing particular solution features that it considers should not occur in a good solution.\nThe difficulty in applying existing search heuristics to newly encountered problems - or even new instances of a similar problem - is a well-known challenge that stems from the No Free Lunch theorem (Wolpert & Macready, 1997). Because all search algorithms have the same performance when averaged over all problems, one must appropriately rely on a prior over problems when selecting a search algorithm to guarantee performance. This challenge has fostered interest in raising the level of generality at which optimization systems operate (Burke et al., 2003) and is the underlying motivation behind hyper-heuristics, defined as \u201dsearch method[s] or learning mechanism[s] for selecting or generating heuristics to solve computation search problems\u201d. Hyper-heuristics aim to be easier to use than problem specific methods by partially abstracting away the knowledge intensive process of selecting heuristics given a combinatorial problem and have been shown to successfully combine human-defined heuristics in superior ways across many tasks (see (Burke et al., 2013) for a survey). However, hyper-heuristics operate on the search space of heuristics, rather than the search space of solutions, therefore still initially relying on human created heuristics.\nThe application of neural networks to combinatorial optimization has a distinguished history, where the majority of research focuses on the Traveling Salesman Problem (Smith, 1999). One of the earliest proposals is the use of Hopfield networks (Hopfield & Tank, 1985) for the TSP. The authors modify the network\u2019s energy function to make it equivalent to TSP objective and use Lagrange multipliers to penalize the violations of the problem\u2019s constraints. A limitation of this approach is that it is sensitive to hyperparameters and parameter initialization as analyzed by (Wilson & Pawley, 1988). Overcoming this limitation is central to the subsequent work in the field, especially by (Aiyer et al., 1990; Gee, 1993). Parallel to the development of Hopfield networks is the work on using deformable template models to solve TSP. Perhaps most prominent is the invention of Elastic Nets as a means to solve TSP (Durbin, 1987), and the application of Self Organizing Map to TSP (Fort, 1988; Angeniol et al., 1988; Kohonen, 1990). Addressing the limitations of deformable template models is central to the following work in this area (Burke, 1994; Favata & Walker, 1991; Vakhutinsky & Golden, 1995). Even though these neural networks have many appealing properties, they are still limited as research work. When being carefully benchmarked, they have not yielded satisfying results compared to algorithmic methods (Sarwar & Bhatti, 2012; La Maire & Mladenov, 2012). Perhaps due to the negative results, this research direction is largely overlooked since the turn of the century.\nMotivated by the recent advancements in sequence-to-sequence learning (Sutskever et al., 2014), neural networks are again the subject of study for optimization in various domains (Yutian et al., 2016), including discrete ones (Zoph & Le, 2016). In particular, the TSP is revisited in the introduction of Pointer Networks (Vinyals et al., 2015b), where a recurrent network with non-parametric softmaxes is trained in a supervised manner to predict the sequence of visited cities. Despite architecural improvements, their models were trained using supervised signals given by an approximate solver.\n3 NEURAL NETWORK ARCHITECTURE FOR TSP\nWe focus on the 2D Euclidean TSP in this paper. Given an input graph, represented as a sequence of n cities in a two dimensional space s = {xi}ni=1 where each xi \u2208 R2, we are concerned with finding a permutation of the points \u03c0, termed a tour, that visits each city once and has the minimum total length. We define the length of a tour defined by a permutation \u03c0 as\nL(\u03c0 | s) = \u2225\u2225x\u03c0(n) \u2212 x\u03c0(1)\u2225\u22252 + n\u22121\u2211\ni=1 \u2225\u2225x\u03c0(i) \u2212 x\u03c0(i+1)\u2225\u22252 , (1) where \u2016\u00b7\u20162 denotes `2 norm. We aim to learn the parameters of a stochastic policy p(\u03c0 | s) that given an input set of points s, assigns high probabilities to short tours and low probabilities to long tours. Our neural network\narchitecture uses the chain rule to factorize the probability of a tour as\np(\u03c0 | s) = n\u220f i=1 p (\u03c0(i) | \u03c0(< i), s) , (2)\nand then uses individual softmax modules to represent each term on the RHS of (2).\nWe are inspired by previous work (Sutskever et al., 2014) that makes use of the same factorization based on the chain rule to address sequence to sequence problems like machine translation. One can use a vanilla sequence to sequence model to address the TSP where the output vocabulary is {1, 2, . . . , n}. However, there are two major issues with this approach: (1) networks trained in this fashion cannot generalize to inputs with more than n cities. (2) one needs to have access to groundtruth output permutations to optimize the parameters with conditional log-likelihood. We address both isssues in this paper.\nFor generalization beyond a pre-specified graph size, we follow the approach of (Vinyals et al., 2015b), which makes use of a set of non-parameteric softmax modules, resembling the attention mechanism from (Bahdanau et al., 2015). This approach, named pointer network, allows the model to effectively point to a specific position in the input sequence rather than predicting an index value from a fixed-size vocabulary. We employ the pointer network architecture, depicted in Figure 2, as our policy model to parameterize p(\u03c0 | s).\n3.1 ARCHITECTURE DETAILS\nOur pointer network comprises two recurrent neural network (RNN) modules, encoder and decoder, both of which consist of Long Short-Term Memory (LSTM) cells (Hochreiter & Schmidhuber, 1997). The encoder network reads the input sequence s, one city at a time, and transforms it into a sequence of latent memory states {enci}ni=1 where enci \u2208 Rd. The input to the encoder network at time step i is a d-dimensional embedding of a 2D point xi, which is obtained via a linear transformation of xi shared across all input steps. The decoder network also maintains its latent memory states {deci}ni=1 where deci \u2208 Rd and, at each step i, uses a pointing mechanism to produce a distribution over the next city to visit in the tour. Once the next city is selected, it is passed as the input to the next decoder step. The input of the first decoder step (denoted by \u3008g\u3009 in Figure 2) is a d-dimensional vector treated as a trainable parameter of our neural network.\nOur attention function, formally defined in Appendix A.1, takes as input a query vector q = deci \u2208 Rd and a set of reference vectors ref = {enc1, . . . , enck} where enci \u2208 Rd, and predicts a distribution A(ref, q) over the set of k references. This probability distribution represents the degree to which the model is pointing to reference ri upon seeing query q.\nVinyals et al. (2015a) also suggest including some additional computation steps, named glimpses, to aggregate the contributions of different parts of the input sequence, very much like (Bahdanau et al., 2015). We discuss this approach in details in Appendix A.1. In our experiments, we find that utilizing one glimpse in the pointing mechanism yields performance gains at an insignificant cost latency.\nAlgorithm 1 Actor-critic training 1: procedure TRAIN(training set S, number of training steps T , batch size B) 2: Initialize pointer network params \u03b8 3: Initialize critic network params \u03b8v 4: for t = 1 to T do 5: si \u223c SAMPLEINPUT(S) for i \u2208 {1, . . . , B} 6: \u03c0i \u223c SAMPLESOLUTION(p\u03b8(.|si)) for i \u2208 {1, . . . , B} 7: bi \u2190 b\u03b8v (si) for i \u2208 {1, . . . , B} 8: g\u03b8 \u2190 1B \u2211B i=1(L(\u03c0i|si)\u2212 bi)\u2207\u03b8 log p\u03b8(\u03c0i|si)\n9: Lv \u2190 1B \u2211B i=1 \u2016bi \u2212 L(\u03c0i)\u2016 2 2\n10: \u03b8 \u2190 ADAM(\u03b8, g\u03b8) 11: \u03b8v \u2190 ADAM(\u03b8v,\u2207\u03b8vLv) 12: end for 13: return \u03b8 14: end procedure\n4 OPTIMIZATION WITH POLICY GRADIENTS\nVinyals et al. (2015b) proposes training a pointer network using a supervised loss function comprising conditional log-likelihood, which factors into a cross entropy objective between the network\u2019s output probabilities and the targets provided by a TSP solver. Learning from examples in such a way is undesirable for NP-hard problems because (1) the performance of the model is tied to the quality of the supervised labels, (2) getting high-quality labeled data is expensive and may be infeasible for new problem statements, (3) one cares about finding a competitive solution more than replicating the results of another algorithm.\nBy contrast, we believe Reinforcement Learning (RL) provides an appropriate paradigm for training neural networks for combinatorial optimization, especially because these problems have relatively simple reward mechanisms that could be even used at test time. We hence propose to use model-free policy-based Reinforcement Learning to optimize the parameters of a pointer network denoted \u03b8. Our training objective is the expected tour length which, given an input graph s, is defined as\nJ(\u03b8 | s) = E\u03c0\u223cp\u03b8(.|s) L(\u03c0 | s) . (3) During training, our graphs are drawn from a distribution S, and the total training objective involves sampling from the distribution of graphs, i.e. J(\u03b8) = Es\u223cS J(\u03b8 | s) . We resort to policy gradient methods and stochastic gradient descent to optimize the parameters. The gradient of (3) is formulated using the well-known REINFORCE algorithm (Williams, 1992):\n\u2207\u03b8J(\u03b8 | s) = E\u03c0\u223cp\u03b8(.|s) [( L(\u03c0 | s)\u2212 b(s) ) \u2207\u03b8 log p\u03b8(\u03c0 | s) ] , (4)\nwhere b(s) denotes a baseline function that does not depend on \u03c0 and estimates the expected tour length to reduce the variance of the gradients.\nBy drawing B i.i.d. sample graphs s1, s2, . . . , sB \u223c S and sampling a single tour per graph, i.e. \u03c0i \u223c p\u03b8(. | si), the gradient in (4) is approximated with Monte Carlo sampling as follows:\n\u2207\u03b8J(\u03b8) \u2248 1\nB B\u2211 i=1 ( L(\u03c0i|si)\u2212 b(si) ) \u2207\u03b8 log p\u03b8(\u03c0i | si) . (5)\nA simple and popular choice of the baseline b(s) is an exponential moving average of the rewards obtained by the network over time to account for the fact that the policy improves with training. While this choice of baseline proved sufficient to improve upon the Christofides algorithm, it suffers from not being able to differentiate between different input graphs. In particular, the optimal tour \u03c0\u2217 for a difficult graph s may be still discouraged if L(\u03c0\u2217|s) > b because b is shared across all instances in the batch.\nUsing a parametric baseline to estimate the expected tour length E\u03c0\u223cp\u03b8(.|s)L(\u03c0 | s) typically improves learning. Therefore, we introduce an auxiliary network, called a critic and parameterized\nAlgorithm 2 Active Search 1: procedure ACTIVESEARCH(input s, \u03b8, number of candidates K, B, \u03b1) 2: \u03c0 \u2190 RANDOMSOLUTION() 3: L\u03c0 \u2190 L(\u03c0 | s) 4: n\u2190 dK\nB e\n5: for t = 1 . . . n do 6: \u03c0i \u223c SAMPLESOLUTION(p\u03b8(. | s)) for i \u2208 {1, . . . , B} 7: j \u2190 ARGMIN(L(\u03c01 | s) . . . L(\u03c0B | s)) 8: Lj \u2190 L(\u03c0j | s) 9: if Lj < L\u03c0 then\n10: \u03c0 \u2190 \u03c0j 11: L\u03c0 \u2190 Lj 12: end if 13: g\u03b8 \u2190 1B \u2211B i=1(L(\u03c0i | s)\u2212 b)\u2207\u03b8 log p\u03b8(\u03c0i | s) 14: \u03b8 \u2190 ADAM(\u03b8, g\u03b8) 15: b\u2190 \u03b1\u00d7 b+ (1\u2212 \u03b1)\u00d7 ( 1\nB \u2211B i=1 bi)\n16: end for 17: return \u03c0 18: end procedure\nby \u03b8v , to learn the expected tour length found by our current policy p\u03b8 given an input sequence s. The critic is trained with stochastic gradient descent on a mean squared error objective between its predictions b\u03b8v (s) and the actual tour lengths sampled by the most recent policy. The additional objective is formulated as\nL(\u03b8v) = 1\nB B\u2211 i=1 \u2225\u2225 b\u03b8v (si)\u2212 L(\u03c0i | si)\u2225\u222522 . (6) Critic\u2019s architecture for TSP. We now explain how our critic maps an input sequence s into a baseline prediction b\u03b8v (s). Our critic comprises three neural network modules: 1) an LSTM encoder, 2) an LSTM process block and 3) a 2-layer ReLU neural network decoder. Its encoder has the same architecture as that of our pointer network\u2019s encoder and encodes an input sequence s into a sequence of latent memory states and a hidden state h. The process block, similarly to (Vinyals et al., 2015a), then performs P steps of computation over the hidden state h. Each processing step updates this hidden state by glimpsing at the memory states as described in Appendix A.1 and feeds the output of the glimpse function as input to the next processing step. At the end of the process block, the obtained hidden state is then decoded into a baseline prediction (i.e a single scalar) by two fully connected layers with respectively d and 1 unit(s).\nOur training algorithm, described in Algorithm 1, is closely related to the asynchronous advantage actor-critic (A3C) proposed in (Mnih et al., 2016), as the difference between the sampled tour lengths and the critic\u2019s predictions is an unbiased estimate of the advantage function. We perform our updates asynchronously across multiple workers, but each worker also handles a mini-batch of graphs for better gradient estimates.\n4.1 SEARCH STRATEGIES\nAs evaluating a tour length is inexpensive, our TSP agent can easily simulate a search procedure at inference time by considering multiple candidate solutions per graph and selecting the best. This inference process resembles how solvers search over a large set of feasible solutions. In this paper, we consider two search strategies detailed below, which we refer to as sampling and active search.\nSampling. Our first approach is simply to sample multiple candidate tours from our stochastic policy p\u03b8(.|s) and select the shortest one. In contrast to heuristic solvers, we do not enforce our model to sample different tours during the process. However, we can control the diversity of the sampled tours with a temperature hyperparameter when sampling from our non-parametric softmax (see Appendix A.2). This sampling process yields significant improvements over greedy decoding, which always selects the index with the largest probability. We also considered perturbing the pointing\nmechanism with random noise and greedily decoding from the obtained modified policy, similarly to (Cho, 2016), but this proves less effective than sampling in our experiments.\nActive Search. Rather than sampling with a fixed model and ignoring the reward information obtained from the sampled solutions, one can refine the parameters of the stochastic policy p\u03b8 during inference to minimize E\u03c0\u223cp\u03b8(.|s)L(\u03c0 | s) on a single test input s. This approach proves especially competitive when starting from a trained model. Remarkably, it also produces satisfying solutions when starting from an untrained model. We refer to these two approaches as RL pretraining-Active Search and Active Search because the model actively updates its parameters while searching for candidate solutions on a single test instance.\nActive Search applies policy gradients similarly to Algorithm 1 but draws Monte Carlo samples over candidate solutions \u03c01 . . . \u03c0B \u223c p\u03b8(\u00b7|s) for a single test input. It resorts to an exponential moving average baseline, rather than a critic, as there is no need to differentiate between inputs. Our Active Search training algorithm is presented in Algorithm 2. We note that while RL training does not require supervision, it still requires training data and hence generalization depends on the training data distribution. In contrast, Active Search is distribution independent. Finally, since we encode a set of cities as a sequence, we randomly shuffle the input sequence before feeding it to our pointer network. This increases the stochasticity of the sampling procedure and leads to large improvements in Active Search.\n5 EXPERIMENTS\nWe conduct experiments to investigate the behavior of the proposed Neural Combinatorial Optimization methods. We consider three benchmark tasks, Euclidean TSP20, 50 and 100, for which we generate a test set of 1, 000 graphs. Points are drawn uniformly at random in the unit square [0, 1]2.\n5.1 EXPERIMENTAL DETAILS\nAcross all experiments, we use mini-batches of 128 sequences, LSTM cells with 128 hidden units, and embed the two coordinates of each point in a 128-dimensional space. We train our models with the Adam optimizer (Kingma & Ba, 2014) and use an initial learning rate of 10\u22123 for TSP20 and TSP50 and 10\u22124 for TSP100 that we decay every 5000 steps by a factor of 0.96. We initialize our parameters uniformly at random within [\u22120.08, 0.08] and clip the L2 norm of our gradients to 1.0. We use up to one attention glimpse. When searching, the mini-batches either consist of replications of the test sequence or its permutations. The baseline decay is set to \u03b1 = 0.99 in Active Search. Our model and training code in Tensorflow (Abadi et al., 2016) will be made availabe soon. Table 1 summarizes the configurations and different search strategies used in the experiments. The variations of our method, experimental procedure and results are as follows.\nSupervised Learning. In addition to the described baselines, we implement and train a pointer network with supervised learning, similarly to (Vinyals et al., 2015b). While our supervised data consists of one million optimal tours, we find that our supervised learning results are not as good as those reported in by (Vinyals et al., 2015b). We suspect that learning from optimal tours is harder for supervised pointer networks due to subtle features that the model cannot figure out only by looking at given supervised targets. We thus refer to the results in (Vinyals et al., 2015b) for TSP20 and TSP50 and report our results on TSP100, all of which are suboptimal compared to other approaches.\nRL pretraining. For the RL experiments, we generate training mini-batches of inputs on the fly and update the model parameters with the Actor Critic Algorithm 1. We use a validation set of 10, 000 randomly generated instances for hyper-parameters tuning. Our critic consists of an encoder network which has the same architecture as that of the policy network, but followed by 3 processing steps and 2 fully connected layers. We find that clipping the logits to [\u221210, 10] with a tanh(\u00b7) activation function, as described in Appendix A.2, helps with exploration and yields marginal performance gains. The simplest search strategy using an RL pretrained model is greedy decoding, i.e. selecting the city with the largest probability at each decoding step. We also experiment with decoding greedily from a set of 16 pretrained models at inference time. For each graph, the tour found by each individual model is collected and the shortest tour is chosen. We refer to those approaches as RL pretraining-greedy and RL pretraining-greedy@16.\nRL pretraining-Sampling. For each test instance, we sample 1, 280, 000 candidate solutions from a pretrained model and keep track of the shortest tour. A grid search over the temperature hyperparameter found respective temperatures of 2.0, 2.2 and 1.5 to yield the best results for TSP20, TSP50 and TSP100. We refer to the tuned temperature hyperparameter as T \u2217. Since sampling does not require parameter udpates and is entirely parallelizable, we use a larger batch size for speed purposes.\nRL pretraining-Active Search. For each test instance, we initialize the model parameters from a pretrained RL model and run Active Search for up to 10, 000 training steps with a batch size of 128, sampling a total of 1, 280, 000 candidate solutions. We set the learning rate to a hundredth of the initial learning rate the TSP agent was trained on (i.e. 10\u22125 for TSP20/TSP50 and 10\u22126 for TSP100).\nActive Search. We allow the model to train much longer to account for the fact that it starts from scratch. For each test graph, we run Active Search for 100, 000 training steps on TSP20/TSP50 and 200, 000 training steps on TSP100.\n5.2 RESULTS AND ANALYSES\nWe compare our methods against 3 different baselines of increasing performance and complexity: 1) Christofides, 2) the vehicle routing solver from OR-Tools (Google, 2016) and 3) optimality. Christofides solutions are obtained in polynomial time and guaranteed to be within a 1.5 ratio of optimality. OR-Tools improves over Christofides\u2019 solutions with simple local search operators, including 2-opt (Johnson, 1990) and a version of the Lin-Kernighan heuristic (Lin & Kernighan, 1973), stopping when it reaches a local minimum. In order to escape poor local optima, ORTools\u2019 local search can also be run in conjunction with different metaheuristics, such as simulated annealing (Kirkpatrick et al., 1983), tabu search (Glover & Laguna, 2013) or guided local search (Voudouris & Tsang, 1999). OR-Tools\u2019 vehicle routing solver can tackle a superset of the TSP and operates at a higher level of generality than solvers that are highly specific to the TSP. While not state-of-the art for the TSP, it is a common choice for general routing problems and provides a reasonable baseline between the simplicity of the most basic local search operators and the sophistication of the strongest solvers. Optimal solutions are obtained via Concorde (Applegate et al., 2006) and LK-H\u2019s local search (Helsgaun, 2012; 2000). While only Concorde provably solves instances to optimality, we empirically find that LK-H also achieves optimal solutions on all of our test sets after 50 trials per graph (which is the default parameter setting).\nWe report the average tour lengths of our approaches on TSP20, TSP50, and TSP100 in Table 2. Notably, results demonstrate that training with RL significantly improves over supervised learning\n(Vinyals et al., 2015b). All our methods comfortably surpass Christofides\u2019 heuristic, including RL pretraining-Greedy which also does not rely on search. Table 3 compares the running times of our greedy methods to the aforementioned baselines, with our methods running on a single Nvidia Tesla K80 GPU, Concorde and LK-H running on an Intel Xeon CPU E5-1650 v3 3.50GHz CPU and ORTool on an Intel Haswell CPU. We find that both greedy approaches are time-efficient but still quite far from optimality.\nSearching at inference time proves crucial to get closer to optimality but comes at the expense of longer running times. Fortunately, the search from RL pretraining-Sampling and RL pretrainingActive Search can be stopped early with a small performance tradeoff in terms of the final objective. This can be seen in Table 4, where we show their performances and corresponding running times as a function of how many solutions they consider.\nWe also find that many of our RL pretraining methods outperform OR-Tools\u2019 local search, including RL pretraining-Greedy@16 which runs similarly fast. Table 6 in Appendix A.3 presents the performance of the metaheuristics as they consider more solutions and the corresponding running times. In our experiments, Neural Combinatorial proves superior than Simulated Annealing but is slightly less competitive that Tabu Search and much less so than Guided Local Search.\nWe present a more detailed comparison of our methods in Figure 3, where we sort the ratios to optimality of our different learning configurations. RL pretraining-Sampling and RL pretrainingActive Search are the most competitive Neural Combinatorial Optimization methods and recover the optimal solution in a significant number of our test cases. We find that for small solution spaces, RL pretraining-Sampling, with a finetuned softmax temperature, outperforms RL pretraining-Active Search with the latter sometimes orienting the search towards suboptimal regions of the solution space (see TSP50 results in Table 4 and Figure 3). Furthermore, RL pretraining-Sampling benefits from being fully parallelizable and runs faster than RL pretraining-Active Search. However, for larger solution spaces, RL-pretraining Active Search proves superior both when controlling for the number of sampled solutions or the running time. Interestingly, Active Search - which starts from an untrained model - also produces competitive tours but requires a considerable amount of time (respectively 7 and 25 hours per instance of TSP50/TSP100). Finally, we show randomly picked example tours found by our methods in Figure 4 in Appendix A.4.\n6 GENERALIZATION TO OTHER PROBLEMS\nIn this section, we discuss how to apply Neural Combinatorial Optimization to other problems than the TSP. In Neural Combinatorial Optimization, the model architecture is tied to the given combinatorial optimization problem. Examples of useful networks include the pointer network, when the output is a permutation or a truncated permutation or a subset of the input, and the classical seq2seq model for other kinds of structured outputs. For combinatorial problems that require to assign labels to elements of the input, such as graph coloring, it is also possible to combine a pointer module and a softmax module to simultaneously point and assign at decoding time. Given a model that encodes an instance of a given combinatorial optimization task and repeatedly branches into subtrees to construct a solution, the training procedures described in Section 4 can then be applied by adapting the reward function depending on the optimization problem being considered.\nAdditionally, one also needs to ensure the feasibility of the obtained solutions. For certain combinatorial problems, it is straightforward to know exactly which branches do not lead to any feasible solutions at decoding time. We can then simply manually assign them a zero probability when decoding, similarly to how we enforce our model to not point at the same city twice in our pointing mechanism (see Appendix A.1). However, for many combinatorial problems, coming up with a feasible solution can be a challenge in itself. Consider, for example, the Travelling Salesman Problem with Time Windows, where the travelling salesman has the additional constraint of visiting each city during a specific time window. It might be that most branches being considered early in the tour do not lead to any solution that respects all time windows. In such cases, knowing exactly which branches are feasible requires searching their subtrees, a time-consuming process that is not much easier than directly searching for the optimal solution unless using problem-specific heuristics.\nRather than explicitly constraining the model to only sample feasible solutions, one can also let the model learn to respect the problem\u2019s constraints. A simple approach, to be verified experimentally in future work, consists in augmenting the objective function with a term that penalizes solutions for violating the problem\u2019s constraints, similarly to penalty methods in constrained optimization. While this does not guarantee that the model consistently samples feasible solutions at inference time, this is not necessarily problematic as we can simply ignore infeasible solutions and resample from the model (for RL pretraining-Sampling and RL-pretraining Active Search). It is also conceivable to combine both approaches by assigning zero probabilities to branches that are easily identifiable as infeasible while still penalizing infeasible solutions once they are entirely constructed.\n6.1 KNAPSACK EXAMPLE\nAs an example of the flexibility of Neural Combinatorial Optimization, we consider the KnapSack problem, another intensively studied problem in computer science. Given a set of n items i = 1...n, each with weight wi and value vi and a maximum weight capacity ofW , the 0-1 KnapSack problem consists in maximizing the sum of the values of items present in the knapsack so that the sum of the\nweights is less than or equal to the knapsack capacity:\nmax S\u2286{1,2,...,n} \u2211 i\u2208S vi\nsubject to \u2211 i\u2208S wi \u2264W (7)\nWith wi, vi and W taking real values, the problem is NP-hard (Kellerer et al., 2004). A naive heuristic is to take the items ordered by their weight-to-value ratios until they fill up the weight capacity. Two simple heuristics are ExpKnap, which employs branch-and-bound with Linear Programming bounds (Pisinger, 1995), and MinKnap, which uses dynamic programming with enumerative bounds (Pisinger, 1997). Exact solutions can also be obtained by quantizing the weights to high precisions and then performing dynamic programming with pseudo-polynomial complexity (Bertsimas & Demir, 2002).\nWe apply the pointer network and encode each KnapSack instance as a sequence of 2D vectors (wi, vi). At decoding time, the pointer network points to items to include in the knapsack and stops when the total weight of the items collected so far exceeds the weight capacity. We generate three datasets, KNAP50, KNAP100 and KNAP200, of a thousand instances with items\u2019 weights and values drawn uniformly at random in [0, 1]. Without loss of generality (since we can scale the items\u2019 weights), we set the capacities to 12.5 for KNAP50 and 25 for KNAP100 and KNAP200. We present the performances of RL pretraining-Greedy and Active Search (which we run for 5, 000 training steps) in Table 5 and compare them to the following baselines: 1) random search (which we let sample as many feasible solutions seen by Active Search), 2) the greedy value-to-weight ratio heuristic, 3) MinKnap, 4) ExpKnap, 5) OR-Tools\u2019 KnapSack solver (Google, 2016) and 6) optimality (which we obtained by quantizing the weights to high precisions and using dynamic programming).\n7 CONCLUSION\nThis paper presents Neural Combinatorial Optimization, a framework to tackle combinatorial optimization with reinforcement learning and neural networks. We focus on the traveling salesman problem (TSP) and present a set of results for each variation of the framework. Experiments demonstrate that Neural Combinatorial Optimization achieves close to optimal results on 2D Euclidean graphs with up to 100 nodes. Our results, while still far from the strongest solvers (especially those which are optimized for one problem), provide an interesting research avenue for using neural networks as a general tool for tackling combinatorial optimization problems.\nACKNOWLEDGMENTS\nThe authors would like to thank Vincent Furnon, Mustafa Ispir, Lukasz Kaiser, Oriol Vinyals, Barret Zoph, the Google Brain team and the anonymous ICLR reviewers for insightful comments and discussion.\nA APPENDIX\nA.1 POINTING AND ATTENDING\nPointing mechanism: Its computations are parameterized by two attention matrices Wref ,Wq \u2208 Rd\u00d7d and an attention vector v \u2208 Rd as follows:\nui = { v> \u00b7 tanh (Wref \u00b7 ri +Wq \u00b7 q) if i 6= \u03c0(j) for all j < i \u2212\u221e otherwise for i = 1, 2, ..., k (8)\nA(ref, q;Wref ,Wq, v) def = softmax(u). (9)\nOur pointer network, at decoder step j, then assigns the probability of visiting the next point \u03c0(j) of the tour as follows:\np(\u03c0(j)|\u03c0(< j), s) def= A(enc1:n, decj). (10)\nSetting the logits of cities that already appeared in the tour to \u2212\u221e, as shown in Equation 8, ensures that our model only points at cities that have yet to be visited and hence outputs valid TSP tours.\nAttending mechanism: Specifically, our glimpse function G(ref, q) takes the same inputs as the attention function A and is parameterized by W gref ,W g q \u2208 Rd\u00d7d and vg \u2208 Rd. It performs the following computations:\np = A(ref, q;W gref ,W g q , v g) (11)\nG(ref, q;W gref ,W g q , v g) def = k\u2211 i=1 ripi. (12)\nThe glimpse functionG essentially computes a linear combination of the reference vectors weighted by the attention probabilities. It can also be applied multiple times on the same reference set ref :\ng0 def = q (13)\ngl def = G(ref, gl\u22121;W g ref ,W g q , v g) (14)\nFinally, the ultimate gl vector is passed to the attention function A(ref, gl;Wref ,Wq, v) to produce the probabilities of the pointing mechanism. We observed empirically that glimpsing more than once with the same parameters made the model less likely to learn and barely improved the results.\nA.2 IMPROVING EXPLORATION\nSoftmax temperature: We modify Equation 9 as follows:\nA(ref, q, T ;Wref ,Wq, v) def = softmax(u/T ), (15)\nwhere T is a temperature hyperparameter set to T = 1 during training. When T > 1, the distribution represented by A(ref, q) becomes less steep, hence preventing the model from being overconfident.\nLogit clipping: We modify Equation 9 as follows:\nA(ref, q;Wref ,Wq, v) def = softmax(C tanh(u)), (16)\nwhereC is a hyperparameter that controls the range of the logits and hence the entropy ofA(ref, q).\nA.3 OR TOOL\u2019S METAHEURISTICS BASELINES FOR TSP\nA.4 SAMPLE TOURS\n(5.934)\nRL pretraining -Greedy\n(5.734)\nRL pretraining -Sampling\n(5.688)\nRL pretraining -Active Search\n(5.827)\nActive Search\n(5.688)\nOptimal\nRL pretraining -Greedy\nRL pretraining -Sampling\nRL pretraining -Active Search\nActive Search\nOptimal\n", "cats": {"READ": 1.0, "DONTREAD": 0.0}, "reviews": [{"IS_META_REVIEW": true, "comments": "This paper proposes to use RNN and reinforcement learning for solving combinatorial optimization problems. The use of pointer network is interesting as it enables generalization to arbitrary input size. The proposed method also \"fintunes\" on test examples with active search to achieve better performance.\n\nThe proposed method is theoretically interesting as it shows that RNN and RL can be combined to solve combinatorial optimization problems and achieve comparable performance to traditional heuristic based algorithms.\n\nHowever, the lack of complexity comparison against baselines make it impossible to tell whether the proposed method has any practical value. The matter is further complicated by the fact that the proposed method runs on GPU while baselines run on CPU: it is hard to even come up with a meaningful unit of complexity. Money spent on hardware and electricity per instance may be a viable option.\n\nFurther more, the performance comparisons should be taken with a grain of salt as traditional heuristic based algorithms can often give better performance if allowed more computation, which is not controlled across algorithms."}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "This was one of the more controversial submissions to this area, and there was extensive discussion over the merits and contributions of the work. The paper also benefitted from ICLRs open review system as additional researchers chimed in on the paper and the authors resubmitted a draft. The authors did a great job responding and updating the work and responding to criticisms. In the end though, even after these consideration, none of the reviewers strongly supported the work and all of them expressed some reservations. \n \n Pros:\n - All agree that the work is extremely clear, going as far as saying the work is \"very well written\" and \"easy to understand\". \n - Generally there was a predisposition to support the work for its originality particularly due to its \"methodological contributions\", and even going so far as a saying it would generally be a natural accept.\n \n Cons:\n - There was a very uncommonly strong backlash to the claims made by the paper, particularly the first draft, but even upon revisions. One reviewer even saying this was an \"excellent example of hype-generation far before having state-of-the-art results\" and that it was \"doing a disservice to our community since it builds up an expectation that the field cannot live up to\" . This does not seem to be an isolated reviewer, but a general feeling across the reviews. Another faulting \"the toy-ness of the evaluation metric\" and the way the comparisons were carried out.\n - A related concern was a feeling that the body of work in operations research was not fully taken account in this work, noting \"operations research literature is replete with a large number of benchmark problems that have become standard to compare solver quality\". The authors did fix some of these issues, but not to the point that any reviewer stood up for the work.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"DATE": "25 Jan 2017", "TITLE": "Summary of new changes to the paper", "IS_META_REVIEW": false, "comments": "We ask reviewers to have a look at the new version of the paper again given the changes outlined below:\n\n- We state clearly in the abstract, introduction, and conclusion that our results are still far from the state-of-the-art (this includes adding an updated version of Figure 1 back into the introduction).\n\n- We include the original KnapSack baselines back into the paper.\n\n- We explain in details how the running time of the LKH baseline is obtained.\n\n- We modify the statement on the performance of greedy approaches: instead of stating that they are \u201cjust a few percents from optimality\u201d, we express that they are \u201cstill quite far from optimality\u201d.\n\nWe thank reviewers for their help in improving the quality of the paper.", "OTHER_KEYS": "Irwan Bello"}, {"DATE": "06 Jan 2017", "TITLE": "Question", "IS_META_REVIEW": false, "comments": "I posted this question in a response below, but it seems to be getting ignored so I thought I'd bring it to the top, with some additional points.\n\nThanks for the update. The natural question to ask, then is - do there exist many (or any) problems that are both interesting and have not been, and cannot be, addressed by the existing combinatorial optimization community? You knock existing algorithms for being \"highly optimized\" to particular problems, but if every worthwhile problem has \"highly optimized\" solutions, what good is your work? \n\nAlso, please stop calling existing TSP solvers such as concorde a heuristic. Concorde produces solutions which are provably correct. Your approach does not, nor is it remotely close. From a practical perspective, this is an important distinction; I don't see why anyone would choose the latter when given the choice. The second paragraphs of the related work and introduction are guilty of this. Also in the related work - you say it solves cities with \"thousands of cities\" when it has solved a 85k problem. \n\nI'd also echo concerns about the toy-ness of the evaluation metrics here - 100 cities is 800x smaller than existing SOTA of 85k from TSPLib - a gap made exponentially larger by the combinatorial nature of the problem.\n\n", "OTHER_KEYS": "(anonymous)"}, {"DATE": "05 Jan 2017", "TITLE": "Summary of paper's revision", "IS_META_REVIEW": false, "comments": "We thank reviewers for their valuable feedback that helped us improve the paper. We appreciate their interest in the method and its novelty. We have made several changes to the paper which are summarized below. We ask reviewers to evaluate the new version of the paper and adjust their reviews if necessary.\n\n1) Previous Figure 1, which was problematic due to different possible interpretations of \u201clocal search\u201d was removed.\n\n2) We added precise running time evaluations for all of the methods in the paper. Table 3 presents running time of the RL pretraining-greedy method and the solvers we compare against. Table 4 presents the performance and corresponding running time of RL pretraining-Sampling and RL pretraining-Active Search as a function of the number solutions considered. It shows how they can be stopped early at the cost of a small performance degradation. Table 6 contains the same information for the metaheuristics from OR-Tools vehicle routing library solver. We controlled the complexity of these approaches by letting all of them evaluate 1,280,000 solutions. Section 5.2 was rewritten in light of the new results.\n\n3) We experimented with a new approach, called RL pretraining-Greedy@16, that decodes greedily from 16 different pretrained models at inference time and selects the shortest tour. It runs as fast as the solvers while only suffering from a small performance cost.\n\n4) We added a discussion in Section 6 (Generalization to other problems) explaining how one may apply Neural Combinatorial Optimization to problems for which coming up with a feasible solution is challenging by itself.\n\n5) We added a more detailed description of the critic network (see Section 4 - Critic\u2019s architecture for TSP).\n\nPlease take a look and let us know your thoughts.", "OTHER_KEYS": "Irwan Bello"}, {"TITLE": "Promising approach to combinatorial optimization", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer4", "comments": "This paper applies the pointer network architecture\u2014wherein an attention mechanism is fashioned to point to elements of an input sequence, allowing a decoder to output said elements\u2014in order to solve simple combinatorial optimization problems such as the well-known travelling salesman problem. The network is trained by reinforcement learning using an actor-critic method, with the actor trained using the REINFORCE method, and the critic used to estimate the reward baseline within the REINFORCE objective.\n\nThe paper is well written and easy to understand. Its use of a reinforcement learning and attention model framework to learn the structure of the space in which combinatorial problems of variable size can be tackled appears novel. Importantly, it provides an interesting research avenue for revisiting classical neural-based solutions to some combinatorial optimization problems, using recently-developed sequence-to-sequence approaches. As such, I think it merits consideration for the conference.\n\nI have a few comments and some important reservations with the paper:\n\n1) I take exception to the conclusion that the pointer network approach can handle general types of combinatorial optimization problems. The crux of combinatorial problems \u2014 for practical applications \u2014 lies in the complex constraints that define feasible solutions (e.g. simple generalizations of the TSP that involve time windows, or multiple salesmen). For these problems, it is no longer so simple to exclude possible solutions from the enumeration of the solution by just \u00ab striking off \u00bb previously-visited instances; in fact, for many of these problems, finding a single feasible solution might in general be a challenge. It would be relevant to include a discussion of whether the Neural Combinatorial Optimization approach could scale to these important classes of problems, and if so, how. My understanding is that this approach, as presented, would be mostly suitable for assignment problems with a very simple constraint structure.\n\n2) The operations research literature is replete with a large number of benchmark problems that have become standard to compare solver quality. For instance, TSPLIB contains a large number of TSP instances (", "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "27 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "Promising method, but biased presentation", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "This paper is methodologically very interesting, and just based on the methodological contribution I would vote for acceptance. However, the paper's sweeping claims of clearly beating existing baselines for TSP have been shown to not hold, with the local search method LK-H solving all the authors' instances to optimality -- in seconds on a CPU, compared to clearly suboptimal results by the authors' method in 25h on a GPU. \n\nSeeing this clear dominance of the local search method LK-H, I find it irresponsible by the authors that they left Figure 1 as it is -- with the line for \"local search\" referring to an obviously poor implementation by Google rather than the LK-H local search method that everyone uses. For example, at NIPS, I saw this Figure 1 being used in a talk (I am not sure anymore by whom, but I don't think it was by the authors), the narrative being \"RNNs now also clearly perform better than local search\". Of course, people would use a figure like that for that purpose, and it is clearly up to the authors to avoid such misconceptions. \n\nThe right course of action upon realizing the real strength of local search with LK-H would've been to make \"local search\" the same line as \"Optimal\", showing that the authors' method is still far worse than proper local search. But the authors chose to leave the figure as it was, still suggesting that their method is far better than local search. Probably the authors didn't even think about this, but this of course will mislead the many superficial readers. To people outside of deep learning, this must look like a sensational yet obviously wrong claim. I thus vote for rejection despite the interesting method. \n\n------------------------\n\nUpdate after rebuttal and changes:\n\nI'm torn about this paper. \n\nOn the one hand, the paper is very well written and I do think the method is very interesting and promising. I'd even like to try it and improve it in the future. So, from that point of view a clear accept.\n\nOn the other hand, the paper was using extremely poor baselines, making the authors' method appear sensationally strong in comparison, and over the course of many iterations of reviewer questions and anonymous feedback, this has come down to the authors' methods being far inferior to the state of the art. That's fine (I expected that all along), but the problem is that the authors don't seem to want this to be true... E.g., they make statements, such as \"We find that both greedy approaches are time-efficient and just a few percents worse than optimality.\"\nThat statement may be true, but it is very well known in the TSP community that it is typically quite trivial to get to a few percent worse than optimality. What's hard and interesting is to push those last few percent. \n(As a side note: the authors probably don't stop LK-H once it has found the optimal solution, like they do with their own method after finding a local optimum. LK-H is an anytime algorithm, so even if it ran for a day that doesn't mean that it didn't find the optimal solution after milliseconds -- and a solution a few percent suboptimal even faster).\n\nNevertheless, since the claims have been toned down over the course of the many iterations, I was starting to feel more positive about this paper when just re-reading it. That is, until I got to the section on Knapsack solving. The version of the paper I reviewed was not bad here, as it at least stated two simple heuristics that yield optimal solutions:\n\n\"Two simple heuristics are ExpKnap, which employs brand-and-bound with Linear Programming bounds (Pisinger, 1995), and MinKnap, which employs dynamic programming with enumerative bounds (Pisinger, 1997). Exact solutions can also be optained by quantizing the weights to high precisions and then performing dynamic programming with a pseudo-polynomial complexity (Bertsimas & Demir, 2002).\" That version then went on to show that these simple heuristics were already optimal, just like their own method.\n\nIn a revision between December 11 and 14, however, that paragraph, along with the optimal results of ExpKnap and MinKnap seems to have been dropped, and the authors instead introduced two new poor baseline methods (random search and greedy). This was likely in an effort to find some methods that are not optimal on these very easy instances. I personally find it pointless to present results for random search here, as nobody would use that for TSP. It's like comparing results on MNIST against a decision stump (yes, you'll do better than that, but that is not surprising). The results for greedy are interesting to see. However, dropping the strong results of the simple heuristics ExpKnap and MinKnap (and their entire discussion) appears unresponsible, since the resulting table in the new version of the paper now suggests that the authors' method is better than all baselines. Of course, if all that one is after is a column of bold numbers for ones own approach that's what one can do, but I don't find it responsible to hide the better baselines. Also, why don't the authors try at least the same OR-tools solver from Google that they tried for TSP? It seems to support Knapsack directly: ", "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "16 Dec 2016 (modified: 19 Jan 2017)", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "No Title", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "This paper proposes to use RNN and reinforcement learning for solving combinatorial optimization problems. The use of pointer network is interesting as it enables generalization to arbitrary input size. The proposed method also \"fintunes\" on test examples with active search to achieve better performance.\n\nThe proposed method is theoretically interesting as it shows that RNN and RL can be combined to solve combinatorial optimization problems and achieve comparable performance to traditional heuristic based algorithms.\n\nHowever, the lack of complexity comparison against baselines make it impossible to tell whether the proposed method has any practical value. The matter is further complicated by the fact that the proposed method runs on GPU while baselines run on CPU: it is hard to even come up with a meaningful unit of complexity. Money spent on hardware and electricity per instance may be a viable option.\n\nFurther more, the performance comparisons should be taken with a grain of salt as traditional heuristic based algorithms can often give better performance if allowed more computation, which is not controlled across algorithms.\n\n", "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "15 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"DATE": "15 Dec 2016", "TITLE": "About Concorde", "IS_META_REVIEW": false, "comments": "This is very interesting to me! Thank you for this.\n\nAfter reading this paper, I tested the Concorde. I think the Concorde allows only integer distances(if use Euclidean distance, they round off), so cannot provide optimal solution of Euclidean TSP.\nBut error can be small if multiply the distance by a large constant.\n\nI want to know that, if I correct, does 'optimal' means a solution which is very closed to optimal?", "OTHER_KEYS": "(anonymous)"}, {"DATE": "09 Dec 2016", "TITLE": "Control for computation cost?", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2"}, {"DATE": "03 Dec 2016", "TITLE": "Code availability", "IS_META_REVIEW": false, "comments": "I am very glad to read \"Our model and training code will be made available soon.\" Thanks for that! My question is: how soon is soon? During the review period? In time for the conference? ", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3"}, {"DATE": "03 Dec 2016", "TITLE": "Is RL pretraining Sampling T=T* actually better than RL pretraining AS when compared with the same number of 10.000 batches?", "IS_META_REVIEW": false, "comments": "In Table 3, what is the performance for the missing values of RL pretraining with 10.000 batches for Sampling T=1 and T=T*? \n\nSince performance improved much more from 100 to 1.000 batches for RL pretraining Sampling T=T* than it did for RL pretraining AS (e.g., 5.79->5.71 vs 5.74->5.71 for TSP50), I would expect RL pretraining Sampling T=T* to do better than RL pretraining AS when you use 10.000 samples. This would also change your qualitative conclusion in Table 2 and the overall result of the paper. You seem to glance over this in the text by saying \"we sample 1000 batches from a pretrained model, afer which we do not see significant improvement\", but seeing the much larger \"gradient\" from 50, 100, and 1000 batches than for RL pretraining AS, and seeing how key the result is to the final take-away from the paper, I would be far more convinced by just seeing the numbers for 10.000 batches.\n\nAlso, what is actually the difference between RL pretraining Sampling T=1 and T=T*? (Maybe I just missed this in the text.)\n", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3"}, {"DATE": "03 Dec 2016", "TITLE": "Christofides algorithm wrong baseline", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3"}, {"DATE": "07 Nov 2016", "TITLE": "Related work incomplete", "IS_META_REVIEW": false, "comments": "There is a large body of work on solving TSP instances that this paper ignores. In particular, the concorde algorithm has produced provably optimal solutions to problems as large as 85,900 cities, and can solve 100+ city problems in a few seconds on a single 500MHz core. Thus, the claims made that this is even close to being a useful tool for solving TSP problems are demonstrably untrue.\n\n", "OTHER_KEYS": "(anonymous)"}, {"IS_META_REVIEW": true, "comments": "This paper proposes to use RNN and reinforcement learning for solving combinatorial optimization problems. The use of pointer network is interesting as it enables generalization to arbitrary input size. The proposed method also \"fintunes\" on test examples with active search to achieve better performance.\n\nThe proposed method is theoretically interesting as it shows that RNN and RL can be combined to solve combinatorial optimization problems and achieve comparable performance to traditional heuristic based algorithms.\n\nHowever, the lack of complexity comparison against baselines make it impossible to tell whether the proposed method has any practical value. The matter is further complicated by the fact that the proposed method runs on GPU while baselines run on CPU: it is hard to even come up with a meaningful unit of complexity. Money spent on hardware and electricity per instance may be a viable option.\n\nFurther more, the performance comparisons should be taken with a grain of salt as traditional heuristic based algorithms can often give better performance if allowed more computation, which is not controlled across algorithms."}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "This was one of the more controversial submissions to this area, and there was extensive discussion over the merits and contributions of the work. The paper also benefitted from ICLRs open review system as additional researchers chimed in on the paper and the authors resubmitted a draft. The authors did a great job responding and updating the work and responding to criticisms. In the end though, even after these consideration, none of the reviewers strongly supported the work and all of them expressed some reservations. \n \n Pros:\n - All agree that the work is extremely clear, going as far as saying the work is \"very well written\" and \"easy to understand\". \n - Generally there was a predisposition to support the work for its originality particularly due to its \"methodological contributions\", and even going so far as a saying it would generally be a natural accept.\n \n Cons:\n - There was a very uncommonly strong backlash to the claims made by the paper, particularly the first draft, but even upon revisions. One reviewer even saying this was an \"excellent example of hype-generation far before having state-of-the-art results\" and that it was \"doing a disservice to our community since it builds up an expectation that the field cannot live up to\" . This does not seem to be an isolated reviewer, but a general feeling across the reviews. Another faulting \"the toy-ness of the evaluation metric\" and the way the comparisons were carried out.\n - A related concern was a feeling that the body of work in operations research was not fully taken account in this work, noting \"operations research literature is replete with a large number of benchmark problems that have become standard to compare solver quality\". The authors did fix some of these issues, but not to the point that any reviewer stood up for the work.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"DATE": "25 Jan 2017", "TITLE": "Summary of new changes to the paper", "IS_META_REVIEW": false, "comments": "We ask reviewers to have a look at the new version of the paper again given the changes outlined below:\n\n- We state clearly in the abstract, introduction, and conclusion that our results are still far from the state-of-the-art (this includes adding an updated version of Figure 1 back into the introduction).\n\n- We include the original KnapSack baselines back into the paper.\n\n- We explain in details how the running time of the LKH baseline is obtained.\n\n- We modify the statement on the performance of greedy approaches: instead of stating that they are \u201cjust a few percents from optimality\u201d, we express that they are \u201cstill quite far from optimality\u201d.\n\nWe thank reviewers for their help in improving the quality of the paper.", "OTHER_KEYS": "Irwan Bello"}, {"DATE": "06 Jan 2017", "TITLE": "Question", "IS_META_REVIEW": false, "comments": "I posted this question in a response below, but it seems to be getting ignored so I thought I'd bring it to the top, with some additional points.\n\nThanks for the update. The natural question to ask, then is - do there exist many (or any) problems that are both interesting and have not been, and cannot be, addressed by the existing combinatorial optimization community? You knock existing algorithms for being \"highly optimized\" to particular problems, but if every worthwhile problem has \"highly optimized\" solutions, what good is your work? \n\nAlso, please stop calling existing TSP solvers such as concorde a heuristic. Concorde produces solutions which are provably correct. Your approach does not, nor is it remotely close. From a practical perspective, this is an important distinction; I don't see why anyone would choose the latter when given the choice. The second paragraphs of the related work and introduction are guilty of this. Also in the related work - you say it solves cities with \"thousands of cities\" when it has solved a 85k problem. \n\nI'd also echo concerns about the toy-ness of the evaluation metrics here - 100 cities is 800x smaller than existing SOTA of 85k from TSPLib - a gap made exponentially larger by the combinatorial nature of the problem.\n\n", "OTHER_KEYS": "(anonymous)"}, {"DATE": "05 Jan 2017", "TITLE": "Summary of paper's revision", "IS_META_REVIEW": false, "comments": "We thank reviewers for their valuable feedback that helped us improve the paper. We appreciate their interest in the method and its novelty. We have made several changes to the paper which are summarized below. We ask reviewers to evaluate the new version of the paper and adjust their reviews if necessary.\n\n1) Previous Figure 1, which was problematic due to different possible interpretations of \u201clocal search\u201d was removed.\n\n2) We added precise running time evaluations for all of the methods in the paper. Table 3 presents running time of the RL pretraining-greedy method and the solvers we compare against. Table 4 presents the performance and corresponding running time of RL pretraining-Sampling and RL pretraining-Active Search as a function of the number solutions considered. It shows how they can be stopped early at the cost of a small performance degradation. Table 6 contains the same information for the metaheuristics from OR-Tools vehicle routing library solver. We controlled the complexity of these approaches by letting all of them evaluate 1,280,000 solutions. Section 5.2 was rewritten in light of the new results.\n\n3) We experimented with a new approach, called RL pretraining-Greedy@16, that decodes greedily from 16 different pretrained models at inference time and selects the shortest tour. It runs as fast as the solvers while only suffering from a small performance cost.\n\n4) We added a discussion in Section 6 (Generalization to other problems) explaining how one may apply Neural Combinatorial Optimization to problems for which coming up with a feasible solution is challenging by itself.\n\n5) We added a more detailed description of the critic network (see Section 4 - Critic\u2019s architecture for TSP).\n\nPlease take a look and let us know your thoughts.", "OTHER_KEYS": "Irwan Bello"}, {"TITLE": "Promising approach to combinatorial optimization", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer4", "comments": "This paper applies the pointer network architecture\u2014wherein an attention mechanism is fashioned to point to elements of an input sequence, allowing a decoder to output said elements\u2014in order to solve simple combinatorial optimization problems such as the well-known travelling salesman problem. The network is trained by reinforcement learning using an actor-critic method, with the actor trained using the REINFORCE method, and the critic used to estimate the reward baseline within the REINFORCE objective.\n\nThe paper is well written and easy to understand. Its use of a reinforcement learning and attention model framework to learn the structure of the space in which combinatorial problems of variable size can be tackled appears novel. Importantly, it provides an interesting research avenue for revisiting classical neural-based solutions to some combinatorial optimization problems, using recently-developed sequence-to-sequence approaches. As such, I think it merits consideration for the conference.\n\nI have a few comments and some important reservations with the paper:\n\n1) I take exception to the conclusion that the pointer network approach can handle general types of combinatorial optimization problems. The crux of combinatorial problems \u2014 for practical applications \u2014 lies in the complex constraints that define feasible solutions (e.g. simple generalizations of the TSP that involve time windows, or multiple salesmen). For these problems, it is no longer so simple to exclude possible solutions from the enumeration of the solution by just \u00ab striking off \u00bb previously-visited instances; in fact, for many of these problems, finding a single feasible solution might in general be a challenge. It would be relevant to include a discussion of whether the Neural Combinatorial Optimization approach could scale to these important classes of problems, and if so, how. My understanding is that this approach, as presented, would be mostly suitable for assignment problems with a very simple constraint structure.\n\n2) The operations research literature is replete with a large number of benchmark problems that have become standard to compare solver quality. For instance, TSPLIB contains a large number of TSP instances (", "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "27 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "Promising method, but biased presentation", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "This paper is methodologically very interesting, and just based on the methodological contribution I would vote for acceptance. However, the paper's sweeping claims of clearly beating existing baselines for TSP have been shown to not hold, with the local search method LK-H solving all the authors' instances to optimality -- in seconds on a CPU, compared to clearly suboptimal results by the authors' method in 25h on a GPU. \n\nSeeing this clear dominance of the local search method LK-H, I find it irresponsible by the authors that they left Figure 1 as it is -- with the line for \"local search\" referring to an obviously poor implementation by Google rather than the LK-H local search method that everyone uses. For example, at NIPS, I saw this Figure 1 being used in a talk (I am not sure anymore by whom, but I don't think it was by the authors), the narrative being \"RNNs now also clearly perform better than local search\". Of course, people would use a figure like that for that purpose, and it is clearly up to the authors to avoid such misconceptions. \n\nThe right course of action upon realizing the real strength of local search with LK-H would've been to make \"local search\" the same line as \"Optimal\", showing that the authors' method is still far worse than proper local search. But the authors chose to leave the figure as it was, still suggesting that their method is far better than local search. Probably the authors didn't even think about this, but this of course will mislead the many superficial readers. To people outside of deep learning, this must look like a sensational yet obviously wrong claim. I thus vote for rejection despite the interesting method. \n\n------------------------\n\nUpdate after rebuttal and changes:\n\nI'm torn about this paper. \n\nOn the one hand, the paper is very well written and I do think the method is very interesting and promising. I'd even like to try it and improve it in the future. So, from that point of view a clear accept.\n\nOn the other hand, the paper was using extremely poor baselines, making the authors' method appear sensationally strong in comparison, and over the course of many iterations of reviewer questions and anonymous feedback, this has come down to the authors' methods being far inferior to the state of the art. That's fine (I expected that all along), but the problem is that the authors don't seem to want this to be true... E.g., they make statements, such as \"We find that both greedy approaches are time-efficient and just a few percents worse than optimality.\"\nThat statement may be true, but it is very well known in the TSP community that it is typically quite trivial to get to a few percent worse than optimality. What's hard and interesting is to push those last few percent. \n(As a side note: the authors probably don't stop LK-H once it has found the optimal solution, like they do with their own method after finding a local optimum. LK-H is an anytime algorithm, so even if it ran for a day that doesn't mean that it didn't find the optimal solution after milliseconds -- and a solution a few percent suboptimal even faster).\n\nNevertheless, since the claims have been toned down over the course of the many iterations, I was starting to feel more positive about this paper when just re-reading it. That is, until I got to the section on Knapsack solving. The version of the paper I reviewed was not bad here, as it at least stated two simple heuristics that yield optimal solutions:\n\n\"Two simple heuristics are ExpKnap, which employs brand-and-bound with Linear Programming bounds (Pisinger, 1995), and MinKnap, which employs dynamic programming with enumerative bounds (Pisinger, 1997). Exact solutions can also be optained by quantizing the weights to high precisions and then performing dynamic programming with a pseudo-polynomial complexity (Bertsimas & Demir, 2002).\" That version then went on to show that these simple heuristics were already optimal, just like their own method.\n\nIn a revision between December 11 and 14, however, that paragraph, along with the optimal results of ExpKnap and MinKnap seems to have been dropped, and the authors instead introduced two new poor baseline methods (random search and greedy). This was likely in an effort to find some methods that are not optimal on these very easy instances. I personally find it pointless to present results for random search here, as nobody would use that for TSP. It's like comparing results on MNIST against a decision stump (yes, you'll do better than that, but that is not surprising). The results for greedy are interesting to see. However, dropping the strong results of the simple heuristics ExpKnap and MinKnap (and their entire discussion) appears unresponsible, since the resulting table in the new version of the paper now suggests that the authors' method is better than all baselines. Of course, if all that one is after is a column of bold numbers for ones own approach that's what one can do, but I don't find it responsible to hide the better baselines. Also, why don't the authors try at least the same OR-tools solver from Google that they tried for TSP? It seems to support Knapsack directly: ", "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "16 Dec 2016 (modified: 19 Jan 2017)", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "No Title", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "This paper proposes to use RNN and reinforcement learning for solving combinatorial optimization problems. The use of pointer network is interesting as it enables generalization to arbitrary input size. The proposed method also \"fintunes\" on test examples with active search to achieve better performance.\n\nThe proposed method is theoretically interesting as it shows that RNN and RL can be combined to solve combinatorial optimization problems and achieve comparable performance to traditional heuristic based algorithms.\n\nHowever, the lack of complexity comparison against baselines make it impossible to tell whether the proposed method has any practical value. The matter is further complicated by the fact that the proposed method runs on GPU while baselines run on CPU: it is hard to even come up with a meaningful unit of complexity. Money spent on hardware and electricity per instance may be a viable option.\n\nFurther more, the performance comparisons should be taken with a grain of salt as traditional heuristic based algorithms can often give better performance if allowed more computation, which is not controlled across algorithms.\n\n", "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "15 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"DATE": "15 Dec 2016", "TITLE": "About Concorde", "IS_META_REVIEW": false, "comments": "This is very interesting to me! Thank you for this.\n\nAfter reading this paper, I tested the Concorde. I think the Concorde allows only integer distances(if use Euclidean distance, they round off), so cannot provide optimal solution of Euclidean TSP.\nBut error can be small if multiply the distance by a large constant.\n\nI want to know that, if I correct, does 'optimal' means a solution which is very closed to optimal?", "OTHER_KEYS": "(anonymous)"}, {"DATE": "09 Dec 2016", "TITLE": "Control for computation cost?", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2"}, {"DATE": "03 Dec 2016", "TITLE": "Code availability", "IS_META_REVIEW": false, "comments": "I am very glad to read \"Our model and training code will be made available soon.\" Thanks for that! My question is: how soon is soon? During the review period? In time for the conference? ", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3"}, {"DATE": "03 Dec 2016", "TITLE": "Is RL pretraining Sampling T=T* actually better than RL pretraining AS when compared with the same number of 10.000 batches?", "IS_META_REVIEW": false, "comments": "In Table 3, what is the performance for the missing values of RL pretraining with 10.000 batches for Sampling T=1 and T=T*? \n\nSince performance improved much more from 100 to 1.000 batches for RL pretraining Sampling T=T* than it did for RL pretraining AS (e.g., 5.79->5.71 vs 5.74->5.71 for TSP50), I would expect RL pretraining Sampling T=T* to do better than RL pretraining AS when you use 10.000 samples. This would also change your qualitative conclusion in Table 2 and the overall result of the paper. You seem to glance over this in the text by saying \"we sample 1000 batches from a pretrained model, afer which we do not see significant improvement\", but seeing the much larger \"gradient\" from 50, 100, and 1000 batches than for RL pretraining AS, and seeing how key the result is to the final take-away from the paper, I would be far more convinced by just seeing the numbers for 10.000 batches.\n\nAlso, what is actually the difference between RL pretraining Sampling T=1 and T=T*? (Maybe I just missed this in the text.)\n", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3"}, {"DATE": "03 Dec 2016", "TITLE": "Christofides algorithm wrong baseline", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3"}, {"DATE": "07 Nov 2016", "TITLE": "Related work incomplete", "IS_META_REVIEW": false, "comments": "There is a large body of work on solving TSP instances that this paper ignores. In particular, the concorde algorithm has produced provably optimal solutions to problems as large as 85,900 cities, and can solve 100+ city problems in a few seconds on a single 500MHz core. Thus, the claims made that this is even close to being a useful tool for solving TSP problems are demonstrably untrue.\n\n", "OTHER_KEYS": "(anonymous)"}]}
{"text": "1 INTRODUCTION\nModern machine learning increasingly relies on highly complex probabilistic models to reason about uncertainty. A key computational challenge is to develop efficient inference techniques to approximate, or draw samples from complex distributions. Currently, most inference methods, including MCMC and variational inference, are hand-designed by researchers or domain experts. This makes it difficult to fully optimize the choice of different methods and their parameters, and exploit the structures in the problems of interest in an automatic way. The hand-designed algorithm can also be inefficient when it requires to make fast inference repeatedly on a large number of different distributions with similar structures. This happens, for example, when we need to reason about a number of observed datasets in settings like online learning, or need fast inference as inner loops for other algorithms such as maximum likelihood training. Therefore, it is highly desirable to develop more intelligent probabilistic inference systems that can adaptively improve its own performance to fully the optimize computational efficiency, and generalize to new tasks with similar structures.\nSpecifically, denote by p(x) a probability density of interest specified up to the normalization constant, which we want to draw sample from, or marginalize to estimate its normalization constant. We want to study the following problem:\nProblem 1. Given a distribution with density p(x) and a function f(\u03b7; \u03be) with parameter \u03b7 and random input \u03be, for which we only have assess to draws of the random input \u03be (without knowing its true distribution q0), and the output values of f(\u03b7; \u03be) and its derivative \u2202\u03b7f(\u03b7; \u03be) given \u03b7 and \u03be. We want to find an optimal parameter \u03b7 so that the density of the random output variable x = f(\u03b7; \u03be) with \u03be \u223c q0 closely matches the target density p(x).\nBecause we have no assumption on the structure of f(\u03b7; \u03be) and the distribution of random input, we can not directly calculate the actual distribution of the output random variable x = f(\u03b7; \u03be); this makes it difficult to solve Problem 1 using the traditional variational inference (VI) methods. Recall that traditional VI approximates p(x) using simple proposal distributions q\u03b7(x) indexed by parameter \u03b7, and finds the optimal \u03b7 by minimizing KL divergence KL(q\u03b7 || p) = Eq\u03b7 [log(q\u03b7/p)], which requires to calculate the density q\u03b7(x) or its derivative that is not computable by our assump-\ntion (even when the Monte Carlo gradient estimation and the reparametrization trick (Kingma & Welling, 2013) are applied).\nIn fact, it is this requirement of calculating q\u03b7(x) that has been the major constraint for the designing of state-of-the-art variational inference methods with rich approximation families; the recent successful algorithms (e.g., Rezende & Mohamed, 2015b; Tran et al., 2015; Ranganath et al., 2015, to name only a few) have to handcraft special variational families to ensure the computational tractability of q\u03b7(x) and simultaneously obtain high approximation accuracy, which require substantial mathematical insights and research effects. Methods that do not require to explicitly calculate q\u03b7(x) can significantly simplify the design and applications of VI methods, allowing practical users to focus more on choosing proposals that work best with their specific tasks. We will use the term wild variational inference to refer to new variants of variational methods that require no tractability q\u03b7(x), to distinguish with the black-box variational inference (Ranganath et al., 2014) which refers to methods that work for generic target distributions p(x) without significant model-by-model consideration (but still require to calculate the proposal density q\u03b7(x)).\nA similar problem also appears in importance sampling (IS), where it requires to calculate the IS proposal density q(x) in order to calculate the importance weight w(x) = p(x)/q(x). However, there exist methods that use no explicit information of q(x), which, seemingly counter-intuitively, give better asymptotic variance or converge rates than the typical IS that uses the proposal information (e.g., Liu & Lee, 2016; Briol et al., 2015; Henmi et al., 2007; Delyon & Portier, 2014). Discussions on this phenomenon dates back to O\u2019Hagan (1987), who argued that \u201cMonte Carlo (that uses the proposal information) is fundamentally unsound\u201d for violating the Likelihood Principle, and developed Bayesian Monte Carlo (O\u2019Hagan, 1991) as an example that uses no information on q(x), yet gives better convergence rate than the typical Monte Carlo O(n\u22121/2) rate (Briol et al., 2015). Despite the substantial difference between IS and VI, these results intuitively suggest the possibility of developing efficient variational inference without calculating q(x) explicitly.\nIn this work, we propose a simple algorithm for Problem 1 by iteratively adjusting the network parameter \u03b7 to make its output random variable changes along a Stein variational gradient direction (SVGD) (Liu & Wang, 2016) that optimally decreases its KL divergence with the target distribution. Critically, the SVGD gradient includes a repulsive term to ensure that the generated samples have the right amount of variability that matches p(x). In this way, we \u201camortize SVGD\u201d using a neural network, which makes it possible for our method to adaptively improve its own efficiency by leveraging fast experience, especially in cases when it needs to perform fast inference repeatedly on a large number of similar tasks. As an application, we use our method to amortize the MLE training of deep energy models, where a neural sampler is adaptively trained to approximate the likelihood function. Our method, which we call SteinGAN, mimics an adversarial game between the energy model and the neural sampler, and obtains realistic-looking images competitive with the state-of-theart results produced by generative adversarial networks (GAN) (Goodfellow et al., 2014; Radford et al., 2015).\nRelated Work The idea of amortized inference (Gershman & Goodman, 2014) has been recently applied in various domains of probabilistic reasoning, including both amortized variational inference (e.g., Kingma & Welling, 2013; Rezende & Mohamed, 2015a), and data-driven proposals for (sequential) Monte Carlo methods (e.g., Paige & Wood, 2016), to name only a few. Most of these methods, however, require to explicitly calculate q(x) (or its gradient). One exception is a very recent paper (Ranganath et al., 2016) that avoids calculating q(x) using an idea related to Stein discrepancy (Gorham & Mackey, 2015; Liu et al., 2016; Oates et al., 2014; Chwialkowski et al., 2016). There is also a raising interest recently on a similar problem of \u201clearning to optimize\u201d (e.g., Andrychowicz et al., 2016; Daniel et al., 2016; Li & Malik, 2016), which is technically easier than the more general problem of \u201clearning to sample\u201d. In fact, we show that our algorithm reduces to \u201clearning to optimize\u201d when only one particle is used in SVGD.\nGenerative adversarial network (GAN) and its variants have recently gained remarkable success on generating realistic-looking images (Goodfellow et al., 2014; Salimans et al., 2016; Radford et al., 2015; Li et al., 2015; Dziugaite et al., 2015; Nowozin et al., 2016). All these methods are set up to train latent variable models (the generator) under the assistant of the discriminator. Our SteinGAN instead performs traditional MLE training for a deep energy model, with the help of a neural sampler that learns to draw samples from the energy model to approximate the likelihood\nfunction; this admits an adversarial interpretation: we can view the neural sampler as a generator that attends to fool the deep energy model, which in turn serves as a discriminator that distinguishes the real samples and the simulated samples given by the neural sampler. This idea of training MLE with neural samplers was first discussed by Kim & Bengio (2016); one of the key differences is that the neural sampler in Kim & Bengio (2016) is trained with the help of a heuristic diversity regularizer based on batch normalization, while SVGD enforces the diversity in a more principled way. Another method by Zhao et al. (2016) also trains an energy score to distinguish real and simulated samples, but within a non-probabilistic framework (see Section 5 for more discussion). Other more traditional approaches for training energy-based models (e.g., Ngiam et al., 2011; Xie et al., 2016) are often based on variants of MCMC-MLE or contrastive divergence (Geyer, 1991; Hinton, 2002; Tieleman, 2008), and have difficulty generating realistic-looking images from scratch.\n2 STEIN VARIATIONAL GRADIENT DESCENT (SVGD)\nStein variational gradient descent (SVGD) (Liu & Wang, 2016) is a general purpose Bayesian inference algorithm motivated by Stein\u2019s method (Stein, 1972; Barbour & Chen, 2005) and kernelized Stein discrepancy (Liu et al., 2016; Chwialkowski et al., 2016; Oates et al., 2014). It uses an efficient deterministic gradient-based update to iteratively evolve a set of particles {xi}ni=1 to minimize the KL divergence with the target distribution. SVGD has a simple form that reduces to the typical gradient descent for maximizing log p when using only one particle (n = 1), and hence can be easily combined with the successful tricks for gradient optimization, including stochastic gradient, adaptive learning rates (such as adagrad), and momentum.\nTo give a quick overview of the main idea of SVGD, let p(x) be a positive density function on Rd which we want to approximate with a set of particles {xi}ni=1. SVGD initializes the particles by sampling from some simple distribution q0, and updates the particles iteratively by\nxi \u2190 xi + \u03c6(xi), \u2200i = 1, . . . , n, (1) where is a step size, and \u03c6(x) is a \u201cparticle gradient direction\u201d chosen to maximumly decrease the KL divergence between the distribution of particles and the target distribution, in the sense that\n\u03c6 = arg max \u03c6\u2208F { \u2212 d d KL(q[ \u03c6] || p) \u2223\u2223 =0 } , (2)\nwhere q[ \u03c6] denotes the density of the updated particle x\u2032 = x + \u03c6(x) when the density of the original particle x is q, and F is the set of perturbation directions that we optimize over. We choose F to be the unit ball of a vector-valued reproducing kernel Hilbert space (RKHS)Hd = H\u00d7\u00b7 \u00b7 \u00b7\u00d7H with eachH associating with a positive definite kernel k(x, x\u2032); note thatH is dense in the space of continuous functions with universal kernels such as the Gaussian RBF kernel.\nCritically, the gradient of KL divergence in (2) equals a simple linear functional of \u03c6, allowing us to obtain a closed form solution for the optimal \u03c6. Liu & Wang (2016) showed that\n\u2212 d d KL(q[ \u03c6] || p) \u2223\u2223 =0 = Ex\u223cq[Tp\u03c6(x)], (3)\nwith Tp\u03c6(x) = \u2207x log p(x)>\u03c6(x) +\u2207x \u00b7 \u03c6(x), (4) where Tp is considered as a linear operator acting on function \u03c6 and is called the Stein operator in connection with Stein\u2019s identity which shows that the RHS of (3) equals zero if p = q:\nEp[Tp\u03c6] = Ep[\u2207x log p>\u03c6 +\u2207x \u00b7 \u03c6] = 0. (5) This is a result of integration by parts assuming the value of p(x)\u03c6(x) vanishes on the boundary of the integration domain.\nTherefore, the optimization in (2) reduces to\nD(q || p) def= max \u03c6\u2208Hd {Ex\u223cq[Tp\u03c6(x)] s.t. ||\u03c6||Hd \u2264 1}, (6)\nwhere D(q || p) is the kernelized Stein discrepancy defined in Liu et al. (2016), which equals zero if and only if p = q under mild regularity conditions. Importantly, the optimal solution of (6) yields a closed form\n\u03c6\u2217(x\u2032) \u221d Ex\u223cq[\u2207x log p(x)k(x, x\u2032) +\u2207xk(x, x\u2032)].\nAlgorithm 1 Amortized SVGD for Problem 1 Set batch size m, step-size scheme { t} and kernel k(x, x\u2032). Initialize \u03b70. for iteration t do\nDraw random {\u03bei}mi=1, calculate xi = f(\u03b7t; \u03bei), and the Stein variational gradient \u2206xi in (7). Update parameter \u03b7 using (8), (9) or (10).\nend for\nBy approximating the expectation under q with the empirical average of the current particles {xi}ni=1, SVGD admits a simple form of update:\nxi \u2190 xi + \u2206xi, \u2200i = 1, . . . , n, where \u2206xi = E\u0302x\u2208{xi}ni=1 [\u2207x log p(x)k(x, xi) +\u2207xk(x, xi)], (7)\nand E\u0302x\u223c{xi}ni=1 [f(x)] = \u2211 i f(xi)/n. The two terms in \u2206xi play two different roles: the term with the gradient \u2207x log p(x) drives the particles toward the high probability regions of p(x), while the term with \u2207xk(x, xi) serves as a repulsive force to encourage diversity; to see this, consider a stationary kernel k(x, x\u2032) = k(x \u2212 x\u2032), then the second term reduces to E\u0302x\u2207xk(x, xi) = \u2212E\u0302x\u2207xik(x, xi), which can be treated as the negative gradient for minimizing the average similarity E\u0302xk(x, xi) in terms of xi. Overall, this particle update produces diverse points for distributional approximation and uncertainty assessment, and also has an interesting \u201cmomentum\u201d effect in which the particles move collaboratively to escape the local optima.\nIt is easy to see from (7) that \u2206xi reduces to the typical gradient \u2207x log p(xi) when there is only a single particle (n = 1) and \u2207xk(x, xi) when x = xi, in which case SVGD reduces to the standard gradient ascent for maximizing log p(x) (i.e., maximum a posteriori (MAP)).\n3 AMORTIZED SVGD: TOWARDS AN AUTOMATIC NEURAL SAMPLER\nSVGD and other particle-based methods become inefficient when we need to repeatedly infer a large number different target distributions for multiple tasks, including online learning or inner loops of other algorithms, because they can not improve based on the experience from the past tasks, and may require a large memory to restore a large number of particles. We propose to \u201camortize SVGD\u201d by training a neural network f(\u03b7; \u03be) to mimic the SVGD dynamics, yielding a solution for Problem 1.\nOne straightforward way to achieve this is to run SVGD to convergence and train f(\u03b7; \u03be) to fit the SVGD results. This, however, requires to run many epochs of fully converged SVGD and can be slow in practice. We instead propose an incremental approach in which \u03b7 is iteratively adjusted so that the network outputs x = f(\u03b7; \u03be) changes along the Stein variational gradient direction in (7) in order to decrease the KL divergence between the target and approximation distribution.\nTo be specific, denote by \u03b7t the estimated parameter at the t-th iteration of our method; each iteration of our method draws a batch of random inputs {\u03bei}mi=1 and calculate their corresponding output xi = f(\u03b7; \u03bei) based on \u03b7t; here m is a mini-batch size (e.g., m = 100). The Stein variational gradient \u2206xi in (7) would then ensure that x\u2032i = xi + \u2206xi forms a better approximation of the target distribution p. Therefore, we should adjust \u03b7 to make its output matches {x\u2032i}, that is, we want to update \u03b7 by\n\u03b7t+1 \u2190 arg min \u03b7 m\u2211 i=1 ||f(\u03b7; \u03bei)\u2212 x\u2032i||22, where x\u2032i = xi + \u2206xi. (8)\nSee Algorithm 1 for the summary of this procedure. If we assume is very small, then (8) reduces to a least square optimization. To see this, note that f(\u03b7; \u03bei) \u2248 f(\u03b7t; \u03bei) + \u2202\u03b7f(\u03b7t; \u03bei)(\u03b7 \u2212 \u03b7t) by Taylor expansion. Since xi = f(\u03b7t; \u03bei), we have\n||f(\u03b7; \u03bei)\u2212 x\u2032i||22 \u2248 ||\u2202\u03b7f(\u03b7t; \u03bei)(\u03b7 \u2212 \u03b7t)\u2212 \u2206xi||22. As a result, (8) reduces to the following least square optimization:\n\u03b7t+1 \u2190 \u03b7t + \u2206\u03b7t, where \u2206\u03b7t = arg min \u03b4 m\u2211 i=1 ||\u2202\u03b7f(\u03b7t; \u03bei)\u03b4 \u2212\u2206xi||22. (9)\nUpdate (9) can still be computationally expensive because of the matrix inversion. We can derive a further approximation by performing only one step of gradient descent of (8) (or (9)), which gives\n\u03b7t+1 \u2190 \u03b7t + m\u2211 i=1 \u2202\u03b7f(\u03b7 t; \u03bei)\u2206xi. (10)\nAlthough update (10) is derived as an approximation of (8)-(9), it is computationally faster and we find it works very effectively in practice; this is because when is small, one step of gradient update can be sufficiently close to the optimum.\nUpdate (10) also has a simple and intuitive form: (10) can be thought as a \u201cchain rule\u201d that backpropagates the Stein variational gradient to the network parameter \u03b7. This can be justified by considering the special case when we use only a single particle (n = 1) in which case \u2206xi in (7) reduces to the typical gradient \u2207x log p(xi) of log p(x), and update (10) reduces to the typical gradient ascent for maximizing E\u03be[log p(f(\u03b7; \u03be))], in which case f(\u03b7; \u03be) is trained to maximize log p(x) (that is, learning to optimize), instead of learning to draw samples from p for which it is crucial to use Stein variational gradient \u2206xi to diversify the network outputs.\nUpdate (10) also has a close connection with the typical variational inference with the reparameterization trick (Kingma & Welling, 2013). Let q\u03b7(x) be the density function of x = f(\u03b7; \u03be), \u03be \u223c q0. Using the reparameterization trick, the gradient of KL(q\u03b7 || p) w.r.t. \u03b7 can be shown to be\n\u2207\u03b7KL(q\u03b7 || p) = \u2212E\u03be\u223cq0 [\u2202\u03b7f(\u03b7; \u03be)(\u2207x log p(x)\u2212\u2207x log q\u03b7(x))].\nWith {\u03bei} i.i.d. drawn from q0 and xi = f(\u03b7; \u03bei), \u2200i, the standard stochastic gradient descent for minimizing the KL divergence is\n\u03b7t+1 \u2190 \u03b7t + \u2211 i \u2202\u03b7f(\u03b7 t; \u03bei)\u2206\u0303xi, where \u2206\u0303xi = \u2207x log p(xi)\u2212\u2207x log q\u03b7(xi). (11)\nThis is similar with (10), but replaces the Stein gradient \u2206xi defined in (7) with \u2206\u0303xi. The advantage of using \u2206xi is that it does not require to explicitly calculate q\u03b7 , and hence admits a solution to Problem 1 in which q\u03b7 is not computable for complex network f(\u03b7; \u03be) and unknown input distribution q0. Further insights can be obtained by noting that\n\u2206xi \u2248 Ex\u223cq[\u2207x log p(x)k(x, xi) +\u2207xk(x, xi)] = Ex\u223cq[(\u2207x log p(x)\u2212\u2207x log q(x))k(x, xi)] (12) = Ex\u223cq[(\u2206\u0303x)k(x, xi)],\nwhere (12) is obtained by using Stein\u2019s identity (5). Therefore, \u2206xi can be treated as a kernel smoothed version of \u2206\u0303xi.\n4 AMORTIZED MLE FOR GENERATIVE ADVERSARIAL TRAINING\nOur method allows us to design efficient approximate sampling methods adaptively and automatically, and enables a host of novel applications. In this paper, we apply it in an amortized MLE method for training deep generative models.\nMaximum likelihood estimator (MLE) provides a fundamental approach for learning probabilistic models from data, but can be computationally prohibitive on distributions for which drawing samples or computing likelihood is intractable due to the normalization constant. Traditional methods such as MCMC-MLE use hand-designed methods (e.g., MCMC) to approximate the intractable likelihood function but do not work efficiently in practice. We propose to adaptively train a generative neural network to draw samples from the distribution during MLE training, which not only provides computational advantage, and also allows us to generate realistic-looking images competitive with, or better than the state-of-the-art generative adversarial networks (GAN) (Goodfellow et al., 2014; Radford et al., 2015) (see Figure 1-5).\nAlgorithm 2 Amortized MLE as Generative Adversarial Learning Goal: MLE training for energy model p(x|\u03b8) = exp(\u2212\u03c6(x, \u03b8)\u2212 \u03a6(\u03b8)). Initialize \u03b7 and \u03b8. for iteration t do\nUpdating \u03b7: Draw \u03bei \u223c q0, xi = f(\u03b7; \u03bei); update \u03b7 using (8), (9) or (10) with p(x) = p(x|\u03b8). Repeat several times when needed. Updating \u03b8: Draw a mini-batch of observed data {xi,obs}, and simulated data xi = f(\u03b7; \u03bei), update \u03b8 by (13).\nend for\nTo be specific, denote by {xi,obs} a set of observed data. We consider the maximum likelihood training of energy-based models of form\np(x|\u03b8) = exp(\u2212\u03c6(x, \u03b8)\u2212 \u03a6(\u03b8)), \u03a6(\u03b8) = log \u222b exp(\u2212\u03c6(x, \u03b8))dx,\nwhere \u03c6(x; \u03b8) is an energy function for x indexed by parameter \u03b8 and \u03a6(\u03b8) is the log-normalization constant. The log-likelihood function of \u03b8 is\nL(\u03b8) = 1\nn n\u2211 i=1 log p(xi,obs|\u03b8),\nwhose gradient is\n\u2207\u03b8L(\u03b8) = \u2212E\u0302obs[\u2202\u03b8\u03c6(x; \u03b8)] + E\u03b8[\u2202\u03b8\u03c6(x; \u03b8)],\nwhere E\u0302obs[\u00b7] and E\u03b8[\u00b7] denote the empirical average on the observed data {xi,obs} and the expectation under model p(x|\u03b8), respectively. The key computational difficulty is to approximate the model expectation E\u03b8[\u00b7]. To address this problem, we use a generative neural network x = f(\u03b7; \u03be) trained by Algorithm 1 to approximately sample from p(x|\u03b8), yielding a gradient update for \u03b8 of form\n\u03b8 \u2190 \u03b8 + \u2207\u0302\u03b8L(\u03b8), \u2207\u0302\u03b8L(\u03b8) = \u2212E\u0302obs[\u2202\u03b8\u03c6(x; \u03b8)] + E\u0302\u03b7[\u2202\u03b8\u03c6(x; \u03b8)], (13)\nwhere E\u0302\u03b7 denotes the empirical average on {xi} where xi = f(\u03b7; \u03bei), {\u03bei} \u223c q0. As \u03b8 is updated by gradient ascent, \u03b7 is successively updated via Algorithm 1 to follow p(x|\u03b8). See Algorithm 2. We call our method SteinGAN, because it can be intuitively interpreted as an adversarial game between the generative network f(\u03b7; \u03be) and the energy model p(x|\u03b8) which serves as a discriminator: The MLE gradient update of p(x|\u03b8) effectively decreases the energy of the training data and increases the energy of the simulated data from f(\u03b7; \u03be), while the SVGD update of f(\u03b7; \u03be) decreases the energy of the simulated data to fit better with p(x|\u03b8). Compared with the traditional methods based on MCMC-MLE or contrastive divergence, we amortize the sampler as we train, which gives much faster speed and simultaneously provides a high quality generative neural network that can generate realistic-looking images; see Kim & Bengio (2016) for a similar idea and discussions.\n5 EMPIRICAL RESULTS\nWe evaluated our SteinGAN on four datasets, MNIST, CIFAR-10, CelebA (Liu et al., 2015), and Large-scale Scene Understanding (LSUN) (Yu et al., 2015), on which we find our method tends to generate realistic-looking images competitive with, sometimes better than DCGAN (Radford et al., 2015) (see Figure 2 - Figure 3). Our code is available at https://github.com/DartML/ SteinGAN.\nModel Setup In order to generate realistic-looking images, we define our energy model based on an autoencoder:\np(x|\u03b8) \u221d exp(\u2212||x\u2212D(E(x; \u03b8); \u03b8)||), (14)\nwhere x denotes the image. This choice is motivated by Energy-based GAN (Zhao et al., 2016) in which the autoencoder loss is used as a discriminator but without a probabilistic interpretation. We\nassume f(\u03b7; \u03be) to be a neural network whose input \u03be is a 100-dimensional random vector drawn by Uniform([\u22121, 1]). The positive definite kernel in SVGD is defined by the RBF kernel on the hidden representation obtained by the autoencoder in (14), that is,\nk(x, x\u2032) = exp(\u2212 1 h2 ||E(x; \u03b8)\u2212 E(x\u2032; \u03b8)||2).\nAs it is discussed in Section 3, the kernel provides a repulsive force to produce an amount of variability required for generating samples from p(x). This is similar to the heuristic repelling regularizer in Zhao et al. (2016) and the batch normalization based regularizer in Kim & Bengio (2016), but is derived in a more principled way. We take the bandwidth to be h = 0.5 \u00d7med, where med is the median of the pairwise distances between E(x) on the image simulated by f(\u03b7; \u03be). This makes the kernel change adaptively based on both \u03b8 (through E(x; \u03b8)) and \u03b7 (through bandwidth h).\nSome datasets include both images x and their associated discrete labels y. In these cases, we train a joint energy model on (x, y) to capture both the inner structure of the images and its predictive relation with the label, allowing us to simulate images with a control on which category it belongs to. Our joint energy model is defined to be\np(x, y|\u03b8) \u221d exp { \u2212 ||x\u2212D(E(x; \u03b8); \u03b8)|| \u2212max[m, \u03c3(y, E(x; \u03b8))] } , (15)\nwhere \u03c3(\u00b7, \u00b7) is the cross entropy loss function of a fully connected output layer. In this case, our neural sampler first draws a label y randomly according to the empirical counts in the dataset, and then passes y into a neural network together with a 100 \u00d7 1 random vector \u03be to generate image x. This allows us to generate images for particular categories by controlling the value of input y.\nStabilization In practice, we find it is useful to modify (13) to be \u03b8 \u2190 \u03b8 \u2212 E\u0302obs[\u2207\u03b8\u03c6(x, \u03b8)] + (1\u2212 \u03b3)E\u0302\u03b7[\u2207\u03b8\u03c6(x, \u03b8)]. (16)\nwhere \u03b3 is a discount factor (which we take to be \u03b3 = 0.7). This is equivalent to maximizing a regularized likelihood:\nmax \u03b8 {log p(x|\u03b8) + \u03b3\u03a6(\u03b8)}\nwhere \u03a6(\u03b8) is the log-partition function; note that exp(\u03b3\u03a6(\u03b8)) is a conjugate prior of p(x|\u03b8). We initialize the weights of both the generator and discriminator from Gaussian distribution N (0, 0.02), and train them using Adam (Kingma & Ba, 2014) with a learning rate of 0.001 for the generator and 0.0001 for the energy model (the discriminator). In order to keep the generator and discriminator approximately aligned during training, we speed up the MLE update (16) of the discriminator (by increasing its learning rate to 0.0005) when the energy of the real data batch is larger than the energy of the simulated images, while slow down it (by freezing the MLE update of \u03b8 in (16)) if the magnitude of the energy difference between the real images and the simulated images goes above a threshold of 0.5. We used the bag of architecture guidelines for stable training suggested in DCGAN (Radford et al., 2015).\nDiscussion The MNIST dataset has a training set of 60, 000 examples. Both DCGAN and our model produce high quality images, both visually indistinguishable from real images; see figure 1.\nCIFAR-10 is very diverse, and with only 50,000 training examples. Figure 2 shows examples of simulated images by DCGAN and SteinGAN generated conditional on each category, which look equally well visually. We also provide quantitively evaluation using a recently proposed inception score (Salimans et al., 2016), as well as the classification accuracy when training ResNet using 50, 000 simulated images as train sets, evaluated on a separate held-out testing set never seen by the GAN models. Besides DCGAN and SteinGAN, we also evaluate another simple baseline obtained by subsampling 500 real images from the training set and duplicating them 100 times. We observe that these scores capture rather different perspectives of image generation: The inception score favors images that look realistic individually and have uniformly distributed labels; as a result, the inception score of the duplicated 500 images is almost as high as the real training set. We find that the inception score of SteinGAN is comparable, or slightly lower than that of DCGAN. On the other hand, the classification accuracy measures the amount information captured in the simulated image sets; we find that SteinGAN achieves the highest classification accuracy, suggesting that it captures more information in the training set.\nFigure 3 and 4 visualize the results on CelebA (with more than 200k face images) and LSUN (with nearly 3M bedroom images), respectively. We cropped and resized both dataset images into 64\u00d764.\n6 CONCLUSION\nWe propose a new method to train neural samplers for given distributions, together with a new SteinGAN method for generative adversarial training. Future directions involve more applications and theoretical understandings for training neural samplers.\n", "cats": {"READ": 1.0, "DONTREAD": 0.0}, "reviews": [{"IS_META_REVIEW": true, "comments": "This paper considers the energy-based model interpretation of GAN, where the discriminator is an unnormalized model for the likelihood of a generative model p(x|theta) and the generator is a directed model that approximates this distribution. The generator is used to draw approximate negative phase samples that are used in stochastic maximum likelihood / contrastive divergence learning of the EBM / discriminator.\n\nThe main idea in the paper is to fit the generator by following the Stein variational gradient. In practice this gradient consists of the usual gradient provided by the discriminator with an added term that provides a repulsive force between the sampled data points to increase sample diversity.\n\nThe idea of using a kernel to push apart the sampled points is interesting, and will work in low dimensions, but it is hard to see how it can work in full scale images. For high dimensional samples x, the proposed kernel is unlikely to provide a useful distance measure between points. There are no convincing experiments in the paper that show otherwise. Specifically:\n\n- There is no experiment that compares between standard GAN and GAN + repulsion, using the same architecture. (please address this in the rebuttal)\n- If the Stein variational idea is taken literally, the right thing to do would be to fully optimize the generator at every step, and then taking a single optimization step on the discriminator. Instead, each is updated in turn, and the learning rates of both steps are adjusted to keep the two \"in line\".\n- The kernel used to fit the generator is defined in the auto-encoder space of the discriminator, and thus depends on the discriminator parameters. The objective that is used to fit the generator thus changes at every step, and the procedure can no longer be interpreted as stochastic gradient descent with respect to any single well defined objective.\n\nThe authors obtain good results: The generated images clearly look better than those generated by DCGAN. However, their approach has a number of changes compared to DCGAN, so it is not clear where the improvement comes from. In addition, by now the DCGAN is no longer a very strong baseline, as various other techniques have been proposed.\n\nNote: The use of phi for both the \"particle gradient direction\" and energy function is confusing"}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "This paper presents an idea with a sensible core (augmenting amortized inference with per-instance optimization) but with an overcomplicated and ad-hoc execution. The reviewers provided clear guidance for how this paper could be improved, and thus I invite the authors to submit this paper to the workshop track.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"DATE": "24 Jan 2017", "TITLE": "Thank you for your review and comments", "IS_META_REVIEW": false, "comments": "We highly appreciate the time and feedback from all the reviewers, all of which we will take into serious consideration in our revision. We will particularly strengthen and clarify the empirical experiments. Below we address some of the major points: \n\n[Testing Accuracy Score]\nWe agree with the reviewers' point on the \"testing accuracy\" score, but think that it still provides some valuable insight about the dataset. Its blindness to the background can be a good thing in that it captures more information about the \"effective amount\" of objects the dataset contains.  The problem is that it is very difficult to obtain a *perfect* score, and reporting more than one metrics (in an objective fashion) can help to gain more comprehensive understandings. \n\n[Repulsive Term in High Dimension]\nOur repulsive force works due to two tricks: 1) scaling the bandwidth with the data diversity using the median trick, which alleviates the exponential decay of RBF kernel. 2) define kernel on the feature space instead of the raw pixels of the images, which allows us to respect the manifold structure of the images. The framework of SVGD allows us to use any positive definite kernels and change it adaptively during iterations, because the kernel only defines the \"tangent space\" for improvement. \n\nSteinGAN without kernel corresponds to Viterbi training of the energy model and we find it work well with careful tuning of parameters, but tend to converge to a small number of bad-looking images after running a large number of iterations; adding the kernel under the same setting helps prevent this problem. Our current results on CIFAR10 shows that SteinGAN without kernel gives an inception score of 6.34, while that SteinGAN with kernel gives 6.76. \n\n[Amortized is slower than non-amortized]\nAlthough the amortized algorithm has the overhead of updating $\\xi$, it stores the information in a generative network, and allows us to simulate as many images as we need. By using the one-step gradient update we proposed, the update of $\\xi$ is the same as standard backpropagation except replacing the Dlogp with the SVGD gradient. ", "OTHER_KEYS": "Dilin Wang"}, {"TITLE": "Insufficient empirical evaluation.", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "This paper proposes an amortized version of the Stein variational gradient descent (SVGD) method in which \"a neural network is trained to mimic the SVGD dynamics\". It applies the method to generative adversarial training to yield a training procedure where the discriminator is interpreted as an energy-based probabilistic model.\n\nOne criticism I have of the presentation is that a lot of time and energy is spent setting the table for a method which is claimed to be widely applicable, and the scope of the empirical evaluation is narrowed down to a single specific setting. In my view, either the paper falls short of its goal of showing how widely applicable the proposed method is, or it spends too much time setting the table for SteinGAN and not enough time evaluating it.\n\nThe consequence of this is that the empirical results are insufficient in justifying the approach proposed by the paper. As another reviewer pointed out, DCGAN is becoming outdated as a benchmark for comparison.\n\nQualitatively, SteinGAN samples don't look significantly better than DCGAN samples, except for the CelebA dataset. In that particular case, the DCGAN samples don't appear to be the ones presented in the original paper; where do they come from?\n\nQuantitatively, DCGAN beats SteinGAN by a small margin for the ImageNet Inception Score and SteinGAN beats DCGAN by an even smaller margin for the CIFAR10 Inception Score. Also, in my opinion, the \"testing accuracy\" score is not a convincing evaluation metric: while it is true that it measures the amount of information captured in the simulated image sets, it is only sensitive to information useful for the discrimination task, not for the more general modeling task. For instance, this score is likely completely blind to information present in the background of the image.\n\nBecause of the reasons outlined above, I don't think the paper is ready for publication at ICLR.", "IS_META_REVIEW": false, "RECOMMENDATION": 4, "DATE": "18 Dec 2016", "REVIEWER_CONFIDENCE": 3}, {"TITLE": "Review: Learning to Draw Samples: With Application to Amortized MLE for Generative Adversarial Learning", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "The authors propose amortized SVGD, an amortized form of prior work on SVGD, which is a particle variational method that maximally decreases the KL divergence at each update. \"amortized SVGD\" is done by training a neural network to learn this dynamic. They then apply this idea to train energy-based models, which admit a tractable unnormalized density.\n\nIn SVGD, the main difference from just MAP is the addition of a \"repulsive force\" that prevents degeneracy by encouraging probability mass to be spread to locations outside the mode. How this is able to still act as a strong enough entropy-like term in high dimensions is curious. From my understanding of their previous work, this was not a problem as the only experiments were on toy and UCI data sets.\n\nIn the experimental results here, they apply the kernel on the hidden representation of an autoencoder, which seems key, similar to Li et al. (2015) where their kernel approach for MMD would not work as well otherwise. However, unlike Li et al. (2015) the autoencoder is part of the model itself and not fixed. This breaks much of the authors' proposed motivation and criticisms of prior work, if they must autoencode onto some low-dimensional space (putting most effort then on the autoencoder, which changes per iteration) before then applying their method.\n\nUnlike previous literature which uses inference networks, their amortized SVGD approach seems in fact slower than the non-amortized approach. This is because they must make the actual update on xi before then regressing to perform the update on eta (in previous approaches, this would be like having to perform local inferences before then updating inference network parameters, or at least partially performing the local inference). This seems quite costly during training.\n\nI recommend the paper be rejected, and that the authors provide more comprehensive experimental results, expecially around the influence of the autoencoder, the incremental updates versus full updates, and the training time of amortized vs non-amortized approaches. The current results are promising but unclear why given the many knobs that the authors are playing with.\n\nReferences\n\nLi, Y., Swersky, K., & Zemel, R. (2015). Generative Moment Matching Networks. Presented at the International Conference on Machine Learning.", "IS_META_REVIEW": false, "RECOMMENDATION": 4, "DATE": "17 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "Decent results, but unclear whether this is due to the proposed Stein variational gradient", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "This paper considers the energy-based model interpretation of GAN, where the discriminator is an unnormalized model for the likelihood of a generative model p(x|theta) and the generator is a directed model that approximates this distribution. The generator is used to draw approximate negative phase samples that are used in stochastic maximum likelihood / contrastive divergence learning of the EBM / discriminator.\n\nThe main idea in the paper is to fit the generator by following the Stein variational gradient. In practice this gradient consists of the usual gradient provided by the discriminator with an added term that provides a repulsive force between the sampled data points to increase sample diversity.\n\nThe idea of using a kernel to push apart the sampled points is interesting, and will work in low dimensions, but it is hard to see how it can work in full scale images. For high dimensional samples x, the proposed kernel is unlikely to provide a useful distance measure between points. There are no convincing experiments in the paper that show otherwise. Specifically:\n\n- There is no experiment that compares between standard GAN and GAN + repulsion, using the same architecture. (please address this in the rebuttal)\n- If the Stein variational idea is taken literally, the right thing to do would be to fully optimize the generator at every step, and then taking a single optimization step on the discriminator. Instead, each is updated in turn, and the learning rates of both steps are adjusted to keep the two \"in line\".\n- The kernel used to fit the generator is defined in the auto-encoder space of the discriminator, and thus depends on the discriminator parameters. The objective that is used to fit the generator thus changes at every step, and the procedure can no longer be interpreted as stochastic gradient descent with respect to any single well defined objective.\n\nThe authors obtain good results: The generated images clearly look better than those generated by DCGAN. However, their approach has a number of changes compared to DCGAN, so it is not clear where the improvement comes from. In addition, by now the DCGAN is no longer a very strong baseline, as various other techniques have been proposed.\n\nNote: The use of phi for both the \"particle gradient direction\" and energy function is confusing", "IS_META_REVIEW": false, "RECOMMENDATION": 4, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 3}, {"DATE": "07 Dec 2016", "TITLE": "equation 10", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1"}, {"DATE": "01 Dec 2016", "TITLE": "Testing accuracy question", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3"}, {"IS_META_REVIEW": true, "comments": "This paper considers the energy-based model interpretation of GAN, where the discriminator is an unnormalized model for the likelihood of a generative model p(x|theta) and the generator is a directed model that approximates this distribution. The generator is used to draw approximate negative phase samples that are used in stochastic maximum likelihood / contrastive divergence learning of the EBM / discriminator.\n\nThe main idea in the paper is to fit the generator by following the Stein variational gradient. In practice this gradient consists of the usual gradient provided by the discriminator with an added term that provides a repulsive force between the sampled data points to increase sample diversity.\n\nThe idea of using a kernel to push apart the sampled points is interesting, and will work in low dimensions, but it is hard to see how it can work in full scale images. For high dimensional samples x, the proposed kernel is unlikely to provide a useful distance measure between points. There are no convincing experiments in the paper that show otherwise. Specifically:\n\n- There is no experiment that compares between standard GAN and GAN + repulsion, using the same architecture. (please address this in the rebuttal)\n- If the Stein variational idea is taken literally, the right thing to do would be to fully optimize the generator at every step, and then taking a single optimization step on the discriminator. Instead, each is updated in turn, and the learning rates of both steps are adjusted to keep the two \"in line\".\n- The kernel used to fit the generator is defined in the auto-encoder space of the discriminator, and thus depends on the discriminator parameters. The objective that is used to fit the generator thus changes at every step, and the procedure can no longer be interpreted as stochastic gradient descent with respect to any single well defined objective.\n\nThe authors obtain good results: The generated images clearly look better than those generated by DCGAN. However, their approach has a number of changes compared to DCGAN, so it is not clear where the improvement comes from. In addition, by now the DCGAN is no longer a very strong baseline, as various other techniques have been proposed.\n\nNote: The use of phi for both the \"particle gradient direction\" and energy function is confusing"}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "This paper presents an idea with a sensible core (augmenting amortized inference with per-instance optimization) but with an overcomplicated and ad-hoc execution. The reviewers provided clear guidance for how this paper could be improved, and thus I invite the authors to submit this paper to the workshop track.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"DATE": "24 Jan 2017", "TITLE": "Thank you for your review and comments", "IS_META_REVIEW": false, "comments": "We highly appreciate the time and feedback from all the reviewers, all of which we will take into serious consideration in our revision. We will particularly strengthen and clarify the empirical experiments. Below we address some of the major points: \n\n[Testing Accuracy Score]\nWe agree with the reviewers' point on the \"testing accuracy\" score, but think that it still provides some valuable insight about the dataset. Its blindness to the background can be a good thing in that it captures more information about the \"effective amount\" of objects the dataset contains.  The problem is that it is very difficult to obtain a *perfect* score, and reporting more than one metrics (in an objective fashion) can help to gain more comprehensive understandings. \n\n[Repulsive Term in High Dimension]\nOur repulsive force works due to two tricks: 1) scaling the bandwidth with the data diversity using the median trick, which alleviates the exponential decay of RBF kernel. 2) define kernel on the feature space instead of the raw pixels of the images, which allows us to respect the manifold structure of the images. The framework of SVGD allows us to use any positive definite kernels and change it adaptively during iterations, because the kernel only defines the \"tangent space\" for improvement. \n\nSteinGAN without kernel corresponds to Viterbi training of the energy model and we find it work well with careful tuning of parameters, but tend to converge to a small number of bad-looking images after running a large number of iterations; adding the kernel under the same setting helps prevent this problem. Our current results on CIFAR10 shows that SteinGAN without kernel gives an inception score of 6.34, while that SteinGAN with kernel gives 6.76. \n\n[Amortized is slower than non-amortized]\nAlthough the amortized algorithm has the overhead of updating $\\xi$, it stores the information in a generative network, and allows us to simulate as many images as we need. By using the one-step gradient update we proposed, the update of $\\xi$ is the same as standard backpropagation except replacing the Dlogp with the SVGD gradient. ", "OTHER_KEYS": "Dilin Wang"}, {"TITLE": "Insufficient empirical evaluation.", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "This paper proposes an amortized version of the Stein variational gradient descent (SVGD) method in which \"a neural network is trained to mimic the SVGD dynamics\". It applies the method to generative adversarial training to yield a training procedure where the discriminator is interpreted as an energy-based probabilistic model.\n\nOne criticism I have of the presentation is that a lot of time and energy is spent setting the table for a method which is claimed to be widely applicable, and the scope of the empirical evaluation is narrowed down to a single specific setting. In my view, either the paper falls short of its goal of showing how widely applicable the proposed method is, or it spends too much time setting the table for SteinGAN and not enough time evaluating it.\n\nThe consequence of this is that the empirical results are insufficient in justifying the approach proposed by the paper. As another reviewer pointed out, DCGAN is becoming outdated as a benchmark for comparison.\n\nQualitatively, SteinGAN samples don't look significantly better than DCGAN samples, except for the CelebA dataset. In that particular case, the DCGAN samples don't appear to be the ones presented in the original paper; where do they come from?\n\nQuantitatively, DCGAN beats SteinGAN by a small margin for the ImageNet Inception Score and SteinGAN beats DCGAN by an even smaller margin for the CIFAR10 Inception Score. Also, in my opinion, the \"testing accuracy\" score is not a convincing evaluation metric: while it is true that it measures the amount of information captured in the simulated image sets, it is only sensitive to information useful for the discrimination task, not for the more general modeling task. For instance, this score is likely completely blind to information present in the background of the image.\n\nBecause of the reasons outlined above, I don't think the paper is ready for publication at ICLR.", "IS_META_REVIEW": false, "RECOMMENDATION": 4, "DATE": "18 Dec 2016", "REVIEWER_CONFIDENCE": 3}, {"TITLE": "Review: Learning to Draw Samples: With Application to Amortized MLE for Generative Adversarial Learning", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "The authors propose amortized SVGD, an amortized form of prior work on SVGD, which is a particle variational method that maximally decreases the KL divergence at each update. \"amortized SVGD\" is done by training a neural network to learn this dynamic. They then apply this idea to train energy-based models, which admit a tractable unnormalized density.\n\nIn SVGD, the main difference from just MAP is the addition of a \"repulsive force\" that prevents degeneracy by encouraging probability mass to be spread to locations outside the mode. How this is able to still act as a strong enough entropy-like term in high dimensions is curious. From my understanding of their previous work, this was not a problem as the only experiments were on toy and UCI data sets.\n\nIn the experimental results here, they apply the kernel on the hidden representation of an autoencoder, which seems key, similar to Li et al. (2015) where their kernel approach for MMD would not work as well otherwise. However, unlike Li et al. (2015) the autoencoder is part of the model itself and not fixed. This breaks much of the authors' proposed motivation and criticisms of prior work, if they must autoencode onto some low-dimensional space (putting most effort then on the autoencoder, which changes per iteration) before then applying their method.\n\nUnlike previous literature which uses inference networks, their amortized SVGD approach seems in fact slower than the non-amortized approach. This is because they must make the actual update on xi before then regressing to perform the update on eta (in previous approaches, this would be like having to perform local inferences before then updating inference network parameters, or at least partially performing the local inference). This seems quite costly during training.\n\nI recommend the paper be rejected, and that the authors provide more comprehensive experimental results, expecially around the influence of the autoencoder, the incremental updates versus full updates, and the training time of amortized vs non-amortized approaches. The current results are promising but unclear why given the many knobs that the authors are playing with.\n\nReferences\n\nLi, Y., Swersky, K., & Zemel, R. (2015). Generative Moment Matching Networks. Presented at the International Conference on Machine Learning.", "IS_META_REVIEW": false, "RECOMMENDATION": 4, "DATE": "17 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "Decent results, but unclear whether this is due to the proposed Stein variational gradient", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "This paper considers the energy-based model interpretation of GAN, where the discriminator is an unnormalized model for the likelihood of a generative model p(x|theta) and the generator is a directed model that approximates this distribution. The generator is used to draw approximate negative phase samples that are used in stochastic maximum likelihood / contrastive divergence learning of the EBM / discriminator.\n\nThe main idea in the paper is to fit the generator by following the Stein variational gradient. In practice this gradient consists of the usual gradient provided by the discriminator with an added term that provides a repulsive force between the sampled data points to increase sample diversity.\n\nThe idea of using a kernel to push apart the sampled points is interesting, and will work in low dimensions, but it is hard to see how it can work in full scale images. For high dimensional samples x, the proposed kernel is unlikely to provide a useful distance measure between points. There are no convincing experiments in the paper that show otherwise. Specifically:\n\n- There is no experiment that compares between standard GAN and GAN + repulsion, using the same architecture. (please address this in the rebuttal)\n- If the Stein variational idea is taken literally, the right thing to do would be to fully optimize the generator at every step, and then taking a single optimization step on the discriminator. Instead, each is updated in turn, and the learning rates of both steps are adjusted to keep the two \"in line\".\n- The kernel used to fit the generator is defined in the auto-encoder space of the discriminator, and thus depends on the discriminator parameters. The objective that is used to fit the generator thus changes at every step, and the procedure can no longer be interpreted as stochastic gradient descent with respect to any single well defined objective.\n\nThe authors obtain good results: The generated images clearly look better than those generated by DCGAN. However, their approach has a number of changes compared to DCGAN, so it is not clear where the improvement comes from. In addition, by now the DCGAN is no longer a very strong baseline, as various other techniques have been proposed.\n\nNote: The use of phi for both the \"particle gradient direction\" and energy function is confusing", "IS_META_REVIEW": false, "RECOMMENDATION": 4, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 3}, {"DATE": "07 Dec 2016", "TITLE": "equation 10", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1"}, {"DATE": "01 Dec 2016", "TITLE": "Testing accuracy question", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3"}]}
{"text": "CONTEXT-CONDITIONAL GENERATIVE ADVERSARIAL NETWORKS\n1 INTRODUCTION\nDeep neural networks have yielded dramatic performance gains in recent years on tasks such as object classification (Krizhevsky et al., 2012), text classification (Zhang et al., 2015) and machine translation (Sutskever et al., 2014; Bahdanau et al., 2015). These successes are heavily dependent on large training sets of manually annotated data. In many settings however, such large collections of labels may not be readily available, motivating the need for methods that can learn from data where labels are rare.\nWe propose a method for harnessing unlabeled image data based on image in-painting. A generative model is trained to generate pixels within a missing hole, based on the context provided by surrounding parts of the image. These in-painted images are then used in an adversarial setting (Goodfellow et al., 2014) to train a large discriminator model whose task is to determine if the image was real (from the unlabeled training set) or fake (an in-painted image). The realistic looking fake examples provided by the generative model cause the discriminator to learn features that generalize to the related task of classifying objects. Thus adversarial training for the in-painting task can be used to regularize large discriminative models during supervised training on a handful of labeled images.\n1.1 RELATED WORK\nLearning From Context: The closest work to ours is the independently developed context-encoder approach of Pathak et al. (2016). This introduces an encoder-decoder framework, shown in Fig. 1(a), that is used to in-paint images where a patch has been randomly removed. After using this as a pre-training task, a classifier is added to the encoder and the model is fine-tuned using the labeled examples. Although both approaches use the concept of in-painting, they differ in several important ways. First, the architectures are different (see Fig. 1): in Pathak et al. (2016), the features for the classifier are taken from the encoder, whereas ours come from the discriminator network. In practice this makes an important difference as we are able to directly train large models such as VGG (Simonyan & Zisserman, 2015) using adversarial loss alone. By contrast, Pathak et al. (2016) report difficulties in training an AlexNet encoder with this loss. This leads to the second difference, namely that on account of these issues, they instead employ an `2 loss when training models for classification and detection (however they do use a joint `2 and adversarial loss to achieve impressive in-painting results). Finally, the unsupervised learning task differs between the two models. The context-encoder learns a feature representation suitable for in-painting whereas our model learns a feature representation suitable for differentiating real/fake in-paintings. Notably, while we also use a neural network to generate the in-paintings, this model is only used as an adversary for the\ndiscriminator, rather than as a feature extractor. In section 4, we compare the performance of our model to the context-encoder on the PASCAL dataset.\nOther forms of spatial context within images have recently been utilized for representation learning. Doersch et al. (2015) propose training a CNN to predict the spatial location of one image patch relative to another. Noroozi & Favaro (2016) propose a model that learns by unscrambling image patches, essentially solving a jigsaw puzzle to learn visual representations. In the text domain, context has been successfully leveraged as an unsupervised criterion for training useful word and sentence level representations (Collobert et al., 2011; Mikolov et al., 2015; Kiros et al., 2015).\nDeep unsupervised and semi-supervised learning: A popular method of utilizing unlabeled data is to layer-wise train a deep autoencoder or restricted Botlzmann machine (Hinton et al., 2006) and then fine tune with labels on a discriminative task. More recently, several autoencoding variants have been proposed for unsupervised and semi-supervised learning, such as the ladder network (Rasmus et al., 2015), stacked what-where autoencoders (Zhao et al., 2016) and variational autoencoders (Kingma & Welling, 2014; Kingma et al., 2014).\nDosovitskiy et al. (2014) achieved state-of-the-art results by training a CNN with a different class for each training example and introducing a set of transformations to provide multiple examples per class. The pseudo-label approach (Lee, 2013) is a simple semi-supervised method that trains using the maximumly predicted class as a label when labels are unavailable. Springenberg (2015) propose a categorical generative adversarial network (CatGAN) which can be used for unsupervised and semi-supervised learning. The discriminator in a CatGAN outputs a distribution over classes and is trained to minimize the predicted entropy for real data and maximize the predicted entropy for fake data. Similar to our model, CatGANs use the feature space learned by the discriminator for the final supervised learning task. Salimans et al. (2016) recently proposed a semi-supervised GAN model in which the discriminator outputs a softmax over classes rather than a probability of real vs. fake. An additional \u2018generated\u2019 class is used as the target for generated samples. This method differs from our work in that it does not utilize context information and has only been applied to datasets of small resolution. However, the discriminator loss is similar to the one we propose and could be combined with our context-conditional approach.\nMore traditional semi-supervised methods include graph-based approaches (Zhou et al., 2004; Zhu, 2006) that show impressive performance when good image representations are available. However, the focus of our work is on learning such representations.\nGenerative models of images: Restricted Boltzmann machines (Salakhutdinov, 2015), de-noising autoencoders (Vincent et al., 2008) and variational autoencoders (Kingma & Welling, 2014) optimize a maximum likelihood criterion and thus learn decoders that map from latent space to image space. More recently, generative adversarial networks (Goodfellow et al., 2014) and generative mo-\nment matching networks (Li et al., 2015; Dziugaite et al., 2015) have been proposed. These methods ignore data likelihoods and instead directly train a generative model to produce realistic samples. Several extensions to the generative adversarial network framework have been proposed to scale the approach to larger images (Denton et al., 2015; Radford et al., 2016; Salimans et al., 2016). Our work draws on the insights of Radford et al. (2016) regarding adversarial training practices and architecture for the generator network, as well as the notion that the discriminator can produce useful features for classification tasks.\nOther models used recurrent approaches to generate images (Gregor et al., 2015; Theis & Bethge, 2015; Mansimov et al., 2016; van den Oord et al., 2016). Dosovitskiy et al. (2015) trained a CNN to generate objects with different shapes, viewpoints and color. Sohl-Dickstein et al. (2015) propose a generative model based on a reverse diffusion process. While our model does involve image generation, it differs from these approaches in that the main focus is on learning a good representation for classification tasks.\nPredictive generative models of videos aim to extrapolate from current frames to future ones and in doing so learn a feature representation that is useful for other tasks. In this vein, Ranzato et al. (2014) used an `2-loss in pixel-space. Mathieu et al. (2015) combined an adversarial loss with `2, giving models that generate crisper images. While our model is also predictive, it only considers interpolation within an image, rather than extrapolation in time.\n2 APPROACH\nWe present a semi-supervised learning framework built on generative adversarial networks (GANs) of Goodfellow et al. (2014). We first review the generative adversarial network framework and then introduce context conditional generative adversarial networks (CC-GANs). Finally, we show how combining a classification objective and a CC-GAN objective provides a unified framework for semi-supervised learning.\n2.1 GENERATIVE ADVERSARIAL NETWORKS\nThe generative adversarial network approach (Goodfellow et al., 2014) is a framework for training generative models, which we briefly review. It consists of two networks pitted against one another in a two player game: A generative model, G, is trained to synthesize images resembling the data distribution and a discriminative model, D, is trained to distinguish between samples drawn from G and images drawn from the training data.\nMore formally, let X = {x1, ...,xn} be a dataset of images of dimensionality d. Let D denote a discriminative function that takes as input an image x \u2208 Rd and outputs a scalar representing the probability of input x being a real sample. Let G denote the generative function that takes as input a random vector z \u2208 Rz sampled from a prior noise distribution pNoise and outputs a synthesized image x\u0303 = G(z) \u2208 Rd. Ideally, D(x) = 1 when x \u2208 X and D(x) = 0 when x was generated from G. The GAN objective is given by:\nmin G max D Ex\u223cX [logD(x)] + Ez\u223cpNoise(z)[log(1\u2212D(G(z)))] (1)\nThe conditional generative adversarial network (Mirza & Osindero, 2014) is an extension of the GAN in which bothD andG receive an additional vector of information y as input. The conditional GAN objective is given by:\nmin G max D Ex,y\u223cX [logD(x,y)] + Ez\u223cpNoise(z)[log(1\u2212D(G(z,y),x))] (2)\n2.2 CONTEXT-CONDITIONAL GENERATIVE ADVERSARIAL NETWORKS\nWe propose context-conditional generative adversarial networks (CC-GANs) which are conditional GANs where the generator is trained to fill in a missing image patch and the generator and discriminator are conditioned on the surrounding pixels.\nIn particular, the generator G receives as input an image with a randomly masked out patch. The generator outputs an entire image. We fill in the missing patch from the generated output and then pass the completed image into D. We pass the completed image into D rather than the context and the patch as two separate inputs so as to prevent D from simply learning to identify discontinuities along the edge of the missing patch.\nMore formally, let m \u2208 Rd denote to a binary mask that will be used to drop out a specified portion of an image. The generator receives as input m x where denotes element-wise multiplication. The generator outputs xG = G(m x, z) \u2208 Rd and the in-painted image xI is given by:\nxI = (1\u2212m) xG +m x (3)\nThe CC-GAN objective is given by:\nmin G max D\nEx\u223cX [logD(x)] + Ex\u223cX ,m\u223cM[log(1\u2212D(xI))] (4)\n2.3 COMBINED GAN AND CC-GAN\nWhile the generator of the CC-GAN outputs a full image, only a portion of it (corresponding to the missing hole) is seen by the discriminator. In the combined model, which we denote by CC-GAN2, the fake examples for the discriminator include both the in-painted image xI and the full image xG produced by the generator (i.e. not just the missing patch). By combining the GAN and CC-GAN approaches, we introduce a wider array of negative examples to the discriminator. The CC-GAN2 objective given by:\nmin G max D\nEx\u223cX [logD(x)] (5)\n+ Ex\u223cX ,m\u223cM[log(1\u2212D(xI))] (6) + Ex\u223cX ,m\u223cM[log(1\u2212D(xG))] (7)\n2.4 SEMI-SUPERVISED LEARNING WITH CC-GANS\nA common approach to semi-supervised learning is to combine a supervised and unsupervised objective during training. As a result unlabeled data can be leveraged to aid the supervised task.\nIntuitively, a GAN discriminator must learn something about the structure of natural images in order to effectively distinguish real from generated images. Recently, Radford et al. (2016) showed that a GAN discriminator learns a hierarchical image representation that is useful for object classification. Such results suggest that combining an unsupervised GAN objective with a supervised classification objective would produce a simple and effective semi-supervised learning method. This approach, denoted by SSL-GAN, is illustrated in Fig. 1(b). The discriminator network receives a gradient from the real/fake loss for every real and generated image. The discriminator also receives a gradient from the classification loss on the subset of (real) images for which labels are available.\nGenerative adversarial networks have shown impressive performance on many diverse datasets. However, samples are most coherent when the set of images the network is trained on comes from a limited domain (eg. churches or faces). Additionally, it is difficult to train GANs on very large images. Both these issues suggest semi-supervised learning with vanilla GANs may not scale well to datasets of large diverse images. Rather than determining if a full image is real or fake, context conditional GANs address a different task: determining if a part of an image is real or fake given the surrounding context.\nFormally, let XL = {(x1, y1), ..., (xn, yn)} denote a dataset of labeled images. Let Dc(x) denote the output of the classifier head on the discriminator (see Fig. 1(c) for details). Then the semisupervised CC-GAN objective is:\nmin G max D Ex\u223cX [logD(x)] + Ex\u223cX ,m\u223cM[log(1\u2212D(xI))] + \u03bbcEx,y\u223cXL [log(Dc(y|x))] (8)\nThe hyperparameter \u03bbc balances the classification and adversarial losses. We only consider the CCGAN in the semi-supervised setting and thus drop the SSL notation when referring to this model.\n2.5 MODEL ARCHITECTURE AND TRAINING DETAILS\nThe architecture of our generative model,G, is inspired by the generator architecture of the DCGAN (Radford et al., 2016). The model consists of a sequence of convolutional layers with subsampling (but no pooling) followed by a sequence of fractionally-strided convolutional layers. For the discriminator, D, we used the VGG-A network (Simonyan & Zisserman, 2015) without the fully connected layers (which we call the VGG-A\u2019 architecture). Details of the generator and discriminator are given\nin Fig. 2. The input to the generator is an image with a patch zeroed out. In preliminary experiments we also tried passing in a separate mask to the generator to make the missing area more explicit but found this did not effect performance.\nEven with the context conditioning it is difficult to generate large image patches that look realistic, making it problematic to scale our approach to high resolution images. To address this, we propose conditioning the generator on both the high resolution image with a missing patch and a low resolution version of the whole image (with no missing region). In this setting, the generators task becomes one of super-resolution on a portion of an image. However, the discriminator does not receive the low resolution image and thus is still faced with the same problem of determining if a given in-painting is viable or not. Where indicated, we used this approach in our PASCAL VOC 2007 experiments, with the original image being downsampled by a factor of 4. This provided enough information for the generator to fill in larger holes but not so much that it made the task trivial. This optional low resolution image is illustrated in Fig. 2(left) with the dotted line.\nWe followed the training procedures of Radford et al. (2016). We used the Adam optimizer (Kingma & Ba, 2015) in all our experiments with learning rate of 0.0002, momentum term \u03b21 of 0.5, and the remaining Adam hyperparameters set to their default values. We set \u03bbc = 1 for all experiments.\n3 EXPERIMENTS\n3.1 STL-10 CLASSIFICATION\nSTL-10 is a dataset of 96\u00d796 color images with a 1:100 ratio of labeled to unlabeled examples, making it an ideal fit for our semi-supervised learning framework. The training set consists of 5000 labeled images, mapped to 10 pre-defined folds of 1000 images each, and 100,000 unlabeled images. The labeled images belong to 10 classes and were extracted from the ImageNet dataset and the unlabeled images come from a broader distribution of classes. We follow the standard testing protocol and train 10 different models on each of the 10 predefined folds of data. We then evaluate classification accuracy of each model on the test set and report the mean and standard deviation.\nWe trained our CC-GAN and CC-GAN2 models on 64\u00d764 crops of the 96\u00d796 image. The hole was 32\u00d732 pixels and the location of the hole varied randomly (see Fig. 3(top)). We trained for 100 epochs and then fine-tuned the discriminator on the 96x96 labeled images, stopping when training accuracy reached 100%. As shown in Table 1, the CC-GAN model performs comparably to current state of the art (Dosovitskiy et al., 2014) and the CC-GAN2 model improves upon it.\nWe also trained two baseline models in an attempt to tease apart the contributions of adversarial training and context conditional adversarial training. The first is a purely supervised training of the VGG-A\u2019 model (the same architecture as the discriminator in the CC-GAN framework). This was trained using a dropout of 0.5 on the final layer and weight decay of 0.001. The performance of this model is significantly worse than the CC-GAN model.\nWe also trained a semi-supervised GAN (SSL-GAN, see Fig. 1(b)) on STL-10. This consisted of the same discriminator as the CC-GAN (VGG-A\u2019 architecture) and generator from the DCGAN model (Radford et al., 2016). The training setup in this case is identical to the CC-GAN model. The SSLGAN performs almost as well as the CC-GAN, confirming our hypothesis that the GAN objective is a useful unsupervised criterion.\n3.2 PASCAL VOC CLASSIFICATION\nIn order to compare against other methods that utilize spatial context we ran the CC-GAN model on PASCAL VOC 2007 dataset. This dataset consists of natural images coming from 20 classes. The dataset contains a large amount of variability with objects varying in size, pose, and position. The training and validation sets combined contain 5,011 images, and the test set contains 4,952 images. The standard measure of performance is mean average precision (mAP).\nWe trained each model on the combined training and validation set for \u223c5000 epochs and evaluated on the test set once1. Following Pathak et al. (2016), we train using random cropping, and then evaluate using the average prediction from 10 random crops.\nOur best performing model was trained on images of resolution 128\u00d7128 with a hole size of 64\u00d764 and a low resolution input of size 32\u00d732. Table 2 compares our CC-GAN method to other feature learning approaches on the PASCAL test set. It outperforms them, beating the current state of the art (Wang & Gupta, 2015) by 3.8%. It is important to note that our feature extractor is the VGGA\u2019 model which is larger than the AlexNet architecture (Krizhevsky et al., 2012) used by other approaches in Table 2. However, purely supervised training of the two models reveals that VGG-A\u2019\n1Hyperparameters were determined by initially training on the training set alone and measuring performance on the validation set.\nis less than 2% better than AlexNet. Furthermore, our model outperforms the supervised VGG-A\u2019 baseline by a 7% margin (62.2% vs. 55.2%). This suggests that our gains stem from the CC-GAN method rather than the use of a better architecture.\nTable 3 shows the effect of training on different resolutions. The CC-GAN improves over the baseline CNN consistently regardless of image size. We found that conditioning on the low resolution image began to help when the hole size was largest (64\u00d764). We hypothesize that the low resolution conditioning would be more important for larger images, potentially allowing the method to scale to larger image sizes than we explored in this work.\n3.3 INPAINTING\nWe now show some sample in-paintings produced by our CC-GAN generators. In our semisupervised learning experiments on STL-10 we remove a single fixed size hole from the image. The top row of Fig. 3 shows in-paintings produced by this model. We can also explored different masking schemes as illustrated in the remaining rows of Fig. 3 (however these did not improve classification results). In all cases we see that training the generator with the adversarial loss produces sharp semantically plausible in-painting results.\nFig. 4 shows generated images and in-painted images from a model trained with the CC-GAN2 criterion. The output of a CC-GAN generator tends to be corrupted outside the patch used to inpaint the image (since gradients only flow back to the missing patch). However, in the CC-GAN2 model, we see that both the in-painted image and the generated image are coherent and semantically consistent with the masked input image.\nFig. 5 shows in-painted images from a generator trained on 128\u00d7128 PASCAL images. Fig. 6 shows the effect of adding a low resolution (32\u00d732) image as input to the generator. For comparison we also show the result of in-painting by filling in with a bi-linearly upsampled image. Here we see the generator produces high-frequency structure rather than simply learning to copy the low resolution patch.\n4 DISCUSSION\nWe have presented a simple semi-supervised learning framework based on in-painting with an adversarial loss. The generator in our CC-GAN model is capable of producing semantically meaningful\nin-paintings and the discriminator performs comparable to or better than existing semi-supervised methods on two classification benchmarks.\nSince discrimination of real/fake in-paintings is more closely related to the target task of object classification than extracting a feature representation suitable for in-filling, it is not surprising that we are able to exceed the performance of Pathak et al. (2016) on PASCAL classification. Furthermore, since our model operates on images half the resolution as those used by other approaches (128\u00d7128 vs. 224\u00d7244), there is potential for further gains if improvements in the generator resolution can be made. Our models and code are available at https://github.com/edenton/cc-gan.\nAcknowledgements: Emily Denton is supported by a Google Fellowship. Rob Fergus is grateful for the support of CIFAR.\n", "cats": {"READ": 1.0, "DONTREAD": 0.0}, "reviews": [{"IS_META_REVIEW": true, "comments": "After rebuttal:\n\nThanks for reporting the AlexNet results. The fact that they are not great is not so bad by itself, and as the authors mention, it would be interesting to understand why this happens. But the fact that these results  were not in the paper (and in fact still are not there) is disturbing. Moreover, some claims in the paper look wrong in the light of these results, for example:\n- \"This suggests that our gains stem from the CC-GAN method rather than the use of a better architecture.\"\n- \"Since discrimination of real/fake in-paintings is more closely related to the target task of object classification than extracting a feature representation suitable for in-filling, it is not surprising that we are able to exceed the performance of Pathak et al. (2016) on PASCAL classification.\"\n\nThese statements, and possibly other parts of the paper, have to be updated. I think the paper cannot be published in its current form. Perhaps after a revision.\n\n--------\nInitial review:\n\nThe paper demonstrates an application of generative adversarial networks (GAN) to unsupervised feature learning. The authors show that the representation learned by the discriminator of a conditional GAN trained for image inpainting performs well on image classification. As a side-effect, fairly convincing inpaintings are produced.\n\nThe proposed method combines two existing ideas: using the discriminator of a GAN as a feature learner [Radford et al. 2015] and performing unsupervised feature learning with image inpainting [Pathak et al. 2016]. Therefore conceptual novelty of the paper is limited. On the plus side, the authors implement their idea well and demonstrate state-of-the-art results on STL-10 and good results on Pascal VOC (although Pascal experiments are incomplete, see below). Overall, I am in the borderline mode, and I will gladly raise the score if the authors address my concerns regarding the experiments.\n\n1) Experimental evaluation on Pascal VOC is not quite satisfactory. Comparison with prior work is unfair because the network architecture used by the authors (VGG) is different from the architecture used by all existing methods (AlexNet). It is great that the authors do not try to hide this fact in the paper, but I do not understand why the authors are not willing to simply run their method with AlexNet architecture, although two commenters asked them to do so. Such an experiment would strongly support authors\u2019 claims. Current reasoning that \u201cwe thought it reasonable to use more current models while making the difference clear\u201d is not convincing. It is great that better architectures lead to better results, but it is also very important to properly compare to prior work. On a related topic, Doersch et al. also tried using VGG architecture, would it be possible to compare to that? Yet another question: why are you not comparing to [Noroozi&Favaro, ECCV 2016] ? I would also like the authors to address the comment by Richard Zhang.\n\n2) Qualitative inpainting results are incomplete: comparison with previous methods (for instance, [Pathak et al 2016]) is missing, and it is impossible to compare different versions of the proposed method because different images are used for different variants. I realize there may be too little space in the main paper to show all the results, but many more results should be shown in the supplementary material. Quantitative results are missing. Currently the inpainting results are just interesting pictures to look at, but they do not add as much to the paper as they could."}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "There has been prior work on semi-supervised GAN, though this paper is the first context conditional variant. The novelty of the approach was questioned by two of the reviewers, as the approach seems more incremental. Furthermore, it would have been helpful if the issues one of the reviewer had with statements in the document were addressed.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"TITLE": "review", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "This paper presents a semi-supervised algorithm for regularizing deep convolutional neural networks.\nThey propose an adversarial approach for image inpainting where the discriminator learns to identify whether an inpainted image comes from the data distribution or the generator, while at the same time it learns to recognize objects in an image from the data distribution. In experiments, they show the usefulness of their algorithm in which the features learned by the discriminator result in comparable or better object recognition performance to the reported state-of-the-art in two datasets.\nOverall, the proposed idea seems a simple yet an effective way for regularize CNNs to improve the classification performance. ", "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "21 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "Review", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "This paper proposes a method to incorporate super-resolution and inpainting in the GAN framework for semi-supervised learning using the GAN discriminative features on larger images.\n\nThe core idea of the paper is not very novel. The usefulness of the GAN discriminative features for semi-supervised learning is already established in previous works such as CatGAN, DCGAN and Salimans et al. However this paper does a good job in actually getting the semi-supervised GAN framework working on larger images such as STL-10 and Pascal datasets using the proposed context conditioning approach, and achieves the state-of-the-art on these datasets.\n\nI think that the authors should provide the SSL-GAN baseline for the PASCAL dataset as it is very important to compare the contribution of the context conditioning idea with the standard way of using GAN for semi-supervised learning, i.e., SSL-GAN. I can't see why the SSL-GAN can not be applied to the 64*64 and 96*96 version of the Pascal dataset (Table 3). If they have trouble training the vanilla GAN on Pascal even on the 64*64 image size, this should be mentioned in the paper and be explained. I am concerned about this specially because CC-GAN almost matches the SSL-GAN baseline on STL-10, and CC-GAN2, to me, seems like a hacky way to improve upon the core CC-GAN idea. So it would be great to compare CC-GAN and SSL-GAN on some other dataset, even if it is a downsampled PASCAL dataset.", "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "19 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "Final review", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "\nAfter rebuttal:\n\nThanks for reporting the AlexNet results. The fact that they are not great is not so bad by itself, and as the authors mention, it would be interesting to understand why this happens. But the fact that these results  were not in the paper (and in fact still are not there) is disturbing. Moreover, some claims in the paper look wrong in the light of these results, for example:\n- \"This suggests that our gains stem from the CC-GAN method rather than the use of a better architecture.\"\n- \"Since discrimination of real/fake in-paintings is more closely related to the target task of object classification than extracting a feature representation suitable for in-filling, it is not surprising that we are able to exceed the performance of Pathak et al. (2016) on PASCAL classification.\"\n\nThese statements, and possibly other parts of the paper, have to be updated. I think the paper cannot be published in its current form. Perhaps after a revision.\n\n--------\nInitial review:\n\nThe paper demonstrates an application of generative adversarial networks (GAN) to unsupervised feature learning. The authors show that the representation learned by the discriminator of a conditional GAN trained for image inpainting performs well on image classification. As a side-effect, fairly convincing inpaintings are produced.\n\nThe proposed method combines two existing ideas: using the discriminator of a GAN as a feature learner [Radford et al. 2015] and performing unsupervised feature learning with image inpainting [Pathak et al. 2016]. Therefore conceptual novelty of the paper is limited. On the plus side, the authors implement their idea well and demonstrate state-of-the-art results on STL-10 and good results on Pascal VOC (although Pascal experiments are incomplete, see below). Overall, I am in the borderline mode, and I will gladly raise the score if the authors address my concerns regarding the experiments.\n\n1) Experimental evaluation on Pascal VOC is not quite satisfactory. Comparison with prior work is unfair because the network architecture used by the authors (VGG) is different from the architecture used by all existing methods (AlexNet). It is great that the authors do not try to hide this fact in the paper, but I do not understand why the authors are not willing to simply run their method with AlexNet architecture, although two commenters asked them to do so. Such an experiment would strongly support authors\u2019 claims. Current reasoning that \u201cwe thought it reasonable to use more current models while making the difference clear\u201d is not convincing. It is great that better architectures lead to better results, but it is also very important to properly compare to prior work. On a related topic, Doersch et al. also tried using VGG architecture, would it be possible to compare to that? Yet another question: why are you not comparing to [Noroozi&Favaro, ECCV 2016] ? I would also like the authors to address the comment by Richard Zhang.\n\n2) Qualitative inpainting results are incomplete: comparison with previous methods (for instance, [Pathak et al 2016]) is missing, and it is impossible to compare different versions of the proposed method because different images are used for different variants. I realize there may be too little space in the main paper to show all the results, but many more results should be shown in the supplementary material. Quantitative results are missing. Currently the inpainting results are just interesting pictures to look at, but they do not add as much to the paper as they could.", "IS_META_REVIEW": false, "RECOMMENDATION": 5, "DATE": "16 Dec 2016 (modified: 24 Jan 2017)", "REVIEWER_CONFIDENCE": 4}, {"DATE": "08 Dec 2016", "TITLE": "question for SSL experiments", "IS_META_REVIEW": false, "comments": "Nice work! I am curious about the SSL experiments: since ", "OTHER_KEYS": "(anonymous)"}, {"DATE": "05 Dec 2016", "TITLE": "questions", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3"}, {"DATE": "02 Dec 2016", "TITLE": "few questions", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2"}, {"DATE": "02 Dec 2016", "TITLE": "pre-review questions", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1"}, {"IS_META_REVIEW": true, "comments": "After rebuttal:\n\nThanks for reporting the AlexNet results. The fact that they are not great is not so bad by itself, and as the authors mention, it would be interesting to understand why this happens. But the fact that these results  were not in the paper (and in fact still are not there) is disturbing. Moreover, some claims in the paper look wrong in the light of these results, for example:\n- \"This suggests that our gains stem from the CC-GAN method rather than the use of a better architecture.\"\n- \"Since discrimination of real/fake in-paintings is more closely related to the target task of object classification than extracting a feature representation suitable for in-filling, it is not surprising that we are able to exceed the performance of Pathak et al. (2016) on PASCAL classification.\"\n\nThese statements, and possibly other parts of the paper, have to be updated. I think the paper cannot be published in its current form. Perhaps after a revision.\n\n--------\nInitial review:\n\nThe paper demonstrates an application of generative adversarial networks (GAN) to unsupervised feature learning. The authors show that the representation learned by the discriminator of a conditional GAN trained for image inpainting performs well on image classification. As a side-effect, fairly convincing inpaintings are produced.\n\nThe proposed method combines two existing ideas: using the discriminator of a GAN as a feature learner [Radford et al. 2015] and performing unsupervised feature learning with image inpainting [Pathak et al. 2016]. Therefore conceptual novelty of the paper is limited. On the plus side, the authors implement their idea well and demonstrate state-of-the-art results on STL-10 and good results on Pascal VOC (although Pascal experiments are incomplete, see below). Overall, I am in the borderline mode, and I will gladly raise the score if the authors address my concerns regarding the experiments.\n\n1) Experimental evaluation on Pascal VOC is not quite satisfactory. Comparison with prior work is unfair because the network architecture used by the authors (VGG) is different from the architecture used by all existing methods (AlexNet). It is great that the authors do not try to hide this fact in the paper, but I do not understand why the authors are not willing to simply run their method with AlexNet architecture, although two commenters asked them to do so. Such an experiment would strongly support authors\u2019 claims. Current reasoning that \u201cwe thought it reasonable to use more current models while making the difference clear\u201d is not convincing. It is great that better architectures lead to better results, but it is also very important to properly compare to prior work. On a related topic, Doersch et al. also tried using VGG architecture, would it be possible to compare to that? Yet another question: why are you not comparing to [Noroozi&Favaro, ECCV 2016] ? I would also like the authors to address the comment by Richard Zhang.\n\n2) Qualitative inpainting results are incomplete: comparison with previous methods (for instance, [Pathak et al 2016]) is missing, and it is impossible to compare different versions of the proposed method because different images are used for different variants. I realize there may be too little space in the main paper to show all the results, but many more results should be shown in the supplementary material. Quantitative results are missing. Currently the inpainting results are just interesting pictures to look at, but they do not add as much to the paper as they could."}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "There has been prior work on semi-supervised GAN, though this paper is the first context conditional variant. The novelty of the approach was questioned by two of the reviewers, as the approach seems more incremental. Furthermore, it would have been helpful if the issues one of the reviewer had with statements in the document were addressed.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"TITLE": "review", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "This paper presents a semi-supervised algorithm for regularizing deep convolutional neural networks.\nThey propose an adversarial approach for image inpainting where the discriminator learns to identify whether an inpainted image comes from the data distribution or the generator, while at the same time it learns to recognize objects in an image from the data distribution. In experiments, they show the usefulness of their algorithm in which the features learned by the discriminator result in comparable or better object recognition performance to the reported state-of-the-art in two datasets.\nOverall, the proposed idea seems a simple yet an effective way for regularize CNNs to improve the classification performance. ", "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "21 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "Review", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "This paper proposes a method to incorporate super-resolution and inpainting in the GAN framework for semi-supervised learning using the GAN discriminative features on larger images.\n\nThe core idea of the paper is not very novel. The usefulness of the GAN discriminative features for semi-supervised learning is already established in previous works such as CatGAN, DCGAN and Salimans et al. However this paper does a good job in actually getting the semi-supervised GAN framework working on larger images such as STL-10 and Pascal datasets using the proposed context conditioning approach, and achieves the state-of-the-art on these datasets.\n\nI think that the authors should provide the SSL-GAN baseline for the PASCAL dataset as it is very important to compare the contribution of the context conditioning idea with the standard way of using GAN for semi-supervised learning, i.e., SSL-GAN. I can't see why the SSL-GAN can not be applied to the 64*64 and 96*96 version of the Pascal dataset (Table 3). If they have trouble training the vanilla GAN on Pascal even on the 64*64 image size, this should be mentioned in the paper and be explained. I am concerned about this specially because CC-GAN almost matches the SSL-GAN baseline on STL-10, and CC-GAN2, to me, seems like a hacky way to improve upon the core CC-GAN idea. So it would be great to compare CC-GAN and SSL-GAN on some other dataset, even if it is a downsampled PASCAL dataset.", "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "19 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "Final review", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "\nAfter rebuttal:\n\nThanks for reporting the AlexNet results. The fact that they are not great is not so bad by itself, and as the authors mention, it would be interesting to understand why this happens. But the fact that these results  were not in the paper (and in fact still are not there) is disturbing. Moreover, some claims in the paper look wrong in the light of these results, for example:\n- \"This suggests that our gains stem from the CC-GAN method rather than the use of a better architecture.\"\n- \"Since discrimination of real/fake in-paintings is more closely related to the target task of object classification than extracting a feature representation suitable for in-filling, it is not surprising that we are able to exceed the performance of Pathak et al. (2016) on PASCAL classification.\"\n\nThese statements, and possibly other parts of the paper, have to be updated. I think the paper cannot be published in its current form. Perhaps after a revision.\n\n--------\nInitial review:\n\nThe paper demonstrates an application of generative adversarial networks (GAN) to unsupervised feature learning. The authors show that the representation learned by the discriminator of a conditional GAN trained for image inpainting performs well on image classification. As a side-effect, fairly convincing inpaintings are produced.\n\nThe proposed method combines two existing ideas: using the discriminator of a GAN as a feature learner [Radford et al. 2015] and performing unsupervised feature learning with image inpainting [Pathak et al. 2016]. Therefore conceptual novelty of the paper is limited. On the plus side, the authors implement their idea well and demonstrate state-of-the-art results on STL-10 and good results on Pascal VOC (although Pascal experiments are incomplete, see below). Overall, I am in the borderline mode, and I will gladly raise the score if the authors address my concerns regarding the experiments.\n\n1) Experimental evaluation on Pascal VOC is not quite satisfactory. Comparison with prior work is unfair because the network architecture used by the authors (VGG) is different from the architecture used by all existing methods (AlexNet). It is great that the authors do not try to hide this fact in the paper, but I do not understand why the authors are not willing to simply run their method with AlexNet architecture, although two commenters asked them to do so. Such an experiment would strongly support authors\u2019 claims. Current reasoning that \u201cwe thought it reasonable to use more current models while making the difference clear\u201d is not convincing. It is great that better architectures lead to better results, but it is also very important to properly compare to prior work. On a related topic, Doersch et al. also tried using VGG architecture, would it be possible to compare to that? Yet another question: why are you not comparing to [Noroozi&Favaro, ECCV 2016] ? I would also like the authors to address the comment by Richard Zhang.\n\n2) Qualitative inpainting results are incomplete: comparison with previous methods (for instance, [Pathak et al 2016]) is missing, and it is impossible to compare different versions of the proposed method because different images are used for different variants. I realize there may be too little space in the main paper to show all the results, but many more results should be shown in the supplementary material. Quantitative results are missing. Currently the inpainting results are just interesting pictures to look at, but they do not add as much to the paper as they could.", "IS_META_REVIEW": false, "RECOMMENDATION": 5, "DATE": "16 Dec 2016 (modified: 24 Jan 2017)", "REVIEWER_CONFIDENCE": 4}, {"DATE": "08 Dec 2016", "TITLE": "question for SSL experiments", "IS_META_REVIEW": false, "comments": "Nice work! I am curious about the SSL experiments: since ", "OTHER_KEYS": "(anonymous)"}, {"DATE": "05 Dec 2016", "TITLE": "questions", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3"}, {"DATE": "02 Dec 2016", "TITLE": "few questions", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2"}, {"DATE": "02 Dec 2016", "TITLE": "pre-review questions", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1"}]}
{"text": "Eligibility traces in reinforcement learning are used as a bias-variance trade-off and can often speed up training time by propagating knowledge back over timesteps in a single update. We investigate the use of eligibility traces in combination with recurrent networks in the Atari domain. We illustrate the benefits of both recurrent nets and eligibility traces in some Atari games, and highlight also the importance of the optimization used in the training.\n1 INTRODUCTION\nDeep reinforcement learning has had many practical successes in game playing (Mnih et al. (2015),Silver et al. (2016)) and robotics (Levine & Abbeel (2014)). Our interest is in further exploring these algorithms in the context of environments with sparse rewards and partial observability. To this end, we investigate the use of two methods that are known to mitigate these problems: recurrent networks, which provide a form of memory summarizing past experiences, and eligibility traces, which allow information to propagate over multiple time steps. Eligibility traces have been shown empirically to provide faster learning (Sutton & Barto (2017), in preparation) but their use with deep RL has been limited so far (van Seijen & Sutton (2014), Hausknecht & Stone (2015)). We provide experiments in the Atari domain showing that eligibility traces boost the performance of Deep RL. We also demonstrate a surprisingly strong effect of the optimization method on the performance of the recurrent networks.\nThe paper is structured as follows. In Sec. 2 we provide background and notation needed for the paper. Sec. 3 describes the algorithms we use. In sec. 4 we present and discuss our experimental results. In Sec. 5 we conclude and present avenues for future work.\n2 BACKGROUND\nA Markov Decision Process (MDP) consists of a tuple \u3008S,A, r,P, \u03b3\u3009, where S is the set of states, A is the set of actions, r : S \u00d7 A 7\u2192 R is the reward function, P(s\u2032|s, a) is the transition function (giving the next state distribution, conditioned on the current state and action), and \u03b3 \u2208 [0, 1) is the discount factor. Reinforcement learning (RL) (Sutton & Barto, 1998) is a framework for solving unknown MDPs, which means finding a good (or optimal) way of behaving, also called a policy. RL works by obtaining transitions from the environment and using them, in order to compute a policy that maximizes the expected return, given by E [\u2211\u221e t=0 \u03b3 trt ] .\nThe state-value function for a policy \u03c0 : S \u00d7 A \u2192 [0, 1], V \u03c0(s), is defined as the expected return obtained by starting at state s and picking actions according to \u03c0. State-action values Q(s, a) are similar to state values, but conditioned also on the initial action a. A policy can be derived from the Q values by picking always the action with the best estimated value at any state.\nMonte Carlo (MC) and Temporal Difference (TD) are two standard methods for updating the value function from data. In MC, an entire trajectory\u2019s return is used as the target value of the current\nstate.\nMC error = \u221e\u2211 i=0 \u03b3irt+i \u2212 V (st) (1)\nIn TD, the estimate of the next state\u2019s value is used to correct the current state\u2019s estimate:\nTD error = rt + \u03b3V (st+1)\u2212 V (st) (2)\nQ-learning is an RL algorithm that allows an agent to learn by imagining that it will take the best possible action in the following step:\nTD error = rt + \u03b3max a\u2032\nQ(st+1, a \u2032)\u2212Q(st, at) (3)\nThis is an instance of off-policy learning, in which the agent gathers data with an exploratory policy, which randomizes the choice of action, but updates its estimates by constructing targets according to a differnet policy (in this case, the policy that is greedy with respect to the current value estimates.\n2.1 ELIGIBILITY TRACES\nEligibility traces are a fundamental reinforcement learning mechanism which allows a trade-off between TD and MC. MC methods suffer from high variance, as many trajectories can be taken from any given state and stochasticity is often present in the MDP. TD suffers from high bias, as it updates values based on its own estimates.\nUsing eligibility traces allows one to design algorithms that cover the middle-ground between MC and TD. The central notion for these are n-step returns, which provide a way of calculating the target by using the value estimate for the state which occurs n steps in the future (compared to the current state):\nR (n) t = n\u22121\u2211 i=0 \u03b3irt+i + \u03b3 nV (st+n). (4)\nWhen n is 1, the results is the TD target, and taking n\u2192\u221e yields the MC target. Eligibility traces use a geometric weighting of these n-step returns, where the weight of the k-step return is \u03bb times the weight of the k \u2212 1-step return. Using a \u03bb = 0 reduces to using TD, as all n-steps for n > 1 have a weight of 0. One of the appealing effects of using eligibility traces is that a single update allows states many steps behind a reward signal to receive credit. This propagates knowledge back at a faster rate, allowing for accelerated learning. Especially in environments where rewards are sparse and/or delayed, eligibility traces can help assign credit to past states and actions. Without traces, seeing a sparse reward will only propagate the value back by one step, which in turn needs to be sampled to send the value back a second step, and so on.\nR\u03bbt = (1\u2212 \u03bb) \u221e\u2211 i=0 \u03bbiR (i) t = (1\u2212 \u03bb) \u221e\u2211 i=1 \u03bbi\u22121 i\u22121\u2211 j=0 \u03b3jrj + \u03b3 i+1V (st+i) (5)\nThis way of viewing eligibility traces is called the forward view, as states are looking ahead at the rewards received in the future. The forward view is rarely used, as it requires a state to wait for the future to unfold before calculating an update, and requires memory to store the experience. There is an equivalent view called the backward view, which allows us to calculate updates for every previous state as we take a single action. This requires no memory and lets us perform updates without having to wait for the future. However, this view has had limited success in the neural network setting as it requires using a trace on each neuron of the network, which tend to be dense and heavily used at each step resulting in noisy signals. For this reason, eligibility traces aren\u2019t heavily used when using deep learning, despite their potential benefits.\n2.1.1 Q(\u03bb)\nQ(\u03bb) is a variant of Q-learning where eligibility traces are used to calculate the TD error. As mentioned previously, the backwards view of traces is traditionally used.\nA few versions of Q(\u03bb) exist, but the most used one is Watkins\u2019s Q(\u03bb). As Q-learning is off-policy, the sequence of actions used in the past trajectory used to calculate the trace might be different from the actions that the current policy might take. In that case, one should not be using the trajectory past the point where actions differ. To handle such a case, Watkins\u2019s Q(\u03bb) sets the trace to 0 if the action that the current policy would select is different from the one used in the past.\n2.2 DEEP Q-NETWORKS\nMnih et al. (2015) introduced deep Q-networks (DQN), one of the first successful reinforcement learning algorithms that use deep learning for function approximation in a way general enough which is applicable to a variety of environments. Applying it to a set of Atari games, they used a convolutional neural network (CNN) which took as input the last four frames of the game, and output Q-values for each possible action.\nEquation 6 shows the DQN cost function, where we are optimizing the \u03b8 parameters. The \u03b8\u2212 parameters represent frozen Q-value weights which are update at a chosen frequency.\nL(st, at|\u03b8) = (rt + \u03b3max a\u2032 Q(st+1, a \u2032|\u03b8\u2212)\u2212Q(st, at|\u03b8))2 (6)\n2.2.1 DEEP RECURRENT Q-NETWORKS\nAs introduced in Hausknecht & Stone (2015), deep recurrent Q-networks (DRQN) are a modification on DQN, where single frames are passed through a CNN, which generates a feature vector that is then fed to an RNN which finally outputs Q-values. This architecture gives the agent a memory, allowing it to learn long-term temporal effects and handle partial observability, which is the case in many environments. The authors showed that randomly blanking out frames was difficult to overcome for DQN, but that DRQN learned to handle without issue.\nTo train DRQN, they proposed two variants of experience replay. The first was to sample entire trajectories and run the RNN from end to end. However this is very computationally demanding as some trajectories can be over 10000 steps long. The second alternative was to sample subtrajectories instead of single transitions. This is required as the RNN needs to fill its hidden state and to allow it to understand the temporal aspect of the data.\n2.3 OPTIMIZERS\nStochastic gradient descent (SGD) is generally the algorithm used to optimize neural networks. However, some information is lost during the process as past gradients might signal that a weight drastically needs to change, or that it is oscillating, requiring a decrease in learning rate. Adaptive SGD algorithms have been built to use this information.\nRMSprop (Tieleman & Hinton (2012)), uses a geometric averaging over gradients squared, and divides the current gradient by its square root. To perform RMSprop, first we calculate the averaging as g = \u03b2g + (1\u2212 \u03b2)\u2207\u03b82 and then update the parameters \u03b8 \u2190 \u03b8 + \u03b1 \u2207\u03b8\u221a\ng+ .\nDQN (Graves (2013)) introduced a variant of RMSprop where the gradient is instead divided by the standard deviation of the running average. First we calculate the running averages m = \u03b2m+ (1\u2212 \u03b2)\u2207\u03b8 and g = \u03b2g + (1 \u2212 \u03b2)\u2207\u03b82, and then update the parameters using \u03b8 \u2190 \u03b8 + \u03b1 \u2207\u03b8\u221a\ng\u2212m2+ . In\nthe rest of the paper, when mentioning RMSprop, we\u2019ll be referring to this version.\nFinally, Kingma & Ba (2014) introduced Adam, which is essentially RMSprop coupled with Nesterov momentum, along with the running averages being corrected for bias. We have a term for the rate of momentum of each of the running averages. To calculate the update with Adam, we start with the updating the averages m = \u03b21m+ (1\u2212 \u03b21)\u2207\u03b8, v = \u03b22v + (1\u2212 \u03b22)\u2207\u03b82, the correct their biases m\u0302 = m/(1\u2212\u03b2t1), v\u0302 = v/(1\u2212\u03b2t2) and finally calculate the gradient update \u03b8 \u2190 \u03b8+\u03b1 m\u0302\u221av\u0302+ .\n3 EXPERIMENTAL SETUP\nAs explained, the forward view of eligibility traces can be useful, but is computationally demanding in terms of memory and time. One must store all transitions and apply the neural network to each state in the trajectory. By using DRQN, experience replay is already part of the algorithm, which removes the memory requirement of the traces. Then, by training on sub-trajectories of data, the states must be run through the RNN with all state values as the output, which eliminates the computational cost. Finally, all that\u2019s left to use eligibility traces is simply to calculate the weighted sum of the targets, which is very cheap to do.\nIn this section we analyze the use of eligibility traces when training DRQN and try both RMSprop and Adam as optimizers. We only tested the algorithms on fully observable games as to compare the learning capacities without the unfair advantage of having a memory, as would be the case on partially observable environments.\n3.1 ARCHITECTURE\nWe tested the algorithms on two Atari 2600 games, part of the Arcade Learning Environment (Bellemare et al. (2012)), Pong and Tennis. The architecture used is similar to the one used in Hausknecht & Stone (2015). The frames are converted to gray-scale and re-sized to 84x84. These are then fed to a CNN with the first layer being 32 8x8 filters and a stride of 4, followed by 64 4x4 filters with a stride of 2, then by 64 3x3 filters with a stride of 1. The output of the CNN is then flattened before being fed to a single dense layer of 512 output neurons, which is finally fed to an LSTM (Hochreiter & Schmidhuber (1997)) with 512 cells. We then have a last linear layer that takes the output of the recurrent layer to output the Q-values. All layers before the LSTM are activated using rectified linear units (ReLU).\nAs mentioned in subsection 2.2.1, we also altered experience replay to sample sub-trajectories. We use backprop through time (BPTT) to train the RNN, but only train on a sub-trajectory of experience. In runtime, the RNN will have had a large sequence of inputs in its hidden state, which can be problematic if always trained with an empty hidden state. Like in Lample & Singh Chaplot (2016), we therefore sample a slightly longer length of trajectory and use the first m states to fill the hidden state. In our experiments, we selected trajectory lengths of 32, where the first 10 states are used as filler and the remaining 22 are used for the traces and TD costs. We used a batch size of 4.\nAll experiments using eligibility traces use \u03bb = 0.8. Furthermore, we use Watkins\u2019s Q(\u03bb). To limit computation costs of using traces, we cut the trace off once it becomes too small. In our experiments, we choose the limit of 0.01, which allows the traces to affect 21 states ahead (when \u03bb = 0.8). We\ncalculate the trace for every state in the trajectory, except for a few in the beginning, use to fill in the hidden state of the RNN.\nWhen using RMSprop, we used a momentum of 0.95, an epsilon of 0.01 and a learning rate of 0.00025. When using Adam, we used a momentum of gradients of 0.9, a momentum of squared gradients of 0.999, an epsilon of 0.001 and a learning rate of 0.00025.\nTesting phases are consistent across all models, with the score being the average over each game played during 125000 frames. We also use an of 0.05 for action selection.\nChoose k as number of trace steps and m as RNN-filler steps Initialize weights \u03b8, experience replay D \u03b8\u2212 \u2190 \u03b8 s\u2190 s0 repeat\nInitialize RNN hidden state to 0. repeat\nChoose a according to \u2212greedy policy on Q(s, a|\u03b8) Take action a in s, observe s\u2032, r Store s, a, r, s\u2032 in Experience Replay Sample 4 sub-trajectories of m+ k sequential transitions (s, a, r, s\u2032) from D\ny\u0302 = { r s\u2019 is terminal, r + \u03b3max\na\u0304 Q(s\u2032, a\u0304|\u03b8\u2212) otherwise\nforeach transition sampled do\n\u03bbt = { \u03bb at = arg maxa\u0304(st, a\u0304|\u03b8), 0 otherwise\nend for l from 0 to k \u2212 1 do\nR\u0302\u03bbt+l = [\u2211k s=l (\u220fs i=l \u03bbt+i ) R (s\u2212l+1) t+s ] / [\u2211k s=l (\u220fs i=l \u03bbt+i )] end Perform gradient descent on\u2202(R\u0302\n\u03bb\u2212Q(s,a|\u03b8))2 \u2202\u03b8\nEvery 10000 steps \u03b8\u2212 \u2190 \u03b8 s\u2190 s\u2032\nuntil s\u2032 is terminal until training complete Algorithm 1: Deep Recurrent Q-Networks with forward view eligibility traces on Atari. The eligibility traces are calculated using the n-step return function R(n)t for time-step t was described in section 2.1.\n4 EXPERIMENTAL RESULTS\nWe describe experiments in two Atari games: Pong and Tennis. We chose Pong because it permits quick experimentation, and Tennis because it is one of the games that has proven difficult in all published results on Atari.\n4.1 PONG\nFirst, we tested an RNN model both with \u03bb = 0 and \u03bb = 0.8, trained with RMSprop. Figure 2 shows that the model without a trace (\u03bb = 0) learned at the same rate as DQN, while the model with traces (\u03bb = 0.8) learned substantially faster and with more stability, without exhibiting any epochs with depressed performance. This is probably due to the eligibility traces propagating rewards back by many steps in a single update. In Pong, when the agent hits the ball, it must wait several time-steps before the ball gets either to or past the opponent. Once this happens, the agent must assign the credit of the event back to the time when it hit the ball, and not to the actions performed after the ball had already left. The traces clearly help send this signal back faster.\nWe then tested the same models but using Adam as the optimizer instead of RMSprop. All models learn much faster with this setting. However, the model with no trace gains significantly more than the model with the trace. Our current intuition is that some hyper-parameters, such as the frozen network\u2019s update frequency, are limiting the rate at which the model can learn. Note also that the DQN model also learns faster with Adam as the optimizer, but remains quite unstable, in comparison with the recurrent net models.\nFinally, the results in Table 1 show that both using eligibility traces and Adam provide performance improvements. While training with RMSProp, the model with traces gets to near optimal performance more than twice as quickly as the other models. With Adam, the model learns to be optimal in just 6 epochs.\n4.2 TENNIS\nThe second Atari 2600 game we tested was Tennis. A match consists of only one set, which is won by the player who is the first to win 6 \u201dgames\u201d (as in regular tennis). The score ranges from 24 to -24, given as the difference between the number of balls won by the two players.\nAs in Pong, we first tried an RNN trained with RMSprop and the standard learning rate of 0.00025, both with and without eligibility traces (using again \u03bb = 0.8 and \u03bb = 0). Figure 3 shows that both RNN models learned to get optimal scores after about 50 epochs. This is in contrast with DQN, which never seems to be able to pass the 0 threshold, with large fluctuations ranging from -24 to 0. After visually inspecting the games played in the testing phase, we noticed that the DQN agent gets stuck in a loop, where it exchanges the ball with the opponent until the timer runs out. In such a case, the agent minimizes the number of points scored against, but never learns to beat the opponent. The score fluctuations depend on how few points the agent allows before entering the loop. We suspect that the agent gets stuck in this policy because the reward for trying to defeat the opponent is delayed, waiting for the ball to reach the opponent and get past it. Furthermore, the experiences of getting a point are relatively sparse. Together, it makes it difficult to propagate the reward back to the action of hitting the ball correctly.\nWe also notice that both the RNN with and without eligibility traces manage to learn a near-optimal policy without getting stuck in the bad policy. The RNN has the capacity of sending the signal back to past states with BPTT, allowing it to do credit assignment implicitly, which might explain their ability to escape the bad policy. Remarkably, this is the only algorithm that succeeds in getting near-optimal scores on Tennis, out of all variants of DQN (Mnih et al. (2015), Munos et al. (2016), Wang et al. (2015), Mnih et al. (2016), Schaul et al. (2015)), which tend to get stuck in the policy of delaying. The model without traces learned at a faster pace than the one with traces, arriving to a score of 20 in 45 epochs as opposed to 62 for its counterpart. It\u2019s possible that the updates for model with traces were smaller, due to the weighting of target values, indirectly leading to a lower learning rate. We also trained the models with RMSprop and a higher learning rate of 0.001. This led to the model with traces getting to 20 points in just 27 epochs, while the model without traces lost its ability to get optimal scores and never passed the 0 threshold.\nWe then tried using Adam as the optimizer, with the original learning rate of 0.00025. Both RNN models learned substantially faster than with RMSprop, with the RNN with traces getting to nearoptimal performance in just 13 epochs. With Adam, the gradient for the positive TD is stored in the momentum part of the equation for quite some time. Once in momentum, the gradient is part of many updates, which makes it enough to overtake the safe strategy. We also notice that the model with traces was much more stable than its counterpart. The model without traces fell back to the policy of delaying the game on two occasions, after having learned to beat the opponent. Finally, we trained DQN with Adam, but the model acted the same way as DQN trained with RMSprop.\n5 DISCUSSION AND CONCLUSION\nIn this paper, we analyzed the effects of using eligibility traces and different optimization functions. We showed that eligibility traces can improve and stabilize learning and using Adam can strongly accelerate learning.\nAs shown in the Pong results, the model using eligibility traces didn\u2019t gain much performance from using Adam. One possible cause is the frozen network. While it has a stabilizing effect in DQN, by stopping policies from drastically changing from a single update, it also stops newly learned values from being propagated back. Double DQN seems to partially go around this issue, allowing\nthe policy of the next state to change, while keeping the values frozen. In future experiments, we must consider eliminating or increasing the frozen network\u2019s update frequency. It would also be interesting to reduce the size of experience replay, as with increased learning speed, old observations can become too off-policy and barely be used in eligibility traces.\n", "cats": {"READ": 0.0, "DONTREAD": 1.0}, "reviews": [{"IS_META_REVIEW": true, "comments": "This paper investigates the use of eligibility traces with recurrent DQN agents. As in other recent work on deep RL, the forward view of Sutton and Barto is used to make eligibility traces practical to use with neural networks. Experiments on the Atari games Pong and Tennis show that traces work better than standard Q-learning.\n\nThe paper is well written and the use of traces in deep RL is indeed underexplored, but the experiments in the paper are too limited and do not answer the most interesting questions.\n\nAs pointed out in the questions, n-step returns have been shown to work better than 1-step returns both in the classical RL literature and more recently with deep networks. [1] shows that using n-step returns in the forward view with neural networks leads to big improvements on both Atari and TORCS. Their n-step Q-learning method also combines returns of different length in expectation, while traces do this explicitly. This paper does not compare traces with n-step returns and simply shows that traces used in the forward view help on two Atari games. This is not a very significant result. It would be much more interesting to see whether traces improve on what is already known to work well with neural networks.\n\nThe other claimed contribution of the paper is showing the strong effect of optimization. As with traces, I find it hard to make any conclusions from experiments on two games with fixed hyperparameter settings. This has already been demonstrated with much more thorough experiments in other papers. One could argue that these experiments show that importance of hyperparameter values and not of the optimization algorithm itself. Without tuning the optimization hyperparameters it's hard to claim anything about the relative merits of the methods.\n\n[1] \"Asynchronous Methods for Deep Reinforcement Learning\", ICML 2016."}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "The reviewers agree that the paper is clear and well-written, but all reviewers raised significant concerns about the novelty of the work, since the proposed algorithm is a combination of well-known techniques in reinforcement learning. It is worth noting that the use of eligibility traces is not very heavily explored in the deep reinforcement learning literature, but since the contribution is primarily empirical rather than conceptual and algorithmic, there is a high bar for the rigorousness of the experiments. The reviewers generally did not find the evaluation to be compelling enough in this regard. Based on this evaluation, the paper is not ready for publication.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"TITLE": "review", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer4", "comments": "This paper combines DRQN with eligibility traces, and also experiment with the Adam optimizer for optimizing the q-network. This direction is worth exploring, and the experiments demonstrate the benefit from using eligibility traces and Adam on two Atari games. The methods themselves are not novel. Thus, the primary contributions are (1) applying eligibility traces and Adam to DRQN and (2) the experimental evaluation. The paper is well-written and easy to understand.\n\nThe experiments provide quantitative results and detailed qualitative intuition for how and why the methods perform as they do. However, with only two Atari games in the results, it is difficult to tell how well it the method would perform more generally. Showing results on several more games and/or other domains would significantly improve the paper. Showing error bars from multiple random seeds would also improve the paper.", "IS_META_REVIEW": false, "RECOMMENDATION": 4, "DATE": "17 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "Review", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer5", "comments": "The paper presents a deep RL with eligibility traces. The authors combine DRQN with eligibility traces for improved training. The new algorithm is evaluated on a two problems, with a single set of hyper-parameters, and compared with DQN.\n\nThe topic is very interesting. Adding eligibility traces to RL updates is not novel, but this family of the algorithms have not been explored for deep RL. The paper is written clearly, and the related literature is well-covered. More experiments would make this promising paper much stronger. As this is an investigative, experimental paper, it is crucial for it to contain a wider range of problems, different hyper-parameter settings, and comparison with vanilla DRQN, Deepmind's DQN implementation, as well as other state of the art methods. ", "IS_META_REVIEW": false, "RECOMMENDATION": 4, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 5}, {"TITLE": "Interesting questions but very limited experiments", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "This paper investigates the use of eligibility traces with recurrent DQN agents. As in other recent work on deep RL, the forward view of Sutton and Barto is used to make eligibility traces practical to use with neural networks. Experiments on the Atari games Pong and Tennis show that traces work better than standard Q-learning.\n\nThe paper is well written and the use of traces in deep RL is indeed underexplored, but the experiments in the paper are too limited and do not answer the most interesting questions.\n\nAs pointed out in the questions, n-step returns have been shown to work better than 1-step returns both in the classical RL literature and more recently with deep networks. [1] shows that using n-step returns in the forward view with neural networks leads to big improvements on both Atari and TORCS. Their n-step Q-learning method also combines returns of different length in expectation, while traces do this explicitly. This paper does not compare traces with n-step returns and simply shows that traces used in the forward view help on two Atari games. This is not a very significant result. It would be much more interesting to see whether traces improve on what is already known to work well with neural networks.\n\nThe other claimed contribution of the paper is showing the strong effect of optimization. As with traces, I find it hard to make any conclusions from experiments on two games with fixed hyperparameter settings. This has already been demonstrated with much more thorough experiments in other papers. One could argue that these experiments show that importance of hyperparameter values and not of the optimization algorithm itself. Without tuning the optimization hyperparameters it's hard to claim anything about the relative merits of the methods.\n\n[1] \"Asynchronous Methods for Deep Reinforcement Learning\", ICML 2016.", "IS_META_REVIEW": false, "RECOMMENDATION": 3, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 5}, {"DATE": "03 Dec 2016", "TITLE": "Some questions", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer4"}, {"DATE": "03 Dec 2016", "TITLE": "Comparison with other methods", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer5"}, {"DATE": "02 Dec 2016", "TITLE": "Questions", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3"}, {"IS_META_REVIEW": true, "comments": "This paper investigates the use of eligibility traces with recurrent DQN agents. As in other recent work on deep RL, the forward view of Sutton and Barto is used to make eligibility traces practical to use with neural networks. Experiments on the Atari games Pong and Tennis show that traces work better than standard Q-learning.\n\nThe paper is well written and the use of traces in deep RL is indeed underexplored, but the experiments in the paper are too limited and do not answer the most interesting questions.\n\nAs pointed out in the questions, n-step returns have been shown to work better than 1-step returns both in the classical RL literature and more recently with deep networks. [1] shows that using n-step returns in the forward view with neural networks leads to big improvements on both Atari and TORCS. Their n-step Q-learning method also combines returns of different length in expectation, while traces do this explicitly. This paper does not compare traces with n-step returns and simply shows that traces used in the forward view help on two Atari games. This is not a very significant result. It would be much more interesting to see whether traces improve on what is already known to work well with neural networks.\n\nThe other claimed contribution of the paper is showing the strong effect of optimization. As with traces, I find it hard to make any conclusions from experiments on two games with fixed hyperparameter settings. This has already been demonstrated with much more thorough experiments in other papers. One could argue that these experiments show that importance of hyperparameter values and not of the optimization algorithm itself. Without tuning the optimization hyperparameters it's hard to claim anything about the relative merits of the methods.\n\n[1] \"Asynchronous Methods for Deep Reinforcement Learning\", ICML 2016."}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "The reviewers agree that the paper is clear and well-written, but all reviewers raised significant concerns about the novelty of the work, since the proposed algorithm is a combination of well-known techniques in reinforcement learning. It is worth noting that the use of eligibility traces is not very heavily explored in the deep reinforcement learning literature, but since the contribution is primarily empirical rather than conceptual and algorithmic, there is a high bar for the rigorousness of the experiments. The reviewers generally did not find the evaluation to be compelling enough in this regard. Based on this evaluation, the paper is not ready for publication.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"TITLE": "review", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer4", "comments": "This paper combines DRQN with eligibility traces, and also experiment with the Adam optimizer for optimizing the q-network. This direction is worth exploring, and the experiments demonstrate the benefit from using eligibility traces and Adam on two Atari games. The methods themselves are not novel. Thus, the primary contributions are (1) applying eligibility traces and Adam to DRQN and (2) the experimental evaluation. The paper is well-written and easy to understand.\n\nThe experiments provide quantitative results and detailed qualitative intuition for how and why the methods perform as they do. However, with only two Atari games in the results, it is difficult to tell how well it the method would perform more generally. Showing results on several more games and/or other domains would significantly improve the paper. Showing error bars from multiple random seeds would also improve the paper.", "IS_META_REVIEW": false, "RECOMMENDATION": 4, "DATE": "17 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "Review", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer5", "comments": "The paper presents a deep RL with eligibility traces. The authors combine DRQN with eligibility traces for improved training. The new algorithm is evaluated on a two problems, with a single set of hyper-parameters, and compared with DQN.\n\nThe topic is very interesting. Adding eligibility traces to RL updates is not novel, but this family of the algorithms have not been explored for deep RL. The paper is written clearly, and the related literature is well-covered. More experiments would make this promising paper much stronger. As this is an investigative, experimental paper, it is crucial for it to contain a wider range of problems, different hyper-parameter settings, and comparison with vanilla DRQN, Deepmind's DQN implementation, as well as other state of the art methods. ", "IS_META_REVIEW": false, "RECOMMENDATION": 4, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 5}, {"TITLE": "Interesting questions but very limited experiments", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "This paper investigates the use of eligibility traces with recurrent DQN agents. As in other recent work on deep RL, the forward view of Sutton and Barto is used to make eligibility traces practical to use with neural networks. Experiments on the Atari games Pong and Tennis show that traces work better than standard Q-learning.\n\nThe paper is well written and the use of traces in deep RL is indeed underexplored, but the experiments in the paper are too limited and do not answer the most interesting questions.\n\nAs pointed out in the questions, n-step returns have been shown to work better than 1-step returns both in the classical RL literature and more recently with deep networks. [1] shows that using n-step returns in the forward view with neural networks leads to big improvements on both Atari and TORCS. Their n-step Q-learning method also combines returns of different length in expectation, while traces do this explicitly. This paper does not compare traces with n-step returns and simply shows that traces used in the forward view help on two Atari games. This is not a very significant result. It would be much more interesting to see whether traces improve on what is already known to work well with neural networks.\n\nThe other claimed contribution of the paper is showing the strong effect of optimization. As with traces, I find it hard to make any conclusions from experiments on two games with fixed hyperparameter settings. This has already been demonstrated with much more thorough experiments in other papers. One could argue that these experiments show that importance of hyperparameter values and not of the optimization algorithm itself. Without tuning the optimization hyperparameters it's hard to claim anything about the relative merits of the methods.\n\n[1] \"Asynchronous Methods for Deep Reinforcement Learning\", ICML 2016.", "IS_META_REVIEW": false, "RECOMMENDATION": 3, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 5}, {"DATE": "03 Dec 2016", "TITLE": "Some questions", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer4"}, {"DATE": "03 Dec 2016", "TITLE": "Comparison with other methods", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer5"}, {"DATE": "02 Dec 2016", "TITLE": "Questions", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3"}]}
{"text": "1 INTRODUCTION\nA daunting challenge in many contexts is to gather annotated data. This can be a long and tedious process, which often slows down the development of a framework and may jeopardize its economic prospects.\nWe refer to active learning Cohn (1994) as the field of machine learning which targets building iteratively the annotated training set with the help of an oracle.\nIn this setting and in a context of pool-based active learning1, a model is trained on a small amount of data (i.e. the initial training set) and a scoring function discriminates samples which should be labeled by the oracle from the ones which do not hold new information for the model. The queried samples are then submitted to an oracle (which can be another decision algorithm for instance in co-training context, or a human expert in interactive learning context) to be labeled. They are then added to the current training set. Finally the model is retrained from scratch. This process is repeated recursively to grow the training set.\nAlthough active learning and deep learning represent two important pillars of machine learning, they have mainly coexisted into independent stream of works owing to the complexity of combining them. The main issues are the scalability and the adaptability of common active learning schemes when considering architectures with a huge number of parameters such as deep networks. Another issue lies in the overall number of training iterations since training a deep architecture remains a computationally expensive process despite all the optimizations through GPU processing. This specificity has prevented deep learning from being prevalent within active learning. Indeed seminal active learning frameworks Cohn (1994) have mainly focused on adding one sample at-a-time. When it comes to selecting a batch of queries, the most intuitive solution is to select top scoring samples.\n1Other settings exist but are not considered here for the sake of clarity, we refer the reader to Settles (2012)\nSuch a solution is immediate in the process but fails to model the correlations between samples. Labeling one sample at-a-time may therefore lead to the labeling of another sample totally useless.\nIn our work, batches of actively selected samples are added at each training iteration. We propose a batch active learning framework designed for deep architectures, especially Deep Convolutional Neural Networks (CNN).\nBatch active learning is highly suitable for deep networks which are trained on minibatches of data at each iterations. Indeed training with minibatches help the training of deep networks, and we empirically noticed that the size of the minibatch is a major hyperparameter. Thus it makes sense to query a batch of unlabelled data whose size would be proportionnal to the size of a minibatch. In our work, batches have the same size as the minibatches but they could be decorrelated by considering importance sampling techniques.\nOur model focuses on log loss which is involved in the training process of most neural networks. To achieve the required scalability of active learning for deep architectures, we step away from traditional active learning methods and focus our attention on a more general setting: Maximum Likelihood Estimation (MLE) and Bayesian inference. Provided certain assumptions, our active selection relies on a criterion which is based on Fisher information and is obtained from the minimization of a stochastic method for variational inference. Our active selection relies on the Fisher matrices on the unlabeled data and on the data selected by the active learning step. An approximation of Fisher information, based on a diagonal Kronecker-block decomposition makes our criterion computationally affordable for an active greedy selection scheme.\nVariational methods have been previously explored like in Graves (2011) as a tractable approximation to Bayesian inference for Artificial Neural Networks (ANNs). One advantage of such a representation is that it leads to a two-term active learning criterion: one related to the prediction accuracy on the observed data and the second term expressing the model complexity. Such a two-fold criterion is, to the best of our knowledge, the first of the kind scalable for active learning. Such an expression may help both to analyze and to optimize ANNs, not only in an active learning framework but also for curriculum learning.\nWe dedicate section Related works to the presentation of active learning literature. Section Covering presents the theoretical aspects of our active learning framework, while section Active learning as a greedy selection scheme details our greedy algorithm. We then assess our active learning method through experiments on MNIST and USPS benchmarks. We discuss possible improvements of our approach and connections with previous MLE-based active learning methods before concluding.\n2 RELATED WORKS\nActive learning is a framework to automatize the selection of instances to be labeled in a learning process. Active learning offers a variety of strategies where the learner actively selects which samples seem \u201coptimal\u201d to annotate, so as to reduce the size of the labeled training set required to achieve equivalent performance.\nWe consider the context of pool-based active learning where the learner selects its queries among a given unlabeled data set. For other variants (query synthesis, (stream-based) selective sampling) we refer the reader to Settles (2012).\n2.1 POOL-BASED ACTIVE LEARNING\nWhen it comes to pool-based active learning, the most intuitive approaches focus on minimizing some error on the target classifier. Uncertainty sampling minimizes the training error by querying unlabeled data on which the current classifier (i.e. from previous training iteration) is assigning labels with the weakest confidence. This method, uncertainty sampling, while being the least computational consuming among all active learning techniques has the main drawback of ignoring much of the output distribution classes, and is prone to querying outliers. Thanks to its low cost and easy setup, uncertainty has been adapted to deep architectures for sentiment classification Zhou et al. (2010). However, deep architectures are subject to adversarial examples, a type of noise we suspect uncertainty selection to be highly sensitive to Szegedy et al. (2014); Goodfellow et al. (2015). Other strategies (expected error reduction, expected output variance reduction) directly minimize the\nerror on a validation set after querying a new unlabeled sample. However they are computationally expensive especially when considering neural networks.\nTraditional active learning techniques handle selection of one sample at-a-time. One of the main drawbacks of the aforementioned active learning techniques is that is does not pay attention to the information held in unlabeled data besides considering it as potential queries. Hence once the strategy for selecting samples to be labeled and added to the training set is defined, the question on the impact of the possible correlation between successive selected samples remains.\nTo that end, one recent class of methods deals with the selection of a batch of samples during the active process, batch mode active learning. Batch mode active learning selects a set of most informative unlabeled sample instead of a unique sample. Such a strategy is highly suitable when retraining the model is not immediate and require to restart the training from scratch at each iteration as it is the case for neural networks. A simple strategy (whose has also been used for previous deep active learning strategy Zhou et al. (2010)) is to select a batch of top scoring instances. However that strategy fails to consider the correlation among pairs of samples. The redundancy between the so-selected samples may therefore hinder the learning process.\nIn the context of a deep learning scenario, if several elements related to the same direction of the gradient are in the same minibatch, the gradient descent in the next learning step may lead at once too close to a local minimum, diverting the process away from the global minimum.\nWhile one sample at-a-time can prevent from being misled that way, it gets prohibitive when considering big data because the number of iterations is equal to the number of learning samples, unlike the batch-based strategies.\nRecently some solutions have been proposed for choosing an appropriate subset of samples so as to minimize any significant loss in performance. Those methods consider the minimization of the Kullback-Leibler (KL) divergence between the resampled distribution of any possible subset selected for the active process and the whole unlabeled data set distribution. A lower bound of the negative of this KL divergence is then defined and rewritten as a submodular function. The minimization of the initial KL divergence becomes then a problem of submodular maximization Hoi et al. (2006). In Wei et al. (2015), Wei et al have designed several submodular functions so as to answer at best the need of specific classifiers (Naive Bayes Classifiers, Logistic Regression Classifier, Nearest Neighbor Classifier). However, their approach is hardly scalable to handle all the information from non-shallow classifiers such as deep networks.\nAnother solution to minimize the correlation among a set of queries is to perform bayesian inference on the weights of a neural network. In a bayesian context, a neural network is considered as a parametric model which assigns a conditional probability on the observed labeled data A given a set of weights w. The weights follow some prior distribution P (\u03b1) depending on the parameter \u03b1 and the posterior distribution of the weights P (w | A, \u03b1) is deduced. The goal is thus to maximize the posterior probability of the weights on the observed data A. Indeed bayesian inference expresses the uncertainty of the weights which consequently leads to a relevant exploration of the underlying distribution of the input data X. When it comes to active learning, the learner needs not only to estimate the posterior given the observed data A but also to consider the impact of new data on that posterior Golovin et al. (2010). In our context, bayesian inference is intractable, partially due to the high number of weights involved in a deep network. To solve this issue, Graves (2011) introduced a variational approximation to perform bayesian inference on neural networks. Specifically he approximates the posterior distribution P (w | A, \u03b1) with a tractable distribution of his choice Q(w | \u03b2) depending on a new parameter \u03b2. The quality of the approximation Q(w | \u03b2) compared to the true posterior P (w | A, \u03b1) is measured by the variational free energy F with respect to the parameters \u03b1 and \u03b2. F has no upper bound but gets closer to zero as both distributions become more and more similar.\nF(\u03b1, \u03b2) = \u2212Ew\u223cQ(\u03b2)\n( ln ( P (A | w)P (w | \u03b1)\nQ(w | \u03b2)\n)) (1)\nFinally in Graves (2011), F is then expressed as a minimum description loss function on \u03b1 and \u03b2: F(\u03b1, \u03b2) = Ew\u223cQ(\u03b2)(L(A;w)) +KL(Q(w) || P(\u03b1)) (2)\nwhere KL is the Kullback Leibler divergence between Q(\u03b2) and P(\u03b1).\nUnder certain assumptions on the family distribution for the posterior and prior of the weights ( diagonal gaussian ...), Graves proposed a backpropagation compatible algorithm to train an ensemble of networks, whose weights are sampled from a shared probability distribution.\nThe primary purpose of the variational free energy is to propose a new training objectif for neural network by learning \u03b1 and \u03b2. In an active learning context, the main drawback of variational free energy based method is that it requires to update by backpropagation the parameters for each unlabeled data submitted as a query. However we know from statistical assumption on the maximum likelihood the posterior and prior distribution of trained weights given the current labeled training set: if and only if we consider trained networks, we know how to build \u03b1 and \u03b2 in a unique iteration without backpropagation. This knowledge helps us to extend Graves first objectives to the use of variational free energy to measure how new observations affect the posterior.\n3 COVERING\n3.1 VARIATIONAL INFERENCE ON NEURAL NETWORK WITH GAUSSIAN POSTERIOR WEIGHT DISTRIBUTION\nAs done for the majority of neural networks, we measure the error of the weights w by the negative log likelihood on an observed set of annotated data A:\nL(A;w) = \u2212 \u2211\n(x,y)\u2208A\nln(P (y | x,w)) (3)\nWe consider the Maximum Likelihood Estimator (MLE) W as the value which makes the data observed A as likely as possible for a fixed architecture. Note than even for a fixed A in the case of neural network,W may not be unique.\nW = argminwL(A;w) (4)\nWhen assuming that an arbitrary parameterW\u2217 is governing the data generation process, we know that the expected negative log likelihood is lower bounded by the expected negative log likelihood of the true parameterW\u2217 governing the data generation process. What it means is that no distribution describes the data as well as the true distribution that generated it. It turns out that, under certain assumptions, we can prove using the central limit theorem that the MLE is asymptotically normal with a mean equal to the true parameter value and variance equal to the inverse of the expected Fisher information evaluated at the true parameter.\nIf we denote by X the underlying data distribution,WX the MLE andW\u2217X the true parameter, we know thatWX is a sample from a multivariate gaussian distribution parametrized byW\u2217X . Note that in this context we assume the Fisher matrices are invertible.\nWX \u223c N (W\u2217X , I\u22121X (W \u2217 X)) (5)\nHowever the expected Fisher information on the underlying distribution is intractable. Eventually, using the law of large numbers, we know that the observed Fisher information converges to the expected Fisher information as the sample size increases. Another theoretical limitation is that the true parameter is unknown but we can approximate its observed Fisher information with the observed Fisher information at the MLE because of the consistency of the MLE. For a sake of simplicity we keep the same notation for observed and expected Fisher matrix.\nLet denote by Y the random variable after resampling the underlying distribution X using an active learning strategy. W\u2217X,W\u2217Y are the true parameters with respect to their respective data distributions and their respective MLE variablesWX,WY, then the following relations hold:\nWX \u223c N (W\u2217X, I\u22121X (W \u2217 X)) WY \u223c N (W\u2217Y, I\u22121Y (W \u2217 Y))\n(6)\nWe thus notice than in an active learning context, the learner is trained on data uniformly sampled from Y, while the optimal solution would be when training on data uniformly sampled from X.\nThe asymptotic behaviour provides us with a prior distribution of the weights based on the data distribution X. In our context of active learning, we approximate the posterior distribution with the MLE distribution induced by the resampling Y. Hence we define a prior and posterior distribution which did not require to be learnt by backpropagation directly but depend on the two data distribution X and Y.\nP (\u03b1) \u2261 P (\u03b1X) = N (W\u2217X, I\u22121X (W \u2217 X)) Q(\u03b2) \u2261 Q(\u03b2Y) = N (W\u2217Y, I\u22121Y (W \u2217 Y))\n(7)\nOur active learning scheme relies on the selection of input data whose induced MLE distribution Q(\u03b2Y) is minimizing the variational free energy.\nY = argminY F(\u03b1X, \u03b2Y) (8)\nIt consists in the minimization of the sum of two terms which we denote respectively by the training factor EW\u223cQ(\u03b2)(L(A;W)) and the generalization factor KL(Q(\u03b2) || P(\u03b1)). It is possible to analyze both terms independently and explain their role into the minimization:\n\u2022 Training factor: Ideally, the Cramer Rao bound implies that the minimum on the training factor is reached whenQ(\u03b2) matches the asymptotically most efficient estimator of the optimal parameter on the error loss on the observed data. Hence the training factor corresponds to the minimization of the error on the observed data A.\n\u2022 Generalization factor: Empirical results Choromanska et al. (2015) tend to show that the variance of the accuracy diminishes as the depth of the classifier increases. So our ultimate goal would be to converge to any set of parameters of the MLE distribution as their effectiveness is similar. The goal of the generalization factor is to converge to the asymptotic distribution on the whole input distribution X and to minimize the error of prediction of new input data.\nIf the expression of the variational free energy provides us a theoretical context to work on, the usage of Fisher matrices of deep networks renders it computationally unaffordable. Indeed the Fisher matrix is a quadratic matrix in terms of the number of parameters of the deep networks. Because of the huge number of parameters involved, such matrices takes a lot of memory and processing them costs a lot of ressources, especially if the operations may be repeated often in the framework (as it would be the case for every possible query processed by an active learning scheme.)\nThe next section 3.2 explains the different approximation proposed to deduce a more friendly user criterion.\n3.2 APPROXIMATIONS\n3.2.1 KRONECKER FACTORED APPROXIMATION OF THE FISHER INFORMATION\nOF A CNN\nRecently an approximation of the Fisher information for deep architectures has been proposed first for fully connected layer in Martens & Grosse (2015), and then for convolutional layer as well in Grosse & Martens (2016). The block kronecker decomposition content (\u03c8, \u03c4 ) is explained in Martens & Grosse (2015); Grosse & Martens (2016)\nBased on their decomposition definition, we define the evaluation of blocks of the Fisher information at a certain point xi(\u03c8xi,l, \u03c4xi,l) and an empirical estimation of the Fisher matrix on a set of data A. A sum up of their decomposition is presented in Eq. (9) while the exact content of the kronecker\nblocks \u03c8 and \u03c4 is left as undescribed in this paper for the sake of concision.\nIA(W) = diag([\u03c8A,l(W)\u2297 \u03c4A,l(W)]Ll=1)\n\u03c8A,l(W) = 1 | A | \u2211 xi\u2208A \u03c8xi,l(W)\n\u03c4A,l(W) = 1 | A | \u2211 xi\u2208A \u03c4xi,l(W)\n(9)\nThe strength of this decomposition lies in the properties of block diagonal combined with those of the kronecker product. \u03c8 and \u03c4 are respectively related to the covariance matrix of the activation and the covariance of the derivative given the input of a layer. Recent deep architectures tend to prevail the depth over the width (i.e. the number of input and output neurons) so this expression becomes really suitable and tractable.\n3.3 APPROXIMATION OF THE TRAINING FACTOR\nDespite the block kronecker product approximation of the Fisher matrix, sampling on Q(\u03b2) requires to compute the inverse. Because the kronecker blocks may still have an important number of parameters involved (especially the first fully connected layer suceeding to a convolutional layer), the inverse of the blocks may be still too computationally expensive. To approximate the training factor, we opt for a second order approximation of the log likelihood for parametersW close to the mean parameterW\u2217Y of Q(\u03b2).\nL(A;W) \u2248 L(A;W\u2217Y) + \u2202L(A;W\u2217Y)\n\u2202W + (W \u2212W\u2217Y)T \u22022L(A;W\u2217Y) \u2202W \u2032\u2202W (W \u2212W\u2217Y) (10)\nOur first approximation consists in assuming that the MLE parameter w\u0302 of the currently trained network is a good approximator ofW\u2217Y. Because the network has converged on the current set of observed data A the first derivative of the log likelihood is also set to zero. Hence Eq. (10) thus becomes:\nL(A;W) \u2248 L(A; w\u0302) + (W \u2212 w\u0302)T I\u22121A (w\u0302)(W \u2212 w\u0302) (11)\nTo compute the expectation over the range of weights sampled fromQ(\u03b2) we need to upperbound the expectation of the dot product ofW given the Fisher matrix. Because we assume our Fisher matrices invertible, and because a covariance matrix is at least semi-definite, our Fisher matrices are positive definite matrix. Hence every eigenvalue is positive and the trace of the Fisher matrix is greater than its maximum eigenvalue. From basic properties of the variance covariance matrix, if we denote by N the number of parameters in the network we obtain the following upperbound for the training factor:\nEW\u223cQ(\u03b2)(L(A;W)) \u2264 L(A; w\u0302) + N\u221a \u03c0 Tr(I\u22121Y (w\u0302) T I\u22121A (w\u0302)I \u22121 Y (w\u0302)) (12)\nWhen it comes to the trace of the inverse, we approximate it by the closest lower bound with the inverse of the trace like in Wei et al. (2015).\nEW\u223cQ(\u03b2)(L(A;W)) \u221d\u223c L(A; w\u0302) + N\u221a \u03c0\nN2\nTr(IY(w\u0302)IA(w\u0302)IY(w\u0302)T ) (13)\n3.4 APPROXIMATION OF THE GENERALIZATION FACTOR\nOur generalization factor corresponds to the KL divergence between the approximation of our posterior Q(\u03b2) and the prior P(\u03b1). Because both distributions are multivariate gaussians, we have a direct formulation of the KL which is always definite since the Fisher matrices are invertible:\nKL(Q(\u03b2) || P(\u03b1)) = 1 2\n( ln ( det(I\u22121X (W\u2217X)) det(I\u22121Y (W\u2217Y)) ) \u2212N+Tr(IX(W\u2217X)I\u22121Y (W \u2217 Y))+(W\u2217X\u2212W\u2217Y)T IX(W\u2217X)(W\u2217X\u2212W\u2217Y) ) (14)\nOur first approximation consists in assuming that the MLE parameter w\u0302 of the currently trained network is a good approximator of both optimal parametersW\u2217X,W\u2217Y like in Zhang & Oles (2000). We also upper bound the determinant with a function of the trace and the number N of parameters. When it comes to the trace of the inverse, we approximate it again by the closest lower bound with the inverse of the trace.\nKL(Q(\u03b2) || P(\u03b1)) \u221d\u223c N\n( ln ( Tr(IY(w\u0302)I\u22121X (w\u0302)) ) +\nN\nTr(IY(w\u0302)I\u22121X (w\u0302))\n) (15)\n3.5 APPROXIMATION OF THE VARIATIONAL FREE ENERGY\nIn the previous subsections, we proposed independent approximations of both our sub-criteria: the training factor and the generalization factor. However the scale of our approximations may not be balanced so we sum up our criterion with an hyperparameter factor \u03b3 which counterparts the difference of scale between the factors:\nF \u221d\u223c \u03b3 ( N\u221a \u03c0\nN2\nTr(IY(w\u0302)IA(w\u0302)IY(w\u0302)T )\n) +N ( ln ( Tr ( IY(w\u0302)I\u22121X (w\u0302) )) +\nN\nTr(IY(w\u0302)I\u22121X (w\u0302)) ) (16)\nWe approximate the expected Fisher matrices on the underlying distribution Y and X by the observed Fisher matrices on a set of data sampled from those distributions. This approximation is relevant due to the consistenty of the MLE.\nAs we are in a pool-based selection case, we dispose at first of two sets of data: A and U which denote respectively the annotated observed data and unlabeled data. Note that the derivatives in the Fisher matrix computation implies to know the label of the samples. Thus at each active learning step, an unknown label is approximated by its prediction from the current trained network. We denote by S the subset of data to be queried by an oracle. The size of S is fixed with | S |= K. S is the subset sampled from Y while U is sampled from X. Finally an approximation of F will be:\nF \u221d\u223c \u03b3 ( N\u221a \u03c0\nN2\nTr(IS(w\u0302)IA(w\u0302)IS(w\u0302)T )\n) +N ( ln ( Tr ( IS(w\u0302)I\u22121U (w\u0302) )) +\nN\nTr(IS(w\u0302)I\u22121U (w\u0302)) ) (17)\nNow we express the trace based on the approximation of the Fisher matrix: we consider that every Fisher matrix for CNN is a L diagonal block matrix, with L the number of layers of the CNN. Every block is made of a kronecker product of two terms \u03c8 and \u03c4 . We rely on the properties involved by the choice of this specific matrix topology to obtain a more computationally compliant approximation of F in Eq. (18):\nF \u221d\u223c\u03b3 ( N\u221a \u03c0 N2\u2211 l\u2208(1,L) Tr(\u03c8S,l(w\u0302)\u03c8 \u22121 A,l(w\u0302)\u03c8S,l(w\u0302) T )Tr(\u03c4S,l(w\u0302)\u03c4 \u22121 A,l(w\u0302)\u03c4S,l(w\u0302) T ) )\n+N ( ln ( \u2211 l\u2208(1,L) Tr ( \u03c8S,l(w\u0302)\u03c8 \u22121 U,l(w\u0302) ) Tr(\u03c4S,l(w\u0302)\u03c4 \u22121 U,l (w\u0302)) )\n+ N\u2211\nl\u2208(1,L) Tr(\u03c8S,l(w\u0302)\u03c8 \u22121 U,l(w\u0302))Tr(\u03c4S,l(w\u0302)\u03c4 \u22121 U,l (w\u0302))\n) (18)\n4 ACTIVE LEARNING AS A GREEDY SELECTION SCHEME ON THE VARIATIATIONAL FREE ENERGY\nThe selected subset S selected at one step of active learning is only involved through the kronecker product of the Fisher matrix IS(w\u0302). We express our approximation of the free energy by a criterion\non the subset S in Eq. (19): C(S;A,U) =\u03b3 ( N\u221a \u03c0 N2\u2211 l\u2208(1,L) Tr(\u03c8S,l(w\u0302)\u03c8A,l(w\u0302)\u03c8S,l(w\u0302) T )Tr(\u03c4S,l(w\u0302)\u03c4A,l(w\u0302)\u03c4S,l(w\u0302)T ) )\n+N ( ln ( \u2211 l\u2208(1,L) Tr(\u03c8S,l(w\u0302)\u03c8 \u22121 U,l(w\u0302))Tr(\u03c4S,l(w\u0302)\u03c4 \u22121 U,l (w\u0302)) )\n+ N\u2211\nl\u2208(1,L) Tr(\u03c8S,l(w\u0302)\u03c8 \u22121 U,l(w\u0302))Tr(\u03c4S,l(w\u0302)\u03c4 \u22121 U,l (w\u0302)) ) (19)\nFinally we estimate our subset S by a greedy procedure: to be more robust to outliers and for reasons of computational efficiency, we select first a pool of samples D \u2282 U which we will use as the set of possible queries. We recursively build S \u2282 D by picking the next sample xi \u2208 D which minimizes C(S \u222a {xi};A,U) among all remaining samples in D. When it comes to the training factor coefficient, we notice that it is a quadratic term in IS(w\u0302) which increases the complexity in a greedy selection scheme. Our choice is to estimate the trace in the following way:\nTr(\u03c8S\u222a{x},l(w\u0302)\u03c8A,l(w\u0302)\u03c8S\u222a{x},l(w\u0302) T ) \u2248 Tr(\u03c8S,l(w\u0302)\u03c8A,l(w\u0302)\u03c8S,l(w\u0302)T )+Tr(\u03c8{x},l(w\u0302)\u03c8A,l(w\u0302)\u03c8{x},l(w\u0302)T )\nPseudo-code and illustration of the algorithm are provided in table 1 in appendix.\n5 EXPERIMENTS\nWe demonstrate the validity of our approach on two datasets: MNIST (28-by-28 pictures, 50.000 training samples, 10.0000 validation samples and 10.000 test samples) and USPS (16-by-16 pictures, 4185 training samples, 464 validation samples and 4649 testing samples) both gray scaled digits image datasets. We describe the CNN configuration and the hyperparameters settings in table 2 in appendix. Note that we do not optimize the hyperparameters specifically for the size of the current annotated training set A. We picked those two similar datasets to judge of the robustness of our method against different size of unlabeled datasets, as expected our method is efficient on both small and large databases.\n5.1 TEST ERROR\nWe run 10 runs of experiments and average the error on the test set of the best validation error before a pass of active learning. We start from an annotated training set of the size of one minibatch selected randomly. We stop both set of experiments after 30% of the training set has been selected (15.000 image for MNIST, 1255 for USPS). We compare the lowest test error achieved so far by our MLE based method against naive baselines: uncertainty sampling, curriculum sampling and a random selection of a minibatch of examples. We measure both uncertainty and curriculum scores based on the log likelihood of a sample using as label its prediction on the full network. While uncertainty selects samples with the highest log likelihood, our version of curriculum does the exact contrary. We select randomly the set of possible queries D among the unlabeled training data. Its size is set to 30 times the minibatch size. We present the results in two phases for the sake of clarity in figure 1 for MNIST and figure 2 for USPS: the first rounds of active learning when the annotated training set is almost empty, and the second round which is more stable in the evolution of the error. In both phases and for both databases we observe a clear difference between the test error achieved by our MLE method with the test error obtained by selecting randomly the data to be queried. Moreover the error achieved by our method on 30 % is close (even equivalent in the case of USPS), to the error achieved using the standard full training sets defined for both datasets (this error rate is defined as yellow line groundtruth on the figures). The experiments made appear that curriculum learning is not a good active learning strategy for both tested datasets. As for the uncertainty selection, it works really well on MNIST while it fails on USPS. While MNIST is a pretty clean database, USPS contains more outliers and noisy samples rendering it more difficult in terms of accuracy even though both databases are designed to assess digit classification. As other works we mentioned in the related work section, we are led to explain uncertainty selection to select useless samples with the amount of outliers and noisy samples in USPS.\n5.2 TIME COMPLEXITY\nTo validate our method in terms of scalability and time complexity, we measured in seconds, the current processor time for one pass of active learning. We repeated this evaluation for different size of query (8 to 128 unlabeled samples added to the currrent training set). For this experiments we used a laptop with a Titan-X (GTX 980 M) with 8 GB RAM GPU memory. Metrics were reported in figure 3. Our criterions takes few seconds to select a batch of query of hundreds of unlabeled data. Moreover the evolution of the time given the size of the query is less than linear.\nPage 1\n9\n6 DISCUSSION\nWe believe our method is a proof of concept for the use of variational inference for active learning on deep neural networks. However our approximations are subject to improvements which may lead to faster convergence and lower generalization error.\nThe first point to raise is that our approximation of the posterior is an asymptotic distribution which may be unstable on a small subset of observed data, as it is the case for active learning. Such a distribution may be regularized by taking the probability provided by the central limit theorem about how well our data fits to the asymptotic gaussian distribution. When it comes to the KFAC approximation, it suffers from the same issue and could be regularized when evaluating on small subset. A refinement of the approximations, especially for the generalization factor, following the approaches of submodular functions may be investigated.\nFinally, an interesting observation is that our formulation of the variational free energy finds similarities with other MLE based active learning criteria previously proposed in the litterature. Indeed, in Zhang & Oles (2000) the authors study active learning by looking among the possible resampling of the input distribution. They formulate their criterion as the minimization of the trace of the inverse Fisher of the resampled distribution multiplied by the Fisher matrix on the input distribution: minS Tr(I\u22121S (w\u0302)IU (w\u0302))\n7 CONCLUSION\nIn a nutshell, we proposed a scalable batch active learning framework for deep networks relying on a variational approximation to perform bayesian inference. We deduced a formulation of the posterior and prior distributions of the weights using statistical knowledge on the Maximum Likelihood Estimator. Those assumptions combined with an existing approximation of the Fisher information for neural network, lead us to a backpropagation free active criterion. Eventually we used our own approximations to obtain a greedy active selection scheme.\nOur criterion is the first of the kind to scale batch active learning to deep networks, especially Convolutional Neural Networks. On different databases, it achieves better test accuracy than random sampling, and is scalable with increasing size of queries. It achieves near optimal error on the test set using a limited percentage (30%) of the annotated training set on larger and more reduced dataset. Our works demonstrated the validity of batch mode active learning for deep networks and the promise of the KFAC approximations for deep Fisher matrices for the active learning community. Such a solution is also interesting as a new technique for curriculum learning approach.\n", "cats": {"READ": 1.0, "DONTREAD": 0.0}, "reviews": [{"IS_META_REVIEW": true, "comments": "This paper introduces a mechanism for active learning with convolutional neural networks (CNNs). I would not go as far as the authors in calling these \"deep\", seeing that they seem to have only 2 hidden layers with only 20 filters each. The active learning criterion is a greedy selection scheme based on variational free energy and a series of approximations.\n\nThe paper is sometimes hard to read, due to (a) many grammatical errors and (b) sloppy notation in some places (e.g., on page 5, line 1, f is used but never introduced before). Overall, I give an accepting score, but a weak one because of the grammatical errors. If the paper is accepted, these should be fixed for the final version, optimally by a native speaker. \n\nThe paper's topic is interesting, and the paper appears to succeed in its goal of showing a proof of concept for active learning in CNNs (if only on toy datasets). I'm surprised by the new results on uncertainty sampling and curriculum learning the authors added: why do these methods both break for USPS? In particular, uncertainty sampling did very well (in fact, better than the authors' new method) on MNIST, but apparently horribly on USPS; some explanation for this would be useful.\n\nI have one more question: why is it necessary to first sample a larger subset D \\subset U, from which we select using active learning? Is this merely done for reasons of computational efficiency, or can it actually somehow improve results? (If so, it would be instrumental to see the worse results when this is not done.)"}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "The reviewers agree that the paper pursues an interesting direction to explore active example selection for CNN training, but have unanimously raised serious concerns with regards to overall presentation which needs further improvement (I still see spelling/grammatical errors/sloppy notation in the latest draft). Some sections in the paper are hard to follow. With regards to technical motivation, the link between depth and need for active example selection is alluded to, but not properly explained in the paper. The PCs think that this paper has too many areas in need of improvement to be accepted to the conference.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"TITLE": "Very interesting but hard to follow.", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "The paper proposes to perform active learning using pool selection of deep learning mini-batches using an approximation of the bayesian posterior. Several terms are in turn approximated.\n\nThe Maximum Likelihood Estimation (MLE) bayesian inference approach to active learning, the various approximations, and more generally the theoretical framework is very interesting but difficult to follow.\n\nThe paper is written in poor English and is sometimes a bit painful to read.\n\nAlternative Active learning strategies and techniques do not need to be described with such detail. On the other hand, the proposed approach has a lot of complex approximations which would benefit from a more detailed/structured presentation. Another dataset would be a big plus (both datasets concern gray digits and USPS and are arguably somewhat similar).", "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "22 Dec 2016", "REVIEWER_CONFIDENCE": 1}, {"TITLE": "Interesting ideas but not well explained", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer4", "comments": "Quality:\nThe paper initiates a framework to incorporate active learning into the deep learning framework, mainly addressing challenges such as scalability that accompanies the training of a deep neural network. \nHowever, I think the paper is not well polished; there are quite a lot of grammatical and typing errors.\n\nClarity:\nThe paper needs major improvements in terms of clarity. The motivations in the introduction, i.e., why it is difficult to do active learning in deep architectures, could be better explained, and tied to the explanation in Section 3 of the paper. For example, the authors motivated the need of (mini)batch label queries, but never mention it again in Section 3, when they describe their main methodology. \nThe related work section, although appearing systematic and thorough, is a little detached from the main body of the paper (related work section should not be a survey of the literature, but help readers locate your work in the relevant literature, and highlight the pros and cons. In this perspective, maybe the authors could shorten some explanations over the related work that are not directly related, while spending more time on discussing/comparing with works that are most related to your current work, e.g., that of Graves '11.\n\nOriginality & Significance:\nThe authors proposed an active learning training framework. The idea is to treat the network parameter optimization problem as a Bayesian inference problem (which is proposed previously by Graves) and formulate the active learning problem as that of sampling the most informative data, where the informativeness is defined by the variational free energy, which depends on the Fisher information. To reconcile the computational burden of computing the inverse of Fisher Information matrix, the authors proposed techniques to approximate it (which seems to be novel)\n\nI think that this paper initiates an interesting direction: one that adapts deep learning to label-expensive problems, via active learning. But the paper needs to be improved in terms of presentation.\n\n", "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "19 Dec 2016", "REVIEWER_CONFIDENCE": 2}, {"TITLE": "Proof of concept for active learning with CNNs", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "This paper introduces a mechanism for active learning with convolutional neural networks (CNNs). I would not go as far as the authors in calling these \"deep\", seeing that they seem to have only 2 hidden layers with only 20 filters each. The active learning criterion is a greedy selection scheme based on variational free energy and a series of approximations.\n\nThe paper is sometimes hard to read, due to (a) many grammatical errors and (b) sloppy notation in some places (e.g., on page 5, line 1, f is used but never introduced before). Overall, I give an accepting score, but a weak one because of the grammatical errors. If the paper is accepted, these should be fixed for the final version, optimally by a native speaker. \n\nThe paper's topic is interesting, and the paper appears to succeed in its goal of showing a proof of concept for active learning in CNNs (if only on toy datasets). I'm surprised by the new results on uncertainty sampling and curriculum learning the authors added: why do these methods both break for USPS? In particular, uncertainty sampling did very well (in fact, better than the authors' new method) on MNIST, but apparently horribly on USPS; some explanation for this would be useful.\n\nI have one more question: why is it necessary to first sample a larger subset D \\subset U, from which we select using active learning? Is this merely done for reasons of computational efficiency, or can it actually somehow improve results? (If so, it would be instrumental to see the worse results when this is not done.)\n", "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "18 Dec 2016", "REVIEWER_CONFIDENCE": 2}, {"DATE": "03 Dec 2016", "TITLE": "Code available?", "IS_META_REVIEW": false, "comments": "Some of the approximations are quite complex; is your code available?", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3"}, {"DATE": "03 Dec 2016", "TITLE": "Groundtruth?", "IS_META_REVIEW": false, "comments": "In Figures 1 and 2, what do you mean by groundtruth?\n", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3"}, {"DATE": "03 Dec 2016", "TITLE": "Overhead of active learning?", "IS_META_REVIEW": false, "comments": "Section 5.2 studies the time complexity of your approach -- up to 30s to select the elements of one minibatch. How does this compare to the time required for using that minibatch to update the model by backprop?", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3"}, {"DATE": "03 Dec 2016", "TITLE": "Simple active learning baseline?", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3"}, {"DATE": "03 Dec 2016", "TITLE": "Experiments", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2"}, {"DATE": "03 Dec 2016", "TITLE": "A strong assumption?", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer4"}, {"IS_META_REVIEW": true, "comments": "This paper introduces a mechanism for active learning with convolutional neural networks (CNNs). I would not go as far as the authors in calling these \"deep\", seeing that they seem to have only 2 hidden layers with only 20 filters each. The active learning criterion is a greedy selection scheme based on variational free energy and a series of approximations.\n\nThe paper is sometimes hard to read, due to (a) many grammatical errors and (b) sloppy notation in some places (e.g., on page 5, line 1, f is used but never introduced before). Overall, I give an accepting score, but a weak one because of the grammatical errors. If the paper is accepted, these should be fixed for the final version, optimally by a native speaker. \n\nThe paper's topic is interesting, and the paper appears to succeed in its goal of showing a proof of concept for active learning in CNNs (if only on toy datasets). I'm surprised by the new results on uncertainty sampling and curriculum learning the authors added: why do these methods both break for USPS? In particular, uncertainty sampling did very well (in fact, better than the authors' new method) on MNIST, but apparently horribly on USPS; some explanation for this would be useful.\n\nI have one more question: why is it necessary to first sample a larger subset D \\subset U, from which we select using active learning? Is this merely done for reasons of computational efficiency, or can it actually somehow improve results? (If so, it would be instrumental to see the worse results when this is not done.)"}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "The reviewers agree that the paper pursues an interesting direction to explore active example selection for CNN training, but have unanimously raised serious concerns with regards to overall presentation which needs further improvement (I still see spelling/grammatical errors/sloppy notation in the latest draft). Some sections in the paper are hard to follow. With regards to technical motivation, the link between depth and need for active example selection is alluded to, but not properly explained in the paper. The PCs think that this paper has too many areas in need of improvement to be accepted to the conference.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"TITLE": "Very interesting but hard to follow.", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "The paper proposes to perform active learning using pool selection of deep learning mini-batches using an approximation of the bayesian posterior. Several terms are in turn approximated.\n\nThe Maximum Likelihood Estimation (MLE) bayesian inference approach to active learning, the various approximations, and more generally the theoretical framework is very interesting but difficult to follow.\n\nThe paper is written in poor English and is sometimes a bit painful to read.\n\nAlternative Active learning strategies and techniques do not need to be described with such detail. On the other hand, the proposed approach has a lot of complex approximations which would benefit from a more detailed/structured presentation. Another dataset would be a big plus (both datasets concern gray digits and USPS and are arguably somewhat similar).", "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "22 Dec 2016", "REVIEWER_CONFIDENCE": 1}, {"TITLE": "Interesting ideas but not well explained", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer4", "comments": "Quality:\nThe paper initiates a framework to incorporate active learning into the deep learning framework, mainly addressing challenges such as scalability that accompanies the training of a deep neural network. \nHowever, I think the paper is not well polished; there are quite a lot of grammatical and typing errors.\n\nClarity:\nThe paper needs major improvements in terms of clarity. The motivations in the introduction, i.e., why it is difficult to do active learning in deep architectures, could be better explained, and tied to the explanation in Section 3 of the paper. For example, the authors motivated the need of (mini)batch label queries, but never mention it again in Section 3, when they describe their main methodology. \nThe related work section, although appearing systematic and thorough, is a little detached from the main body of the paper (related work section should not be a survey of the literature, but help readers locate your work in the relevant literature, and highlight the pros and cons. In this perspective, maybe the authors could shorten some explanations over the related work that are not directly related, while spending more time on discussing/comparing with works that are most related to your current work, e.g., that of Graves '11.\n\nOriginality & Significance:\nThe authors proposed an active learning training framework. The idea is to treat the network parameter optimization problem as a Bayesian inference problem (which is proposed previously by Graves) and formulate the active learning problem as that of sampling the most informative data, where the informativeness is defined by the variational free energy, which depends on the Fisher information. To reconcile the computational burden of computing the inverse of Fisher Information matrix, the authors proposed techniques to approximate it (which seems to be novel)\n\nI think that this paper initiates an interesting direction: one that adapts deep learning to label-expensive problems, via active learning. But the paper needs to be improved in terms of presentation.\n\n", "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "19 Dec 2016", "REVIEWER_CONFIDENCE": 2}, {"TITLE": "Proof of concept for active learning with CNNs", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "This paper introduces a mechanism for active learning with convolutional neural networks (CNNs). I would not go as far as the authors in calling these \"deep\", seeing that they seem to have only 2 hidden layers with only 20 filters each. The active learning criterion is a greedy selection scheme based on variational free energy and a series of approximations.\n\nThe paper is sometimes hard to read, due to (a) many grammatical errors and (b) sloppy notation in some places (e.g., on page 5, line 1, f is used but never introduced before). Overall, I give an accepting score, but a weak one because of the grammatical errors. If the paper is accepted, these should be fixed for the final version, optimally by a native speaker. \n\nThe paper's topic is interesting, and the paper appears to succeed in its goal of showing a proof of concept for active learning in CNNs (if only on toy datasets). I'm surprised by the new results on uncertainty sampling and curriculum learning the authors added: why do these methods both break for USPS? In particular, uncertainty sampling did very well (in fact, better than the authors' new method) on MNIST, but apparently horribly on USPS; some explanation for this would be useful.\n\nI have one more question: why is it necessary to first sample a larger subset D \\subset U, from which we select using active learning? Is this merely done for reasons of computational efficiency, or can it actually somehow improve results? (If so, it would be instrumental to see the worse results when this is not done.)\n", "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "18 Dec 2016", "REVIEWER_CONFIDENCE": 2}, {"DATE": "03 Dec 2016", "TITLE": "Code available?", "IS_META_REVIEW": false, "comments": "Some of the approximations are quite complex; is your code available?", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3"}, {"DATE": "03 Dec 2016", "TITLE": "Groundtruth?", "IS_META_REVIEW": false, "comments": "In Figures 1 and 2, what do you mean by groundtruth?\n", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3"}, {"DATE": "03 Dec 2016", "TITLE": "Overhead of active learning?", "IS_META_REVIEW": false, "comments": "Section 5.2 studies the time complexity of your approach -- up to 30s to select the elements of one minibatch. How does this compare to the time required for using that minibatch to update the model by backprop?", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3"}, {"DATE": "03 Dec 2016", "TITLE": "Simple active learning baseline?", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3"}, {"DATE": "03 Dec 2016", "TITLE": "Experiments", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2"}, {"DATE": "03 Dec 2016", "TITLE": "A strong assumption?", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer4"}]}
{"text": "1 INTRODUCTION\nPolynomial feature expansion has long been used in statistics to approximate nonlinear functions Gergonne (1974); Smith (1918). The compressed sparse row (CSR) matrix format is a widelyused data structure to hold design matrices for statistics and machine learning applications. However, polynomial expansions are typically not performed directly on sparse CSR matrices, nor on any sparse matrix format for that matter, without intermediate densification steps. This densification not only adds extra overhead, but wastefully computes combinations of features that have a product of zero, which are then discarded during conversion into a sparse format.\nWe provide an algorithm that allows CSR matrices to be the input of a polynomial feature expansion without any densification. The algorithm leverages the CSR format to only compute products of features that result in nonzero values. This exploits the sparsity of the data to achieve an improved time complexity of O(dkDk) on each vector of the matrix where k is the degree of the expansion, D is the dimensionality, and d is the density. The standard algorithm has time complexity O(Dk). Since 0 \u2264 d \u2264 1, our algorithm is a significant improvement. While the algorithm we describe uses CSR matrices, it could be modified to operate on other sparse formats.\n2 PRELIMINARIES\nMatrices are denoted by uppercase bold letters thus: A. The ithe row of A is written ai. All vectors are written in bold, and a, with no subscript, is a vector.\nA compressed sparse row (CSR) matrix representation of an r-row matrix A consists of three vectors: c, d, and p and a single number: the number of columns of A. The vectors c and d contain the same number of elements, and hold the column indices and data values, respectively, of all nonzero elements of A. The vector p has r entries. The values in p index both c and d. The ith entry pi of p tells where the data describing nonzero columns of ai are within the other two vectors: cpi:pi+1 contain the column indices of those entries; dpi:pi+1 contain the entries themselves. Since only nonzero elements of each row are held, the overall number of columns of A must also be stored, since it cannot be derived from the other data.\nScalars, vectors, and matrices are often referenced with the superscript k. This is not to be interpreted as an exponent, but to indicate that it is the analogous aspect of that which procedes it, but in its polynomial expansion form. For example, c2 is the vector that holds columns for nonzero values in A\u2019s quadratic feature expansion CSR representation.\nFor simplicity in the presentation, we work with polynomial expansions of degree 2, but continue to use the exponent k to show how the ideas apply in the general case. \u2217Now at Google \u2020The authors contributed equally important and fundamental aspects of this work.\nWe do provide an algorithm for third degree expansions, and derive the big-O time complexity of the general case.\nWe have also developed an algorithm for second and third degree interaction features (combinations without repetition), which can be found in the implementation.\n3 MOTIVATION\nIn this section, we present a strawman algorithm for computing polynomial feature expansions on dense matrices. We then modify the algorithm slightly to operate on a CSR matrix, in order to expose its infeasibility in that context. We then show how the algorithm would be feasible with an added component, which we then derive in the following section.\n3.1 DENSE EXPANSION ALGORITHM\nA natural way to calculate polynomial features for a matrix A is to walk down its rows and, for each row, take products of all k-combinations of elements. To determine in which column of Aki products of elements in Ai belong, a simple counter can be set to zero for each row of A and incremented efter each polynomial feature is generated. This counter gives the column of Aki into which each expansion feature belongs.\nSECOND ORDER (k = 2) DENSE POLYNOMIAL EXPANSION ALGORITHM(A) 1 N = row count of A 2 D = column count of A 3 Ak = empty N \u00d7 ( D 2 ) matrix 4 for i = 0 to N \u2212 1 5 cp = 0 6 for j1 = 0 to D \u2212 1 7 for j2 = j1 to D \u2212 1 8 Akicp = Aij1 \u00b7Aij2 9 cp = cp + 1\n3.2 IMPERFECT CSR EXPANSION ALGORITHM\nNow consider how this algorithm might be modified to accept a CSR matrix. Instead of walking directly down rows of A, we will walk down sections of c and d partitioned by p, and instead of inserting polynomial features into Ak, we will insert column numbers into ck and data elements into dk.\nINCOMPLETE SECOND ORDER (k = 2) CSR POLYNOMIAL EXPANSION ALGORITHM(A) 1 N = row count of A 2 pk = vector of size N + 1 3 pk0 = 0 4 nnzk = 0 5 for i = 0 to N \u2212 1 6 istart = pi 7 istop = pi+1 8 ci = cistart:istop 9 nnzki = (|ci| 2\n) 10 nnzk = nnzk + nnzki 11 pki+1 = p k i + nnz k i\n// Build up the elements of pk, ck, and dk\n12 pk = vector of size N + 1 13 ck = vector of size nnzk 14 dk = vector of size nnzk 15 n = 0 16 for i = 0 to N \u2212 1 17 istart = pi 18 istop = pi+1 19 ci = cistart:istop 20 di = distart:istop 21 for c1 = 0 to |ci| \u2212 1 22 for c2 = c1 to |ci| \u2212 1 23 dkn = dc0 \u00b7 dc1 24 ckn =? 25 n = n+ 1\nThe crux of the problem is at line 24. Given the arbitrary columns involved in a polynomial feature of Ai, we need to determine the corresponding column of Aki . We cannot simply reset a counter for each row as we did in the dense algorithm, because only columns corresponding to nonzero values are stored. Any time a column that would have held a zero value is implicitly skipped, the counter would err.\nTo develop a general algorithm, we require a mapping from columns of A to a column of Ak. If there are D columns of A and ( D k ) columns of Ak, this can be accomplished by a bijective mapping of the following form:\n(j0, j1, . . . , jk\u22121) pj0j1...ik\u22121 \u2208 {0, 1, . . . , ( D\nk\n) \u2212 1} (1)\nsuch that 0 \u2264 j0 \u2264 j1 \u2264 \u00b7 \u00b7 \u00b7 \u2264 jk\u22121 < D where (j0, j1, . . . , jk\u22121) are elements of c and pj0j1...ik\u22121 is an element of ck.\n4 CONSTRUCTION OF MAPPING\nWithin this section, i, j, and k denote column indices. For the second degree case, we seek a map from matrix indices (i, j) (with 0 \u2264 i < j < D ) to numbers f(i, j) with 0 \u2264 f(i, j) < D(D\u22121)2 , one that follows the pattern indicated by x 0 1 3x x 2 4x x x 5\nx x x x  (2) where the entry in row i, column j, displays the value f(i, j). We let T2(n) = 12n(n + 1) be the nth triangular number; then in Equation 2, column j (for j > 0) contains entries with T2(j \u2212 1) \u2264\ne < T2(j); the entry in the ith row is just i + T2(j \u2212 1). Thus we have f(i, j) = i + T2(j \u2212 1) = 1 2 (2i + j\n2 \u2212 j). For instance, in column j = 2 in our example (the third column), the entry in row i = 1 is i+ T2(j \u2212 1) = 1 + 1 = 2. With one-based indexing in both the domain and codomain, the formula above becomes f1(i, j) = 1 2 (2i+ j\n2 \u2212 3j + 2). For polynomial features, we seek a similar map g, one that also handles the case i = j. In this case, a similar analysis yields g(i, j) = i+ T2(j) = 12 (2i+ j 2 + j + 1).\nTo handle three-way interactions, we need to map triples of indices in a 3-index array to a flat list, and similarly for higher-order interactions. For this, we\u2019ll need the tetrahedral numbers T3(n) =\u2211n\ni=1 T2(n) = 1 6 (n 3 + 3n2 + 2n).\nFor three indices, i, j, k, with 0 \u2264 i < j < k < D, we have a similar recurrence. Calling the mapping h, we have\nh(i, j, k) = i+ T2(j \u2212 1) + T3(k \u2212 2); (3) if we define T1(i) = i, then this has the very regular form\nh(i, j, k) = T1(i) + T2(j \u2212 1) + T3(k \u2212 2); (4) and from this the generalization to higher dimensions is straightforward. The formulas for \u201chigher triangular numbers\u201d, i.e., those defined by\nTk(n) = n\u2211 i=1 Tk\u22121(n) (5)\nfor k > 1 can be determined inductively.\nThe explicit formula for 3-way interactions, with zero-based indexing, is\nh(i, j, k) = 1 + (i\u2212 1) + (j \u2212 1)j 2 + (6)\n(k \u2212 2)3 + 3(k \u2212 2)2 + 2(k \u2212 2) 6 . (7)\n5 FINAL CSR EXPANSION ALGORITHM\nWith the mapping from columns of A to a column of Ak, we can now write the final form of the innermost loop of the algorithm from 3.2. Let the mapping for k = 2 be denoted h2. Then the innermost loop becomes:\nfor c2 = c1 to |ci| \u2212 1 j0 = cc0 j1 = cc1 cp = h\n2(j0, j1) dkn = dc0 \u00b7 dc1 ckn = cp n = n+ 1\nThe algorithm can be generalized to higher degrees by simply adding more nested loops, using higher order mappings, modifying the output dimensionality, and adjusting the counting of nonzero polynomial features in line 9.\n6 TIME COMPLEXITY\n6.1 ANALYTICAL\nCalculating k-degree polynomial features via our method for a vector of dimensionality D and density d requires ( dD k ) (with repetition) products. The complexity of the algorithm, for fixed k\ndD, is therefore\nO\n(( dD + k \u2212 1\nk\n)) = O ( (dD + k \u2212 1)! k!(dD \u2212 1)! ) (8)\n= O\n( (dD + k \u2212 1)(dD + k \u2212 2) . . . (dD)\nk!\n) (9)\n= O ((dD + k \u2212 1)(dD + k \u2212 2) . . . (dD)) for k dD (10) = O ( dkDk ) (11)\n6.2 EMPIRICAL\nTo demonstrate how our algorithm scales with the density of a matrix, we compare it to the traditional polynomial expansion algorithm in the popular machine library scikit-learn Pedregosa et al. (2011) in the task of generating second degree polynomial expansions. Matrices of size 100\u00d7 5000 were randomly generated with densities of 0.2, 0.4, 0.6, 0.8, and 1.0. Thirty matrices of each density were randomly generated, and the mean times (gray) of each algorithm were plotted. The red or blue width around the mean marks the third standard deviation from the mean. The time to densify the input to the standard algorithm was not counted.\nThe standard algorithm\u2019s runtime stays constant no matter the density of the matrix. This is because it does not avoid products that result in zero, but simply multiplies all second order combinations of features. Our algorithm scales quadratically with respect to the density. If the task were third degree expansions rather than second, the plot would show cubic scaling.\nThe fact that our algorithm is approximately 6.5 times faster than the scikit-learn algorithm on 100\u00d7 5000 matrices that are entirely dense is likely a language implementation difference. What matters is that the time of our algorithm increases quadratically with respect to the density in accordance with the big-O analysis.\n7 CONCLUSION\nWe have developed an algorithm for performing polynomial feature expansions on CSR matrices that scales polynomially with respect to the density of the matrix. The areas within machine learning that this work touches are not en vogue, but they are workhorses of industry, and every improvement in core representations has an impact across a broad range of applications.\n", "cats": {"READ": 0.0, "DONTREAD": 1.0}, "reviews": [{"IS_META_REVIEW": true, "comments": "The paper is beyond my expertise. I cannot give any solid review comments regarding the techniques that are better than an educated guess.\n\nHowever, it seems to me that the topic is not very relevant to the focus of ICLR. Also the quality of writing requires improvement, especially literature review and experiment analysis."}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "The approach/problem seems interesting, and several reviewers commented on this. However, the experimental evaluation is quite preliminary and the paper would be helped a lot with a connection to a motivating application. All of the reviewers pointed out that the work is not written in the usual in the scope of ICLR papers, and putting these together at this time it makes sense to reject the paper.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"TITLE": "Interesting algorithm, but poor fit with ICLR", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "The authors present here a new algorithm for the effective calculation of polynomial features on Sparse Matrices. The key idea is to use a proper mapping between matrices and their polynomial versions, in order to derive an effective CSR expansion algorithm. The authors analyse the time complexity in a convincing way with experiments.\n\nOverall, the algorithm is definitely interesting, quite simple and nice, with many possible applications. The paper is however very superficial in terms of experiments, or applications of the proposed scheme. Most importantly, the fit with the main scope of ICLR is far from obvious with this work, that should probably re-submitted to better targets. ", "IS_META_REVIEW": false, "RECOMMENDATION": 3, "DATE": "21 Dec 2016", "REVIEWER_CONFIDENCE": 3}, {"TITLE": "No Title", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "This paper proposes an algorithm for polynomial feature expansion on CSR matrices, which reduces the time complexity of the standard method by a factor d^k where d is the density of the sparse matrix. The main contribution of this work is not significant enough. The experiments are incomplete and not convincing.\n\nThe background of the problem is not sufficiently introduced. There are only two references in the introduction part (overall only three papers are cited), which are from decades ago. Many more relevant papers should be cited from the recent literature.\n\nThe experiment part is very weak. This paper claims that the time complexity of their algorithm is O(d^k D^k), which is an improvement over standard method O(D^k) by a factor d^k. But in the experiments, when d=1, there is still a large gap (~14s vs. ~90s) between the proposed method and the standard one. The authors explain this as \"likely a language implementation\", which is not convincing. To fairly compare the two methods, of course you need to implement both in the same programming language and run experiments in the same environment. For higher degree feature expansion, there is no empirical experiments to show the advantage of the proposed method.\n\nSome minor problems are listed below.\n1) In Section 2, the notation \"p_i:p_i+1\" is not clearly defined.\n2) In Section 3.1, typo: \"efter\" - \"after\"\n3) All the algorithms in this paper are not titled. The input and output is not clearly listed.\n4) In Figure 1, the meaning of the colored area is not described. Is it standard deviation or some quantile of the running time? How many runs of each algorithm are used to generate the ribbons? Many details of the experimental settings are missing.\n", "IS_META_REVIEW": false, "RECOMMENDATION": 3, "DATE": "19 Dec 2016", "REVIEWER_CONFIDENCE": 3}, {"TITLE": "This is not quite related with ICLR's field of interests", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "The paper is beyond my expertise. I cannot give any solid review comments regarding the techniques that are better than an educated guess.\n\nHowever, it seems to me that the topic is not very relevant to the focus of ICLR. Also the quality of writing requires improvement, especially literature review and experiment analysis. ", "IS_META_REVIEW": false, "RECOMMENDATION": 3, "DATE": "18 Dec 2016", "REVIEWER_CONFIDENCE": 1}, {"IS_META_REVIEW": true, "comments": "The paper is beyond my expertise. I cannot give any solid review comments regarding the techniques that are better than an educated guess.\n\nHowever, it seems to me that the topic is not very relevant to the focus of ICLR. Also the quality of writing requires improvement, especially literature review and experiment analysis."}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "The approach/problem seems interesting, and several reviewers commented on this. However, the experimental evaluation is quite preliminary and the paper would be helped a lot with a connection to a motivating application. All of the reviewers pointed out that the work is not written in the usual in the scope of ICLR papers, and putting these together at this time it makes sense to reject the paper.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"TITLE": "Interesting algorithm, but poor fit with ICLR", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "The authors present here a new algorithm for the effective calculation of polynomial features on Sparse Matrices. The key idea is to use a proper mapping between matrices and their polynomial versions, in order to derive an effective CSR expansion algorithm. The authors analyse the time complexity in a convincing way with experiments.\n\nOverall, the algorithm is definitely interesting, quite simple and nice, with many possible applications. The paper is however very superficial in terms of experiments, or applications of the proposed scheme. Most importantly, the fit with the main scope of ICLR is far from obvious with this work, that should probably re-submitted to better targets. ", "IS_META_REVIEW": false, "RECOMMENDATION": 3, "DATE": "21 Dec 2016", "REVIEWER_CONFIDENCE": 3}, {"TITLE": "No Title", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "This paper proposes an algorithm for polynomial feature expansion on CSR matrices, which reduces the time complexity of the standard method by a factor d^k where d is the density of the sparse matrix. The main contribution of this work is not significant enough. The experiments are incomplete and not convincing.\n\nThe background of the problem is not sufficiently introduced. There are only two references in the introduction part (overall only three papers are cited), which are from decades ago. Many more relevant papers should be cited from the recent literature.\n\nThe experiment part is very weak. This paper claims that the time complexity of their algorithm is O(d^k D^k), which is an improvement over standard method O(D^k) by a factor d^k. But in the experiments, when d=1, there is still a large gap (~14s vs. ~90s) between the proposed method and the standard one. The authors explain this as \"likely a language implementation\", which is not convincing. To fairly compare the two methods, of course you need to implement both in the same programming language and run experiments in the same environment. For higher degree feature expansion, there is no empirical experiments to show the advantage of the proposed method.\n\nSome minor problems are listed below.\n1) In Section 2, the notation \"p_i:p_i+1\" is not clearly defined.\n2) In Section 3.1, typo: \"efter\" - \"after\"\n3) All the algorithms in this paper are not titled. The input and output is not clearly listed.\n4) In Figure 1, the meaning of the colored area is not described. Is it standard deviation or some quantile of the running time? How many runs of each algorithm are used to generate the ribbons? Many details of the experimental settings are missing.\n", "IS_META_REVIEW": false, "RECOMMENDATION": 3, "DATE": "19 Dec 2016", "REVIEWER_CONFIDENCE": 3}, {"TITLE": "This is not quite related with ICLR's field of interests", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "The paper is beyond my expertise. I cannot give any solid review comments regarding the techniques that are better than an educated guess.\n\nHowever, it seems to me that the topic is not very relevant to the focus of ICLR. Also the quality of writing requires improvement, especially literature review and experiment analysis. ", "IS_META_REVIEW": false, "RECOMMENDATION": 3, "DATE": "18 Dec 2016", "REVIEWER_CONFIDENCE": 1}]}
{"text": "1 INTRODUCTION\nMachine intelligence has had some notable successes, however often in narrow domains which are sometimes of little practical use to humans \u2013 for instance games like chess (Campbell et al., 2002) or Go (Silver et al., 2016). If we aimed to build a general AI that would be able to efficiently assist humans in a wide range of settings, we would want it to have a much larger set of skills \u2013 among them would be an ability to understand human language, to perform common-sense reasoning and to be able to generalize its abilities to new situations like humans do.\nIf we want to achieve this goal through Machine Learning, we need data to learn from. A lot of data if the task at hand is complex \u2013 which is the case for many useful tasks. One way to achieve wide applicability would be to provide training data for each specific task we would like the machine to perform. However it is unrealistic to obtain a sufficient amount of training data for some domains \u2013 it may for instance require expensive human annotation or all domains of application may be difficult to predict in advance \u2013 while the amount of training data in other domains is practically unlimited, (e.g. in language modelling or Cloze-style question answering).\nThe way to bridge this gap \u2013 and to achieve the aforementioned adaptability \u2013 is transfer learning (Pan & Yang, 2010) and closely related semi-supervised learning (Zhu & Goldberg, 2009) which allow the system to acquire a set of skills on domains where data are abundant and then use these skills to succeed on previously unseen domains. Despite how important generalization is for general AI, a lot of research keeps focusing on solving narrow tasks.\nIn this paper we would like to examine transfer of learnt skills and knowledge within the domain of text comprehension, a field that has lately attracted a lot of attention within the NLP community (Hermann et al., 2015; Hill et al., 2015; Kobayashi et al., 2016; Kadlec et al., 2016b; Chen et al., 2016; Sordoni et al., 2016; Dhingra et al., 2016; Trischler et al., 2016; Weissenborn, 2016; Cui et al., 2016b;a;\n\u2217These authors contributed equally to this work.\nLi et al., 2016; Shen et al., 2016). Specifically, we would like to address the following research questions:\n1. Whether we could train models on natural-language tasks where data are abundant and transfer the learnt skills to tasks where in-domain training data may be difficult to obtain. We will first look into what reasoning abilities a model learns from two large-scale readingcomprehension datasets using artificial tasks, and then check whether it can transfer its skills to real world tasks. Spoiler: both these transfers are very poor if we allow no training at all on the target task.\n2. Whether pre-training on large-scale datasets does help if we allow the model to train on a small sample of examples from the target tasks. Here the results are much more positive.\n3. Finally we examine whether the benefits of pre-training are concentrated in any particular part of the model - namely the word-embedding part or the context encoder (the reasoning part). It turns out that pre-training is useful for both components.\nAlthough our results do not improve current state of the art in any of the studied tasks, they show a clear positive effect of large-dataset pre-training on the performance of our baseline machine-learning model. Previous studies of transfer learning and semi-supervised learning in NLP focused on text classification (Dai & Le, 2015; Mou et al., 2016) and various parsing tasks (Collobert et al., 2011; Hashimoto et al., 2016). To our knowledge this work is the first study of transfer learning in reading comprehension, and we hope it will stimulate further work in this important area.\nWe will first briefly introduce the datasets we will be using on the pre-training and target sides, then our baseline model and afterwards in turn describe the method and results of each of the three experiments.\n2 DATASETS\n2.1 PRE-TRAINING DATASETS\nWe have mentioned that for the model pre-training we would want to use a task where training data are abundant. An example of such task is context-dependent cloze-style-question answering since the training data for this task can be generated automatically from a suitable corpus. We will use two such pre-training datasets in our experiments: the BookTest (Bajgar et al., 2016) and the CNN/Daily Mail (CNN/DM) news dataset (Hermann et al., 2015).\nThe task associated with both datasets is to answer a cloze-style question (i.e. fill in a blank in a sentence) the answer to which needs to be inferred from a context document provided with the question.\n2.1.1 BOOKTEST\nIn the BookTest dataset, the context document is formed from 20 consecutive sentences from a book. The question is then formed by omitting a common noun or a named entity from the subsequent 21st sentence. Among datasets of this kind, the BookTest is among the largest with more than 14 million training examples coming from 3555 copyright-free books avalable thanks to Project Gutenberg.\n2.1.2 CNN/DAILY MAIL\nIn the CNN/DM dataset the context document is formed from a news article while the cloze-style question is formed by removing a named entity from one of the short summary sentences which often appear at the top of the article.\nTo stop the model from using world knowledge from outside the context article (and hence truly test the comprehension of the article), all named entities were replaced by anonymous tags, which are further shuffled for each example. This may make the comprehension more difficult; however, since the answer is always one of the anonymized entities, it also reduces the number of possible answers making guessing easier.\n2.2 TARGET DATASETS\n2.2.1 BABI\nThe first target dataset are the bAbI tasks (Weston et al., 2016) \u2013 a set of artificial tasks each of which is designed to test a specific kind of reasoning. This toy dataset will allow us to observe what particular skills the model may be learning from each of the three training datasets.\nFor our experiments we will be using an architecture designed to select one word from the context document as the answer. Hence we have selected Tasks 1,2,3,4,5,11,12,13,14 and 16 which fulfill this requirement and added task 15 which required a slight modification. Furthermore because both pre-training datasets are cloze-style we converted also the bAbI task questions into cloze style (e.g. \u201dWhere is John?\u201d to \u201dJohn is in the XXXXX.\u201d).\nFor the models pre-trained on CNN/DM we also anonymized the tasks in a way similar to the pre-training dataset - i.e. we replaced all names of characters and also all words that can appear as answers for the given task by anonymous tags in the style of CNN/DM. This gives even models that have not seen any training examples from the target domain a chance to answer the questions.\nFull details about these alterations can be found in Appendix A.\n2.2.2 SQUAD\nSecondly, we will look on transfer to the SQuAD dataset (Rajpurkar et al., 2016); here the associated task may be already useful in the real world. Although cloze-style questions have the huge advantage in the possibility of being automatically generated from a suitable corpus \u2013 the path taken by CNN/DM and the BookTest \u2013 in practice humans would use a proper question, not its cloze-style substitute. This brings us to the need of transfer from the data-rich cloze-style training to the domain of proper questions where data are much scarcer due to the necessary human annotation.\nThe SQuAD dataset is a great target dataset to use for this. As opposed to the bAbI tasks, the goal of this dataset is actually a problem whose solving would be useful to humans - answering natural questions based on an natural language encyclopedic knowledge base.\nFor our experiments we selected only a subset of the SQuAD training and development examples where the answer is only a single word, since this is an inherent assumption of our machine learning model. This way we extracted 28,346 training examples out of the original 100,000 examples and 3,233 development examples out of 10,570.\n3 MACHINE LEARNING MODEL: AS READER\nWe perform our experiments using the Attention Sum Reader (AS Reader) (Kadlec et al., 2016b) model. The AS Reader is simple to implement while it achieves strong performance on several text comprehension tasks (Kadlec et al., 2016b; Bajgar et al., 2016; Chu et al., 2016). Since the AS Reader is a building block of many recent text-comprehension models (Trischler et al., 2016; Sordoni et al., 2016; Dhingra et al., 2016; Cui et al., 2016b;a; Shen et al., 2016; Munkhdalai & Yu, 2016) it is a good representative of current research in this field.\nA high level structure of the AS Reader is shown in Figure 1. The words from the document and the question are first converted into vector embeddings using a look-up matrix. The document is then read by a bidirectional Gated Recurrent Unit (GRU) network (Cho et al., 2014). A concatenation of the hidden states of the forward and backward GRUs at each word is then used as a contextual embedding of this word, intuitively representing the context in which the word is appearing. We can also understand it as representing the set of questions to which this word may be an answer.\nSimilarly the question is read by a bidirectional GRU but in this case only the final hidden states are concatenated to form the question embedding.\nThe attention over each word in the context is then calculated as the dot product of its contextual embedding with the question embedding. This attention is then normalized by the softmax function and summed across all occurrences of each answer candidate. The candidate with most accumulated attention is selected as the final answer.\nFor a more detailed description of the model including equations check Kadlec et al. (2016b).\n4 EXPERIMENTS: TRANSFER LEARNING IN TEXT COMPREHENSION\nNow let us turn in more detail to the three kinds of experiments that we performed.\n4.1 PRE-TRAINED WITHOUT TARGET ADJUSTMENT\nIn the first experiment we tested how a model trained on one of the large-scale pre-training datasets performs on the bAbI tasks without any opportunity to train on bAbI. Since the BookTest and CNN/DM tasks involve only cloze-style questions, we can\u2019t expect a model trained on them to answer natural ?-style questions. Hence we did not study the transfer to SQuAD in this case, only the transfer to the (cloze-converted) bAbI tasks.\n4.1.1 METHOD\nFirst we tested how the AS Reader architecture (Kadlec et al., 2016b) can handle the tasks if trained directly on the bAbI training data for each task. Then we tested the degree of transfer from the BookTest and CNN/DM data to the 11 selected bAbI tasks.\nIn the first part of the experiment we trained a separate instance of the AS Reader on the 10,000- example version of the bAbI training data for each of the 11 tasks (for more details see Appendix B.1). On 8 of them the architecture was able to learn the task with accuracy at least 95% 1 (results for each task can be found in Table 4 in Appendix C). Hence if given appropriate training the AS Reader is capable of the reasoning needed to solve most of the selected bAbI tasks. Now when we know that the AS Reader is powerful enough to learn the target tasks we can turn to transfer from the two large-scale datasets.\nThe main part of this first experiment was then straightforward: we pre-trained multiple models on the BookTest and CNN/DM datasets and then simply evaluated them on the test datasets of the 11 selected bAbI tasks.\n4.1.2 RESULTS\nTable 1 summarizes the results of this experiment. Both the models trained on the BookTest and those trained on the CNN/DM dataset perform quite poorly on bAbI and achieve much lower accuracy than\n1It should be noted that there are several machine learning models that perform better than the AS Reader in the 10k weakly supervised setting, e.g. (Sukhbaatar et al., 2015; Xiong et al., 2016; Graves et al., 2016), however they often need significant fine-tuning. On the other hand we trained plain AS Reader model without any modifications. Hyperparameter and feature fine-tuning could probably further increase its performance on individual tasks however it goes directly against the idea of generality that is at the heart of this work. For comparison with state of the art we include results of DMN+ (Xiong et al., 2016) in Table 1 which had the best average performance over the original 20 tasks.\nthe models trained directly on each individual bAbI task. However there is some transfer between the tasks since the AS Reader trained on either the BookTest or CNN/DM outperforms a random baseline2 and even an improved baseline which selects the most frequent word from the context that also appears as an answer in the training data for this task.\nThe results also show that the models trained on CNN/DM perform somewhat better on most tasks than the BookTest models. This may be due to the fact that bAbI tasks generally require the model to summarize information from the context document, which is also what the CNN/DM dataset is testing. On the other hand, the BookTest requires prediction of a possible continuation of a story, where the required kind of reasoning is much less clear but certainly different from pure summarization. Another explanation for better performance of CNN/DM models might be that they solve slightly simpler task since the candidate answers were already pre-selected in the entity anonymization step.\nReaders interested in how the training-dataset size affects this kind of transfer can check (Kadlec et al., 2016a) where we show that the target-task performance is a bit better if we use the large BookTest as opposed to its smaller subset, the Children\u2019s Book Test (CBT) (Hill et al., 2015).\nConclusions from this experiment are that the skills learned from two large-scale datasets generalize surprisingly poorly to even simple toy tasks. This may make us ask whether most teams\u2019 focus on solving narrow tasks is truly beneficial if the skills learnt on these tasks are hard to apply elsewhere. However it also brings us to our next experiment, where we try to provide some help to the struggling pre-trained models.\n4.2 PRE-TRAINED WITH TARGET ADJUSTMENT\nAfter showing that the skills learnt from the BookTest and CNN/DM datasets are by themselves insufficient for solving the toy tasks, the next natural question is whether they are useful if helped by training on a small sample of examples from the target task. We call this additional phase of training target adjustment. For this experiment we again use the bAbI tasks, however we also test transfer to a subset of the SQuAD dataset, which is much closer to real-world natural-language question answering.\nThe results presented in this and the following section are based on training 3701 model instances.\n4.2.1 METHOD\nCommon to bAbI and SQuAD datasets. In this experiment we started with a pre-trained model which we used in the previous experiment. However, after it finished training on one of the large pre-training datasets, we allowed it to train on a subset of training examples from the target dataset. We tried subsets of various sizes ranging from a single example to thousands. We tried training four different pre-trained models and also, for comparison, four randomly-initialized models with the same hyperparameters (see Appendix B.2 for details). The experiment with each task-model couple was run on 4 different data samples of each size which were randomly drawn from the training dataset\n2The random baseline selects randomly uniformly between all unique words contained in the context document.\nof the task to account for variations between these random samples \u2013 which may be substantial given the small sample size.3\nbAbI. For each of these models we observed the test accuracy at the best-validation epoch and compared this number between the randomly initialized and pre-trained models. Validation was done using 100 examples which were set aside from the task\u2019s original 10k training data.4 We perform the experiment with models pre-trained on the BookTest and also on CNN/DM.\nSQuAD subset. In the SQuAD experiment, we trained the model on a subset of the original training dataset where answers were only single words and its sub-subsets. We report the best-validation accuracy on a development set filtered in the same way. This experiment was performed only with the models pre-trained on BookTest.\n4.2.2 RESULTS\nThe results of these experiments are summarized in Figures 2 and 3.\n3We are planning to release the split training datasets soon. 4The other models trained on the full 10k dataset usually use 1000 validation examples (Sukhbaatar et al., 2015; Xiong et al., 2016), however we wanted to focus on low data regime thus we used 10 times less examples.\nbAbI. Sub-figure 2a shows mean test accuracy of the models that achieved the best validation result for each single task. The results for both BookTest and CNN/DM experiments confirm positive effect of pre-training compared to randomly initialized baseline. Figure 3 shows performance on selected bAbI tasks where pre-training has clearly positive effect, such plot for each of the target tasks is provided in Appendix C.2 (Figure 4).\nNote that the CNN/DM models cannot be directly compared to BookTest results due to entity anonymization that seems to simplify the task when the model is trained on smaller datasets.\nSince our evaluation methodology with different training set sizes is novel, we can compare our result only to MemN2N (Sukhbaatar et al., 2015) trained on a 1k dataset. MemN2N is the only weakly supervised model that reports accuracy when trained on less than 10k examples. MemN2N achieves average accuracy 93.2%5 on the eleven selected tasks. This is substantially better than both our random baseline (78.0%) and the BookTest-pre-trained model (79.5%), however our model is not tuned in any way towards this particular task. One important conceptual difference is that the AS Reader processes the whole context as one sequence of words, whereas MemN2N receives the context split into single sentences, which simplifies the task for the network.\nSQuAD subset. The results of SQuAD experiment also confirm positive effect of pre-training, see Sub-figure 2b, for now compare just lines showing performance of the fully pre-trained model and the randomly initialized model \u2013 the meaning of the remaining two lines shall become clear in the next section.\nMore detailed statistics about the results of this experiment can be found in Appendix D.\nWe should note that performance of our model is not competitive with the state of the art models on this dataset. For instance the DCR model (Yu et al., 2016) trained on our SQuAD subset achieves validation accuracy 74.9% in this task which is better than our randomly initialized (35.4%) and pre-trained (51.6%) models6. However, the DCR model is designed specifically for the SQuAD task, for instance it utilizes features that are not used by our model.\n4.3 PARTIALLY PRE-TRAINED MODEL\nSince our previous experiment confirmed positive effect of pre-training if followed by target-domain adjustment, we wondered which part of the model contains the knowledge transferable to new domains. To examine this we performed the following experiment.\n4.3.1 METHOD\nOur machine learning model, the AS Reader, consists of two main parts: the word-embedding look-up and the bidirectional GRUs used to encode the document and question (see Figure 1). Therefore a natural question was what the contribution of each of these parts is.\nTo test this we created two models out of each pre-trained model used in the previous experiment. The first model variant uses the pre-trained word embeddings from the original model while the GRU encoders are randomly initialized. We say that this model has pre-trained embeddings. The second model variant uses the opposite setting where the word embeddings are randomly initialized while the encoders are taken form a pre-trained model. We call this pre-trained encoders.\nbAbI. For this experiment we selected only a subset of tasks with training set of 100 examples where there was significant difference in accuracy between randomly-initialized and pre-trained models. For evaluation we use the same methodology as in the previous experiment, that is, we report accuracy of the best-validation model averaged over 4 training splits.\nSQuAD subset. We evaluated both model variants on all training sets from the previous SQuAD experiment using the same methodology.\n5MemN2N trained on each single task with PE LS RN features, see (Sukhbaatar et al., 2015) for details. 6We would like to thank Yu et al. (2016) for training their system on our dataset.\n4.3.2 RESULTS\nbAbI. Table 2 shows improvement of pre-trained models over a randomly initialized baseline. In most cases (all except Task 5) the fully pre-trained model achieved the best accuracy.\nSQuAD subset. The accuracies of the four model variants are plotted in Figure 2b together with results of the previous SQuAD experiment. The graph shows that both pre-trained embeddings and pre-trained encoders alone improve performance over the randomly initialized baseline, however the fully pre-trained model is always the best.\nThe overall result of this experiment is that both pre-training of the word embeddings and pre-training of the encoder parameters are important since the fully pre-trained model outperforms both partially pre-trained variants.\n5 CONCLUSION\nOur experiments show that transfer from two large cloze-style question-answering datasets to our two target tasks is suprisingly poor, if the models aren\u2019t provided with any examples from the target domain. However we show that models that pre-trained models perform significantly better than a randomly initialized model if they are shown at least a few training examples from the target domain. The usefulness of pre-trained word embeddings is well known in the NLP community however we show that the power of our pre-trained model does not lie just in the embeddings. This suggests that once the text-comprehension community agrees on sufficiently versatile model, much larger parts of the model could start being reused than just the word-embeddings.\nThe generalization of skills from a training domain to new tasks is an important ingredient of any system we would want to call intelligent. This work is an early step to explore this direction.\nA CLOZE STYLE BABI DATASET\nSince our AS Reader architecture is designed to select a single word from the context document as an answer (the task of CBT and BookTest), we selected 10 bAbI tasks that fulfill this requirement out of the original 20. These tasks are: 1. single supporting fact, 2. two supporting facts, 3. three supporting facts, 4. two argument relations, 5. three argument relations, 11. basic coreference, 12. conjunction, 13. compound coreference, 14. time reasoning and 16. basic induction.\nTask 15 needed a slight modification to satisfy this requirement: we converted the answers into plural (e.g. \u201dQ: What is Gertrude afraid of? A: wolf.\u201d was converted into \u201dA: wolves\u201d which also seems to be the more natural way to formulate the answer to such a question.).\nAlso since CBT and BookTest train the model for Cloze-style question answering, we modify the original bAbI dataset by reformulating the questions into Cloze-style. For example we translate a question \u201dWhere is John ?\u201d to \u201dJohn is in the XXXXX .\u201d\nFor the models pre-trained on CNN/DM we also replace two kinds of words by anonymized tags (e.g. \u201d@entity56\u201d) in a style similar to the pre-training dataset. Specifically we replace two (largely overlapping) categories of words:\n1. Proper names of story characters (e.g. John, Sandra) 2. Any word that can appear as an answer for the particular task (e.g. kitchen, garden if the\ntask is asking about locations).\nB METHOD DETAILS\nB.1 DIRECT TRAINING ON BABI \u2013 METHOD\nHere we give a more detailed description of the method we used to arrive to our results. We highlight only facts particular to this experiment. A more detailed general description of training the AS Reader is given in (Kadlec et al., 2016b).\nThe results given for AS Reader trained on bAbI are each for a single model with 64 hidden units in each direction of the GRU context encoder and embedding dimension 32 trained on the 10k training data provided with that particular task.\nThe results for AS Reader trained on the BookTest and the CNN/DM are for a greedy ensemble consisting of 4 models whose predictions were simply averaged. The models and ensemble were all validated on the validation set corresponding to the training dataset. The performance on the bAbI tasks oscillated notably during training however the ensemble averaging does somewhat mitigate this to get more representative numbers.\nB.2 HYPERPARAMETERS FOR THE TARGET-ADJUSTMENT EXPERIMENTS\nTable 3 lists hyperparameters of the pre-trained AS Reader instances used in our experiments with target adjustment.\nC DETAILED RESULTS\nC.1 EXPERIMENTS WITHOUT TARGET ADJUSTMENT\nTable 4 shows detailed results for the experiments on models which were just pre-trained on one of the pre-training datasets without any target-adjustment. It also shows several baselines and results of a state-of-the-art model.\nC.2 TARGET-ADJUSTMENT EXPERIMENTS\nC.2.1 RESULTS FOR ALL BABI TASKS\nFigure 4 shows the test accuracies of all models that we trained in the target-adjustment experiments as well as lines joining the accuracies of the best-validation models.\nTa bl\ne 4:\nPe rf\nor m\nan ce\nof th\ne A\nS R\nea de\nr w\nhe n\ntr ai\nne d\non th\ne bA\nbI 10\nk, B\noo kT\nes ta\nnd C\nN N\n/D M\nda ta\nse ts\nan d\nth en\nev al\nua te\nd on\nbA bI\nte st\nda ta\n. T\nhe D\nyn am ic M em or y N et w or k (D M N +) is th e st at eof -t he -a rt m od el in a w ea kl y su pe rv is ed se tti ng on th e bA bI 10 k da ta se t. It s re su lts ar e ta ke n fr om (X io ng et al ., 20 16 ). M em N 2N (S uk hb aa ta re ta l., 20 15 )i s th e st at eof -t he -a rt m od el on th e 1k tr ai ni ng da ta se t; fo rc om pl et en es s w e al so in cl ud e its re su lts w ith th e 10 k tr ai ni ng .\nM od\nel :\nR an\ndo m\nR nd\nca nd\n. M\nem N 2N (s in gl e) (P E L S R\nN )\nM em\nN 2N\n(s in\ngl e)\n(P E\nL S\nLW R\nN )\nD M\nN +\n(s in\ngl e)\nA SR\nea de\nr\na a\na a\na a a\na a a\nTe st\nda ta\nse t Tr ai\nn da\nta se\nt no\nt tr ai ne\nd bA\nbI 10\nk bA\nbI 1k\nbA bI 10 k\nbA bI 10 k\nbA bI 10 k\nB oo\nkT es t 14\nM D\nM +C\nN N\n1. 2M\n1 Si\nng le\nsu pp\nor tin\ng fa\nct 7.\n80 31\n.2 0\n10 0.\n00 10\n0. 00\n10 0.\n00 10\n0. 00\n37 .3\n0 51\n.5 0\n2 Tw\no su\npp or\ntin g\nfa ct\ns 4.\n40 26\n.9 6\n91 .7\n0 99\n.7 0\n99 .7\n0 91\n.9 0\n25 .8\n0 28\n.9 0\n3 T\nhr ee\nsu pp\nor tin\ng fa\nct s\n3. 40\n19 .1\n4 59\n.7 0\n97 .9\n0 98\n.9 0\n86 .0\n0 22\n.2 0\n27 .4 0 4 Tw oar gu m en tr el at io ns 10 .5 0 33 .5 8 97 .2 0 10 0. 00 10 0. 00 10 0. 00 50 .3 0 54 .9 0 5 T hr ee -a rg um en tr el at io ns 4. 40 21 .4 2 86 .9 0 99 .2 0 99 .5 0 99 .8 0 67 .6 0 68 .1 0 11 B as ic co re fe re nc e 6. 20 30 .4 2 99 .1 0 99 .9 0 10 0. 00 10 0. 00 33 .0 0 20 .8 0 12 C on ju nc tio n 6. 70 27 .2 5 99 .8 0 10 0. 00 10 0. 00 10 0. 00 30 .4 0 37 .7 0 13 C om po un d co re fe re nc e 5. 60 27 .7 3 99 .6 0 10 0. 00 10 0. 00 10 0. 00 33 .8 0 14 .0 0 14 Ti m e re as on in g 5. 00 27 .8 2 98 .3 0 99 .9 0 99 .8 0 95 .0 0 27 .6 0 50 .5 0 15 B as ic de du ct io n 5. 20 37 .2 0 10 0. 00 10 0. 00 10 0. 00 96 .7 0 39 .9 0 17 .6 0 16 B as ic in du ct io n 7. 50 45 .6 5 98 .7 0 48 .2 0 54 .7 0 50 .3 0 15 .1 0 48 .0 0 bA bI m ea n (1 1 ta sk s) 6. 06 29 .8 5 93 .7 3 94 .9 8 95 .6 9 92 .7 0 34 .8 2 38 .1 3\nC.2.2 AVERAGE OVER ALL MODELS TRAINED ON BABI TASKS\nFigure 5 plots mean accuracy of all models trained in our experiments. This suggests that pre-training helped all models, not only the top performing ones selected by validation as already shown in Figure 2a.\nD MEANS, STANDARD DEVIATIONS AND P-VALUES BY EXPERIMENT\nTable 5 shows the mean accuracy across all models trained for each combination of task, pre-training dataset and target-adjustment dataset size. Table 6 shows the corresponding standard deviations.\nTable 7 then shows the p-value that whether the expected accuracy of pre-trained models is greater than the expected accuracy of randomly initialized models. This shows that the pre-trained models are statistically significantly better for all target-adjustment set sizes on the SQuAD dataset. On bAbI the BookTest pre-trained models perform convincingly better especially for target-adjustment dataset sizes 100, 500 and 1000, with Task 16 being the main exception to this because the AS Reader struggles to learn it in any setting. For the CNN+DM pre-training the results are not conclusive.\nTa sk\nPr et\nra in\nin g\nTa rg\net -a\ndj us\ntm en\nts et\nsi ze\n0 1\n10 10\n0 50\n0 10\n00 50\n00 10\n00 0\n28 17 4 SQ uA D B oo kT es t 1. 01 e45 4. 07 e05 7. 40 e05 7. 82 e08 N A 5. 17 e08 N A 3. 93 e08 8.\n52 e03 Ta sk 1 B oo kT es t 3. 34 e83 1. 81 e03 1. 33 e01 2. 35 e19 9. 41 e04 1. 67 e02 1. 32 e01 N A N A Ta sk 2 B oo kT es t 1. 24 e34 3. 86 e07 7. 29 e03 2. 59 e01 1. 39 e08 2. 63 e06 7. 54 e09 2. 04 e01 N A Ta sk 3 B oo kT es t 9. 84 e55 1. 27 e05 7. 66 e03 1. 48 e03 3. 18 e04 2. 18 e03 2. 16 e04 1. 03 e01 N A Ta sk 4 B oo kT es t 7. 25 e78 9. 50 e01 9. 71 e01 1. 04 e05 6. 38 e03 1. 70 e02 1. 81 e02 N A N A Ta sk 5 B oo kT es t 6. 55 e11 5 9. 88 e22 8. 87 e19 5. 25 e05 3. 66 e03 8. 61 e02 5. 65 e03 N A N A Ta sk 11 B oo kT es t 6. 78 e15 2 1. 00 e+ 00 9. 94 e01 4. 07 e09 2. 50 e04 2. 28 e02 6. 37 e02 N A N A Ta sk 12 B oo kT es t 2. 27 e90 9. 10 e01 6. 46 e01 1. 89 e05 2. 78 e04 1. 43 e02 2. 36 e02 N A N A Ta sk 13 B oo kT es t 5. 30 e91 9. 75 e01 9. 99 e01 2. 88 e02 2. 74 e02 1. 03 e01 7. 06 e02 N A N A Ta sk 14 B oo kT es t 1. 97 e20 0 1. 01 e03 6. 79 e01 2. 22 e14 3. 40 e05 2. 93 e03 3. 66 e06 3. 97 e01 N A Ta sk 15 B oo kT es t 3. 64 e09 4. 75 e01 4. 12 e01 6. 70 e01 1. 68 e03 3. 70 e03 1. 03 e05 4. 54 e01 N A Ta sk 16 B oo kT es t 1. 81 e05 8. 28 e04 4. 38 e01 2. 72 e01 4. 89 e01 5. 71 e01 7. 40 e03 N A N A Ta sk 1 C N N +D M 9. 43 e09 2. 99 e01 1. 11 e01 1. 05 e01 9. 54 e02 1. 45 e01 3. 97 e03 N A N A Ta sk 2 C N N +D M 9. 38 e17 6. 93 e01 9. 02 e01 9. 15 e01 1. 05 e03 4. 20 e01 2. 64 e03 8. 49 e02 N A Ta sk 3 C N N +D M 2. 42 e16 4. 95 e02 6. 30 e01 1. 75 e01 2. 13 e03 6. 59 e04 4. 68 e02 1. 24 e01 N A Ta sk 4 C N N +D M 5. 84 e03 9. 70 e01 1. 37 e01 4. 83 e03 3. 33 e01 8. 84 e01 1. 08 e01 N A N A Ta sk 5 C N N +D M 1. 17 e10 7. 00 e03 7. 93 e04 5. 20 e01 9. 70 e01 5. 66 e01 1. 83 e01 N A N A Ta sk 11 C N N +D M 1. 00 e+ 00 9. 84 e01 9. 73 e01 2. 58 e01 7. 17 e01 1. 45 e01 6. 95 e01 N A N A Ta sk 12 C N N +D M 1. 93 e14 9. 32 e01 9. 92 e01 2. 57 e02 4. 06 e01 6. 65 e02 2. 09 e01 N A N A Ta sk 13 C N N +D M 8. 69 e02 9. 61 e01 9. 72 e01 9. 89 e01 6. 22 e01 9. 44 e01 2. 83 e01 N A N A Ta sk 14 C N N +D M 2. 17 e12 6. 64 e02 1. 11 e01 2. 05 e02 3. 66 e02 4. 52 e01 9. 10 e01 8. 24 e01 N A Ta sk 15 C N N +D M 1. 36 e52 5. 30 e03 3. 48 e02 7. 21 e02 8. 36 e01 3. 09 e01 8. 47 e01 9. 84 e01 N A Ta sk 16 C N N +D M 6. 39 e35 4. 56 e02 9. 66 e01 5. 95 e01 7. 19 e01 4. 09 e02 2. 51 e02 2. 22 e03 N A\nTa bl\ne 7:\nO ne\n-s id\ned p-\nva lu\ne w\nhe th\ner th\ne m\nea n\nac cu\nra cy\nof pr\netr\nai ne\nd m\nod el\ns is\ngr ea\nte rt\nha n\nth e\nac cu\nra cy\nof th\ne ra\nnd om\nly in\niti al\niz ed\non es\nfo re\nac h\nco m\nbi na\ntio n of ta sk pr etr ai ni ng da ta se t. pva lu es be lo w 0. 05 ar e m ar ke d in gr ee n.\n", "cats": {"READ": 0.0, "DONTREAD": 1.0}, "reviews": [{"IS_META_REVIEW": true, "comments": "This work investigates the performance of transfer learning from resource-rich setup (BookTest, CNN/Daily Mail corpora) to low-resource (bAbI, SQuAD benchmarks) settings. Experiments show poor improvements in 0-shot learning. However, when the model is exposed to few training instances some improvements are observed.\n\nThe claims made here require a more comprehensive analysis. I criticize the use of bAbI as a low-resource real-world scenario. bAbI is designed as a unit test and is far from representing many natural language phenomena. Thus, the claims related to bAbI can only be weak evidence for questioning transfer learning high-resource to low-resource in real-world scenarios. I highly recommend using recently proposed real-world scenarios [1,2].\n\nMore importantly, the work does not explain why and how do we get improvement using transfer learning. They remotely address this by hypothesizing the knowledge of transfer is not just encoded in embeddings but also in the model. Considering the related work [3], these claims bring a marginal novelty and still \"how and why\" should be central in this work.  \n\n[1]"}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "The area chair agrees with reviewers 1 and 3 that the paper does not meet the bar for ICLR. Reviewer 3 in particular points out how the paper can be strengthened for future revisions.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"DATE": "16 Jan 2017", "TITLE": "Any further thoughts?", "IS_META_REVIEW": false, "comments": "Dear authors and reviewers, this paper is currently very close to the decision boundary for acceptance and would benefit from a bit more discussion.\n\n", "OTHER_KEYS": "ICLR 2017 conference"}, {"TITLE": "Interesting ; needs to improve clarity", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "First I would like to apologize for the delay in reviewing.\n\nsummary : This work explores several experiments to transfer training a specific model of reading comprehension ( AS Reader), in an artificial and well populated dataset in order to perform in another target dataset. \n\nHere is what I understand are their several experiments to transfer learning, but I am not 100% sure.\n1. The model is trained on the big artificial dataset and tested on the small target datasets (section 4.1)\n2. The model is pre-trained on the big artificial dataset like before, then fine-tuned on a few examples from the target dataset and tested on the remaining target examples. Several such models are trained using different sub-sets of fine-tuning examples. The results are tested against the performance of randomly intialized then fine-tuned models (section 4.2).\n3. The model is pre-trained on the big artificial dataset like before. The model is made of an embedding component and an encoder  component. Alternatively, each component is reset to a random initialization, to test the importance of the pre-training in each component. Then the model is fine-tuned on a few examples from the target dataset and tested on the remaining target examples. (section 4.3)\n\nI think what makes things difficult to follow is the fact that the test set is composed by several sub tasks, and sometimes what is reported is the mean performance across the tasks, sometimes the performance on a few tasks. Sometimes what we see is the mean performance of several models? You should report standard deviations also. Could you better explain what you mean by best validation ?\n\nInteresting and unpretentious work. The clarity of the presentation could be improved maybe by simplifying the experimental setup? The interesting conclusion I think is reported at the end of the section 4.1, when the nuanced difference between the datasets are exposed.\n\nMinor: unexplained acronyms: GRU, BT, CBT.\nbenfits p. 2\nsubsubset p. 6", "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "27 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "review", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "This paper proposes a study of transfer learning in the context of QA from stories. A system is presented with a a short story and has to answer a question about it. This paper studies how a system trained to answer questions on a dataset can eventually be used to answer questions from another dataset. The results are mostly negative: transfer seems almost non-existant.\n\nThis paper is centered around presenting negative results. Indeed the main hypothesis of transferring between QA datasets with the attention sum reader turns out impossible and one needs a small portion of labeled data from the target dataset to get meaningful performance.\n\nHaving only negative results could be fine if the paper was bringing some value with a sharp analysis of the failure modes and of the reasons behind it. Because this might indicate some research directions to follow. However, there is not much of that. \n\nThe answers to the pre-review questions actually start to give some insights: typing seems to be transferred for instance. How about the impact of syntax (very different between bAbI, Gutenberg books, and CNN news articles)? And the word/entity/ngrams distributions overlap between the 3 datasets?\n\nUnfortunately, there is not much to take-away from this paper.\n ", "IS_META_REVIEW": false, "RECOMMENDATION": 4, "DATE": "19 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "needs more thorough analysis", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "This work investigates the performance of transfer learning from resource-rich setup (BookTest, CNN/Daily Mail corpora) to low-resource (bAbI, SQuAD benchmarks) settings. Experiments show poor improvements in 0-shot learning. However, when the model is exposed to few training instances some improvements are observed.\n\nThe claims made here require a more comprehensive analysis. I criticize the use of bAbI as a low-resource real-world scenario. bAbI is designed as a unit test and is far from representing many natural language phenomena. Thus, the claims related to bAbI can only be weak evidence for questioning transfer learning high-resource to low-resource in real-world scenarios. I highly recommend using recently proposed real-world scenarios [1,2].\n\nMore importantly, the work does not explain why and how do we get improvement using transfer learning. They remotely address this by hypothesizing the knowledge of transfer is not just encoded in embeddings but also in the model. Considering the related work [3], these claims bring a marginal novelty and still \"how and why\" should be central in this work.  \n\n[1] ", "IS_META_REVIEW": false, "RECOMMENDATION": 3, "DATE": "17 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"DATE": "04 Dec 2016", "TITLE": "multi-task learning", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2"}, {"DATE": "02 Dec 2016", "TITLE": "Questions", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1"}, {"DATE": "02 Dec 2016", "TITLE": "qualitative analysis", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3"}, {"DATE": "07 Nov 2016", "TITLE": "ICLR Paper Format", "IS_META_REVIEW": false, "comments": "Dear Authors,\n\nPlease resubmit your paper in the ICLR 2017 format with the correct font for your submission to be considered. Thank you!", "OTHER_KEYS": "Tara N Sainath"}, {"IS_META_REVIEW": true, "comments": "This work investigates the performance of transfer learning from resource-rich setup (BookTest, CNN/Daily Mail corpora) to low-resource (bAbI, SQuAD benchmarks) settings. Experiments show poor improvements in 0-shot learning. However, when the model is exposed to few training instances some improvements are observed.\n\nThe claims made here require a more comprehensive analysis. I criticize the use of bAbI as a low-resource real-world scenario. bAbI is designed as a unit test and is far from representing many natural language phenomena. Thus, the claims related to bAbI can only be weak evidence for questioning transfer learning high-resource to low-resource in real-world scenarios. I highly recommend using recently proposed real-world scenarios [1,2].\n\nMore importantly, the work does not explain why and how do we get improvement using transfer learning. They remotely address this by hypothesizing the knowledge of transfer is not just encoded in embeddings but also in the model. Considering the related work [3], these claims bring a marginal novelty and still \"how and why\" should be central in this work.  \n\n[1]"}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "The area chair agrees with reviewers 1 and 3 that the paper does not meet the bar for ICLR. Reviewer 3 in particular points out how the paper can be strengthened for future revisions.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"DATE": "16 Jan 2017", "TITLE": "Any further thoughts?", "IS_META_REVIEW": false, "comments": "Dear authors and reviewers, this paper is currently very close to the decision boundary for acceptance and would benefit from a bit more discussion.\n\n", "OTHER_KEYS": "ICLR 2017 conference"}, {"TITLE": "Interesting ; needs to improve clarity", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "First I would like to apologize for the delay in reviewing.\n\nsummary : This work explores several experiments to transfer training a specific model of reading comprehension ( AS Reader), in an artificial and well populated dataset in order to perform in another target dataset. \n\nHere is what I understand are their several experiments to transfer learning, but I am not 100% sure.\n1. The model is trained on the big artificial dataset and tested on the small target datasets (section 4.1)\n2. The model is pre-trained on the big artificial dataset like before, then fine-tuned on a few examples from the target dataset and tested on the remaining target examples. Several such models are trained using different sub-sets of fine-tuning examples. The results are tested against the performance of randomly intialized then fine-tuned models (section 4.2).\n3. The model is pre-trained on the big artificial dataset like before. The model is made of an embedding component and an encoder  component. Alternatively, each component is reset to a random initialization, to test the importance of the pre-training in each component. Then the model is fine-tuned on a few examples from the target dataset and tested on the remaining target examples. (section 4.3)\n\nI think what makes things difficult to follow is the fact that the test set is composed by several sub tasks, and sometimes what is reported is the mean performance across the tasks, sometimes the performance on a few tasks. Sometimes what we see is the mean performance of several models? You should report standard deviations also. Could you better explain what you mean by best validation ?\n\nInteresting and unpretentious work. The clarity of the presentation could be improved maybe by simplifying the experimental setup? The interesting conclusion I think is reported at the end of the section 4.1, when the nuanced difference between the datasets are exposed.\n\nMinor: unexplained acronyms: GRU, BT, CBT.\nbenfits p. 2\nsubsubset p. 6", "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "27 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "review", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "This paper proposes a study of transfer learning in the context of QA from stories. A system is presented with a a short story and has to answer a question about it. This paper studies how a system trained to answer questions on a dataset can eventually be used to answer questions from another dataset. The results are mostly negative: transfer seems almost non-existant.\n\nThis paper is centered around presenting negative results. Indeed the main hypothesis of transferring between QA datasets with the attention sum reader turns out impossible and one needs a small portion of labeled data from the target dataset to get meaningful performance.\n\nHaving only negative results could be fine if the paper was bringing some value with a sharp analysis of the failure modes and of the reasons behind it. Because this might indicate some research directions to follow. However, there is not much of that. \n\nThe answers to the pre-review questions actually start to give some insights: typing seems to be transferred for instance. How about the impact of syntax (very different between bAbI, Gutenberg books, and CNN news articles)? And the word/entity/ngrams distributions overlap between the 3 datasets?\n\nUnfortunately, there is not much to take-away from this paper.\n ", "IS_META_REVIEW": false, "RECOMMENDATION": 4, "DATE": "19 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "needs more thorough analysis", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "This work investigates the performance of transfer learning from resource-rich setup (BookTest, CNN/Daily Mail corpora) to low-resource (bAbI, SQuAD benchmarks) settings. Experiments show poor improvements in 0-shot learning. However, when the model is exposed to few training instances some improvements are observed.\n\nThe claims made here require a more comprehensive analysis. I criticize the use of bAbI as a low-resource real-world scenario. bAbI is designed as a unit test and is far from representing many natural language phenomena. Thus, the claims related to bAbI can only be weak evidence for questioning transfer learning high-resource to low-resource in real-world scenarios. I highly recommend using recently proposed real-world scenarios [1,2].\n\nMore importantly, the work does not explain why and how do we get improvement using transfer learning. They remotely address this by hypothesizing the knowledge of transfer is not just encoded in embeddings but also in the model. Considering the related work [3], these claims bring a marginal novelty and still \"how and why\" should be central in this work.  \n\n[1] ", "IS_META_REVIEW": false, "RECOMMENDATION": 3, "DATE": "17 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"DATE": "04 Dec 2016", "TITLE": "multi-task learning", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2"}, {"DATE": "02 Dec 2016", "TITLE": "Questions", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1"}, {"DATE": "02 Dec 2016", "TITLE": "qualitative analysis", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3"}, {"DATE": "07 Nov 2016", "TITLE": "ICLR Paper Format", "IS_META_REVIEW": false, "comments": "Dear Authors,\n\nPlease resubmit your paper in the ICLR 2017 format with the correct font for your submission to be considered. Thank you!", "OTHER_KEYS": "Tara N Sainath"}]}
{"text": "IMPROVING NEURAL CONVERSATION MODELS\n1 INTRODUCTION\nChatbots are one of the classical applications of artificial intelligence and are now ubiquitous in technology, business and everyday life. Many corporate entities are now increasingly using chatbots to either replace or assist humans in customer service contexts. For example, Microsoft is currently actively building a chat bot to optimise and streamline its technical support service.\nIn these scenarios, there is usually an abundance of historical data since past conversations between customers and human customer service agents are usually recorded by organisations. An apparently straightforward solution would be to train chatbots to reproduce the responses by human agents using standard techniques such as maximum likelihood. While this seems natural, it is far from desirable for several reasons. It has been observed that such procedures have a tendency to produce very generic responses (Sordoni et al., 2015). For instance, when we trained chatbots via maximum likelihood on a restaurant recommendations dataset, they repeatedly output responses to the effect of How large is your group?, What is your budget? etc. Further, they also produce responses such as Let me look that up. or Give me a second. which, although permissible for a human agent to say, are not appropriate for a chatbot. Although there are ways to increase the diversity of responses (Li et al., 2015), our focus is on encouraging the bot to meaningfully advance the conversation. One way to address this problem is to provide some form of weak supervision for responses generated by a chatbot. For example, a human labeller, such as a quality assurance agent, could score each response generated by a chatbot in a conversation with a customer. This brings us to the reinforcement learning (RL) paradigm where these rewards (scores) are to be used to train a good chatbot. In this paper we will use the terms score, label, and reward interchangeably. Labelled data will mean conversations which have been assigned a reward of some form as explained above.\nNonetheless, there are some important differences in the above scenario when compared to the more popular approaches for RL.\n\u2022 Noisy and expensive rewards: Obtaining labels for each conversation can be time consuming and economically expensive. As a result, there is a limited amount of labelled data available. Moreover, labels produced by humans are invariably noisy due to human error and subjectivity.\n\u2022 Off-line evaluations: Unlike conventional RL settings, such as games, where we try to find the optimal policy while interacting with the system, the rewards here are not immediately available. Previous conversations are collected, labelled by human experts, and then given to an algorithm which has to manage with the data it has.\n\u2022 Unlabelled Data: While labelled data is limited, a large amount of unlabelled data is available.\nIf labelled data is in short supply, reinforcement learning could be hopeless. However, if unlabelled data can be used to train a decent initial bot, say via maximum likelihood, we can use policy iteration techniques to refine this bot by making local improvements using the labelled data (Bellman, 1956). Besides chatbots, this framework also finds applications in tasks such as question answering (Ferrucci et al., 2010; Hermann et al., 2015; Sachan et al., 2016), generating image descriptions (Karpathy & Fei-Fei, 2015) and machine translation (Bahdanau et al., 2014) where a human labeller can provide weak supervision in the form of a score to a sentence generated by a bot.\nTo contextualise the work in this paper, we make two important distinctions in policy iteration methods in reinforcement learning. The first is on-policy vs off-policy. In on-policy settings, the goal is to improve the current policy on the fly while exploring the space. On-policy methods are used in applications where it is necessary to be competitive (achieve high rewards) while simultaneously exploring the environment. In off-policy, the environment is explored using a behaviour policy, but the goal is to improve a different target policy. The second distinction is on-line vs batch (off-line). In on-line settings one can interact with the environment. In batch methods, which is the setting for this work, one is given past exploration data from possibly several behaviour policies and the goal is to improve a target policy using this data. On-line methods can be either on-policy or off-policy whereas batch methods are necessarily off-policy.\nIn this paper, we study reinforcement learning in batch settings, for improving chat bots with Seq2Seq recurrent neural network (RNN) architectures. One of the challenges when compared to on-line learning is that we do not have interactive control over the environment. We can only hope to do as well as our data permits us to. On the other hand, the batch setting affords us some luxuries. We can reuse existing data and use standard techniques for hyper-parameter tuning based on cross validation. Further, in on-line policy updates, we have to be able to \u201cguess\u201d how an episode will play out, i.e. actions the behaviour/target policies would take in the future and corresponding rewards. However, in batch learning, the future actions and rewards are directly available in the data. This enables us to make more informed choices when updating our policy.\nRELATED WORK\nRecently there has been a surge of interest in deep learning approaches to reinforcement learning, many of them adopting Q-learning, e.g. (He et al., 2015; Mnih et al., 2013; Narasimhan et al., 2015). In Q-learning, the goal is to estimate the optimal action value function Q\u2217. Then, when an agent is at a given state, it chooses the best greedy action according to Q\u2217. While Q-learning has been successful in several applications, it is challenging in the settings we consider since estimating Q\u2217 over large action and state spaces will require a vast number of samples. In this context, policy iteration methods are more promising since we can start with an initial policy and make incremental local improvements using the data we have. This is especially true given that we can use maximum likelihood techniques to estimate a good initial bot using unlabelled data.\nPolicy gradient methods, which fall within the paradigm of policy iteration, make changes to the parameters of a policy along the gradient of a desired objective (Sutton et al., 1999). Recently, the natural language processing (NLP) literature has turned its attention to policy gradient methods for improving language models. Ranzato et al. (2015) present a method based on the classical REINFORCE algorithm (Williams, 1992) for improving machine translation after preliminary training with maximum likelihood objectives. Bahdanau et al. (2016) present an actor-critic method also for machine translation. In both cases, as the reward, the authors use the BLEU (bilingual evaluation understudy) score of the output and the translation in the training dataset. This setting, where the rewards are deterministic and cheaply computable, does not reflect difficulties inherent to training chatbots where labels are noisy and expensive. Li et al. (2016) develop a policy gradient method bot for chatbots. However, they use user defined rewards (based on some simple rules) which, once again, are cheaply obtained and deterministic. Perhaps the closest to our work is that of Williams & Zweig (2016) who use a REINFORCE based method for chat bots. We discuss the differences of\nthis and other methods in greater detail in Section 3. The crucial difference between all of the above efforts and ours is that they use on-policy and/or on-line updates in their methods.\nThe remainder of this manuscript is organised as follows. In Section 2 we review Seq2Seq models and Markov decision processes (MDP) and describe our framework for batch reinforcement learning. Section 3 presents our method BPG and compares it with prior work in the RL and NLP literature. Section 4 presents experiments on a synthetic task and a customer service dataset for restaurant recommendations.\n2 PRELIMINARIES\n2.1 A REVIEW OF SEQ2SEQ MODELS\nThe goal of a Seq2Seq model in natural language processing is to produce an output sequence y = [a1, a2, . . . , aT ] given an input sequence x (Cho et al., 2014; Kalchbrenner & Blunsom, 2013; Sutskever et al., 2014). Here ai \u2208 A where A is a vocabulary of words. For example, in machine translation from French to English, x is the input sequence in French, and y is its translation in English. In customer service chatbots, x is the conversation history until the customer\u2019s last query and y is the response by an agent/chatbot. In a Seq2Seq model, we use an encoder network to represent the input sequence as a euclidean vector and then a decoder network to convert this vector to an output sequence. Typically, both the encoder and decoder networks are recurrent neural networks (RNN) (Mikolov et al., 2010) where the recurrent unit processes each word in the input/output sequences one at a time. In this work, we will use the LSTM (long short term memory) (Hochreiter & Schmidhuber, 1997) as our recurrent unit due to its empirical success in several applications.\nIn its most basic form, the decoder RNN can be interpreted as assigning a probability distribution over A given the current \u201cstate\u201d. At time t, the state st is the input sequence x and the words yt\u22121 = [a1, . . . , at\u22121] produced by the decoder thus far, i.e. st = (x, yt\u22121). We sample the next word at from this probability distribution \u03c0(\u00b7|st), then update our state st+1 = (x, yt) where yt = [yt\u22121, at], and proceed in a similar fashion. The vocabulary A contains an end-of-statement token <EOS>. If we sample <EOS> at time T + 1, we terminate the sequence and output yT .\n2.2 A REVIEW OF MARKOV DECISION PROCESSES (MDP)\nWe present a formalism for MDPs simplified to our setting. In an MDP, an agent takes an action a in a state s and transitions to a state s\u2032. An episode refers to a sequence of transitions s1 \u2192 a1 \u2192 s2 \u2192 a2 \u2192 \u00b7 \u00b7 \u00b7 \u2192 aT \u2192 sT+1 until the agent reaches a terminal state sT+1. At a terminal state, the agent receives a reward. Formally, an MDP is the triplet (S,A, R). Here, S is a set of states andA is a set of actions. When we take an action a at state s we transition to a new state s\u2032 = s\u2032(s, a) which, in this work, will be deterministic. A will be a finite but large discrete set and S will be discrete but potentially infinite. R : S \u2192 R is the expected reward function such that when we receive a reward r at state s \u2208 S, E[r] = R(s). Let S0 \u2282 S be a set of terminal states. When we transition to any s \u2208 S0, the episode ends. In this work, we will assume that the rewards are received only at a terminal state, i.e R(s) is nonzero only on S0. A policy \u03c0 is a rule to select an action at a given state. We will be focusing on stochastic policies \u03c0 : A\u00d7S \u2192 R+ where \u03c0(a|s) denotes the probability an agent will execute action a at state s. We define the value function V \u03c0 : S \u2192 R of policy \u03c0, where V (s) is the expected reward at the end of the episode when we follow policy \u03c0 from state s. For any terminal state s \u2208 S0, V \u03c0(s) = R(s) regardless of \u03c0. We will also find it useful to define the action-value function Q\u03c0 : S \u00d7 A :\u2192 R, where Q\u03c0(s, a) is the expected reward of taking action a at state s and then following policy \u03c0. With deterministic state transitions this is simply Q\u03c0(s, a) = V \u03c0(s\u2032(s, a)). It can be verified that V \u03c0(s) = Ea\u223c\u03c0(\u00b7|s) [Q\u03c0(s, a)] (Sutton & Barto, 1998).\n2.3 SET UP\nWe now frame our learning from labels scenario for RNN chatbots as an MDP. The treatment has similarities to some recent RL work in the NLP literature discussed above.\nLet x be the input and yt\u22121 = [a1, . . . , at\u22121] be the words output by the decoder until time t. The state of our MDP at time t of the current episode will be st = (x, yt\u22121). Therefore, the set of states S will be all possible pairs of inputs and partial output sequences. The actions A will be the vocabulary. The terminal states S0 will be (x, y) such that the last literal of y is <EOS>. The stochastic policy \u03c0 will be a Seq2Seq RNN which produces a distribution overA given state st. When we wish to make the dependence of the policy on the RNN parameters \u03b8 explicit, we will write \u03c0\u03b8. When we sample an action at \u223c \u03c0(\u00b7|st), we deterministically transition to state (x, [yt\u22121, at]). If we sample aT+1 = <EOS> at time T + 1, the episode terminates and we observe a stochastic reward.\nWe are given a dataset of input-output-reward triples {(x(i), y(i), r(i))}ni=1 where y(i) = (a\n(i) 1 , . . . , a (i) Ti , <EOS>) is the sequence of output words. This data was collected from possibly multiple behaviour policies which output y(i) for the given input x(i). In the above customer service example, the behaviour policies could be chatbots, or even humans, which were used for conversations with a customer. The rewards ri are scores assigned by a human quality assurance agent to each response of the chatbot. Our goal is to use this data to improve a given target policy \u03c0\u03b8. We will use q to denote the distribution of the data. q(s) is the distribution of the states in the dataset, q(a|s) is the conditional distribution of an action given a state, and q(s, a) = q(s)q(a|s) is the joint distribution over states and actions. q will be determined by the initial distribution of the inputs x(i) and the behaviour policies used to collect the training data. Our aim is to find a policy that does well with respect to q. Specifically, we wish to maximise the following objective,\nJ(\u03b8) = \u2211 s\u2208S q(s)V \u03c0\u03b8 (s). (1)\nHere, the value function V \u03c0\u03b8 is not available to us but has to be estimated from the data. This is similar to objectives used in on-line off-policy policy gradient literature where q is replaced by the limiting distribution of the behaviour policy (Degris et al., 2012). In the derivation of our algorithm, we will need to know q(a|s) to compute the gradient of our objective. In off-policy reinforcement learning settings this is given by the behaviour policy which is readily available. If the behaviour policy if available to us, then we can use this too. Otherwise, a simple alternative is to \u201clearn\u201d a behaviour policy. For example, in our experiments we used an RNN trained using the unlabelled data to obtain values for q(a|s). As long as this learned policy can capture the semantics of natural language (for example, the word apple is more likely than car when the current state is (x, I ate an)), then it can be expected to do reasonably well. In the following section, we will derive a stochastic gradient descent (SGD) procedure that will approximately minimise (1).\nBefore we proceed, we note that it is customary in the RL literature to assume stochastic transitions between states and use rewards at all time steps instead of the terminal step. Further, the future rewards are usually discounted by a discount factor \u03b3 < 1. While we use the above formalism to simplify the exposition, the ideas presented here extend naturally to more conventional settings.\n3 BATCH POLICY GRADIENT\nOur derivation follows the blueprint in Degris et al. (2012) who derive an off-policy on-line actor critic algorithm. Following standard policy gradient methods, we will aim to update the policy by taking steps along the gradient of the objective\u2207J(\u03b8).\n\u2207J(\u03b8) = \u2207Es\u223cq [\u2211 a\u2208A \u03c0\u03b8(a|s)Q\u03c0\u03b8 (s, a) ] = Es\u223cq [\u2211 a\u2208A \u2207\u03c0\u03b8(a|s)Q\u03c0\u03b8 (s, a) + \u03c0\u03b8(a|s)\u2207Q\u03c0\u03b8 (s, a) ] .\nThe latter term inside the above summation is difficult to work with, so the first step is to ignore it and work with the approximate gradient g(\u03b8) = Es\u223cq[ \u2211 a\u2208A\u2207\u03c0\u03b8(a|s)Q\u03c0\u03b8 (s, a)] \u2248 \u2207J(\u03b8). Degris et al. (2012) provide theoretical justification for this approximation in off policy settings by establishing that J(\u03b8) \u2264 J(\u03b8 + \u03b1g(\u03b8)) for all small enough \u03b1. Expanding on g(\u03b8), we obtain:\ng(\u03b8) = Es\u223cq [\u2211 a\u2208A \u03c0\u03b8(a|s) \u2207\u03c0\u03b8(a|s) \u03c0\u03b8(a|s) Q\u03c0\u03b8 (s, a) ] = E s\u223cq a\u223cq(\u00b7|s) [ \u03c1(s, a)\u03c8(a, s)Q\u03c0\u03b8 (s, a) ] = E(st,at)\u223cq(\u00b7,\u00b7) [\u03c1(st, at)\u03c8(at, st)(Q \u03c0\u03b8 (st, at)\u2212 V \u03c0\u03b8 (st))] . (2)\nHere \u03c8(a, s) = \u2207\u03c0\u03b8(a|s)\u03c0\u03b8(a|s) = \u2207 log \u03c0\u03b8(a|s) is the score function of the policy and \u03c1(s, a) = \u03c0\u03b8(a|s)/q(a|s) is the importance sampling coefficient. In the last step, we have used the fact that E[\u03c0(a|s)\u03c8(a|s)h(s)] = 0 for any function h : S \u2192 R of the current state (Szepesva\u0301ri, 2010). The purpose of introducing the value function V \u03c0\u03b8 is to reduce the variance of the SGD updates \u2013 we want to assess how good/bad action at is relative to how well \u03c0\u03b8 will do at state st in expectation. If at is a good action (Q\u03c0\u03b8 (st, at) is large relative to V \u03c0\u03b8 (st)), the coefficient of the score function is positive and it will change \u03b8 so as to assign a higher probability to action at at state st.\nThe Q\u03c0\u03b8 , V \u03c0\u03b8 functions are not available to us so we will replace them with estimates. For V \u03c0\u03b8 (st) we will use an estimate V\u0302 (st) \u2013 we will discuss choices for this shortly. However, the action value function is usually not estimated in RL policy gradient settings to avoid the high sample complexity. A sensible stochastic approximation for Q\u03c0\u03b8 (st, at) is to use the sum of future rewards from the current state (Sutton & Barto, 1998)1. If we receive reward r at the end of the episode, we can then use Q\u03c0\u03b8 (st, at) \u2248 r for all time steps t in the episode. However, since q(at|st) is different from \u03c0\u03b8(at|st) we will need to re-weight future rewards via importance sampling r \u220fT i=t \u03c1(si, ai). This is to account for the fact that an action a given s may have been more likely under the policy \u03c0\u03b8(\u00b7|s) than it was under q(\u00b7|s) or vice versa. Instead of directly using the re-weighted rewards, we will use the so called \u03bb\u2013return which is a convex combination of the re-weighted rewards and the value function (Sutton, 1988; 1984). In our setting, they are defined recursively from the end of the episode t = T + 1 to t = 1 as follows. For \u03bb \u2208 (0, 1],\nr\u0303\u03bbT+1 = r, r\u0303 \u03bb t = (1\u2212 \u03bb)V \u03c0\u03b8 (st+1) + \u03bb\u03c1(st, at)r\u0303\u03bbt+1 for t = T, . . . , 1. (3)\nThe purpose of introducing \u03bb is to reduce the variance of using the future rewards alone as an estimate for Q\u03c0\u03b8 (st, at). This is primarily useful when rewards are noisy. If the rewards are deterministic, \u03bb = 1 which ignores the value function is the best choice. In noisy settings, it is recommended to use \u03bb < 1 (see Sec 3.1 of (Szepesva\u0301ri, 2010)). In our algorithm, we will replace r\u0303\u03bbt with r \u03bb t where V \u03c0\u03b8 is replaced with the estimate V\u0302 . Putting it all together, and letting \u03b1 denote the step size, we have the following update rule for the parameters \u03b8 of our policy:\n\u03b8 \u2190 \u03b8 + \u03b1\u03c1(st, at)\u03c8(st, at)(r\u03bbt \u2212 V\u0302 (st)). In Algorithm 1, we have summarised the procedure where the updates are performed after an entire pass through the dataset. In practice, we perform the updates in mini-batches.\nAn Estimator for the Value Function: All that is left to do is to specify an estimator V\u0302 for the value function. We first need to acknowledge that this is a difficult problem: S is quite large and for typical applications for this work there might not be enough data since labels are expensive. That said, the purpose of V\u0302 in (2), (3) is to reduce the variance of our SGD updates and speed up convergence so it is not critical that this be precise \u2013 even a bad estimator will converge eventually. Secondly, standard methods for estimating the value function based on minimising the projected Bellman error require the second derivatives, which might be intractable for highly nonlinear parametrisations of V\u0302 (Maei, 2011). For these two statistical and computational reasons, we resort to simple estimators for V \u03c0\u03b8 . We will study two options. The first is a simple heuristic used previously in the RL literature, namely a constant estimator for V\u0302 which is equal to the mean of all rewards in the dataset (Williams, 1992). The second uses the parametrisation V\u0302 (s) = \u03c3(\u03be>\u03c6(s)) where \u03c3 is the logistic function and \u03c6(s) \u2208 Rd is a Euclidean representation of the state. For V\u0302 (s) of the above form, the Hessian \u22072\u03beV\u0302 (s) can be computed in O(d) time. To estimate this value function, we use the GTD(\u03bb) estimator from Maei (2011). As \u03c6(s) we will be using the hidden state of the LSTM. The rationale for this is as follows. In an LSTM trained using maximum likelihood, the hidden state contains useful information about the objective. If there is overlap between the maximum likelihood and reinforcement learning objectives, we can expect the hidden state to also carry useful information about the RL objective. Therefore, we can use the hidden state to estimate the value function whose expectation is the RL objective. We have described our implementation of GTD(\u03bb) in Appendix A and specified some implementation details in Section 4.\n1 Note Q\u03c0\u03b8 (st, at) = V \u03c0\u03b8 (st+1) for deterministic transitions. However, it is important not to interpret the term in (2) as the difference in the value function between successive states. Conditioned on the current time step, V \u03c0\u03b8 (st) is deterministic, while V \u03c0\u03b8 (st+1) is stochastic. In particular, while a crude estimate suffices for the former, the latter is critical and should reflect the rewards received during the remainder of the episode.\nAlgorithm 1 Batch Policy Gradient (BPG) Given: Data {(xi, yi, ri)}ni=1, step size \u03b1, return coefficient \u03bb, initial \u03b80.\n\u2013 Set \u03b8 \u2190 \u03b80. \u2013 For each epoch k = 1, 2, . . .\nI Set \u2206\u03b8 \u2190 0 I For each episode i = 1, . . . , n \u2022 r\u03bbT+1 \u2190 ri \u2022 \u03c1t \u2190 \u03c0\u03b8(a(i)t |s (i) t )/q(a (i) t |s (i) t ) for t = 1, . . . , T\n(i). \u2022 For each time step in reverse t = T (i), . . . , 1\n(i) r\u03bbt \u2190 (1\u2212 \u03bb)V\u0302 (s (i) t+1) + \u03bb\u03c1tr \u03bb t+1\n(ii) \u2206\u03b8 \u2190 \u2206\u03b8 + 1 T (i) \u03c1t\u03c8(s (i) t , a (i) t )(r \u03bb t \u2212 V\u0302 (s (i) t ))\n(iii) Compute updates for the value function estimate V\u0302 . I Update the policy \u03b8 \u2190 \u03b8 + \u03b1\u2206\u03b8 I Update the value function estimate V\u0302 .\nCOMPARISON WITH OTHER RL APPROACHES IN NLP\nPolicy gradient methods have been studied extensively in on policy settings where the goal is to improve the current policy on the fly (Amari, 1998; Williams, 1992). To our knowledge, all RL approaches in Seq2Seq models have also adopted on-policy policy gradient updates (Bahdanau et al., 2016; Li et al., 2016; Ranzato et al., 2015; Williams & Zweig, 2016). However, on policy methods break down in off-policy settings, because any update must account for the probability of the action under the target policy. For example, suppose the behaviour policy took action a at state s and received a low reward. Then we should modify the target policy \u03b8 so as to reduce \u03c0\u03b8(a|s). However, if the target policy is already assigning low probability to a|s then we should not be as aggressive when making the updates. The re-weighting \u03c1(s, a) via importance sampling does precisely this.\nA second difference is that we study batch RL. Standard on-line methods are designed for settings where we have to continually improve the target while exploring using the behaviour policy. Critical to such methods are the estimation of future rewards at the current state and the future actions that will be taken by both the behaviour and target policies. In order to tackle this, previous research either ignore future rewards altogether (Williams, 1992), resort to heuristics to distribute a delayed reward to previous time steps (Bahdanau et al., 2016; Williams & Zweig, 2016), or make additional assumptions about the distribution of the states such as stationarity of the Markov process (Degris et al., 2012; Maei, 2011). However, in batch settings, the \u03bb-return from a given time step can be computed directly (3) since the future action and rewards are available in the dataset. Access to this information provides a crucial advantage over techniques designed for on-line settings.\n4 EXPERIMENTS\nImplementation Details: We implement our methods using Chainer (Tokui et al., 2015), and group sentences of the same length together in the same batch to make use of GPU parallelisation. Since different batches could be of different length, we do not normalise the gradients by the batch size as we should take larger steps after seeing more data. However, we normalise by the length of the output sequence to allocate equal weight to all sentences. We truncate all output sequences to length 64 and use a maximum batch size of 32. We found it necessary to use a very small step size (10\u22125), otherwise the algorithm has a tendency to get stuck at bad parameter values. While importance reweighting is necessary in off-policy settings, it can increase the variance of the updates, especially when q(at|st) is very small. A common technique to alleviate this problem is to clip the \u03c1(st, at) value (Swaminathan & Joachims, 2015). In addition to single \u03c1(st, at) values, our procedure has a product of \u03c1(st, at) values when computing the future rewards (3). The effect of large \u03c1 values is a large weight \u03c1t(r\u03bbt \u2212V\u0302 (st)) for the score function in step (ii) of Algorithm 1. In our implementation,\nLSTM LSTM LSTM\nLSTM LSTM LSTM\nsoftmax softmax softmax\nLSTM LSTM\nLSTM LSTM\nEncoder Decoder\nwe clip this weight at 5 which controls the variance of the updates and ensures that a single example does not disproportionately affect the gradient.\nRNN Design: In both experiments we use deep LSTMs with two layers for the encoder and decoder RNNs. The output of the bottom layer is fed to the top layer and in the decoder RNN, the output of the top layer is fed to a softmax layer of size |A|. When we implement GTD(\u03bb) to estimate V \u03c0\u03b8 we use the hidden state of the bottom LSTM as \u03c6(s). When performing our policy updates, we only change the parameters of the top LSTM and the softmax layer in our decoder RNN. If we were to change the bottom LSTM too, then the state representation \u03c6(s) would also change as the policy changes. This violates the MDP framework. In other words, we treat the bottom layer as part of the environment in our MDP. To facilitate a fair comparison, we only modify the top LSTM and softmax layers in all methods. We have illustrated this set up in Fig. 1. We note that if one is content with using the constant estimator, then one can change all parameters of the RNN.\n4.1 SOME SYNTHETIC EXPERIMENTS ON THE EUROPARL DATASET\nTo convey the main intuitions of our method, we compare our methods against other baselines on a synthetic task on the European parliament proceedings corpus (Koehn, 2005). We describe the experimental set up briefly, deferring details to Appendix B.1. The input sequence to the RNN was each sentence in the dataset. Given an input, the goal was to reproduce the words in the input without repeating words in a list of forbidden words. The RL algorithm does not explicitly know either goal of the objective but has to infer it from the stochastic rewards assigned to input output sequences in the dataset. We used a training set of 500 input-output-reward triplets for the RL methods.\nWe initialised all methods by maximum likelihood training on 6000 input output sequences where the output sequence was the reverse of the input sequence. The maximum likelihood objective captures part of the RL objective. This set up reflects naturally occurring practical scenarios for the algorithm where a large amount unlabelled data can be used to bootstrap a policy if the maximum likelihood and reinforcement learning objectives are at least partially aligned. We trained the RL algorithms for 200 epochs on the training set. At the end of each epoch, we generated outputs from the policy on test set of 500 inputs and scored them according to our criterion. We plot the test set error against the number of epochs for various methods in Fig. 2.\nFig. 2(a) compares 3 methods: BPG with and without maximum likelihood initialisation and a version of BPG which does not use importance sampling. Clearly, bootstrapping an RL algorithm with ML can be advantageous especially if data is abundantly available for ML training. Further, without importance sampling, the algorithm is not as competitive for reasons described in Section 3. In all 3 cases, we used a constant estimator for V\u0302 and \u03bb = 0.5. The dashed line indicates the performance of ML training alone. BPG-NIS is similar to the algorithms of Ranzato et al. (2015); Williams & Zweig (2016) except that there, their methods implicitly use \u03bb = 1.\nFig. 2(b) compares 4 methods: BPG and its on-line version OPG with constant (CONST) and GTD(\u03bb) estimators for V\u0302 . The on-line versions of the algorithms are a direct implementation of the method in Degris et al. (2012) which do not use the future rewards as we do. The first observation is that while GTD(\u03bb) is slightly better in the early iterations, it performs roughly the same as using a constant estimator in the long run. Next, BPG performs significantly better than OPG. We believe this is due to the following two reasons. First, the online updates assume stationarity of the MDP. When this does not hold, such as in limited data instances like ours, the SGD updates can be\nvery noisy. Secondly, the value function estimate plays a critical role in the online version. While obtaining a reliable estimate V\u0302 is reasonable in on-line settings where we can explore indefinitely to collect a large number of samples, it is difficult when one only has a limited number of labelled samples. Finally, we compare BPG with different choices for \u03bb in Fig. 2(c). As noted previously, \u03bb < 1 is useful with stochastic rewards, but choosing too small a value is detrimental. The optimal \u03bb value may depend on the problem.\n4.2 RESTAURANT RECOMMENDATIONS\nWe use data from an on-line restaurant recommendation service. Customers log into the service and chat with a human agent asking recommendations for restaurants. The agents ask a series of questions such as food preferences, group size etc. before recommending a restaurant. The goal is to train a chatbot (policy) which can replace or assist the agent. For reasons explained in Section 1, maximum likelihood training alone will not be adequate. By obtaining reward labels for responses produced by various other bots, we hope to improve on a bot initialised using maximum likelihood.\nData Collection: We collected data for RL as follows. We trained five different RNN chatbots with different LSTM parameters via maximum likelihood on a dataset of 6000 conversations from this dataset. The bots were trained to reproduce what the human agent said (output y) given the past conversation history (input x). While the dataset is relatively small, we can still expect our bots to do reasonably well since we work in a restricted domain. Next, we generated responses from these bots on 1216 separate conversations and had them scored by workers on Amazon Mechanical Turk (AMT). For each response by the bots in each conversation, the workers were shown the history before the particular response and asked to score (label) each response on a scale of 0\u2212 1\u2212 2. We collected scores from three different workers for each response and used the mean as the reward.\nPolicies and RL Application: Next, we initialised 2 bots via maximum likelihood and then used BPG to improve them using the labels collected from AMT. For the 2 bots we used the following LSTM hidden state size H , word embedding size E and BPG parameters. These parameters were chosen arbitrarily and are different from those of the bots used in data collection described above.\n\u2022 Bot-1: H = 512, E = 256. BPG: \u03bb = 0.5, GTD(\u03bb) estimator for V\u0302 .\n\u2022 Bot-2: H = 400, E = 400. BPG: \u03bb = 0.5, constant estimator for V\u0302 .\nTesting: We used a separate test set of 500 conversations which had a total of more than 3500 inputoutput (conversation history - response) pairs. For each Bot-1 and Bot-2 we generated responses before and after applying BPG, totalling 4 responses per input. We then had them scored by workers on AMT using the same set up described above. The same worker labels the before-BPG and afterBPG responses from the same bot. This controls spurious noise effects and allows us to conduct a paired test. We collected 16, 808 before and after label pairs each for Bot-1 and Bot-2 and compare them using a paired t-test and a Wilcoxon signed rank test.\nResults: The results are shown in Table 1. The improvements on Bot-2 are statistically significant at the 10% level on both tests, while Bot-1 is significant on the Wilcoxon test. The large p-values for Bot-1 are due to the noisy nature of AMT experiments and we believe that we can attain significance if we collect more labels which will reduce the standard error in both tests. In Appendix B.2 we present some examples of conversation histories and the responses generated by the bots before and after applying BPG. We qualitatively discuss specific kinds of issues that we were able to overcome via reinforcement learning.\n5 CONCLUSION\nWe presented a policy gradient method for batch reinforcement learning to train chatbots. The data to this algorithm are input-output sequences generated using other chatbots/humans and stochastic rewards for each output in the dataset. This setting arises in many applications, such as customer service systems, where there is usually an abundance of unlabelled data, but labels (rewards) are expensive to obtain and can be noisy. Our algorithm is able to efficiently use minimal labelled data to improve chatbots previously trained through maximum likelihood on unlabelled data. While our method draws its ideas from previous policy gradient work in the RL and NLP literature, there are some important distinctions that contribute to its success in the settings of interest for this work. Via importance sampling we ensure that the probability of an action is properly accounted for in off-policy updates. By explicitly working in the batch setting, we are able to use knowledge of future actions and rewards to converge faster to the optimum. Further, we use the unlabelled data to initialise our method and also learn a reasonable behaviour policy. Our method outperforms baselines on a series of synthetic and real experiments.\nThe ideas presented in this work extend beyond chatbots. They can be used in applications such as question answering, generating image descriptions and machine translation where an output sentence generated by a policy is scored by a human labeller to provide a weak supervision signal.\nACKNOWLEDGEMENTS\nWe would like to thank Christoph Dann for the helpful conversations and Michael Armstrong for helping us with the Amazon Mechanical Turk experiments.\nAPPENDIX\nA IMPLEMENTATION OF GTD(\u03bb)\nWe present the details of the GTD(\u03bb) algorithm (Maei, 2011) to estimate a value function in Algorithm 2. However, while Maei (2011) give an on-line version we present the batch version here where the future rewards of an episode are known. We use a parametrisation of the form V\u0302 (s) = V\u0302\u03be(s) = \u03c3(\u03be\n>\u03c6(s)) where \u03be \u2208 Rd is the parameter to be estimated. \u03c3(z) = 1/(1 + e\u2212z) is the logistic function.\nThe algorithm requires two step sizes \u03b1\u2032, \u03b1\u2032\u2032 below for the updates to \u03be and the ancillary parameter w. Following the recommendations in Borkar (1997), we use \u03b1\u2032\u2032 \u03b1. In our implementations, we used \u03b1\u2032 = 10\u22125 and \u03b1\u2032\u2032 = 10\u22126. When we run BPG, we perform steps (a)-(f) of Algorithm 2 in step (iii) of Algorithm 1 and the last two update steps of Algorithm 2 in the last update step of Algorithm 1.\nThe gradient and Hessian of V\u0302\u03be have the following forms,\n\u2207\u03beV\u0302\u03be(s) = V\u0302\u03be(s)(1\u2212 V\u0302\u03be(s))\u03c6(s), \u22072\u03beV\u0302\u03be(s) = V\u0302\u03be(s)(1\u2212 V\u0302\u03be(s))(1\u2212 2V\u0302\u03be(s))\u03c6(s)\u03c6(s)>.\nThe Hessian product in step (d) of Algorithm 2 can be computed in O(d) time via, \u22072\u03beV\u0302\u03be(s) \u00b7 w = [ V\u0302\u03be(s)(1\u2212 V\u0302\u03be(s))(1\u2212 2V\u0302\u03be(s))(\u03c6(s)>w) ] \u03c6(s).\nAlgorithm 2 GTD(\u03bb) Given: Data {(xi, yi, ri)}ni=1, step sizes \u03b1\u2032, \u03b1\u2032\u2032, return coefficient \u03bb, initial \u03be0.\n\u2013 Set \u03be \u2190 \u03be0, w \u2190 0. \u2013 For each epoch k = 1, 2, . . .\nI Set \u2206\u03be \u2190 0, \u2206w \u2190 0. I For each episode i = 1, . . . , n \u2022 Set r\u03bbT+1 \u2190 ri, g\u03bbT+1 \u2190 0, q\u03bbT+1 \u2190 0 \u2022 \u03c1t \u2190 \u03c0\u03b8(a(i)t |s (i) t )/q(a (i) t |s (i) t ) for t = 1, . . . , T\n(i). \u2022 For each time step in reverse t = T (i), . . . , 1:\n(a) g\u03bbt \u2190 \u03c1t ( (1\u2212 \u03bb)V\u0302\u03be(s(i)t+1) + \u03bb\u03c1tr\u03bbt+1 )\n(b) q\u03bbt \u2190 \u03c1t ( (1\u2212 \u03bb)\u2207\u03beV\u0302\u03be(s(i)t+1) + \u03bbq\u03bbt+1 )\n(c) \u03b4t \u2190 g\u03bbt \u2212 V\u0302\u03be(s (i) t ) (d) ht \u2190 ( \u03b4t \u2212 w>\u2207\u03beV\u0302\u03be(s(i)t ) ) \u22072\u03beV\u0302\u03be(s (i) t ) \u00b7 w\n(e) \u2206w \u2190 \u2206w + 1 T (i) ( \u03b4t \u2212 w>\u2207\u03beV\u0302\u03be(s(i)t ) ) \u2207\u03beV\u0302\u03be(s(i)t )\n(f) \u2206\u03be \u2190 \u2206\u03be + 1 T (i) ( \u03b4t\u2207\u03beV\u0302\u03be(s(i)t )\u2212 q\u03bbt w>\u2207\u03beV\u0302\u03be(s (i) t )\u2212 ht ) I w \u2190 w + \u03b1\u2032\u2032\u2206w. I \u03be \u2190 \u03be + \u03b1\u2032\u2206\u03be.\nB ADDENDUM TO EXPERIMENTS\nB.1 DETAILS OF THE SYNTHETIC EXPERIMENT SET UP\nGiven an input and output sequence, we used the average of five Bernoulli rewards Bern(r), where the parameter r was r = 0.75 \u00d7 rr + 0.25 \u00d7 rf. Here rr was the fraction of common words in the input and output sequences while rf = 0.01pf where pf is the fraction of forbidden words in the dataset. As the forbidden words, we used the 50 most common words in the dataset. So if an input\nhad 10 words of which 2 were forbidden, an output sequence repeating 7 of the allowed words and 1 forbidden word would receive an expected score of 0.75\u00d7 (8/10) + 0.25\u00d7 0.01(1/8) = 0.7406. The training and testing set for reinforcement learning were obtained as follows. We trained 4 bots using maximum likelihood on 6000 input output sequences as indicated in Section 4.1. The LSTM hidden state size H and word embedding size E for the 4 bots were, (H,E) = (256, 128), (128, 64), (64, 32), (32, 16). The vocabulary size was |A| = 12000. We used these bots to generate outputs for 500 different input sequences each. This collection of input and output pairs was scored stochastically as described above to produce a pool of 2000 input-output-score triplets. From this pool we use a fixed set of 500 triplets for testing across all our experiments. From the remaining 1500 data points, we randomly select 500 for training for each execution of an algorithm. For all RL algorithms, we used an LSTM with 16 layers and 16 dimensional word embeddings.\nB.2 ADDENDUM TO THE AMT RESTAURANT RECOMMENDATIONS EXPERIMENT\nMORE DETAILS ON THE EXPERIMENTAL SET UP\nWe collected the initial batch of training data for RL as follows: We trained, via maximum likelihood on 6000 conversations, five RNN bots whose LSTM hidden sizeH and word embedding sizeE were (H,E) = (512, 512), (256, 256), (128, 128), (512, 256), (256, 64). The inputs x were all words from the history of the conversation truncated at length 64, i.e. the most recent 64 words in the conversation history. The outputs were the actual responses of the agent which were truncated to length 64. As the vocabulary we use the |A| = 4000 most commonly occurring words in the dataset and replace the rest with an <UNK> token.\nUsing the bots trained this way we generate responses on 1216 separate conversations. This data was sent to AMT workers who were asked to label the conversations on the following scale.\n\u2022 2: The response is coherent and appropriate given the history and advances the conversation forward.\n\u2022 1: The response has some minor flaws but is discernible and appropriate. \u2022 0: The response is either completely incoherent or inappropriate and fails to advance the\nconversation forward.\nSOME QUALITATIVE RESULTS\nIn Tables 2 and 3 we have presented some examples. The text in black/grey shows the conversation history, the response in blue is by the bot trained via maximum likelihood (ML) alone and in red is the bot after improvement using our BPG reinforcement learning algorithm. The first two examples of Table 2 present examples where the ML algorithm repeated generic questions (on budget, group size etc.) even though they had already been answered previously. After applying BPG, we are able to correct such issues, even though there are some grammatical errors. In the second, third and fourth example, we see that the ML+BPG bot is able to take context into consideration well when responding. For example, the customer asks for oriental/Mexican/Italian food. While the ML bot doesn\u2019t take this into consideration, the ML+BPG bot is able to provide relevant answers. However, in the third example, the name of the restaurant suggests that the food might be Indian and not Mexican. In the final example of Table 2 the customer asks a direct question about smoking. The ML bot provides an irrelevant answer where as the ML+BPG bot directly responds to the question.\nIn some examples, the ML bot had a tendency to produce sentences that were grammatically correct but nonsensical, sensible but grammatically incorrect, or just complete gibberish. We were able to correct such issues via RL. The first three examples of Table 3 present such cases. Occasionally the opposite happened. The last example of Table 3 is one such instance.\n", "cats": {"READ": 1.0, "DONTREAD": 0.0}, "reviews": [{"IS_META_REVIEW": true, "comments": "The paper discuss a \"batch\" method for RL setup to improve chat-bots.\nThe authors provide nice overview of the RL setup they are using and present an algorithm which is similar to previously published on line setup for the same problem. They make a comparison to the online version and explore several modeling choices. \n\nI find the writing clear, and the algorithm a natural extension of the online version.\n\nBelow are some constructive remarks:\n- Comparison of the constant vs. per-state value function: In the artificial experiment there was no difference between the two while on the real-life task there was. It will be good to understand why, and add this to the discussion. Here is one option:\n- For the artificial task it seems like you are giving the constant value function an unfair advantage, as it can update all the weights of the model, and not just the top layer, like the per-state value function.\n- section 2.2:\n   sentence before last: s' is not defined. \n   last sentence: missing \"... in the stochastic case.\" at the end.\n- Section 4.1 last paragraph: \"While Bot-1 is not significant ...\" => \"While Bot-1 is not significantly different from ML ...\""}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "This is an interesting and timely paper combining off-policy learning with seq2seq models to train a chatbot on a restaurant reservation task, using labels collected through Amazon Mechanical Turk while using the bot with a baseline maximum likelihood policy. \n The paper is clear, well-written and well-executed. Although the improvements are modest and the actual novelty of the paper is limited (combining known pieces in a rather straightforward way), this is still an interesting and informative read, and will probably be of interest to many people at ICLR.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"DATE": "03 Jan 2017", "TITLE": "Revisions", "IS_META_REVIEW": false, "comments": "Once again, we'd like to thank all reviewers for the feedback. We have updated the manuscript accordingly. The changes are done in magenta so that it would be easier to identify them.\n\n@Reviewer2:\ns' is defined in the first para of section 2.2. Also, the expectation in the statement at the end of section 2.2 is to account for the stochasticity in the rewards.", "OTHER_KEYS": "Kirthevasan Kandasamy"}, {"TITLE": "Review", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer4", "comments": "This paper extends neural conversational models into the batch reinforcement learning setting. The idea is that you can collect human scoring data for some responses from a dialogue model, however such scores are expensive. Thus, it is natural to use off-policy learning \u2013 training a base policy on unsupervised data, deploying that policy to collect human scores, and then learning off-line from those scores.\n\nWhile the overall contribution is modest (extending off-policy actor-critic to the application of dialogue generation), the approach is well-motivated, and the paper is written clearly and is easy to understand. \n\nMy main concern is that the primary dataset used (restaurant recommendations) is very small (6000 conversations). In fact, it is several orders of magnitude smaller than other datasets used in the literature (e.g. Twitter, the Ubuntu Dialogue Corpus) for dialogue generation. It is a bit surprising to me that RNN chatbots (with no additional structure) are able to generate reasonable utterances on such a small dataset. Wen et al. (2016) are able to do this on a similarly small restaurant dataset, but this is mostly because they map directly from dialogue states to surface form, rather than some embedding representation of the context. Thus, it remains to be seen if the approaches in this paper also result in improvements when much more unsupervised data is available.\n\nReferences:\n\nWen, Tsung-Hsien, Milica Gasic, Nikola Mrksic, Lina M. Rojas-Barahona, Pei-Hao Su, Stefan Ultes, David Vandyke, and Steve Young. \"A Network-based End-to-End Trainable Task-oriented Dialogue System.\" arXiv preprint arXiv:1604.04562 (2016).\n", "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 3}, {"TITLE": "No Title", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "The author propose to use a off-policy actor-critic algorithm in a batch-setting to improve chat-bots.\nThe approach is well motivated and the paper is well written, except for some intuitions for why the batch version outperforms the on-line version (see comments on \"clarification regarding batch vs. online setting\").\nThe artificial experiments are instructive, and the real-world experiments were performed very thoroughly although the results show only modest improvement. ", "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "15 Dec 2016", "REVIEWER_CONFIDENCE": 3}, {"TITLE": "clearly written, natural extension of previous work", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "The paper discuss a \"batch\" method for RL setup to improve chat-bots.\nThe authors provide nice overview of the RL setup they are using and present an algorithm which is similar to previously published on line setup for the same problem. They make a comparison to the online version and explore several modeling choices. \n\nI find the writing clear, and the algorithm a natural extension of the online version.\n\nBelow are some constructive remarks:\n- Comparison of the constant vs. per-state value function: In the artificial experiment there was no difference between the two while on the real-life task there was. It will be good to understand why, and add this to the discussion. Here is one option:\n- For the artificial task it seems like you are giving the constant value function an unfair advantage, as it can update all the weights of the model, and not just the top layer, like the per-state value function.\n- section 2.2:\n   sentence before last: s' is not defined. \n   last sentence: missing \"... in the stochastic case.\" at the end.\n- Section 4.1 last paragraph: \"While Bot-1 is not significant ...\" => \"While Bot-1 is not significantly different from ML ...\"\n\n\n\n", "IS_META_REVIEW": false, "RECOMMENDATION": 8, "DATE": "13 Dec 2016", "REVIEWER_CONFIDENCE": 3}, {"DATE": "08 Dec 2016", "TITLE": "Clarification regarding batch vs. online setting", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3"}, {"DATE": "02 Dec 2016", "TITLE": "synthetic experiment", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer4"}, {"IS_META_REVIEW": true, "comments": "The paper discuss a \"batch\" method for RL setup to improve chat-bots.\nThe authors provide nice overview of the RL setup they are using and present an algorithm which is similar to previously published on line setup for the same problem. They make a comparison to the online version and explore several modeling choices. \n\nI find the writing clear, and the algorithm a natural extension of the online version.\n\nBelow are some constructive remarks:\n- Comparison of the constant vs. per-state value function: In the artificial experiment there was no difference between the two while on the real-life task there was. It will be good to understand why, and add this to the discussion. Here is one option:\n- For the artificial task it seems like you are giving the constant value function an unfair advantage, as it can update all the weights of the model, and not just the top layer, like the per-state value function.\n- section 2.2:\n   sentence before last: s' is not defined. \n   last sentence: missing \"... in the stochastic case.\" at the end.\n- Section 4.1 last paragraph: \"While Bot-1 is not significant ...\" => \"While Bot-1 is not significantly different from ML ...\""}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "This is an interesting and timely paper combining off-policy learning with seq2seq models to train a chatbot on a restaurant reservation task, using labels collected through Amazon Mechanical Turk while using the bot with a baseline maximum likelihood policy. \n The paper is clear, well-written and well-executed. Although the improvements are modest and the actual novelty of the paper is limited (combining known pieces in a rather straightforward way), this is still an interesting and informative read, and will probably be of interest to many people at ICLR.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"DATE": "03 Jan 2017", "TITLE": "Revisions", "IS_META_REVIEW": false, "comments": "Once again, we'd like to thank all reviewers for the feedback. We have updated the manuscript accordingly. The changes are done in magenta so that it would be easier to identify them.\n\n@Reviewer2:\ns' is defined in the first para of section 2.2. Also, the expectation in the statement at the end of section 2.2 is to account for the stochasticity in the rewards.", "OTHER_KEYS": "Kirthevasan Kandasamy"}, {"TITLE": "Review", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer4", "comments": "This paper extends neural conversational models into the batch reinforcement learning setting. The idea is that you can collect human scoring data for some responses from a dialogue model, however such scores are expensive. Thus, it is natural to use off-policy learning \u2013 training a base policy on unsupervised data, deploying that policy to collect human scores, and then learning off-line from those scores.\n\nWhile the overall contribution is modest (extending off-policy actor-critic to the application of dialogue generation), the approach is well-motivated, and the paper is written clearly and is easy to understand. \n\nMy main concern is that the primary dataset used (restaurant recommendations) is very small (6000 conversations). In fact, it is several orders of magnitude smaller than other datasets used in the literature (e.g. Twitter, the Ubuntu Dialogue Corpus) for dialogue generation. It is a bit surprising to me that RNN chatbots (with no additional structure) are able to generate reasonable utterances on such a small dataset. Wen et al. (2016) are able to do this on a similarly small restaurant dataset, but this is mostly because they map directly from dialogue states to surface form, rather than some embedding representation of the context. Thus, it remains to be seen if the approaches in this paper also result in improvements when much more unsupervised data is available.\n\nReferences:\n\nWen, Tsung-Hsien, Milica Gasic, Nikola Mrksic, Lina M. Rojas-Barahona, Pei-Hao Su, Stefan Ultes, David Vandyke, and Steve Young. \"A Network-based End-to-End Trainable Task-oriented Dialogue System.\" arXiv preprint arXiv:1604.04562 (2016).\n", "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 3}, {"TITLE": "No Title", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "The author propose to use a off-policy actor-critic algorithm in a batch-setting to improve chat-bots.\nThe approach is well motivated and the paper is well written, except for some intuitions for why the batch version outperforms the on-line version (see comments on \"clarification regarding batch vs. online setting\").\nThe artificial experiments are instructive, and the real-world experiments were performed very thoroughly although the results show only modest improvement. ", "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "15 Dec 2016", "REVIEWER_CONFIDENCE": 3}, {"TITLE": "clearly written, natural extension of previous work", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "The paper discuss a \"batch\" method for RL setup to improve chat-bots.\nThe authors provide nice overview of the RL setup they are using and present an algorithm which is similar to previously published on line setup for the same problem. They make a comparison to the online version and explore several modeling choices. \n\nI find the writing clear, and the algorithm a natural extension of the online version.\n\nBelow are some constructive remarks:\n- Comparison of the constant vs. per-state value function: In the artificial experiment there was no difference between the two while on the real-life task there was. It will be good to understand why, and add this to the discussion. Here is one option:\n- For the artificial task it seems like you are giving the constant value function an unfair advantage, as it can update all the weights of the model, and not just the top layer, like the per-state value function.\n- section 2.2:\n   sentence before last: s' is not defined. \n   last sentence: missing \"... in the stochastic case.\" at the end.\n- Section 4.1 last paragraph: \"While Bot-1 is not significant ...\" => \"While Bot-1 is not significantly different from ML ...\"\n\n\n\n", "IS_META_REVIEW": false, "RECOMMENDATION": 8, "DATE": "13 Dec 2016", "REVIEWER_CONFIDENCE": 3}, {"DATE": "08 Dec 2016", "TITLE": "Clarification regarding batch vs. online setting", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3"}, {"DATE": "02 Dec 2016", "TITLE": "synthetic experiment", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer4"}]}
{"text": "We present a scalable approach for semi-supervised learning on graph-structured data that is based on an efficient variant of convolutional neural networks which operate directly on graphs. We motivate the choice of our convolutional architecture via a localized first-order approximation of spectral graph convolutions. Our model scales linearly in the number of graph edges and learns hidden layer representations that encode both local graph structure and features of nodes. In a number of experiments on citation networks and on a knowledge graph dataset we demonstrate that our approach outperforms related methods by a significant margin.\n1 INTRODUCTION\nWe consider the problem of classifying nodes (such as documents) in a graph (such as a citation network), where labels are only available for a small subset of nodes. This problem can be framed as graph-based semi-supervised learning, where label information is smoothed over the graph via some form of explicit graph-based regularization (Zhu et al., 2003; Zhou et al., 2004; Belkin et al., 2006; Weston et al., 2012), e.g. by using a graph Laplacian regularization term in the loss function:\nL = L0 + \u03bbLreg , with Lreg = \u2211 i,j Aij\u2016f(Xi)\u2212 f(Xj)\u20162 = f(X)>\u2206f(X) . (1)\nHere, L0 denotes the supervised loss w.r.t. the labeled part of the graph, f(\u00b7) can be a neural networklike differentiable function, \u03bb is a weighing factor and X is a matrix of node feature vectors Xi. \u2206 = D \u2212 A denotes the unnormalized graph Laplacian of an undirected graph G = (V, E) with N nodes vi \u2208 V , edges (vi, vj) \u2208 E , an adjacency matrix A \u2208 RN\u00d7N (binary or weighted) and a degree matrix Dii = \u2211 j Aij . The formulation of Eq. 1 relies on the assumption that connected nodes in the graph are likely to share the same label. This assumption, however, might restrict modeling capacity, as graph edges need not necessarily encode node similarity, but could contain additional information.\nIn this work, we encode the graph structure directly using a neural network model f(X,A) and train on a supervised target L0 for all nodes with labels, thereby avoiding explicit graph-based regularization in the loss function. Conditioning f(\u00b7) on the adjacency matrix of the graph will allow the model to distribute gradient information from the supervised loss L0 and will enable it to learn representations of nodes both with and without labels.\nOur contributions are two-fold. Firstly, we introduce a simple and well-behaved layer-wise propagation rule for neural network models which operate directly on graphs and show how it can be motivated from a first-order approximation of spectral graph convolutions (Hammond et al., 2011). Secondly, we demonstrate how this form of a graph-based neural network model can be used for fast and scalable semi-supervised classification of nodes in a graph. Experiments on a number of datasets demonstrate that our model compares favorably both in classification accuracy and efficiency (measured in wall-clock time) against state-of-the-art methods for semi-supervised learning.\n2 FAST APPROXIMATE CONVOLUTIONS ON GRAPHS\nIn this section, we provide theoretical motivation for a specific graph-based neural network model f(X,A) that we will use in the rest of this paper. We consider a multi-layer Graph Convolutional Network (GCN) with the following layer-wise propagation rule:\nH(l+1) = \u03c3 ( D\u0303\u2212 1 2 A\u0303D\u0303\u2212 1 2H(l)W (l) ) . (2)\nHere, A\u0303 = A + IN is the adjacency matrix of the undirected graph G with added self-connections. IN is the identity matrix, D\u0303ii = \u2211 j A\u0303ij and W\n(l) is a layer-specific trainable weight matrix. \u03c3(\u00b7) denotes an activation function, such as the ReLU(\u00b7) = max(0, \u00b7). H(l) \u2208 RN\u00d7D is the matrix of activations in the lth layer; H(0) = X . In the following, we show that the form of this propagation rule can be motivated1 via a first-order approximation of localized spectral filters on graphs (Hammond et al., 2011; Defferrard et al., 2016).\n2.1 SPECTRAL GRAPH CONVOLUTIONS\nWe consider spectral convolutions on graphs defined as the multiplication of a signal x \u2208 RN (a scalar for every node) with a filter g\u03b8 = diag(\u03b8) parameterized by \u03b8 \u2208 RN in the Fourier domain, i.e.:\ng\u03b8 ? x = Ug\u03b8U >x , (3)\nwhere U is the matrix of eigenvectors of the normalized graph Laplacian L = IN \u2212D\u2212 1 2AD\u2212 1 2 = U\u039bU>, with a diagonal matrix of its eigenvalues \u039b and U>x being the graph Fourier transform of x. We can understand g\u03b8 as a function of the eigenvalues of L, i.e. g\u03b8(\u039b). Evaluating Eq. 3 is computationally expensive, as multiplication with the eigenvector matrix U isO(N2). Furthermore, computing the eigendecomposition of L in the first place might be prohibitively expensive for large graphs. To circumvent this problem, it was suggested in Hammond et al. (2011) that g\u03b8(\u039b) can be well-approximated by a truncated expansion in terms of Chebyshev polynomials Tk(x) up to K th order:\ng\u03b8\u2032(\u039b) \u2248 K\u2211 k=0 \u03b8\u2032kTk(\u039b\u0303) , (4)\nwith a rescaled \u039b\u0303 = 2\u03bbmax \u039b \u2212 IN . \u03bbmax denotes the largest eigenvalue of L. \u03b8 \u2032 \u2208 RK is now a vector of Chebyshev coefficients. The Chebyshev polynomials are recursively defined as Tk(x) = 2xTk\u22121(x) \u2212 Tk\u22122(x), with T0(x) = 1 and T1(x) = x. The reader is referred to Hammond et al. (2011) for an in-depth discussion of this approximation.\nGoing back to our definition of a convolution of a signal x with a filter g\u03b8\u2032 , we now have:\ng\u03b8\u2032 ? x \u2248 K\u2211 k=0 \u03b8\u2032kTk(L\u0303)x , (5)\nwith L\u0303 = 2\u03bbmaxL \u2212 IN ; as can easily be verified by noticing that (U\u039bU >)k = U\u039bkU>. Note that this expression is nowK-localized since it is aK th-order polynomial in the Laplacian, i.e. it depends only on nodes that are at maximum K steps away from the central node (K th-order neighborhood). The complexity of evaluating Eq. 5 is O(|E|), i.e. linear in the number of edges. Defferrard et al. (2016) use this K-localized convolution to define a convolutional neural network on graphs.\n2.2 LAYER-WISE LINEAR MODEL\nA neural network model based on graph convolutions can therefore be built by stacking multiple convolutional layers of the form of Eq. 5, each layer followed by a point-wise non-linearity. Now, imagine we limited the layer-wise convolution operation to K = 1 (see Eq. 5), i.e. a function that is linear w.r.t. L and therefore a linear function on the graph Laplacian spectrum.\n1We provide an alternative interpretation of this propagation rule based on the Weisfeiler-Lehman algorithm (Weisfeiler & Lehmann, 1968) in Appendix A.\nIn this way, we can still recover a rich class of convolutional filter functions by stacking multiple such layers, but we are not limited to the explicit parameterization given by, e.g., the Chebyshev polynomials. We intuitively expect that such a model can alleviate the problem of overfitting on local neighborhood structures for graphs with very wide node degree distributions, such as social networks, citation networks, knowledge graphs and many other real-world graph datasets. Additionally, for a fixed computational budget, this layer-wise linear formulation allows us to build deeper models, a practice that is known to improve modeling capacity on a number of domains (He et al., 2016).\nIn this linear formulation of a GCN we further approximate \u03bbmax \u2248 2, as we can expect that neural network parameters will adapt to this change in scale during training. Under these approximations Eq. 5 simplifies to:\ng\u03b8\u2032 ? x \u2248 \u03b8\u20320x+ \u03b8\u20321 (L\u2212 IN )x = \u03b8\u20320x\u2212 \u03b8\u20321D\u2212 1 2AD\u2212 1 2x , (6)\nwith two free parameters \u03b8\u20320 and \u03b8 \u2032 1. The filter parameters can be shared over the whole graph. Successive application of filters of this form then effectively convolve the kth-order neighborhood of a node, where k is the number of successive filtering operations or convolutional layers in the neural network model.\nIn practice, it can be beneficial to constrain the number of parameters further to address overfitting and to minimize the number of operations (such as matrix multiplications) per layer. This leaves us with the following expression:\ng\u03b8 ? x \u2248 \u03b8 ( IN +D \u2212 12AD\u2212 1 2 ) x , (7)\nwith a single parameter \u03b8 = \u03b8\u20320 = \u2212\u03b8\u20321. Note that IN + D\u2212 1 2AD\u2212 1 2 now has eigenvalues in the range [0, 2]. Repeated application of this operator can therefore lead to numerical instabilities and exploding/vanishing gradients when used in a deep neural network model. To alleviate this problem, we introduce the following renormalization trick: IN +D\u2212 1 2AD\u2212 1 2 \u2192 D\u0303\u2212 12 A\u0303D\u0303\u2212 12 , with\nA\u0303 = A+ IN and D\u0303ii = \u2211 j A\u0303ij .\nWe can generalize this definition to a signalX \u2208 RN\u00d7C withC input channels (i.e. aC-dimensional feature vector for every node) and F filters or feature maps as follows:\nZ = D\u0303\u2212 1 2 A\u0303D\u0303\u2212 1 2X\u0398 , (8)\nwhere \u0398 \u2208 RC\u00d7F is now a matrix of filter parameters and Z \u2208 RN\u00d7F is the convolved signal matrix. This filtering operation has complexity O(|E|FC), as A\u0303X can be efficiently implemented as a product of a sparse matrix with a dense matrix.\n3 SEMI-SUPERVISED NODE CLASSIFICATION\nHaving introduced a simple, yet flexible model f(X,A) for efficient information propagation on graphs, we can return to the problem of semi-supervised node classification. As outlined in the introduction, we can relax certain assumptions typically made in graph-based semi-supervised learning by conditioning our model f(X,A) both on the data X and on the adjacency matrix A of the underlying graph structure. We expect this setting to be especially powerful in scenarios where the adjacency matrix contains information not present in the data X , such as citation links between documents in a citation network or relations in a knowledge graph. The overall model, a multi-layer GCN for semi-supervised learning, is schematically depicted in Figure 1.\n3.1 EXAMPLE\nIn the following, we consider a two-layer GCN for semi-supervised node classification on a graph with a symmetric adjacency matrix A (binary or weighted). We first calculate A\u0302 = D\u0303\u2212 1 2 A\u0303D\u0303\u2212 1 2 in a pre-processing step. Our forward model then takes the simple form:\nZ = f(X,A) = softmax ( A\u0302 ReLU ( A\u0302XW (0) ) W (1) ) . (9)\n1\n30\nHere, W (0) \u2208 RC\u00d7H is an input-to-hidden weight matrix for a hidden layer with H feature maps. W (1) \u2208 RH\u00d7F is a hidden-to-output weight matrix. The softmax activation function, defined as softmax(xi) = 1 Z exp(xi) with Z = \u2211 i exp(xi), is applied row-wise. For semi-supervised multiclass classification, we then evaluate the cross-entropy error over all labeled examples:\nL = \u2212 \u2211 l\u2208YL F\u2211 f=1 Ylf lnZlf , (10)\nwhere YL is the set of node indices that have labels. The neural network weights W (0) and W (1) are trained using gradient descent. In this work, we perform batch gradient descent using the full dataset for every training iteration, which is a viable option as long as datasets fit in memory. Using a sparse representation for A, memory requirement is O(|E|), i.e. linear in the number of edges. Stochasticity in the training process is introduced via dropout (Srivastava et al., 2014). We leave memory-efficient extensions with mini-batch stochastic gradient descent for future work.\n3.2 IMPLEMENTATION\nIn practice, we make use of TensorFlow (Abadi et al., 2015) for an efficient GPU-based implementation2 of Eq. 9 using sparse-dense matrix multiplications. The computational complexity of evaluating Eq. 9 is then O(|E|CHF ), i.e. linear in the number of graph edges.\n4 RELATED WORK\nOur model draws inspiration both from the field of graph-based semi-supervised learning and from recent work on neural networks that operate on graphs. In what follows, we provide a brief overview on related work in both fields.\n4.1 GRAPH-BASED SEMI-SUPERVISED LEARNING\nA large number of approaches for semi-supervised learning using graph representations have been proposed in recent years, most of which fall into two broad categories: methods that use some form of explicit graph Laplacian regularization and graph embedding-based approaches. Prominent examples for graph Laplacian regularization include label propagation (Zhu et al., 2003), manifold regularization (Belkin et al., 2006) and deep semi-supervised embedding (Weston et al., 2012).\n2Code to reproduce our experiments is available at https://github.com/tkipf/gcn.\nRecently, attention has shifted to models that learn graph embeddings with methods inspired by the skip-gram model (Mikolov et al., 2013). DeepWalk (Perozzi et al., 2014) learns embeddings via the prediction of the local neighborhood of nodes, sampled from random walks on the graph. LINE (Tang et al., 2015) and node2vec (Grover & Leskovec, 2016) extend DeepWalk with more sophisticated random walk or breadth-first search schemes. For all these methods, however, a multistep pipeline including random walk generation and semi-supervised training is required where each step has to be optimized separately. Planetoid (Yang et al., 2016) alleviates this by injecting label information in the process of learning embeddings.\n4.2 NEURAL NETWORKS ON GRAPHS\nNeural networks that operate on graphs have previously been introduced in Gori et al. (2005); Scarselli et al. (2009) as a form of recurrent neural network. Their framework requires the repeated application of contraction maps as propagation functions until node representations reach a stable fixed point. This restriction was later alleviated in Li et al. (2016) by introducing modern practices for recurrent neural network training to the original graph neural network framework. Duvenaud et al. (2015) introduced a convolution-like propagation rule on graphs and methods for graph-level classification. Their approach requires to learn node degree-specific weight matrices which does not scale to large graphs with wide node degree distributions. Our model instead uses a single weight matrix per layer and deals with varying node degrees through an appropriate normalization of the adjacency matrix (see Section 3.1).\nA related approach to node classification with a graph-based neural network was recently introduced in Atwood & Towsley (2016). They report O(N2) complexity, limiting the range of possible applications. In a different yet related model, Niepert et al. (2016) convert graphs locally into sequences that are fed into a conventional 1D convolutional neural network, which requires the definition of a node ordering in a pre-processing step.\nOur method is based on spectral graph convolutional neural networks, introduced in Bruna et al. (2014) and later extended by Defferrard et al. (2016) with fast localized convolutions. In contrast to these works, we consider here the task of transductive node classification within networks of significantly larger scale. We show that in this setting, a number of simplifications (see Section 2.2) can be introduced to the original frameworks of Bruna et al. (2014) and Defferrard et al. (2016) that improve scalability and classification performance in large-scale networks.\n5 EXPERIMENTS\nWe test our model in a number of experiments: semi-supervised document classification in citation networks, semi-supervised entity classification in a bipartite graph extracted from a knowledge graph, an evaluation of various graph propagation models and a run-time analysis on random graphs.\n5.1 DATASETS\nWe closely follow the experimental setup in Yang et al. (2016). Dataset statistics are summarized in Table 1. In the citation network datasets\u2014Citeseer, Cora and Pubmed (Sen et al., 2008)\u2014nodes are documents and edges are citation links. Label rate denotes the number of labeled nodes that are used for training divided by the total number of nodes in each dataset. NELL (Carlson et al., 2010; Yang et al., 2016) is a bipartite graph dataset extracted from a knowledge graph with 55,864 relation nodes and 9,891 entity nodes.\nCitation networks We consider three citation network datasets: Citeseer, Cora and Pubmed (Sen et al., 2008). The datasets contain sparse bag-of-words feature vectors for each document and a list of citation links between documents. We treat the citation links as (undirected) edges and construct a binary, symmetric adjacency matrix A. Each document has a class label. For training, we only use 20 labels per class, but all feature vectors.\nNELL NELL is a dataset extracted from the knowledge graph introduced in (Carlson et al., 2010). A knowledge graph is a set of entities connected with directed, labeled edges (relations). We follow the pre-processing scheme as described in Yang et al. (2016). We assign separate relation nodes r1 and r2 for each entity pair (e1, r, e2) as (e1, r1) and (e2, r2). Entity nodes are described by sparse feature vectors. We extend the number of features in NELL by assigning a unique one-hot representation for every relation node, effectively resulting in a 61,278-dim sparse feature vector per node. The semi-supervised task here considers the extreme case of only a single labeled example per class in the training set. We construct a binary, symmetric adjacency matrix from this graph by setting entries Aij = 1, if one or more edges are present between nodes i and j.\nRandom graphs We simulate random graph datasets of various sizes for experiments where we measure training time per epoch. For a dataset with N nodes we create a random graph assigning 2N edges uniformly at random. We take the identity matrix IN as input feature matrix X , thereby implicitly taking a featureless approach where the model is only informed about the identity of each node, specified by a unique one-hot vector. We add dummy labels Yi = 1 for every node.\n5.2 EXPERIMENTAL SET-UP\nUnless otherwise noted, we train a two-layer GCN as described in Section 3.1 and evaluate prediction accuracy on a test set of 1,000 labeled examples. We provide additional experiments using deeper models with up to 10 layers in Appendix B. We choose the same dataset splits as in Yang et al. (2016) with an additional validation set of 500 labeled examples for hyperparameter optimization (dropout rate for all layers, L2 regularization factor for the first GCN layer and number of hidden units). We do not use the validation set labels for training.\nFor the citation network datasets, we optimize hyperparameters on Cora only and use the same set of parameters for Citeseer and Pubmed. We train all models for a maximum of 200 epochs (training iterations) using Adam (Kingma & Ba, 2015) with a learning rate of 0.01 and early stopping with a window size of 10, i.e. we stop training if the validation loss does not decrease for 10 consecutive epochs. We initialize weights using the initialization described in Glorot & Bengio (2010) and accordingly (row-)normalize input feature vectors. On the random graph datasets, we use a hidden layer size of 32 units and omit regularization (i.e. neither dropout nor L2 regularization).\n5.3 BASELINES\nWe compare against the same baseline methods as in Yang et al. (2016), i.e. label propagation (LP) (Zhu et al., 2003), semi-supervised embedding (SemiEmb) (Weston et al., 2012), manifold regularization (ManiReg) (Belkin et al., 2006) and skip-gram based graph embeddings (DeepWalk) (Perozzi et al., 2014). We omit TSVM (Joachims, 1999), as it does not scale to the large number of classes in one of our datasets.\nWe further compare against the iterative classification algorithm (ICA) proposed in Lu & Getoor (2003) in conjunction with two logistic regression classifiers, one for local node features alone and one for relational classification using local features and an aggregation operator as described in Sen et al. (2008). We first train the local classifier using all labeled training set nodes and use it to bootstrap class labels of unlabeled nodes for relational classifier training. We run iterative classification (relational classifier) with a random node ordering for 10 iterations on all unlabeled nodes (bootstrapped using the local classifier). L2 regularization parameter and aggregation operator (count vs. prop, see Sen et al. (2008)) are chosen based on validation set performance for each dataset separately.\nLastly, we compare against Planetoid (Yang et al., 2016), where we always choose their bestperforming model variant (transductive vs. inductive) as a baseline.\n6 RESULTS\n6.1 SEMI-SUPERVISED NODE CLASSIFICATION\nResults are summarized in Table 2. Reported numbers denote classification accuracy in percent. For ICA, we report the mean accuracy of 100 runs with random node orderings. Results for all other baseline methods are taken from the Planetoid paper (Yang et al., 2016). Planetoid* denotes the best model for the respective dataset out of the variants presented in their paper.\nWe further report wall-clock training time in seconds until convergence (in brackets) for our method (incl. evaluation of validation error) and for Planetoid. For the latter, we used an implementation provided by the authors3 and trained on the same hardware (with GPU) as our GCN model. We trained and tested our model on the same dataset splits as in Yang et al. (2016) and report mean accuracy of 100 runs with random weight initializations. We used the following sets of hyperparameters for Citeseer, Cora and Pubmed: 0.5 (dropout rate), 5 \u00b7 10\u22124 (L2 regularization) and 16 (number of hidden units); and for NELL: 0.1 (dropout rate), 1 \u00b7 10\u22125 (L2 regularization) and 64 (number of hidden units).\nIn addition, we report performance of our model on 10 randomly drawn dataset splits of the same size as in Yang et al. (2016), denoted by GCN (rand. splits). Here, we report mean and standard error of prediction accuracy on the test set split in percent.\n6.2 EVALUATION OF PROPAGATION MODEL\nWe compare different variants of our proposed per-layer propagation model on the citation network datasets. We follow the experimental set-up described in the previous section. Results are summarized in Table 3. The propagation model of our original GCN model is denoted by renormalization trick (in bold). In all other cases, the propagation model of both neural network layers is replaced with the model specified under propagation model. Reported numbers denote mean classification accuracy for 100 repeated runs with random weight matrix initializations. In case of multiple variables \u0398i per layer, we impose L2 regularization on all weight matrices of the first layer.\n6.3 TRAINING TIME PER EPOCH\nHere, we report results for the mean training time per epoch (forward pass, cross-entropy calculation, backward pass) for 100 epochs on simulated random graphs, measured in seconds wall-clock time. See Section 5.1 for a detailed description of the random graph dataset used in these experiments. We compare results on a GPU and on a CPU-only implementation4 in TensorFlow (Abadi et al., 2015). Figure 2 summarizes the results.\n7 DISCUSSION\n7.1 SEMI-SUPERVISED MODEL\nIn the experiments demonstrated here, our method for semi-supervised node classification outperforms recent related methods by a significant margin. Methods based on graph-Laplacian regularization (Zhu et al., 2003; Belkin et al., 2006; Weston et al., 2012) are most likely limited due to their assumption that edges encode mere similarity of nodes. Skip-gram based methods on the other hand are limited by the fact that they are based on a multi-step pipeline which is difficult to optimize. Our proposed model can overcome both limitations, while still comparing favorably in terms of efficiency (measured in wall-clock time) to related methods. Propagation of feature information from neighboring nodes in every layer improves classification performance in comparison to methods like ICA (Lu & Getoor, 2003), where only label information is aggregated.\nWe have further demonstrated that the proposed renormalized propagation model (Eq. 8) offers both improved efficiency (fewer parameters and operations, such as multiplication or addition) and better predictive performance on a number of datasets compared to a na\u0131\u0308ve 1st-order model (Eq. 6) or higher-order graph convolutional models using Chebyshev polynomials (Eq. 5).\n7.2 LIMITATIONS AND FUTURE WORK\nHere, we describe several limitations of our current model and outline how these might be overcome in future work.\nMemory requirement In the current setup with full-batch gradient descent, memory requirement grows linearly in the size of the dataset. We have shown that for large graphs that do not fit in GPU memory, training on CPU can still be a viable option. Mini-batch stochastic gradient descent can alleviate this issue. The procedure of generating mini-batches, however, should take into account the number of layers in the GCN model, as the K th-order neighborhood for a GCN with K layers has to be stored in memory for an exact procedure. For very large and densely connected graph datasets, further approximations might be necessary.\nDirected edges and edge features Our framework currently does not naturally support edge features and is limited to undirected graphs (weighted or unweighted). Results on NELL however show that it is possible to handle both directed edges and edge features by representing the original directed graph as an undirected bipartite graph with additional nodes that represent edges in the original graph (see Section 5.1 for details).\nLimiting assumptions Through the approximations introduced in Section 2, we implicitly assume locality (dependence on the K th-order neighborhood for a GCN with K layers) and equal importance of self-connections vs. edges to neighboring nodes. For some datasets, however, it might be beneficial to introduce a trade-off parameter \u03bb in the definition of A\u0303:\nA\u0303 = A+ \u03bbIN . (11) 4Hardware used: 16-core Intel R\u00a9 Xeon R\u00a9 CPU E5-2640 v3 @ 2.60GHz, GeForce R\u00a9 GTX TITAN X\nThis parameter now plays a similar role as the trade-off parameter between supervised and unsupervised loss in the typical semi-supervised setting (see Eq. 1). Here, however, it can be learned via gradient descent.\n8 CONCLUSION\nWe have introduced a novel approach for semi-supervised classification on graph-structured data. Our GCN model uses an efficient layer-wise propagation rule that is based on a first-order approximation of spectral convolutions on graphs. Experiments on a number of network datasets suggest that the proposed GCN model is capable of encoding both graph structure and node features in a way useful for semi-supervised classification. In this setting, our model outperforms several recently proposed methods by a significant margin, while being computationally efficient.\nACKNOWLEDGMENTS\nWe would like to thank Christos Louizos, Taco Cohen, Joan Bruna, Zhilin Yang, Dave Herman, Pramod Sinha and Abdul-Saboor Sheikh for helpful discussions. This research was funded by SAP.\nA RELATION TO WEISFEILER-LEHMAN ALGORITHM\nA neural network model for graph-structured data should ideally be able to learn representations of nodes in a graph, taking both the graph structure and feature description of nodes into account. A well-studied framework for the unique assignment of node labels given a graph and (optionally) discrete initial node labels is provided by the 1-dim Weisfeiler-Lehman (WL-1) algorithm (Weisfeiler & Lehmann, 1968):\nAlgorithm 1: WL-1 algorithm (Weisfeiler & Lehmann, 1968)\nInput: Initial node coloring (h(0)1 , h (0) 2 , ..., h (0) N ) Output: Final node coloring (h(T )1 , h (T ) 2 , ..., h (T ) N ) t\u2190 0; repeat\nfor vi \u2208 V do h (t+1) i \u2190 hash (\u2211 j\u2208Ni h (t) j ) ;\nt\u2190 t+ 1; until stable node coloring is reached;\nHere, h(t)i denotes the coloring (label assignment) of node vi (at iteration t) and Ni is its set of neighboring node indices (irrespective of whether the graph includes self-connections for every node or not). hash(\u00b7) is a hash function. For an in-depth mathematical discussion of the WL-1 algorithm see, e.g., Douglas (2011).\nWe can replace the hash function in Algorithm 1 with a neural network layer-like differentiable function with trainable parameters as follows:\nh (l+1) i = \u03c3 \u2211 j\u2208Ni 1 cij h (l) j W (l)  , (12) where cij is an appropriately chosen normalization constant for the edge (vi, vj). Further, we can take h(l)i now to be a vector of activations of node i in the l\nth neural network layer. W (l) is a layer-specific weight matrix and \u03c3(\u00b7) denotes a differentiable, non-linear activation function.\nBy choosing cij = \u221a didj , where di = |Ni| denotes the degree of node vi, we recover the propagation rule of our Graph Convolutional Network (GCN) model in vector form (see Eq. 2)5.\nThis\u2014loosely speaking\u2014allows us to interpret our GCN model as a differentiable and parameterized generalization of the 1-dim Weisfeiler-Lehman algorithm on graphs.\nA.1 NODE EMBEDDINGS WITH RANDOM WEIGHTS\nFrom the analogy with the Weisfeiler-Lehman algorithm, we can understand that even an untrained GCN model with random weights can serve as a powerful feature extractor for nodes in a graph. As an example, consider the following 3-layer GCN model:\nZ = tanh ( A\u0302 tanh ( A\u0302 tanh ( A\u0302XW (0) ) W (1) ) W (2) ) , (13)\nwith weight matricesW (l) initialized at random using the initialization described in Glorot & Bengio (2010). A\u0302, X and Z are defined as in Section 3.1.\nWe apply this model on Zachary\u2019s karate club network (Zachary, 1977). This graph contains 34 nodes, connected by 154 (undirected and unweighted) edges. Every node is labeled by one of four classes, obtained via modularity-based clustering (Brandes et al., 2008). See Figure 3a for an illustration.\n5Note that we here implicitly assume that self-connections have already been added to every node in the graph (for a clutter-free notation).\nWe take a featureless approach by setting X = IN , where IN is the N by N identity matrix. N is the number of nodes in the graph. Note that nodes are randomly ordered (i.e. ordering contains no information). Furthermore, we choose a hidden layer dimensionality6 of 4 and a two-dimensional output (so that the output can immediately be visualized in a 2-dim plot).\nFigure 3b shows a representative example of node embeddings (outputs Z) obtained from an untrained GCN model applied to the karate club network. These results are comparable to embeddings obtained from DeepWalk (Perozzi et al., 2014), which uses a more expensive unsupervised training procedure.\nA.2 SEMI-SUPERVISED NODE EMBEDDINGS\nOn this simple example of a GCN applied to the karate club network it is interesting to observe how embeddings react during training on a semi-supervised classification task. Such a visualization (see Figure 4) provides insights into how the GCN model can make use of the graph structure (and of features extracted from the graph structure at later layers) to learn embeddings that are useful for a classification task.\nWe consider the following semi-supervised learning setup: we add a softmax layer on top of our model (Eq. 13) and train using only a single labeled example per class (i.e. a total number of 4 labeled nodes). We train for 300 training iterations using Adam (Kingma & Ba, 2015) with a learning rate of 0.01 on a cross-entropy loss.\nFigure 4 shows the evolution of node embeddings over a number of training iterations. The model succeeds in linearly separating the communities based on minimal supervision and the graph structure alone. A video of the full training process can be found on our website7.\n6We originally experimented with a hidden layer dimensionality of 2 (i.e. same as output layer), but observed that a dimensionality of 4 resulted in less frequent saturation of tanh(\u00b7) units and therefore visually more pleasing results.\n7http://tkipf.github.io/graph-convolutional-networks/\nB EXPERIMENTS ON MODEL DEPTH\nIn these experiments, we investigate the influence of model depth (number of layers) on classification performance. We report results on a 5-fold cross-validation experiment on the Cora, Citeseer and Pubmed datasets (Sen et al., 2008) using all labels. In addition to the standard GCN model (Eq. 2), we report results on a model variant where we use residual connections (He et al., 2016) between hidden layers to facilitate training of deeper models by enabling the model to carry over information from the previous layer\u2019s input:\nH(l+1) = \u03c3 ( D\u0303\u2212 1 2 A\u0303D\u0303\u2212 1 2H(l)W (l) ) +H(l) . (14)\nOn each cross-validation split, we train for 400 epochs (without early stopping) using the Adam optimizer (Kingma & Ba, 2015) with a learning rate of 0.01. Other hyperparameters are chosen as follows: 0.5 (dropout rate, first and last layer), 5 \u00b7 10\u22124 (L2 regularization, first layer), 16 (number of units for each hidden layer) and 0.01 (learning rate). Results are summarized in Figure 5.\nFor the datasets considered here, best results are obtained with a 2- or 3-layer model. We observe that for models deeper than 7 layers, training without the use of residual connections can become difficult, as the effective context size for each node increases by the size of its K th-order neighborhood (for a model with K layers) with each additional layer. Furthermore, overfitting can become an issue as the number of parameters increases with model depth.\n", "cats": {"READ": 1.0, "DONTREAD": 0.0}, "reviews": [{"IS_META_REVIEW": true, "comments": "This paper proposes the graph convolutional networks, motivated from approximating graph convolutions.  In one propagation step, what the model does can be simplified as, first linearly transform the node representations for each node, and then multiply the transformed node representations with the normalized affinity matrix (with self-connections added), and then pass through nonlinearity.\n\nThis model is used for semi-supervised learning on graphs, and in the experiments it demonstrated quite impressive results compared to other baselines, outperforming them by a significant margin.  The evaluation of propagation model is also interesting, where different variants of the model and design decisions are evaluated and compared.\n\nIt is surprising that such a simple model works so much better than all the baselines.  Considering that the model used is just a two-layer model in most experiments, this is really surprising as a two-layer model is very local, and the output of a node can only be affected by nodes in a 2-hop neighborhood, and no longer range interactions can play any roles in this.  Since computation is quite efficient (sec. 6.3), I wonder if adding more layers helped anything or not.\n\nEven though motivated from graph convolutions, when simplified as the paper suggests, the operations the model does are quite simple.  Compared to Duvenaud et al. 2015 and Li et al. 2016, the proposed method is simpler and does almost strictly less things.  So how would the proposed GCN compare against these methods?\n\nOverall I think this model is simple, but the connection to graph convolutions is interesting, and the experiment results are quite good.  There are a few questions that still remain, but I feel this paper can be accepted."}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "The reviewers are in agreement that this paper is well written and constitutes a solid contribution to graph-based semi-supervised learning based on variants of CNNs.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"DATE": "14 Jan 2017", "TITLE": "General response", "IS_META_REVIEW": false, "comments": "Dear Reviewers,\n\nThanks a lot for reviewing our paper and for your valuable comments. \n\nTo incorporate your feedback, we have uploaded a revision of our paper with the following changes:\n\n1) We have added the Iterative Classification Algorithm (ICA) from Lu & Getoor (2003) as a baseline as suggested by Reviewer 2. Thanks a lot for pointing out the references on iterative classification. ICA is indeed a powerful baseline that we have not considered previously and it compares favorably against some of the other baselines. We have put the code to reproduce the ICA baseline experiments on Github: ", "OTHER_KEYS": "Thomas N. Kipf"}, {"TITLE": "Simple and reasonable approach", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "The paper develops a simple and reasonable algorithm for graph node prediction/classification. The formulations are very intuitive and lead to a simple CNN based training and can easily leverage existing GPU speedups. \n\nExperiments are thorough and compare with many reasonable baselines on large and real benchmark datasets. Although, I am not quite aware of the literature on other methods and there may be similar alternatives as link and node prediction is an old problem. I still think the approach is quite simple and reasonably supported by good evaluations.  ", "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "29 Dec 2016", "REVIEWER_CONFIDENCE": 3}, {"TITLE": "No Title", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "The paper introduces a method for semi-supervised learning in graphs that exploits the spectral structure of the graph in a convolutional NN implementation. The proposed algorithm has a limited complexity and it is shown to scale well on a large dataset. The comparison with baselines on different datasets show a clear jump of performance with the proposed method.\n\nThe paper is technically fine and clear, the algorithm seems to scale well, and the results on the different datasets compare very favorably with the different baselines. The algorithm is simple and training seems easy. Concerning the originality, the proposed algorithm is a simple adaptation of graph convolutional networks (ref Defferrard 2016 in the paper) to a semi-supervised transductive setting. This is clearly mentioned in the paper, but the authors could better highlight the differences and novelty wrt this reference paper. Also, there is no comparison with the family of iterative classifiers, which usually compare favorably, both in performance and training time, with regularization based approaches, although they are mostly used in inductive settings. Below are some references for this family of methods.\n\nThe authors mention that more complex filters could be learned by stacking layers but they limit their architecture to one hidden layer. They should comment on the interest of using more layers for graph classification.\n\n\nSome references on iterative classification Qing Lu and Lise Getoor. 2003. Link-based classification. In ICML, Vol. 3. 496\u2013503.\n\nGideon S Mann and Andrew McCallum. 2010. Generalized expectation criteria for semi-supervised learning with weakly labeled data. The Journal of Machine Learning Research 11 (2010), 955\u2013984.\nDavid Jensen, Jennifer Neville, and Brian Gallagher. 2004. Why collective inference improves relational classification. In Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining. ACM, 593\u2013598.\nJoseph J Pfeiffer III, Jennifer Neville, and Paul N Bennett. 2015. Overcoming Relational Learning Biases\nto Accurately Predict Preferences in Large Scale Networks. In Proceedings of the 24th International Conference on World Wide Web. International World Wide Web Conferences Steering Committee, 853\u2013\n863.\nStephane Peters, Ludovic Denoyer, and Patrick Gallinari. 2010. Iterative annotation of multi-relational social networks. In Advances in Social Networks Analysis and Mining (ASONAM), 2010 International Conference on. IEEE, 96\u2013103.\n", "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "19 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "Solid results.", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "This paper proposes the graph convolutional networks, motivated from approximating graph convolutions.  In one propagation step, what the model does can be simplified as, first linearly transform the node representations for each node, and then multiply the transformed node representations with the normalized affinity matrix (with self-connections added), and then pass through nonlinearity.\n\nThis model is used for semi-supervised learning on graphs, and in the experiments it demonstrated quite impressive results compared to other baselines, outperforming them by a significant margin.  The evaluation of propagation model is also interesting, where different variants of the model and design decisions are evaluated and compared.\n\nIt is surprising that such a simple model works so much better than all the baselines.  Considering that the model used is just a two-layer model in most experiments, this is really surprising as a two-layer model is very local, and the output of a node can only be affected by nodes in a 2-hop neighborhood, and no longer range interactions can play any roles in this.  Since computation is quite efficient (sec. 6.3), I wonder if adding more layers helped anything or not.\n\nEven though motivated from graph convolutions, when simplified as the paper suggests, the operations the model does are quite simple.  Compared to Duvenaud et al. 2015 and Li et al. 2016, the proposed method is simpler and does almost strictly less things.  So how would the proposed GCN compare against these methods?\n\nOverall I think this model is simple, but the connection to graph convolutions is interesting, and the experiment results are quite good.  There are a few questions that still remain, but I feel this paper can be accepted.", "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "18 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"DATE": "07 Dec 2016", "TITLE": "Why is GPU not much faster than CPU", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3"}, {"IS_META_REVIEW": true, "comments": "This paper proposes the graph convolutional networks, motivated from approximating graph convolutions.  In one propagation step, what the model does can be simplified as, first linearly transform the node representations for each node, and then multiply the transformed node representations with the normalized affinity matrix (with self-connections added), and then pass through nonlinearity.\n\nThis model is used for semi-supervised learning on graphs, and in the experiments it demonstrated quite impressive results compared to other baselines, outperforming them by a significant margin.  The evaluation of propagation model is also interesting, where different variants of the model and design decisions are evaluated and compared.\n\nIt is surprising that such a simple model works so much better than all the baselines.  Considering that the model used is just a two-layer model in most experiments, this is really surprising as a two-layer model is very local, and the output of a node can only be affected by nodes in a 2-hop neighborhood, and no longer range interactions can play any roles in this.  Since computation is quite efficient (sec. 6.3), I wonder if adding more layers helped anything or not.\n\nEven though motivated from graph convolutions, when simplified as the paper suggests, the operations the model does are quite simple.  Compared to Duvenaud et al. 2015 and Li et al. 2016, the proposed method is simpler and does almost strictly less things.  So how would the proposed GCN compare against these methods?\n\nOverall I think this model is simple, but the connection to graph convolutions is interesting, and the experiment results are quite good.  There are a few questions that still remain, but I feel this paper can be accepted."}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "The reviewers are in agreement that this paper is well written and constitutes a solid contribution to graph-based semi-supervised learning based on variants of CNNs.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"DATE": "14 Jan 2017", "TITLE": "General response", "IS_META_REVIEW": false, "comments": "Dear Reviewers,\n\nThanks a lot for reviewing our paper and for your valuable comments. \n\nTo incorporate your feedback, we have uploaded a revision of our paper with the following changes:\n\n1) We have added the Iterative Classification Algorithm (ICA) from Lu & Getoor (2003) as a baseline as suggested by Reviewer 2. Thanks a lot for pointing out the references on iterative classification. ICA is indeed a powerful baseline that we have not considered previously and it compares favorably against some of the other baselines. We have put the code to reproduce the ICA baseline experiments on Github: ", "OTHER_KEYS": "Thomas N. Kipf"}, {"TITLE": "Simple and reasonable approach", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "The paper develops a simple and reasonable algorithm for graph node prediction/classification. The formulations are very intuitive and lead to a simple CNN based training and can easily leverage existing GPU speedups. \n\nExperiments are thorough and compare with many reasonable baselines on large and real benchmark datasets. Although, I am not quite aware of the literature on other methods and there may be similar alternatives as link and node prediction is an old problem. I still think the approach is quite simple and reasonably supported by good evaluations.  ", "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "29 Dec 2016", "REVIEWER_CONFIDENCE": 3}, {"TITLE": "No Title", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "The paper introduces a method for semi-supervised learning in graphs that exploits the spectral structure of the graph in a convolutional NN implementation. The proposed algorithm has a limited complexity and it is shown to scale well on a large dataset. The comparison with baselines on different datasets show a clear jump of performance with the proposed method.\n\nThe paper is technically fine and clear, the algorithm seems to scale well, and the results on the different datasets compare very favorably with the different baselines. The algorithm is simple and training seems easy. Concerning the originality, the proposed algorithm is a simple adaptation of graph convolutional networks (ref Defferrard 2016 in the paper) to a semi-supervised transductive setting. This is clearly mentioned in the paper, but the authors could better highlight the differences and novelty wrt this reference paper. Also, there is no comparison with the family of iterative classifiers, which usually compare favorably, both in performance and training time, with regularization based approaches, although they are mostly used in inductive settings. Below are some references for this family of methods.\n\nThe authors mention that more complex filters could be learned by stacking layers but they limit their architecture to one hidden layer. They should comment on the interest of using more layers for graph classification.\n\n\nSome references on iterative classification Qing Lu and Lise Getoor. 2003. Link-based classification. In ICML, Vol. 3. 496\u2013503.\n\nGideon S Mann and Andrew McCallum. 2010. Generalized expectation criteria for semi-supervised learning with weakly labeled data. The Journal of Machine Learning Research 11 (2010), 955\u2013984.\nDavid Jensen, Jennifer Neville, and Brian Gallagher. 2004. Why collective inference improves relational classification. In Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining. ACM, 593\u2013598.\nJoseph J Pfeiffer III, Jennifer Neville, and Paul N Bennett. 2015. Overcoming Relational Learning Biases\nto Accurately Predict Preferences in Large Scale Networks. In Proceedings of the 24th International Conference on World Wide Web. International World Wide Web Conferences Steering Committee, 853\u2013\n863.\nStephane Peters, Ludovic Denoyer, and Patrick Gallinari. 2010. Iterative annotation of multi-relational social networks. In Advances in Social Networks Analysis and Mining (ASONAM), 2010 International Conference on. IEEE, 96\u2013103.\n", "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "19 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "Solid results.", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "This paper proposes the graph convolutional networks, motivated from approximating graph convolutions.  In one propagation step, what the model does can be simplified as, first linearly transform the node representations for each node, and then multiply the transformed node representations with the normalized affinity matrix (with self-connections added), and then pass through nonlinearity.\n\nThis model is used for semi-supervised learning on graphs, and in the experiments it demonstrated quite impressive results compared to other baselines, outperforming them by a significant margin.  The evaluation of propagation model is also interesting, where different variants of the model and design decisions are evaluated and compared.\n\nIt is surprising that such a simple model works so much better than all the baselines.  Considering that the model used is just a two-layer model in most experiments, this is really surprising as a two-layer model is very local, and the output of a node can only be affected by nodes in a 2-hop neighborhood, and no longer range interactions can play any roles in this.  Since computation is quite efficient (sec. 6.3), I wonder if adding more layers helped anything or not.\n\nEven though motivated from graph convolutions, when simplified as the paper suggests, the operations the model does are quite simple.  Compared to Duvenaud et al. 2015 and Li et al. 2016, the proposed method is simpler and does almost strictly less things.  So how would the proposed GCN compare against these methods?\n\nOverall I think this model is simple, but the connection to graph convolutions is interesting, and the experiment results are quite good.  There are a few questions that still remain, but I feel this paper can be accepted.", "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "18 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"DATE": "07 Dec 2016", "TITLE": "Why is GPU not much faster than CPU", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3"}]}
{"text": "A COMPARE-AGGREGATE MODEL FOR MATCHING TEXT SEQUENCES\n1 INTRODUCTION\nMany natural language processing problems involve matching two or more sequences to make a decision. For example, in textual entailment, one needs to determine whether a hypothesis sentence can be inferred from a premise sentence (Bowman et al., 2015). In machine comprehension, given a passage, a question needs to be matched against it in order to find the correct answer (Richardson et al., 2013; Tapaswi et al., 2016). Table 1 gives two example sequence matching problems. In the first example, a passage, a question and four candidate answers are given. We can see that to get the correct answer, we need to match the question against the passage and identify the last sentence to be the answer-bearing sentence. In the second example, given a question and a set of candidate answers, we need to find the answer that best matches the question. Because of the fundamental importance of comparing two sequences of text to judge their semantic similarity or relatedness, sequence matching has been well studied in natural language processing.\nWith recent advances of neural network models in natural language processing, a standard practice for sequence modeling now is to encode a sequence of text as an embedding vector using models such as RNN and CNN. To match two sequences, a straightforward approach is to encode each sequence as a vector and then to combine the two vectors to make a decision (Bowman et al., 2015; Feng et al., 2015). However, it has been found that using a single vector to encode an entire sequence is not sufficient to capture all the important information from the sequence, and therefore advanced techniques such as attention mechanisms and memory networks have been applied to sequence matching problems (Hermann et al., 2015; Hill et al., 2016; Rockta\u0308schel et al., 2015).\nA common trait of a number of these recent studies on sequence matching problems is the use of a \u201ccompare-aggregate\u201d framework (Wang & Jiang, 2016b; He & Lin, 2016; Parikh et al., 2016). In such a framework, comparison of two sequences is not done by comparing two vectors each representing an entire sequence. Instead, these models first compare vector representations of smaller units such as words from these sequences and then aggregate these comparison results to make the final decision. For example, the match-LSTM model proposed by Wang & Jiang (2016b) for textual entailment first compares each word in the hypothesis with an attention-weighted version of the premise. The comparison results are then aggregated through an LSTM. He & Lin (2016) proposed a pairwise word interaction model that first takes each pair of words from two sequences and applies a comparison unit on the two words. It then combines the results of these word interactions using a similarity focus layer followed by a multi-layer CNN. Parikh et al. (2016) proposed a decomposable attention model for textual entailment, in which words from each sequence are compared with an\nattention-weighted version of the other sequence to produce a series of comparison vectors. The comparison vectors are then aggregated and fed into a feed forward network for final classification.\nAlthough these studies have shown the effectiveness of such a \u201ccompare-aggregate\u201d framework for sequence matching, there are at least two limitations with these previous studies: (1) Each of the models proposed in these studies is tested on one or two tasks only, but we hypothesize that this general framework is effective on many sequence matching problems. There has not been any study that empirically verifies this. (2) More importantly, these studies did not pay much attention to the comparison function that is used to compare two small textual units. Usually a standard feedforward network is used (Hu et al., 2014; Wang & Jiang, 2016b) to combine two vectors representing two units that need to be compared, e.g., two words. However, based on the nature of these sequence matching problems, we essentially need to measure how semantically similar the two sequences are. Presumably, this property of these sequence matching problems should guide us in choosing more appropriate comparison functions. Indeed He & Lin (2016) used cosine similarity, Euclidean distance and dot product to define the comparison function, which seem to be better justifiable. But they did not systematically evaluate these similarity or distance functions or compare them with a standard feedforward network.\nIn this paper, we argue that the general \u201ccompare-aggregate\u201d framework is effective for a wide range of sequence matching problems. We present a model that follows this general framework and test it on four different datasets, namely, MovieQA, InsuranceQA, WikiQA and SNLI. The first three datasets are for Question Answering, but the setups of the tasks are quite different. The last dataset is for textual entailment. More importantly, we systematically present and test six different comparison functions. We find that overall a comparison function based on element-wise subtraction and multiplication works the best on the four datasets.\nThe contributions of this work are twofold: (1) Using four different datasets, we show that our model following the \u201ccompare-aggregate\u201d framework is very effective when compared with the state-ofthe-art performance on these datasets. (2) We conduct systematic evaluation of different comparison functions and show that a comparison function based on element-wise operations, which is not widely used for word-level matching, works the best across the different datasets. We believe that these findings will be useful for future research on sequence matching problems. We have also made our code available online.1\n2 METHOD\nIn this section, we propose a general model following the \u201ccompare-aggregate\u201d framework for matching two sequences. This general model can be applied to different tasks. We focus our discussion on six different comparison functions that can be plugged into this general \u201ccompare-aggregate\u201d model. In particular, we hypothesize that two comparison functions based on element-wise operations, SUB and MULT, are good middle ground between highly flexible functions using standard neural network models and highly restrictive functions based on cosine similarity and/or Euclidean\n1https://github.com/shuohangwang/SeqMatchSeq\ndistance. As we will show in the experiment section, these comparison functions based on elementwise operations can indeed perform very well on a number of sequence matching problems.\n2.1 PROBLEM DEFINITION AND MODEL OVERVIEW\nThe general setup of the sequence matching problem we consider is the following. We assume there are two sequences to be matched. We use two matrices Q \u2208 Rd\u00d7Q and A \u2208 Rd\u00d7A to represent the word embeddings of the two sequences, where Q and A are the lengths of the two sequences, respectively, and d is the dimensionality of the word embeddings. In other words, each column vector of Q or A is an embedding vector representing a single word. Given a pair of Q and A, the goal is to predict a label y. For example, in textual entailment, Q may represent a premise and A a hypothesis, and y indicates whether Q entails A or contradicts A. In question answering, Q may be a question and A a candidate answer, and y indicates whether A is the correct answer to Q.\nWe treat the problem as a supervised learning task. We assume that a set of training examples in the form of (Q,A, y) is given and we aim to learn a model that maps any pair of (Q,A) to a y.\nAn overview of our model is shown in Figure 1. The model can be divided into the following four layers:\n1. Preprocessing: We use a preprocessing layer (not shown in the figure) to process Q and A to obtain two new matrices Q \u2208 Rl\u00d7Q and A \u2208 Rl\u00d7A. The purpose here is to use some gate values to control the importance of different words in making the predictions on the sequence pair. For example, qi \u2208 Rl, which is the ith column vector of Q, encodes the ith word in Q.\n2. Attention: We apply a standard attention mechanism on Q and A to obtain attention weights over the column vectors in Q for each column vector in A. With these attention weights, for each column vector aj in A, we obtain a corresponding vector hj , which is an attention-weighted sum of the column vectors of Q.\n3. Comparison: We use a comparison function f to combine each pair of aj and hj into a vector tj .\n4. Aggregation: We use a CNN layer to aggregate the sequence of vectors tj for the final classification.\nAlthough this model follows more or less the same framework as the model proposed by Parikh et al. (2016), our work has some notable differences. First, we will pay much attention to the comparison function f and compare a number of options, including some uncommon ones based on elementwise operations. Second, we apply our model to four different datasets representing four different tasks to evaluate its general effectiveness for sequence matching problems. There are also some other differences from the work by Parikh et al. (2016). For example, we use a CNN layer instead of summation and concatenation for aggregation. Our attention mechanism is one-directional instead of two-directional.\nIn the rest of this section we will present the model in detail. We will focus mostly on the comparison functions we consider.\n2.2 PREPROCESSING AND ATTENTION\nInspired by the use of gates in LSTM and GRU, we preprocess Q and A with the following formulas:\nQ = \u03c3(WiQ+ bi \u2297 eQ) tanh(WuQ+ bu \u2297 eQ), A = \u03c3(WiA+ bi \u2297 eA) tanh(WuA+ bu \u2297 eA), (1)\nwhere is element-wise multiplication, and Wi,Wu \u2208 Rl\u00d7d and bi,bu \u2208 Rl are parameters to be learned. The outer product (\u00b7 \u2297 eX) produces a matrix or row vector by repeating the vector or scalar on the left for X times. Here \u03c3(WiQ + bi \u2297 eQ) and \u03c3(WiA + bi \u2297 eA) act as gate values to control the degree to which the original values of Q and A are preserved in Q and A. For example, for stop words, their gate values would likely be low for tasks where stop words make little difference to the final predictions.\nIn this preprocessing step, the word order does not matter. Although a better way would be to use RNN such as LSTM and GRU to chain up the words such that we can capture some contextual information, this could be computationally expensive for long sequences. In our experiments, we only incorporated LSTM into the formulas above for the SNLI task.\nThe general attention (Luong et al., 2015) layer is built on top of the resulting Q and A as follows: G = softmax ( (WgQ+ bg \u2297 eQ)TA ) ,\nH = QG, (2)\nwhere Wg \u2208 Rl\u00d7l and bg \u2208 Rl are parameters to be learned, G \u2208 RQ\u00d7A is the attention weight matrix, and H \u2208 Rl\u00d7A are the attention-weighted vectors. Specifically, hj , which is the jth column vector of H, is a weighted sum of the column vectors of Q and represents the part of Q that best matches the jth word in A. Next we will combine hj and aj using a comparison function.\n2.3 COMPARISON\nThe goal of the comparison layer is to match each aj , which represents the jth word and its context in A, with hj , which represents a weighted version of Q that best matches aj . Let f denote a comparison function that transforms aj and hj into a vector tj to represent the comparison result.\nA natural choice of f is a standard neural network layer that consists of a linear transformation followed by a non-linear activation function. For example, we can consider the following choice:\nNEURALNET (NN): tj = f(aj ,hj) = ReLU(W [ aj hj ] + b), (3)\nwhere matrix W \u2208 Rl\u00d72l and vector b \u2208 Rl are parameters to be learned. Alternatively, another natural choice is a neural tensor network (Socher et al., 2013) as follows:\nNEURALTENSORNET (NTN): tj = f(aj ,hj) = ReLU(aTjT [1...l]hj + b), (4)\nwhere tensor T[1...l] \u2208 Rl\u00d7l\u00d7l and vector b \u2208 Rl are parameters to be learned.\nHowever, we note that for many sequence matching problems, we intend to measure the semantic similarity or relatedness of the two sequences. So at the word level, we also intend to check how similar or related aj is to hj . For this reason, a more natural choice used in some previous work is Euclidean distance or cosine similarity between aj and hj . We therefore consider the following definition of f :\nEUCLIDEAN+COSINE (EUCCOS): tj = f(aj ,hj) = [ \u2016aj \u2212 hj\u20162 cos(aj ,hj) ] . (5)\nNote that with EUCCOS, the resulting vector tj is only a 2-dimensional vector. Although EUCCOS is a well-justified comparison function, we suspect that it may lose some useful information from the original vectors aj and hj . On the other hand, NN and NTN are too general and thus do not capture the intuition that we care mostly about the similarity between aj and hj .\nTo use something that is a good compromise between the two extreme cases, we consider the following two new comparison functions, which operate on the two vectors in an element-wise manner. These functions have been used previously by Mou et al. (2016).\nSUBTRACTION (SUB): tj = f(aj ,hj) = (aj \u2212 hj) (aj \u2212 hj), (6) MULTIPLICATION (MULT): tj = f(aj ,hj) = aj hj . (7)\nNote that the operator is element-wise multiplication. For both comparison functions, the resulting vector tj has the same dimensionality as aj and hj .\nWe can see that SUB is closely related to Euclidean distance in that Euclidean distance is the sum of all the entries of the vector tj produced by SUB. But by not summing up these entries, SUB preserves some information about the different dimensions of the original two vectors. Similarly, MULT is closely related to cosine similarity but preserves some information about the original two vectors.\nFinally, we consider combining SUB and MULT followed by an NN layer as follows: SUBMULT+NN: tj = f(aj ,hj) = ReLU(W [ (aj \u2212 hj) (aj \u2212 hj)\naj hj\n] + b). (8)\nIn summary, we consider six different comparison functions: NN, NTN, EUCCOS, SUB, MULT and SUBMULT+NN. Among these functions, the last three (SUB, MULT and SUBMULT+NN) have not been widely used in previous work for word-level matching.\n2.4 AGGREGATION\nAfter we apply the comparison function to each pair of aj and hj to obtain a series of vectors tj , finally we aggregate these vectors using a one-layer CNN (Kim, 2014):\nr = CNN([t1, . . . , tA]). (9)\nr \u2208 Rnl is then used for the final classification, where n is the number of windows in CNN.\n3 EXPERIMENTS\nIn this section, we evaluate our model on four different datasets representing different tasks. The first three datasets are question answering tasks while the last one is on textual entailment. The statistics of the four datasets are shown in Table 2. We will fist introduce the task settings and the way we customize the \u201ccompare-aggregate\u201d structure to each task. Then we will show the baselines for the different datasets. Finally, we discuss the experiment results shown in Table 3 and the ablation study shown in Table 4.\n3.1 TASK-SPECIFIC MODEL STRUCTURES\nIn all these tasks, we use matrix Q \u2208 Rd\u00d7Q to represent the question or premise and matrix Ak \u2208 Rd\u00d7Ak (k \u2208 [1,K]) to represent the kth answer or the hypothesis. For the machine comprehension task MovieQA (Tapaswi et al., 2016), there is also a matrix P \u2208 Rd\u00d7P that represents the plot of a movie. Here Q is the length of the question or premise, Ak the length of the kth answer, and P the length of the plot.\nFor the SNLI (Bowman et al., 2015) dataset, the task is text entailment, which identifies the relationship (entailment, contradiction or neutral) between a premise sentence and a hypothesis sentence. Here K = 1, and there are exactly two sequences to match. The actual model structure is what we have described before.\nFor the InsuranceQA (Feng et al., 2015) dataset, the task is an answer selection task which needs to select the correct answer for a question from a candidate pool. For the WikiQA (Yang et al., 2015) datasets, we need to rank the candidate answers according to a question. For both tasks,\nthere are K candidate answers for each question. Let us use rk to represent the resulting vector produced by Eqn. 9 for the kth answer. In order to select one of the K answers, we first define R = [r1, r2, . . . , rK ]. We then compute the probability of the kth answer to be the correct one as follows:\np(k|R) = softmax(wT tanh(WsR+ bs \u2297 eK) + b\u2297 eK), (10) where Ws \u2208 Rl\u00d7nl, w \u2208 Rl, bs \u2208 Rl, b \u2208 R are parameters to be learned. For the machine comprehension task MovieQA, each question is related to Plot Synopses written by fans after watching the movie and each question has five candidate answers. So for each candidate answer there are three sequences to be matched: the plot P, the question Q and the answer Ak. For each k, we first match Q and P and refer to the matching result at position j as tqj , as generated by one of the comparison functions f . Similarly, we also match Ak with P and refer to the matching result at position j as tak,j . We then define\ntk,j = [ tqj tak,j ] ,\nand\nrk = CNN([tk,1, . . . , tk,P ]).\nTo select an answer from the K candidate answers, again we use Eqn. 10 to compute the probabilities.\nThe implementation details of the modes are as follows. The word embeddings are initialized from GloVe (Pennington et al., 2014). During training, they are not updated. The word embeddings not found in GloVe are initialized with zero.\nThe dimensionality l of the hidden layers is set to be 150. We use ADAMAX (Kingma & Ba, 2015) with the coefficients \u03b21 = 0.9 and \u03b22 = 0.999 to optimize the model. We do not use L2regularization. The main parameter we tuned is the dropout on the embedding layer. For WikiQA, which is a relatively small dataset, we also tune the learning rate and the batch size. For the others, we set the batch size to be 30 and the learning rate 0.002.\n3.2 BASELINES\nHere, we will introduce the baselines for each dataset. We did not re-implement these models but simply took the reported performance for the purpose of comparison.\nSNLI: \u2022W-by-W Attention: The model by Rockta\u0308schel et al. (2015), who first introduced attention mechanism into text entailment. \u2022 match-LSTM: The model by Wang & Jiang (2016b), which concatenates the matched words as the inputs of an LSTM. \u2022 LSTMN: Long short-term memorynetworks proposed by Cheng et al. (2016). \u2022 Decomp Attention: Another \u201ccompare-aggregate\u201d model proposed by Parikh et al. (2016). \u2022 EBIM+TreeLSTM: The state-of-the-art model proposed by Chen et al. (2016) on the SNLI dataset.\nInsuranceQA: \u2022 IR model: This model by Bendersky et al. (2010) learns the concept information to help rank the candidates. \u2022 CNN with GESD: This model by Feng et al. (2015) uses Euclidean distance and dot product between sequence representations built through convolutional neural networks to select the answer. \u2022 Attentive LSTM: Tan et al. (2016) used soft-attention mechanism to select the most important information from the candidates according to the representation of the questions. \u2022 IARNN-Occam: This model by Wang et al. (2016) adds regularization on the attention weights. \u2022 IARNN-Gate: This model by Wang et al. (2016) uses the representation of the question to build the GRU gates for each candidate answer.\nWikiQA: \u2022 IARNN-Occam and IARNN-Gate as introduced before. \u2022 CNN-Cnt: This model by Yang et al. (2015) combines sentence representations built by a convolutional neural network with logistic regression. \u2022 ABCNN: This model is Attention-Based Convolutional Neural Network proposed by Yin et al. (2015). \u2022 CubeCNN proposed by He & Lin (2016) builds a CNN on all pairs of word similarity.\nMovieQA: All the baselines we consider come from Tapaswi et al. (2016)\u2019s work: \u2022 Cosine Word2Vec: A sliding window is used to select the answer according to the similarities computed\nthrough Word2Vec between the sentences in plot and the question/answer. \u2022 Cosine TFIDF: This model is similar to the previous method but uses bag-of-word with tf-idf scores to compute similarity. \u2022 SSCB TFIDF: Instead of using the sliding window method, a convolutional neural network is built on the sentence level similarities.\n3.3 ANALYSIS OF RESULTS\nWe use accuracy as the evaluation metric for the datasets MovieQA, InsuranceQA and SNLI, as there is only one correct answer or one label for each instance. For WikiQA, there may be multiple correct answers, so evaluation metrics we use are Mean Average Precision (MAP) and Mean Reciprocal Rank (MRR).\nWe observe the following from the results. (1) Overall, we can find that our general \u201ccompareaggregate\u201d structure achieves the best performance on MovieQA, InsuranceQA, WikiQA datasets and very competitive performance on the SNLI dataset. Especially for the InsuranceQA dataset, with any comparison function we use, our model can outperform all the previous models. (2) The comparison method SUBMULT+NN is the best in general. (3) Some simple comparison functions can achieve better performance than the neural networks or neural tensor network comparison functions. For example, the simplest comparison function EUCCOS achieves nearly the best performance in the MovieQA dataset, and the element-wise comparison functions, which do not need parameters can achieve the best performance on the WikiQA dataset. (4) We find the preprocessing layer and the attention layer for word selection to be important in the \u201ccompare-aggregate\u201d structure through the experiments of removing these two layers separately. We also see that for sequence matching with big difference in length, such as the MovieQA and InsuranceQA tasks, the attention layer plays a more important role. For sequence matching with smaller difference in length, such as the WikiQA and SNLI tasks, the pre-processing layer plays a more important role. (5) For the MovieQA, InsuranceQA and WikiQA tasks, our preprocessing layer is order-insensitive so that it will not take the context information into consideration during the comparison, but our model can still outperform the previous work with order-sensitive preprocessing layer. With this finding, we believe the word-by-word comparison part plays a very important role in these tasks. We will further explore the preprocessing layer in the future.\n3.4 FURTHER ANALYSES\nTo further explain how our model works, we visualize the max values in each dimension of the convolutional layer. We use two examples shown in Table 1 from MovieQA and InsuranceQA datasets respectively. In the top of Figure 2, we can see that the plot words that also appear in either the question or the answer will draw more attention by the CNN. We hypothesize that if the nearby words in the plot can match both the words in question and the words in one answer, then this answer is more likely to be the correct one. Similarly, the bottom one of Figure 2 also shows that the CNN will focus more on the matched word representations. If the words in one answer continuously match the words in the question, this answer is more likely to be the correct one.\n4 RELATED WORK\nWe review related work in three types of general structures for matching sequences.\nSiamense network: These kinds of models use the same structure, such as RNN or CNN, to build the representations for the sequences separately and then use them for classification. Then cosine similarity (Feng et al., 2015; Yang et al., 2015), element-wise operation (Tai et al., 2015; Mou et al., 2016) or neural network-based combination Bowman et al. (2015) are used for sequence matching.\nAttentive network: Soft-attention mechanism (Bahdanau et al., 2014; Luong et al., 2015) has been widely used for sequence matching in machine comprehension (Hermann et al., 2015), text entailment (Rockta\u0308schel et al., 2015) and question answering (Tan et al., 2016). Instead of using the final state of RNN to represent a sequence, these studies use weighted sum of all the states for the sequence representation.\nCompare-Aggregate network: This kind of framework is to perform the word level matching (Wang & Jiang, 2016a; Parikh et al., 2016; He & Lin, 2016; Trischler et al., 2016; Wan et al.,\n2016). Our work is under this framework. But our structure is different from previous models and our model can be applied on different tasks. Besides, we analyzed different word-level comparison functions separately.\n5 CONCLUSIONS\nIn this paper, we systematically analyzed the effectiveness of a \u201ccompare-aggregate\u201d model on four different datasets representing different tasks. Moreover, we compared and tested different kinds of word-level comparison functions and found that some element-wise comparison functions can outperform the others. According to our experiment results, many different tasks can share the same \u201ccompare-aggregate\u201d structure. In the future work, we would like to test its effectiveness on multi-task learning.\n6 ACKNOWLEDGMENTS\nThis research is supported by the National Research Foundation, Prime Ministers Office, Singapore under its International Research Centres in Singapore Funding Initiative.\n", "cats": {"READ": 1.0, "DONTREAD": 0.0}, "reviews": [{"IS_META_REVIEW": true, "comments": "The paper presents a general approach to modeling for natural language understanding problems with two distinct textual inputs (such as a question and a source text) that can be aligned in some way. In the approach, soft attention is first used to derive alignments between the tokens of the two texts, then a comparison function uses the resulting alignments (represented as pairs of attention queries and attention results) to derive a representations that are aggregated by CNN into a single vector from which an output can be computed. The paper both presents this as an overall modeling strategy that can be made to work quite well, and offers a detailed empirical analysis of the comparison component of the model.\n\nThis work is timely. Language understanding problems of this kind are a major open issue in NLP, and are just at the threshold of being addressable with representation learning methods. The work presents a general approach which is straightforward and reasonable, and shows that it can yield good results. The work borders on incremental (relative to their earlier work or that of Parikh et al.), but it contributes in enough substantial ways that I'd strongly recommend acceptance.\n\nDetail: \n- The model, at least as implemented for the problems with longer sequences (everything but SNLI), is not sensitive to word order. It is empirically competitive, but this insensitivity places a strong upper bound on its performance. The paper does make this clear, but it seems salient enough to warrant a brief mention in the introduction or discussion sections.\n- If I understand correctly, your attention strategy is based more closely on the general/bilinear strategy of Luong et al. '15 than it is on the earlier Bahdanau work. You should probably cite the former (or some other more directly relevant reference for that strategy).\n- Since the NTN risks overfitting because of its large number of parameters, did you try using a version with input dimension l and a smaller output dimension m (so an l*l*m tensor)?\n- You should probably note that SubMultNN looks a lot like the strategy for *sentence*-level matching in the Lili Mou paper you cite.\n- Is there a reason you use the same parameters for preprocessing the question and answer in (1)? These could require different things to be weighted highly."}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "This paper proposes a framework whereby, to an attention mechanism relating one text segment to another piecewise, an aggregation mechanism is added to yield an architecture matching words of one segment to another. Different vector comparison operations are explored in this framework. The reviewers were satisfied that this work is relevant, timely, clearly presented, and that the empirical validation was sound. ", "OTHER_KEYS": "ICLR 2017 pcs"}, {"IMPACT": 3, "RECOMMENDATION_UNOFFICIAL": 5, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "This paper proposed a compare-aggregate model for the NLP tasks that require semantically comparing the text sequences, such as question answering and textual entailment.\nThe basic framework of this model is to apply a convolutional neural network (aggregation) after a element-wise operation (comparison) over the attentive outputs of the LSTMs. \nThe highlighted part is the comparison, where this paper compares several different methods for matching text sequences, and the element-wise subtraction/multiplication operations are demonstrated to achieve generally better performance on four different datasets.\nWhile the weak point is that this is an incremental work and a bit lack of innovation. A qualitative evaluation about how subtraction, multiplication and other comparison functions perform on varied kinds of sentences would be more interesting.   \n\n", "IS_ANNOTATED": true, "TITLE": "Official Review", "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "16 Dec 2016", "CLARITY": 5, "REVIEWER_CONFIDENCE": 4}, {"SUBSTANCE": 4, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "TITLE": "A solid empirical study", "comments": "This paper proposes a compare-aggregate framework that performs word-level matching followed by aggregation with convolutional neural networks. It compares six different comparison functions and evaluates them on four datasets. Extensive experimental results have been reported and compared against various published baselines.\n\nThe paper is well written overall.\n\nA few detailed comments:\n* page 4, line5: including a some -> including some\n* What's the benefit of the preprocessing and attention step? Can you provide the results without it?\n* Figure 2 is hard to read, esp. when on printed hard copy. Please enhance the quality.\n", "SOUNDNESS_CORRECTNESS": 5, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 5}, {"OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "TITLE": "Effective model design, great evaluation", "comments": "The paper presents a general approach to modeling for natural language understanding problems with two distinct textual inputs (such as a question and a source text) that can be aligned in some way. In the approach, soft attention is first used to derive alignments between the tokens of the two texts, then a comparison function uses the resulting alignments (represented as pairs of attention queries and attention results) to derive a representations that are aggregated by CNN into a single vector from which an output can be computed. The paper both presents this as an overall modeling strategy that can be made to work quite well, and offers a detailed empirical analysis of the comparison component of the model.\n\nThis work is timely. Language understanding problems of this kind are a major open issue in NLP, and are just at the threshold of being addressable with representation learning methods. The work presents a general approach which is straightforward and reasonable, and shows that it can yield good results. The work borders on incremental (relative to their earlier work or that of Parikh et al.), but it contributes in enough substantial ways that I'd strongly recommend acceptance.\n\nDetail: \n- The model, at least as implemented for the problems with longer sequences (everything but SNLI), is not sensitive to word order. It is empirically competitive, but this insensitivity places a strong upper bound on its performance. The paper does make this clear, but it seems salient enough to warrant a brief mention in the introduction or discussion sections.\n- If I understand correctly, your attention strategy is based more closely on the general/bilinear strategy of Luong et al. '15 than it is on the earlier Bahdanau work. You should probably cite the former (or some other more directly relevant reference for that strategy).\n- Since the NTN risks overfitting because of its large number of parameters, did you try using a version with input dimension l and a smaller output dimension m (so an l*l*m tensor)?\n- You should probably note that SubMultNN looks a lot like the strategy for *sentence*-level matching in the Lili Mou paper you cite.\n- Is there a reason you use the same parameters for preprocessing the question and answer in (1)? These could require different things to be weighted highly.\n", "ORIGINALITY": 3, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "RECOMMENDATION": 8, "DATE": "11 Dec 2016", "REVIEWER_CONFIDENCE": 5}, {"IMPACT": 3, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "RECOMMENDATION_UNOFFICIAL": 5, "comments": "", "IS_ANNOTATED": true, "TITLE": "Lengths of the sequences", "IS_META_REVIEW": false, "DATE": "03 Dec 2016", "CLARITY": 5}, {"DATE": "30 Nov 2016", "TITLE": "Attention or Matching Matrix?", "IS_META_REVIEW": false, "comments": "Text matching models based on Attention mechanism make sense. \nThere are also some matching models based on Matching Matrix.\nAttention mechanism also computes a matching matrix implicitly and the attention weights before softmax are the values of the matching matrix. \nI wonder which way is better, Attention or Matching Matrix, and Why?\nHow do you think\uff1f\nI will appreciate it if you could compare these models in your future works.\n\nReference of Matching Matrix Models:\n1. A Deep Architecture for Semantic Matching with Multiple Positional Sentence Representations. AAAI 2016.\n2. Match-SRNN: Modeling the Recursive Matching Structure with Spatial RNN. IJCAI 2016.\n3. Text Matching as Image Recognition. AAAI 2016.", "OTHER_KEYS": "(anonymous)"}, {"TITLE": "Clarification for Sec. 2.2/Eqn. 1", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "", "ORIGINALITY": 3, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "DATE": "19 Nov 2016"}, {"IS_META_REVIEW": true, "comments": "The paper presents a general approach to modeling for natural language understanding problems with two distinct textual inputs (such as a question and a source text) that can be aligned in some way. In the approach, soft attention is first used to derive alignments between the tokens of the two texts, then a comparison function uses the resulting alignments (represented as pairs of attention queries and attention results) to derive a representations that are aggregated by CNN into a single vector from which an output can be computed. The paper both presents this as an overall modeling strategy that can be made to work quite well, and offers a detailed empirical analysis of the comparison component of the model.\n\nThis work is timely. Language understanding problems of this kind are a major open issue in NLP, and are just at the threshold of being addressable with representation learning methods. The work presents a general approach which is straightforward and reasonable, and shows that it can yield good results. The work borders on incremental (relative to their earlier work or that of Parikh et al.), but it contributes in enough substantial ways that I'd strongly recommend acceptance.\n\nDetail: \n- The model, at least as implemented for the problems with longer sequences (everything but SNLI), is not sensitive to word order. It is empirically competitive, but this insensitivity places a strong upper bound on its performance. The paper does make this clear, but it seems salient enough to warrant a brief mention in the introduction or discussion sections.\n- If I understand correctly, your attention strategy is based more closely on the general/bilinear strategy of Luong et al. '15 than it is on the earlier Bahdanau work. You should probably cite the former (or some other more directly relevant reference for that strategy).\n- Since the NTN risks overfitting because of its large number of parameters, did you try using a version with input dimension l and a smaller output dimension m (so an l*l*m tensor)?\n- You should probably note that SubMultNN looks a lot like the strategy for *sentence*-level matching in the Lili Mou paper you cite.\n- Is there a reason you use the same parameters for preprocessing the question and answer in (1)? These could require different things to be weighted highly."}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "This paper proposes a framework whereby, to an attention mechanism relating one text segment to another piecewise, an aggregation mechanism is added to yield an architecture matching words of one segment to another. Different vector comparison operations are explored in this framework. The reviewers were satisfied that this work is relevant, timely, clearly presented, and that the empirical validation was sound. ", "OTHER_KEYS": "ICLR 2017 pcs"}, {"IMPACT": 3, "RECOMMENDATION_UNOFFICIAL": 5, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "This paper proposed a compare-aggregate model for the NLP tasks that require semantically comparing the text sequences, such as question answering and textual entailment.\nThe basic framework of this model is to apply a convolutional neural network (aggregation) after a element-wise operation (comparison) over the attentive outputs of the LSTMs. \nThe highlighted part is the comparison, where this paper compares several different methods for matching text sequences, and the element-wise subtraction/multiplication operations are demonstrated to achieve generally better performance on four different datasets.\nWhile the weak point is that this is an incremental work and a bit lack of innovation. A qualitative evaluation about how subtraction, multiplication and other comparison functions perform on varied kinds of sentences would be more interesting.   \n\n", "IS_ANNOTATED": true, "TITLE": "Official Review", "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "16 Dec 2016", "CLARITY": 5, "REVIEWER_CONFIDENCE": 4}, {"SUBSTANCE": 4, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "TITLE": "A solid empirical study", "comments": "This paper proposes a compare-aggregate framework that performs word-level matching followed by aggregation with convolutional neural networks. It compares six different comparison functions and evaluates them on four datasets. Extensive experimental results have been reported and compared against various published baselines.\n\nThe paper is well written overall.\n\nA few detailed comments:\n* page 4, line5: including a some -> including some\n* What's the benefit of the preprocessing and attention step? Can you provide the results without it?\n* Figure 2 is hard to read, esp. when on printed hard copy. Please enhance the quality.\n", "SOUNDNESS_CORRECTNESS": 5, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 5}, {"OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "TITLE": "Effective model design, great evaluation", "comments": "The paper presents a general approach to modeling for natural language understanding problems with two distinct textual inputs (such as a question and a source text) that can be aligned in some way. In the approach, soft attention is first used to derive alignments between the tokens of the two texts, then a comparison function uses the resulting alignments (represented as pairs of attention queries and attention results) to derive a representations that are aggregated by CNN into a single vector from which an output can be computed. The paper both presents this as an overall modeling strategy that can be made to work quite well, and offers a detailed empirical analysis of the comparison component of the model.\n\nThis work is timely. Language understanding problems of this kind are a major open issue in NLP, and are just at the threshold of being addressable with representation learning methods. The work presents a general approach which is straightforward and reasonable, and shows that it can yield good results. The work borders on incremental (relative to their earlier work or that of Parikh et al.), but it contributes in enough substantial ways that I'd strongly recommend acceptance.\n\nDetail: \n- The model, at least as implemented for the problems with longer sequences (everything but SNLI), is not sensitive to word order. It is empirically competitive, but this insensitivity places a strong upper bound on its performance. The paper does make this clear, but it seems salient enough to warrant a brief mention in the introduction or discussion sections.\n- If I understand correctly, your attention strategy is based more closely on the general/bilinear strategy of Luong et al. '15 than it is on the earlier Bahdanau work. You should probably cite the former (or some other more directly relevant reference for that strategy).\n- Since the NTN risks overfitting because of its large number of parameters, did you try using a version with input dimension l and a smaller output dimension m (so an l*l*m tensor)?\n- You should probably note that SubMultNN looks a lot like the strategy for *sentence*-level matching in the Lili Mou paper you cite.\n- Is there a reason you use the same parameters for preprocessing the question and answer in (1)? These could require different things to be weighted highly.\n", "ORIGINALITY": 3, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "RECOMMENDATION": 8, "DATE": "11 Dec 2016", "REVIEWER_CONFIDENCE": 5}, {"IMPACT": 3, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "RECOMMENDATION_UNOFFICIAL": 5, "comments": "", "IS_ANNOTATED": true, "TITLE": "Lengths of the sequences", "IS_META_REVIEW": false, "DATE": "03 Dec 2016", "CLARITY": 5}, {"DATE": "30 Nov 2016", "TITLE": "Attention or Matching Matrix?", "IS_META_REVIEW": false, "comments": "Text matching models based on Attention mechanism make sense. \nThere are also some matching models based on Matching Matrix.\nAttention mechanism also computes a matching matrix implicitly and the attention weights before softmax are the values of the matching matrix. \nI wonder which way is better, Attention or Matching Matrix, and Why?\nHow do you think\uff1f\nI will appreciate it if you could compare these models in your future works.\n\nReference of Matching Matrix Models:\n1. A Deep Architecture for Semantic Matching with Multiple Positional Sentence Representations. AAAI 2016.\n2. Match-SRNN: Modeling the Recursive Matching Structure with Spatial RNN. IJCAI 2016.\n3. Text Matching as Image Recognition. AAAI 2016.", "OTHER_KEYS": "(anonymous)"}, {"TITLE": "Clarification for Sec. 2.2/Eqn. 1", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "", "ORIGINALITY": 3, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "DATE": "19 Nov 2016"}]}
{"text": "CLASSLESS ASSOCIATION USING NEURAL NETWORKS\n1 INTRODUCTION\nInfants are able to learn the binding between abstract concepts to the real world via their sensory input. For example, the abstract concept ball is binding to the visual representation of a rounded object and the auditory representation of the phonemes /b/ /a/ /l/. This scenario can be seen as the Symbol Grounding Problem (Harnad, 1990). Moreover, infants are also able to learn the association between different sensory input signals while they are still learning the binding of the abstract concepts. Several results have shown a correlation between object recognition (visual) and vocabulary acquisition (auditory) in infants (Balaban & Waxman, 1997; Asano et al., 2015). One example of this correlation is the first words that infants have learned. In that case, the words are mainly nouns, which are visible concepts, such as, dad, mom, ball, dog, cat (Gershkoff-Stowe & Smith, 2004). As a result, we can define the previous scenario in terms of a machine learning tasks. More formally, the task is defined by learning the association between two parallel streams of data that represent the same unknown class (or abstract concept). Note that this task is different from the supervised association where the data has labels. First, the semantic concept is unknown in our scenario whereas it is known in the supervised case. Second, both classifiers needs to agree on the same coding scheme for each sample pair during training. In contrast, the coding-scheme is already pre-defined before training in the supervised case. Figure 1 shows an example of the difference between a supervised association task and our scenario.\nUsually, classifiers requires labeled data for training. However, the presented scenario needs an alternative training mechanism. One way is to train based on statistical distributions. Casey (1986) proposed to solve the OCR problem using language statistics for inferring form images to characters. Later on, Knight et al. (2006) applied a similar idea to machine translation. Recently, Sutskever et al. (2015) defined the Output Distribution Matching (ODM) cost function for dual autoencoders and generative networks.\nIn this paper, we are proposing a novel model that is trained based on the association of two input samples of the same unknown class. The presented model has two parallel Multilayer Perceptrons (MLPs) with an Expectation-Maximization (EM) (Dempster et al., 1977) training rule that matches the network output against a statistical distribution. Also, both networks agree on the same classification because one network is used as target of the other network, and vice versa. Our model has some\nsimilarities with Siamese Networks proposed by Chopra et al. (2005). They introduced their model for supervised face verification where training is based on constraints of pairs of faces. The constraints exploit the relation of two faces that may or may not be instances of the same person. However, there are some differences to our work. First, our training rule does not have pre-defined classes before training, whereas the Siamese Network requires labeled samples. Second, our model only requires instances of the same unknown class, whereas the Siamese network requires two types of input pairs: a) instances of the same person and b) instances of two different persons. Our contributions in this paper are\n\u2022 We define a novel training rule based on matching the output vectors of the presented model and a statistical distribution. Note that the output vectors are used as symbolic features similar to the Symbol Grounding Problem. Furthermore, the proposed training rule is based on an EM-approach and classified each sample based on generated pseudo-classes (Section 2.1).\n\u2022 We propose a novel architecture for learning the association in the classless scenario. Moreover, the presented model uses two parallel MLPs, which require to agree on the same class for each input sample. This association is motivated by the correlation between different sensory input signals in infants development. In more detail, one network is the target of the other network, and vice versa. Also, note that our model is gradient-based and can be extended to deeper architectures (Section 2.2).\n\u2022 We evaluate our classless association task against two cases: totally supervised and totally unsupervised. In this manner, we can verify the range of our results in terms of supervised and unsupervised cases since our model is neither totally supervised nor totally unsupervised. We compare against a MLP trained with labels as the supervised scenario (upper bound) and two clustering algorithms (K-means and Hierarchical Agglomerative) as the unsupervised scenario (lower bound). First, our model reaches better results than the clustering. Second, our model shows promising results with respect to the supervised scenario (Sections 3 and 4).\n2 METHODOLOGY\nIn this paper, we are interested in the classless association task in the following scenario: two input instances x(1) and x(2) belong to the same unknown class c, where x(1) \u2208 X(1) and x(2) \u2208 X(2) are two disjoint sets, and the goal is to learn the output classification of x(1) and x(2) is the same c(1) = c(2), where c(1) and c(2) \u2208 C is the set of possible classes. With this in mind, we present a model that has two parallel Multilayer Perceptrons (MLPs) that are trained with an EM-approach that associates both networks in the following manner: one network uses the other network as a target, and vice versa. We explain how the output vectors of the network are matched to a statistical distribution in Section 2.1 and the classless association learning is presented in Section 2.2.\n2.1 STATISTICAL CONSTRAINT\nOne of our constraint is to train a MLP without classes. As a result, we use an alternative training rule based on matching the output vectors and a statistical distribution. For simplicity, we explain our training rule using a single MLP with one hidden layer, which is defined by\nz = network(x;\u03b8) (1)\nwhere x \u2208 Rn is the input vector, \u03b8 encodes the parameters of the MLP, and z \u2208 Rc is the output vector. Moreover, the output vectors (z1, . . . ,zm) of a mini-batch of size m are matched to a target distribution (E[z1, . . . ,zm] \u223c \u03c6 \u2208 Rc), e.g., uniform distribution. We have selected a uniform distribution because it is an ideal case to have a balanced dataset for any classifier. However, it is possible to extend to different distribution. We introduce a new parameter that is a weighting vector \u03b3 \u2208 Rc. The intuition behind it is to guide the network based on a set of generated pseudo-classes c. These pseudo-classes can be seen as cluster indexes that group similar elements. With this in mind, we also propose an EM-training rule for learning the unknown class given a desired target distribution. We want to point out that the pseudo-classes are internal representations of the network that are independent of the labels.\nThe E-step obtains the current statistical distribution given the output vectors (z1, . . . ,zm) and the weighting vector (\u03b3). In this case, an approximation of the distribution is obtained by the following equation\nz\u0302 = 1\nM M\u2211 i=1 power(zi,\u03b3) (2)\nwhere \u03b3 is the weighting vector, zi is the output vector of the network, M is the number of elements, and the function power1 is the element-wise power operation between the output vector and the weighting vector. We have used the power function because the output vectors (z1, . . . , zm) are quite similar between them at the initial state of the network, and the power function provides an initial boost for learning to separate the input samples in different pseudo-classes in the first iterations. Moreover, we can retrieve the pseudo-classes by the maximum value of the following equation\nc\u2217 = arg maxc power(zi,\u03b3) (3)\nwhere c\u2217 is the pseudo-class, which are used in the M-step for updating the MLP weights. Also, note that the pseudo-classes are not updated in an online manner. Instead, the pseudo-classes are updated after a certain number of iterations. The reason is the network requires a number of iterations to learn the common features.\nThe M-step updates the weighting vector \u03b3 given the current distribution z\u0302. Also, the MLP parameters (\u03b8) are updated given the current classification given by the pseudo-classes. The cost function is the variance between the distribution and the desired statistical distribution, which is defined by\ncost = (z\u0302 \u2212 \u03c6)2 (4) 1We decide to use power function instead of z\u03b3i in order to simplify the index notation\nwhere z\u0302 is the current statistical distribution of the output vectors, and \u03c6 is a vector that represent the desired statistical distribution, e.g. uniform distribution. Then, the weighting vector is updated via gradient descent\n\u03b3 = \u03b3 \u2212 \u03b1 \u2217 \u2207\u03b3cost (5)\nwhere \u03b1 is the learning rate and \u2207\u03b3cost is the derivative w.r.t \u03b3. Also, the MLP weights are updated via the generated pseudo-classes, which are used as targets in the backpropagation step.\nIn summary, we propose an EM-training rule for matching the network output vectors and a desired target statistical distribution. The E-Step generates pseudo-classes and finds an approximation of the current statistical distribution of the output vectors. The M-Step updates the MLP parameters and the weighting vector. With this in mind, we adapt the mentioned training rule for the classless association task. Figure 2 summarizes the presented EM training rule and its components.\n2.2 CLASSLESS ASSOCIATION LEARNING\nOur second constraint is to classify both input samples as the same class and different from the other classes. Note that the pseudo-class (Equation 3) is used as identification for each input sample and it is not related to the semantic concept or labels. The presented classless association model is trained based on a statistical constraint. Formally, the input is represented by the pair x(1) \u2208 Rn1 and x(2) \u2208 Rn2 where x(1) and x(2) are two different instances of the same unknown label. The classless association model has two parallel Multilayer Perceptron MLP (1) and MLP (2) with training rule that follows an EM-approach (cf. Section 2.1). Moreover, input samples are divided into several mini-batches of size m.\nInitially, all input samples have random pseudo-classes c(1)i and c (2) i . The pseudo-classes have the same desired statistical distribution \u03c6. Also, the weighting vectors \u03b3(1) and \u03b3(2) are initialized to one. Then each input element from the mini-batch is propagated forward to each MLP. Afterwards, an estimation of the statistical distribution for each MLP (z\u0302(1) and z\u0302(2)) is obtained. Furthermore, a new set of pseudo-classes (c(1)1 , . . . , c (1) m and c (2) 1 , . . . , c (2) m ) is obtained for each network. Note that this first part can be seen as an E-step from Section 2.1. We want to point out that the pseudo-classes are updated only after a number of iterations.\nThe second part of our association training updates the MLP parameters and the weighting vector (\u03b3(1) and \u03b3(2)). In this step, one network (MLP (1)) uses pseudo-classes (c(2)1 , . . . , c (2) m ) obtained from the other network (MLP (2)), and vice versa. In addition, the weighting vector is updated\nbetween the output approximation (z\u0302(1) and z\u0302(2)) and the desired target distribution (\u03c6). Figure 3 shows an overview of the presented model.\n3 EXPERIMENTS\nIn this paper, we are interested in a simplified scenario inspired by the Symbol Grounding Problem and the association learning between sensory input signal in infants. We evaluated our model in four classless datasets that are generated from MNIST (Lecun & Cortes, 2010). The procedure of generating classless datasets from labeled datasets have been already applied in (Sutskever et al., 2015; Hsu & Kira, 2015). Each dataset has two disjoint sets input 1 and input 2. The first dataset (MNIST) has two different instances of the same digit. The second dataset (Rotated-90 MNIST) has two different instances of the same digit, and all input samples in input 2 are rotated 90 degrees. The third dataset (Inverted MNIST ) follows a similar procedures as the second dataset, but the transformation of the elements in input 2 is the invert function instead of rotation. The last dataset (Random Rotated MNIST) is more challenging because all elements in input 2 are randomly rotated between 0 and 2\u03c0. All datasets have a uniform distribution between the digits and the dataset size is 21,000 samples for training and 4,000 samples for validation and testing.\nThe following parameters turned out being optimal on the validation set. For the first three datasets, each internal MLP relies on two fully connected layers of 200 and 100 neurons respectively. The learning rate for the MLPs was set to start at 1.0 and was continuously decaying by half after every 1,000 iterations. We set the initial weighting vector to 1.0 and updated after every 1,000 iterations as well. Moreover, the best parameters for the fourth dataset were the same forMLP (1) and different for MLP (2), which has two fully connected layers of 400 and 150 neurons respectively and the learning rate stars at 1.2. The target distribution \u03c6 is uniform for all datasets. The decay of the learning rate (Equation 5) for the weighting vector was given by 1/(100+epoch)0.3, where epoch was the number of training iterations so far. The mini-batch size M is 5,250 sample pairs (corresponding to 25% of the training set) and the mean of the derivatives for each mini-batch is used for the back-propagation step of MLPs. Note that the mini-batch is quite big comparing with common setups. We infer from this parameter that the model requires a sample size big enough for estimating the uniform distribution and also needs to learn slower than traditional approaches. Our model was implemented in Torch.\nMLP(1) MLP (2) Association Matrix (%) Purity (%)\n10.9 10.9 __________Initial State\nEpoch 1,000\n0 1 2 3 4 5 6 7 8 9 MLP (2)\n0 1 2 3 4 5 6 7 8 9 M LP (1 ) 0.7 0.6 2.4 0.4 3.0 2.2 1.0 1.4 1.2 0.6\n0\n2\n4\n6\n8\n10\n12\n14\n16\n18\n20\n24.8 22.6\nEpoch 3,000\n0 1 2 3 4 5 6 7 8 9 MLP (2)\n0 1 2 3 4 5 6 7 8 9 M LP (1 ) 0.1 1.7 2.8 0.0 5.7 8.9 0.3 10.0 8.1 0.1\n0\n2\n4\n6\n8\n10\n12\n14\n16\n18\n20\n64.4 65.8\nEpoch 49,000\n0 1 2 3 4 5 6 7 8 9 MLP (2)\n0 1 2 3 4 5 6 7 8 9 M LP (1 ) 9.8 8.9 9.4 9.0 9.6 9.6 9.7 9.6 9.6 9.0\n0\n2\n4\n6\n8\n10\n12\n14\n16\n18\n20\n95.5 95.6\nFigure 4: Example of the presented model during classless training. In this example, there are ten pseudo-classes represented by each row of MLP (1) and MLP (2). Note that the output classification are randomly selected (not cherry picking). Initially, the pseudo-classes are assigned randomly to all input pair samples, which holds a uniform distribution (first row). Then, the classless association model slowly start learning the features and grouping similar input samples. Afterwards, the output classification of both MLPs slowly agrees during training, and the association matrix shows the relation between the occurrences of the pseudo-classes.\nTo determine the baseline of our classless constraint, we compared our model against two cases: totally supervised and totally unsupervised. In the supervised case, we used the same MLP parameters and training for a fair comparison. In the unsupervised scenario, we used k-means and agglomerative clustering to each set (input 1 and input 2) independently. The clustering algorithm implementation are provided by scikit-learn (Pedregosa et al., 2011).\n4 RESULTS AND DISCUSSION\nIn this work, we have generated ten different folds for each dataset and report the average results. We introduce the Association Accuracy for measuring association, and it is defined by the following equation\nAssociation Accuracy = 1\nN N\u2211 i=1 1(c (1) i = c (2) i ) (6)\nwhere the indicator function is one if c(1)i = c (2) i , zero otherwise; c (1) i and c (2) i are the pseudo-classes for MLP (1) and MLP (2), respectively, and N is the number of elements. In addition, we also reported the Purity of each set (input 1 and input 2). Purity is defined by\nPurity(\u2126, C) = 1 N k\u2211 i=1 maxj |ci \u2229 gtj | (7)\nwhere \u2126 = {gt1, gt2, . . . , gtj} is the set of ground-truth labels and C = {c1, c2, . . . , ck} is the set of pseudo-classes in our model or the set of cluster indexes of K-means or Hierarchical Agglomerative clustering, and N is the number of elements.\nTable 1 shows the Association Accuracy between our model and the supervised association task and the Purity between our model and two clustering algorithms. First, the supervised association task performances better that the presented model. This was expected because our task is more complex in relation to the supervised scenario. However, we can infer from our results that the presented model has a good performance in terms of the classless scenario and supervised method. Second, our model not only learns the association between input samples but also finds similar elements covered under the same pseudo-class. Also, we evaluate the purity of our model and found that the performance of our model reaches better results than both clustering methods for each set (input 1 and input 2).\nFigure 4 illustrates an example of the proposed learning rule. The first two columns (MLP (1) and MLP (2)) are the output classification (Equation 3) and each row represents a pseudo-class. We have randomly selected 15 output samples for each MLP (not cherry picking). Initially, the pseudo classes are random selected for each MLP. As a result, the output classification of both networks does not show any visible discriminant element and the initial purity is close to random choice (first row). After 1,000 epochs, the networks start learning some features in order to discriminate the input samples. Some groups of digits are grouped together after 3,000 epochs. For example, the first row of MLP (2) shows several digits zero, whereas MLP (1) has not yet agree on the same digit for that pseudo-class. In contrast, both MLPs have almost agree on digit one at the fifth row. Finally, the association is learned using only the statistical distribution of the input samples and each digit is represented by each pseudo-class.\nBest Results\nWorst Results\nMLP (1) MLP (2) Association Matrix (%)\n0 1 2 3 4 5 6 7 8 9 MLP (2)\n0 1 2 3 4 5 6 7 8 9 M LP (1 ) 9.7 8.9 9.0 8.7 9.7 9.0 9.2 9.1 9.4 8.5\n0\n2\n4\n6\n8\n10\n12\n14\n16\n18\n20\nPurity (%)\n0 1 2 3 4 5 6 7 8 9 MLP (2)\n0 1 2 3 4 5 6 7 8 9 M LP (1 ) 9.7 7.9 5.1 4.5 0.0 5.1 9.2 0.0 10.3 12.8\n0\n2\n4\n6\n8\n10\n12\n14\n16\n18\n20\n95.9 95.2\n72.9 59.4\nFigure 5: Example of the best and worst results among all folds and datasets. It can be observed our model is able to learn to discriminate each digit (first row). However, the presented model has a limitation that two or more digits are assigned to the same pseudo-class (last row of MLP (1) and MLP (2)).\nFigure 5 shows the best and worst results of our model in two cases. The first row is the best result from MNIST dataset. Each row of MLP (1) and MLP (2) represent a pseudo-class, and it can be observed that all digits are grouped together. In addition, the association matrix shows a distribution per digit close to the desired uniform distribution, and the purity of each input is close to the supervised scenario. In contrast, the second row is our worst result from Random Rotated MNIST dataset. In this example, we can observe that some digits are recognized by the same pseudo-class, for example, digit one and seven (first two rows). However, there two or more digits that are recognized by the samepseudo-class. For example, the last row shows that nine and four are merged. Our model is still able to reach better results than the unsupervised scenario.\n5 CONCLUSION\nIn this paper, we have shown the feasibility to train a model that has two parallel MLPs under the following scenario: pairs of input samples that represent the same unknown classes. This scenario was motivated by the Symbol Grounding Problem and association learning between sensory input signal in infants development. We proposed a model based on gradients for solving the classless association. Our model has an EM-training that matches the network output against a statistical distribution and uses one network as a target of the other network, and vice versa. Our model reaches better performance than K-means and Hierarchical Agglomerative clustering. In addition, we compare the presented model against a supervised method. We find that the presented model with respect to the supervised method reaches good results because of two extra conditions in the unsupervised association: unlabeled data and agree on the same pseudo-class. We want to point out that our model was evaluated in an optimal case where the input samples are uniform distributed and the number of classes is known. However, we will explore the performance of our model if the number of classes and the statistical distrubtion are unknown. One way is to change the number of pseudo-classes. This can be seen as changing the number of clusters k in k-means. With this in mind, we are planning to do more exhaustive analysis of the learning behavior with deeper architectures. Moreover, we will work on how a small set of labeled classes affects the performance of our model (similar to semi-supervised learning). Furthermore, we are interested in replicating our findings in more complex scenarios, such as, multimodal datasets like TVGraz (Khan et al., 2009) or Wikipedia featured articles (Rasiwasia et al., 2010). Finally, our work can be applied to more classless scenarios where the data can be extracted simultaneously from different input sources at the same time. Also, transformation functions can be applied to input samples for creating the association without classes.\nACKNOWLEDGMENTS\nWe would like to thank Damian Borth, Christian Schulze, J\u00f6rn Hees, Tushar Karayil, and Philipp Blandfort for helpful discussions.\n", "cats": {"READ": 1.0, "DONTREAD": 0.0}, "reviews": [{"IS_META_REVIEW": true, "comments": "The paper looks correct but still i am not convinced about the experimentation performed. Perhaps another experiment with more challenging data would be welcome. Honestly i don't find a clear motivation for this work however it could have some potential and it would be interested to be presented in conference."}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "The paper explores neural-network learning on pairs of samples that are labeled as either similar or dissimilar. The proposed model appears to be different from standard siamese architectures, but it is poorly motivated. The experimental evaluation of the proposed model is very limited.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"DATE": "13 Jan 2017", "TITLE": "New revision", "IS_META_REVIEW": false, "comments": "We have updated our paper.  The changes are\n* We have improved the clarity and motivation of our model\n* We have evaluated our model to three more classless datasets (Rotated-90 MNIST, Inverted MNIST, and Random Rotation MNIST).\n* We have updated Figure 4 and 5 for showing some random output classification samples instead of the mean of all images.\n* We have added two more examples and demo as supplemental material ", "OTHER_KEYS": "Federico Raue"}, {"TITLE": "Review", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "The paper presents an alternative way of supervising the training of neural network without explicitly using labels when only link/not-link information is available between pairs of examples. A pair of network is trained each of which is used to supervise the other one.\n\n\nThe presentation of the paper is not very clear, the writing can be improved.\nSome design choice are not explained: Why is the power function used in the E-step for approximating the distribution (section 2.1)? Why do the authors only consider a uniform distribution? I understand that using a different prior breaks the assumption that nothing is known about the classes. However I do not see a practical situations where the proposed setting/work would be useful.  \n\nAlso, there exist a large body of work in semi-supervised learning with co-training based on a similar idea. \n\nOverall, I think this work should be clarified and improved to be a good fit for this venue.", "IS_META_REVIEW": false, "RECOMMENDATION": 5, "DATE": "20 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "No Title", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "The paper explores a new technique for classless association, a milder unsupervised learning where we do not know the class labels exactly, but we have a prior about the examples that belong to the same class. Authors proposed a two stream architecture with two neural networks, as streams process examples from the same class simultaneously. Both streams rely on the target (pseudo classes or cluster indices) of each other, and the outputs an intermediate representation z, which is forced to match with a statistical distribution (uniform in their case). The model is trained with EM where the E step obtains the current statistical distribution given output vectors z, and M step updates the weights of the architecture given z and pseudo-classes. Experimental results on re-organized MNIST exhibits better performance compared to classical clustering algorithms (in terms of association accuracy and purity). The authors further provide comparison against a supervised method, where proposed architecture expectedly performs worse but with promising results.\n\nThe basic motivation of the architecture apparently relies on unlabeled data and agreement of the same pseudo-labels generated by two streams. But the paper is hard to follow and the motivation for the proposed architecture itself, is hidden in details. What is trying to be achieved by matching distributions and using the pseudo-targets of the each other? Perhaps the statistical distribution of the classes is assumed to be uniform but how will it extend to other priors, or even the case where we do not assume that we know the prior? The current setup needs justifications. \n\nWhat would be very interesting is to see two examples having the same class but one from MNIST, the other from Rotated-MNIST or Background-MNIST. Because it is hard to guess how different the examples in two streams. \n\nAt the end, I feel like the authors have found a very interesting approach for classless association which can be extended to lots of many-to-one problems. This is a good catch. I would like to see the idea in the future with some extensive experiments on large scale datasets and tasks. But the current version lacks the theoretical motivations and convincing experiments. I would definitely recommend this paper to be presented in ICLR workshop.\n\nFew more points:\nTypo: Figure1. second line in the caption \"that\" -> \"than\"\nNecessity of Equation 2 is not clear\nBatch size M is enormous compared to classical models, there is no explanation for this\nWhy uniform? should be clarified (of course it is the simplest prior to pick but just a few words about it would be good for completeness)\nTypo: Page 6, second paragraph line 3: \"that\" -> \"than\"", "IS_META_REVIEW": false, "RECOMMENDATION": 5, "DATE": "17 Dec 2016", "REVIEWER_CONFIDENCE": 3}, {"TITLE": "Classes association", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "The paper looks correct but still i am not convinced about the experimentation performed. Perhaps another experiment with more challenging data would be welcome. Honestly i don't find a clear motivation for this work however it could have some potential and it would be interested to be presented in conference.\n", "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 3}, {"DATE": "02 Dec 2016", "TITLE": "Any prior info?", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2"}, {"IS_META_REVIEW": true, "comments": "The paper looks correct but still i am not convinced about the experimentation performed. Perhaps another experiment with more challenging data would be welcome. Honestly i don't find a clear motivation for this work however it could have some potential and it would be interested to be presented in conference."}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "The paper explores neural-network learning on pairs of samples that are labeled as either similar or dissimilar. The proposed model appears to be different from standard siamese architectures, but it is poorly motivated. The experimental evaluation of the proposed model is very limited.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"DATE": "13 Jan 2017", "TITLE": "New revision", "IS_META_REVIEW": false, "comments": "We have updated our paper.  The changes are\n* We have improved the clarity and motivation of our model\n* We have evaluated our model to three more classless datasets (Rotated-90 MNIST, Inverted MNIST, and Random Rotation MNIST).\n* We have updated Figure 4 and 5 for showing some random output classification samples instead of the mean of all images.\n* We have added two more examples and demo as supplemental material ", "OTHER_KEYS": "Federico Raue"}, {"TITLE": "Review", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "The paper presents an alternative way of supervising the training of neural network without explicitly using labels when only link/not-link information is available between pairs of examples. A pair of network is trained each of which is used to supervise the other one.\n\n\nThe presentation of the paper is not very clear, the writing can be improved.\nSome design choice are not explained: Why is the power function used in the E-step for approximating the distribution (section 2.1)? Why do the authors only consider a uniform distribution? I understand that using a different prior breaks the assumption that nothing is known about the classes. However I do not see a practical situations where the proposed setting/work would be useful.  \n\nAlso, there exist a large body of work in semi-supervised learning with co-training based on a similar idea. \n\nOverall, I think this work should be clarified and improved to be a good fit for this venue.", "IS_META_REVIEW": false, "RECOMMENDATION": 5, "DATE": "20 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "No Title", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "The paper explores a new technique for classless association, a milder unsupervised learning where we do not know the class labels exactly, but we have a prior about the examples that belong to the same class. Authors proposed a two stream architecture with two neural networks, as streams process examples from the same class simultaneously. Both streams rely on the target (pseudo classes or cluster indices) of each other, and the outputs an intermediate representation z, which is forced to match with a statistical distribution (uniform in their case). The model is trained with EM where the E step obtains the current statistical distribution given output vectors z, and M step updates the weights of the architecture given z and pseudo-classes. Experimental results on re-organized MNIST exhibits better performance compared to classical clustering algorithms (in terms of association accuracy and purity). The authors further provide comparison against a supervised method, where proposed architecture expectedly performs worse but with promising results.\n\nThe basic motivation of the architecture apparently relies on unlabeled data and agreement of the same pseudo-labels generated by two streams. But the paper is hard to follow and the motivation for the proposed architecture itself, is hidden in details. What is trying to be achieved by matching distributions and using the pseudo-targets of the each other? Perhaps the statistical distribution of the classes is assumed to be uniform but how will it extend to other priors, or even the case where we do not assume that we know the prior? The current setup needs justifications. \n\nWhat would be very interesting is to see two examples having the same class but one from MNIST, the other from Rotated-MNIST or Background-MNIST. Because it is hard to guess how different the examples in two streams. \n\nAt the end, I feel like the authors have found a very interesting approach for classless association which can be extended to lots of many-to-one problems. This is a good catch. I would like to see the idea in the future with some extensive experiments on large scale datasets and tasks. But the current version lacks the theoretical motivations and convincing experiments. I would definitely recommend this paper to be presented in ICLR workshop.\n\nFew more points:\nTypo: Figure1. second line in the caption \"that\" -> \"than\"\nNecessity of Equation 2 is not clear\nBatch size M is enormous compared to classical models, there is no explanation for this\nWhy uniform? should be clarified (of course it is the simplest prior to pick but just a few words about it would be good for completeness)\nTypo: Page 6, second paragraph line 3: \"that\" -> \"than\"", "IS_META_REVIEW": false, "RECOMMENDATION": 5, "DATE": "17 Dec 2016", "REVIEWER_CONFIDENCE": 3}, {"TITLE": "Classes association", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "The paper looks correct but still i am not convinced about the experimentation performed. Perhaps another experiment with more challenging data would be welcome. Honestly i don't find a clear motivation for this work however it could have some potential and it would be interested to be presented in conference.\n", "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 3}, {"DATE": "02 Dec 2016", "TITLE": "Any prior info?", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2"}]}
{"text": "SKIP-GRAM NEGATIVE SAMPLING\n1 INTRODUCTION\nIn this paper, we consider the problem of embedding words into a low-dimensional space in order to measure the semantic similarity between them. As an example, how to find whether the word \u201ctable\u201d is semantically more similar to the word \u201cstool\u201d than to the word \u201csky\u201d? That is achieved by constructing a low-dimensional vector representation for each word and measuring similarity between the words as the similarity between the corresponding vectors.\nOne of the most popular word embedding models by Mikolov et al. (2013) is a discriminative neural network that optimizes Skip-Gram Negative Sampling (SGNS) objective (see Equation 3). It aims at predicting whether two words can be found close to each other within a text. As shown in Section 2, the process of word embeddings training using SGNS can be divided into two general steps with clear objectives:\nStep 1. Search for a low-rank matrix X that provides a good SGNS objective value; Step 2. Search for a good low-rank representation X = WC\u22a4 in terms of linguistic metrics,\nwhere W is a matrix of word embeddings and C is a matrix of so-called context embeddings.\nUnfortunately, most previous approaches mixed these two steps into a single one, what entails a not completely correct formulation of the optimization problem. For example, popular approaches to train embeddings (including the original \u201cword2vec\u201d implementation) do not take into account that the objective from Step 1 depends only on the product X = WC\u22a4: instead of straightforward computing of the derivative w.r.t. X , these methods are explicitly based on the derivatives w.r.t. W and C, what complicates the optimization procedure. Moreover, such approaches do not take into account that parametrization WC\u22a4 of matrix X is non-unique and Step 2 is required. Indeed, for any invertible matrix S, we have X = W1C\u22a41 = W1SS \u22121C\u22a41 = W2C \u22a4 2 , therefore, solutions W1C1 and W2C2 are equally good in terms of the SGNS objective but entail different cosine similarities between embeddings and, as a result, different performance in terms of linguistic metrics (see Section 4.2 for details).\nA successful attempt to follow the above described steps, which outperforms the original SGNS optimization approach in terms of various linguistic tasks, was proposed by Levy & Goldberg (2014). In order to obtain a low-rank matrix X on Step 1, the method reduces the dimensionality of Shifted Positive Pointwise Mutual Information (SPPMI) matrix via Singular Value Decomposition (SVD). On Step 2, it computes embeddings W and C via a simple formula that depends on the factors obtained by SVD. However, this method has one important limitation: SVD provides a solution to a surrogate optimization problem, which has no direct relation to the SGNS objective. In fact, SVD minimizes the Mean Squared Error (MSE) between X and SPPMI matrix, what does not lead to minimization of SGNS objective in general (see Section 6.1 and Section 4.2 in Levy & Goldberg (2014) for details).\nThese issues bring us to the main idea of our paper: while keeping the low-rank matrix search setup on Step 1, optimize the original SGNS objective directly. This leads to an optimization problem over matrixX with the low-rank constraint, which is often (Mishra et al. (2014)) solved by applying Riemannian optimization framework (Udriste (1994)). In our paper, we use the projector-splitting algorithm (Lubich & Oseledets (2014)), which is easy to implement and has low computational complexity. Of course, Step 2 may be improved as well, but we regard this as a direction of future work.\nAs a result, our approach achieves the significant improvement in terms of SGNS optimization on Step 1 and, moreover, the improvement on Step 1 entails the improvement on Step 2 in terms of linguistic metrics. That is why, the proposed two-step decomposition of the problem makes sense, what, most importantly, opens the way to applying even more advanced approaches based on it (e.g., more advanced Riemannian optimization techniques for Step 1 or a more sophisticated treatment of Step 2).\nTo summarize, the main contributions of our paper are:\n\u2022 We reformulated the problem of SGNS word embedding learning as a two-step procedure with clear objectives;\n\u2022 For Step 1, we developed an algorithm based on Riemannian optimization framework that optimizes SGNS objective over low-rank matrix X directly;\n\u2022 Our algorithm outperforms state-of-the-art competitors in terms of SGNS objective and the semantic similarity linguistic metric (Levy & Goldberg (2014); Mikolov et al. (2013); Schnabel et al. (2015)).\n2 PROBLEM SETTING\n2.1 SKIP-GRAM NEGATIVE SAMPLING\nIn this paper, we consider the Skip-Gram Negative Sampling (SGNS) word embedding model (Mikolov et al. (2013)), which is a probabilistic discriminative model. Assume we have a text corpus given as a sequence of words w1, . . . , wn, where n may be larger than 1012 and wi \u2208 VW belongs to a vocabulary of words VW . A context c \u2208 VC of the word wi is a word from set {wi\u2212L, ..., wi\u22121, wi+1, ..., wi+L} for some fixed window size L. Letw, c \u2208 Rd be the word embeddings of word w and context c, respectively. Assume they are specified by the following mappings:\nW : VW \u2192 Rd, C : VC \u2192 Rd. The ultimate goal of SGNS word embedding training is to fit good mappingsW and C. In the SGNS model, the probability that pair (w, c) is observed in the corpus is modeled as a following function:\nP ((w, c) \u2208 D|w, c) = \u03c3(\u27e8w, c\u27e9) = 1 1 + exp(\u2212\u27e8w, c\u27e9) , (1)\nwhere D is the multiset of all word-context pairs (w, c) observed in the corpus and \u27e8x,y\u27e9 is the scalar product of vectors x and y. Number d is a hyperparameter that adjusts the flexibility of the model. It usually takes values from tens to hundreds.\nIn order to collect a training set, we take all pairs (w, c) fromD as positive examples and k randomly generated pairs (w, c) as negative ones. Let #(w, c) be the number of times the pair (w, c) appears\nin D. Thereby the number of times the word w and the context c appear in D can be computed as #(w) = \u2211 c\u2208Vc #(w, c) and #(c) = \u2211 w\u2208Vw #(w, c) accordingly. Then negative examples are generated from the distribution defined by #(c) counters: PD(c) = #(c) |D| . In this way, we have a model maximizing the following logarithmic likelihood objective for each word pair (w, c):\n#(w, c)(log \u03c3(\u27e8w, c\u27e9) + k \u00b7 Ec\u2032\u223cPD log \u03c3(\u2212\u27e8w, c\u2032\u27e9)). (2) In order to maximize the objective over all observations for each pair (w, c), we arrive at the following SGNS optimization problem over all possible mappingsW and C:\nl = \u2211\nw\u2208VW \u2211 c\u2208VC #(w, c)(log \u03c3(\u27e8w, c\u27e9) + k \u00b7 Ec\u2032\u223cPD log \u03c3(\u2212\u27e8w, c\u2032\u27e9)) \u2192 maxW, . (3)\nUsually, this optimization is done via the stochastic gradient descent procedure that is performed during passing through the corpus (Mikolov et al. (2013); Rong (2014)).\n2.2 OPTIMIZATION OVER LOW-RANK MATRICES\nRelying on the prospect proposed by Levy & Goldberg (2014), let us show that the optimization problem given by (3) can be considered as a problem of searching for a matrix that maximizes a certain objective function and has the rank-d constraint (Step 1 in the scheme described in Section 1).\n2.2.1 SGNS LOSS FUNCTION\nAs shown by Levy & Goldberg (2014), the logarithmic likelihood (3) can be represented as the sum of lw,c(w, c) over all pairs (w, c), where lw,c(w, c) has the following form:\nlw,c(w, c) =#(w, c) log \u03c3(\u27e8w, c\u27e9) + k #(w)#(c)\n|D| log \u03c3(\u2212\u27e8w, c\u27e9). (4)\nA crucial observation is that this loss function depends only on the scalar product \u27e8w, c\u27e9 but not on embeddings w and c separately:\nlw,c(w, c) = fw,c(xw,c),\nfw,c(xw,c) = aw,c log \u03c3(xw,c) + bw,c log \u03c3(\u2212xw,c), where xw,c is the scalar product \u27e8w, c\u27e9 and aw,c = #(w, c), bw,c = k#(w)#(c)|D| are constants.\n2.2.2 MATRIX NOTATION\nDenote |VW | as n and |VC | as m. Let W \u2208 Rn\u00d7d and C \u2208 Rm\u00d7d be matrices, where each row w \u2208 Rd of matrix W is the word embedding of the corresponding word w and each row c \u2208 Rd of matrix C is the context embedding of the corresponding context c. Then the elements of the product of these matrices\nX = WC\u22a4\nare the scalar products xw,c of all pairs (w, c):\nX = (xw,c), w \u2208 VW , c \u2208 VC . Note that this matrix has rank d, becauseX equals to the product of two matrices with sizes (n\u00d7 d) and (d\u00d7m). Now we can write SGNS objective given by (3) as a function of X:\nF (X) = \u2211\nw\u2208VW \u2211 c\u2208VC fw,c(xw,c), F : Rn\u00d7m \u2192 R. (5)\nThis arrives us at the following proposition:\nProposition 1 SGNS optimization problem given by (3) can be rewritten in the following constrained form:\nmaximize X\u2208Rn\u00d7m F (X), subject to X \u2208 Md, (6)\nwhereMd is the manifold (Udriste (1994)) of all matrices in Rn\u00d7m with rank d: Md = {X \u2208 Rn\u00d7m : rank(X) = d}.\nThe key idea of this paper is to solve the optimization problem given by (6) via the framework of Riemannian optimization, which we introduce in Section 3.\nImportant to note that this prospect does not suppose the optimization over parameters W and C directly. This entails the optimization in the space with ((n + m \u2212 d) \u00b7 d) degrees of freedom (Mukherjee et al. (2015)) instead of ((n + m) \u00b7 d), what simplifies the optimization process (see Section 5 for the experimental results).\n2.3 COMPUTING EMBEDDINGS FROM A LOW-RANK SOLUTION\nOnceX is found, we need to recoverW andC such thatX = WC\u22a4 (Step 2 in the scheme described in Section 1). This problem does not have a unique solution, since if (W,C) satisfy this equation, thenWS\u22121 and CS\u22a4 satisfy it as well for any non-singular matrix S. Moreover, different solutions may achieve different values of the linguistic metrics (see Section 4.2 for details). While our paper focuses on Step 1, we use, for Step 2, a heuristic approach that was proposed by Levy et al. (2015) and it shows good results in practice. We compute SVD of X in the form X = U\u03a3V \u22a4, where U and V have orthonormal columns, and \u03a3 is the diagonal matrix, and use\nW = U \u221a \u03a3, C = V \u221a \u03a3\nas matrices of embeddings.\nA simple justification of this solution is the following: we need to map words into vectors in a way that similar words would have similar embeddings in terms of cosine similarities:\ncos(w1,w2) = \u27e8w1,w2\u27e9\n\u2225w1\u2225 \u00b7 \u2225w2\u2225 .\nIt is reasonable to assume that two words are similar, if they share contexts. Therefore, we can estimate the similarity of two words w1, w2 as s(w1, w2) = \u2211 c\u2208VC xw1,c \u00b7 xw2,c, what is the element of the matrix XX\u22a4 with indices (w1, w2). Note that XX\u22a4 = U\u03a3V \u22a4V \u03a3U\u22a4 = U\u03a32U\u22a4. If we choose W = U\u03a3, we exactly obtain \u27e8w1,w2\u27e9 = s(w1, w2), since WW\u22a4 = XX\u22a4 in this case. That is, the cosine similarity of the embeddingsw1,w2 coincides with the intuitive similarity s(w1, w2). However, scaling by \u221a \u03a3 instead of \u03a3 was shown by Levy et al. (2015) to be a better solution in experiments.\n3 PROPOSED METHOD\n3.1 RIEMANNIAN OPTIMIZATION\n3.1.1 GENERAL SCHEME\nThe main idea of Riemannian optimization (Udriste (1994)) is to consider (6) as a constrained optimization problem. Assume we have an approximated solution Xi on a current step of the optimization process, where i is the step number. In order to improve Xi, the next step of the standard gradient ascent outputs Xi + \u2207F (Xi), where \u2207F (Xi) is the gradient of objective F at the point Xi. Note that the gradient \u2207F (Xi) can be naturally considered as a matrix in Rn\u00d7m. Point Xi + \u2207F (Xi) leaves the manifold Md, because its rank is generally greater than d. That is why Riemannian optimization methods map point Xi + \u2207F (Xi) back to manifold Md. The standard Riemannian gradient method first projects the gradient step onto the tangent space at the current point Xi and then retracts it back to the manifold:\nXi+1 = R (PTM (Xi +\u2207F (Xi))),\nwhere R is the retraction operator, and PTM is the projection onto the tangent space.\n3.1.2 PROJECTOR-SPLITTING ALGORITHM\nIn our paper, we use a much simpler version of such approach that retracts point Xi + \u2207F (Xi) directly to the manifold, as illustrated on Figure 1: Xi+1 = R(Xi +\u2207F (Xi)).\nUnder review as a conference paper at ICLR 2017\nFine-tuning word embeddings xxxxx xxxxx xxxxx xxxx xxxx xxxx xxx xxxxx xxxx xxxxx ABSTRACT Blah-blah\nKeywords word embeddings, SGNS, word2vec, GLOVE\n1. INTRODUCTION sdfdsf\n2. CONCLUSIONS\n3. RELATED WORK Mikolov main [?] Levi main [?]\nrFi\nXi = UiSiV T i\nXi+1 = Ui+1Si+1V T i+1\nretraction\n4. CONCLUSIONS\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. WOODSTOCK \u201997 El Paso, Texas USA Copyright 20XX ACM X-XXXXX-XX-X/XX/XX ...$15.00.\nFine-tuning word embeddings xxxxx xxxxx xxxxx xxxx xxxx xxxx xxx xxxxx xxxx xxxxx ABSTRACT Blah-blah Keywords word embeddings, SGNS, word2vec, GLOVE\n1. INTRODUCTION sdfdsf\n2. CONCLUSIONS\n3. RELATED WORK Mikolov main [?] Levi main [?]\nrFi\nXi = UiSiV T i\nXi+1 = Ui+1Si+1V T i+1\nretraction\nMd\n4. CONCLUSIONS\nPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. WOODSTOCK \u201997 El Paso, Texas USA Copyright 20XX ACM X-XXXXX-XX-X/XX/XX ...$15.00.\nFine-tuning word embeddings xxxxx xxxxx xxxxx xxxx xxxx xxxx xxx xxxxx xxxx xxxxx\nABSTRACT Blah-blah\nKeywords word embeddings, SGNS, word2vec, GLOVE\n1. INTRODUCTION sdfdsf\n2. CONCLUSIONS\n3. RELATED WORK Mikolov main [?] Levi main [?]\nrF (Xi)\nXi +rF (Xi)\nXi = UiSiV T i\nXi\nXi+1\nXi+1 = Ui+1Si+1V T i+1\nretraction\nMd\n4. CONCLUSIONS\nPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. WOODSTOCK \u201997 El Paso, Texas USA Copyright 20XX ACM X-XXXXX-XX-X/XX/XX ...$15.00.\nFine-tuning word embeddings xxxxx xxxxx xxxxx xxx xxxx xxxx xxx xxxxx xxxx xxxxx ABSTRACT Blah-blah\nKeywords word embeddings, SGNS, word2vec, GLOVE\n1. INTRODUCTION\nsdfdsf\n2. CONCLUSIONS\n3. RELATED WORK Mikolov main [?] Levi main [?]\nrF (Xi)\nXi +rF (Xi)\nXi = UiSiV T i\nXi\nXi+1\nXi+1 = Ui+1Si+1V T i+1\nretraction\nMd\n4. CONCLUSIONS\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for profit or commercial advantage and that copies\nbear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. WOODSTOCK \u201997 El Paso, Texas USA Copyright 20XX ACM X-XXXXX-XX-X/XX/XX ...$15.00.\nFine-tuning word embeddings xxxxx xxxxx xxxxx xxxx xxxx xxxx xxx xxxxx xxxx xxxxx\nABSTRACT Blah-blah\nKeywords word embeddings, SGNS, word2vec, GLOVE\n1. INTRODUCTION sdfdsf\n2. CONCLUSIONS\n3. RELATED WORK Mikolov main [?] Levi main [?]\nrF (Xi)\nXi +rF (Xi)\nXi = UiSiV T i\nXi\nXi+1\nXi+1 = Ui+1Si+1V T i+1\nretraction\nMd\n4. CONCLUSIONS\nPermission to make digital or hard copies of all or part of this work for personal or classro m us is r nted without f e provided tha copies are not made or distributed for profit or commercial advantage an that copies bear this notice and the full citation n the first page. To copy otherwise, to republish, to post on servers or to redistribute o lists, requires prior specific permission and/or a fee. WOODSTOCK \u201997 El Paso, Texas USA Copyright 20XX ACM X-XXXXX-XX-X/XX/XX ...$15.00.\nFine-tuning word embeddings xxxxx xx xxxxx xxxx xxxx xxxx xxx xxxxx xxxx xxxxx ABSTRACT Blah-blah\nKeywords word embeddings, SGNS, word2vec, GLOVE\n1. INT ODUCTION sdfdsf\n2. CONCLUSIONS\n3. RELATED WORK Mikolov main [?] Levi main [?]\nrF (Xi)\nXi +rF (Xi)\nXi = UiSiV T i\nXi\nXi+1\nXi+1 = Ui+1Si+1V T i+1\nretraction\nMd\n4. CONCLUSIONS\nPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. WOODSTOCK \u201997 El Paso, Texas USA Copyright 20XX ACM X-XXXXX-XX-X/XX/XX ...$15.00.\nFigure 1: Geometric interpretation of one step of projector-splitting optimization procedure: the gradient step an the retraction of the high-rank atr x Xi + \u2207F (Xi) to the manifold of low-rank matrices Md.\nIntuitively, retractor R finds rank-d matrix on t e manifold Md tha is similar to high-rank matrixXi+\u2207F (Xi) in terms o Frobeniu orm. How can e do it? The most straightforward way to reduce the rank ofXi +\u2207F (Xi) is to perform the SVD, which keeps d largest singular values of it:\n1: Ui+1, Si+1, V \u22a4i+1 \u2190 SVD(Xi +\u2207F (Xi)), 2: Xi+1 \u2190 Ui+1Si+1V \u22a4i+1.\n(7)\nHowever, it is computationally expensive. Instead of this approach, we use the projector-splitting method (Lubich & Oseledets (2014)), which is a second-order retraction onto the manifold (for details, see the review by Absil & Oseledets (2015)). Its practical implementation is also quite intuitive: instead of computing the full SVD of Xi +\u2207F (Xi) according to the gradient projection method, we use just one step of the block power numerical method (Bentbib & Kanber (2015)) which computes the SVD, what reduces the computational complexity.\nLet us keep the current point in the following factorized form: Xi = UiSiV \u22a4 i , (8) where matrices Ui \u2208 Rn\u00d7d and Vi \u2208 Rm\u00d7d have d orthonormal columns and Si \u2208 Rd\u00d7d. Then we need to perform two QR-decompositions to retract point Xi +\u2207F (Xi) back to the manifold:\n1: Ui+1, Si+1 \u2190 QR ((Xi +\u2207F (Xi))Vi) , 2: Vi+1, S\u22a4i+1 \u2190 QR ( (Xi +\u2207F (Xi))\u22a4Ui+1 ) ,\n3: Xi+1 \u2190 Ui+1Si+1V \u22a4i+1. In this way, we always keep the solution Xi+1 = Ui+1Si+1V \u22a4i+1 on the manifold Md and in the form (8).\nWhat is important, we only need to compute \u2207F (Xi), so the gradients with respect to U , S and V are never computed explicitly, thus avoiding the subtle case where S is close to singular (so-called singular (critical) point on the manifold). Indeed, the gradient with respect to U (while keeping the orthogonality constraints) can be written (Koch & Lubich (2007)) as:\n\u2202F \u2202U = \u2202F \u2202X V S\u22121,\nwhich means that the gradient will be large if S is close to singular. The projector-splitting scheme is free from this problem.\n3.2 ALGORITHM\nIn case of SGNS objective given by (5), an element of gradient \u2207F has the form:\n(\u2207F (X))w,c = \u2202fw,c(xw,c)\n\u2202xw,c = #(w, c) \u00b7 \u03c3 (\u2212xw,c)\u2212 k\n#(w)#(c)\n|D| \u00b7 \u03c3 (xw,c) .\nTo make the method more flexible in terms of convergence properties, we additionally use \u03bb \u2208 R, which is a step size parameter. In this case, retractor R returns Xi + \u03bb\u2207F (Xi) instead of Xi +\u2207F (Xi) onto the manifold. The whole optimization procedure is summarized in Algorithm 1.\nAlgorithm 1 Riemannian Optimization for SGNS Require: Dimentionality d, initializationW0 and C0, step size \u03bb, gradient function\u2207F : Rn\u00d7m \u2192\nRn\u00d7m, number of iterationsK Ensure: FactorW \u2208 Rn\u00d7d 1: X0 \u2190 W0C\u22a40 # get an initial point at the manifold 2: U0, S0, V \u22a40 \u2190 SVD(X0) # compute the first point satisfying the low-rank constraint 3: i \u2190 0 4: while i < K do 5: Ui+1, Si+1 \u2190 QR ((Xi + \u03bb\u2207F (Xi))Vi) # perform one step of the block power method\nwith two QR-decompositions 6: Vi+1, S\u22a4i+1 \u2190 QR ( (Xi + \u03bb\u2207F (Xi))\u22a4Ui+1 ) 7: Xi+1 \u2190 Ui+1Si+1V \u22a4i+1 # update the point at the manifold 8: i \u2190 i+ 1 9: end while 10: U,\u03a3, V \u22a4 \u2190 SVD(XK) 11: W \u2190 U \u221a \u03a3 # compute word embeddings 12: return W\n4 EXPERIMENTAL SETUP\n4.1 TRAINING MODELS\nWe compare our method (\u201cRO-SGNS\u201d in the tables) performance to two baselines: SGNS embeddings optimized via Stochastic Gradient Descent, implemented in the original \u201cword2vec\u201d, (\u201cSGDSGNS\u201d in the tables) by Mikolov et al. (2013) and embeddings obtained by SVD over SPPMI matrix (\u201cSVD-SPPMI\u201d in the tables) by Levy & Goldberg (2014). We have also experimented with the blockwise alternating optimization over factors W and C, but the results are almost the same to SGD results, that is why we do not to include them into the paper. The source code of our experiments is available online1.\nThe models were trained on English Wikipedia \u201cenwik9\u201d corpus2, which was previously used in most papers on this topic. Like in previous studies, we counted only the words which occur more than 200 times in the training corpus (Levy & Goldberg (2014); Mikolov et al. (2013)). As a result, we obtained a vocabulary of 24292 unique tokens (set of words VW and set of contexts VC are equal). The size of the context window was set to 5 for all experiments, as it was done by Levy & Goldberg (2014); Mikolov et al. (2013). We conduct two series of experiments: for dimensionality d = 100 and d = 200.\nOptimization step size is chosen to be small enough to avoid huge gradient values. However, thorough choice of \u03bb does not result in a significant difference in performance (this parameter was tuned on the training data only, the exact values used in experiments are reported below).\n4.2 EVALUATION\nWe evaluate word embeddings via the word similarity task. We use the following popular datasets for this purpose: \u201cwordsim-353\u201d (Finkelstein et al. (2001); 3 datasets), \u201csimlex-999\u201d (Hill et al. (2016)) and \u201cmen\u201d (Bruni et al. (2014)). Original \u201cwordsim-353\u201d dataset is a mixture of the word pairs for both word similarity and word relatedness tasks. This dataset was split (Agirre et al. (2009)) into two intersecting parts: \u201cwordsim-sim\u201d (\u201cws-sim\u201d in the tables) and \u201cwordsim-rel\u201d (\u201cws-rel\u201d in the tables) to separate the words from different tasks. In our experiments, we use both of them on a par with the full version of \u201cwordsim-353\u201d (\u201cws-full\u201d in the tables). Each dataset contains word pairs together with assessor-assigned similarity scores for each pair. As a quality measure, we use Spearman\u2019s correlation between these human ratings and cosine similarities for each pair. We call this quality metric linguistic in our paper.\n1https://github.com/newozz/riemannian_sgns 2Enwik9 corpus can be found here: http://mattmahoney.net/dc/textdata\n5 RESULTS OF EXPERIMENTS\nFirst of all, we compare the value of SGNS objective obtained by the methods. The comparison is demonstrated in Table 1.\nWe see that SGD-SGNS and SVD-SPPMI methods provide quite similar results, however, the proposed method obtains significantly better SGNS values, what proves the feasibility of using Riemannian optimization framework in SGNS optimization problem. It is interesting to note that SVDSPPMI method, which does not optimize SGNS objective directly, obtains better results than SGDSGNS method, which aims at optimizing SGNS. This fact additionally confirms the idea described in Section 2.2.2 that the independent optimization over parameters W and C may decrease the performance.\nHowever, the target performance measure of embedding models is the correlation between semantic similarity and human assessment (Section 4.2). Table 2 presents the comparison of the methods in terms of it. We see that our method outperforms the competitors on all datasets except for \u201cmen\u201d dataset where it obtains slightly worse results. Moreover, it is important that the higher dimension entails higher performance gain of our method in comparison to the competitors.\nIn order to understand how exactly our model improves or degrades the performance in comparison to the baseline, we found several words, whose neighbors in terms of cosine distance change significantly. Table 3 demonstrates neighbors of words \u201cfive\u201d, \u201che\u201d and \u201cmain\u201d in terms of our model and its nearest competitor according to the similarity task \u2014 SVD-SPPMI. These words were chosen as representative examples whose neighborhoods in terms of SVD-SPPMI and RO-SGNS models are strikingly different. A neighbour of a source word is bold if we suppose that it has a similar semantic meaning to the source word. First of all, we notice that our model produces much better neighbors of the words describing digits or numbers (see word \u201cfive\u201d as an example). The similar situation happens for many other words, e.g. in case of word \u201cmain\u201d \u2014 the nearest neighbors contain 4 similar words in case of our model instead of 2 in case of SVD-SPPMI. The neighbourhood of word \u201che\u201d contains less semantically similar words in case of our model. However, it filters out completely irrelevant words, such as \u201cpromptly\u201d and \u201cdumbledore\u201d.\nTalking about the optimal number K of iterations in the optimization procedure and step size \u03bb, we found that they depend on the particular value of dimensionality d. For d = 100, we have K = 25, \u03bb \u2248 5 \u00b7 10\u22125, and for d = 200, we have K = 13, \u03bb = 10\u22124. Moreover, it is interesting that the best results were obtained when SVD-SPPMI embeddings were used as an initialization of Riemannian optimization process.\n6 RELATED WORK\n6.1 WORD EMBEDDINGS\nSkip-Gram Negative Sampling was introduced by Mikolov et al. (2013). The \u201cnegative sampling\u201d approach was thoroughly described by Goldberg & Levy (2014), and the learning method is ex-\nplained by Rong (2014). There are several open-source implementations of SGNS neural network, which is widely known as \u201cword2vec\u201d 34.\nAs shown in Section 2.2, Skip-Gram Negative Sampling optimization can be reformulated as a problem of searching for a low-rank matrix. In order to be able to use out-of-the-box SVD for this task, Levy & Goldberg (2014) used the surrogate version of SGNS as the objective function. There are two general assumptions made in their algorithm that distinguish it from the SGNS optimization:\n1. SVD optimizes Mean Squared Error (MSE) objective instead of SGNS loss function.\n2. In order to avoid infinite elements in SPMI matrix, it is transformed in ad-hoc manner (SPPMI matrix) before applying SVD.\nThis makes the objective not interpretable in terms of the original task (3). As mentioned by Levy & Goldberg (2014), SGNS objective weighs different (w, c) pairs differently, unlike the SVD, which works with the same weight for all pairs, what may entail the performance fall. The comprehensive explanation of the relation between SGNS, SPPMI, SVD-over-SPPMI methods is provided by Keerthi et al. (2015). Lai et al. (2015); Levy et al. (2015) give a good overview of highly practical methods to improve these word embedding models.\n6.2 RIEMANNIAN OPTIMIZATION\nAn introduction to optimization over Riemannian manifolds can be found in the paper of Udriste (1994). The overview of retractions of high rank matrices to low-rank manifolds is provided by Absil & Oseledets (2015). The projector-splitting algorithm was introduced by Lubich & Oseledets (2014), and also was mentioned by Absil & Oseledets (2015) as \u201cLie-Trotter retraction\u201d.\nRiemannian optimization is succesfully applied to various data science problems: for example, matrix completion (Vandereycken (2013)), large-scale recommender systems (Tan et al. (2014)), and tensor completion (Kressner et al. (2014)).\n7 CONCLUSIONS AND FUTURE WORK\nIn our paper, we proposed the general two-step scheme of training SGNS word embedding model and introduced the algorithm that performs the search of a solution in the low-rank form via Riemannian optimization framework. We also demonstrated the superiority of the proposed method, by providing the experimental comparison to the existing state-of-the-art approaches.\nIt seems to be an interesting direction of future work to apply more advanced optimization techniques to Step 1 of the scheme proposed in Section 1 and to explore the Step 2 \u2014 obtaining embeddings with a given low-rank matrix.\n3Original Google word2vec: https://code.google.com/archive/p/word2vec/ 4Gensim word2vec: https://radimrehurek.com/gensim/models/word2vec.html\n", "cats": {"READ": 1.0, "DONTREAD": 0.0}, "reviews": [{"IS_META_REVIEW": true, "comments": "This paper presents a principled optimization method for SGNS (word2vec).\n\nWhile the proposed method is elegant from a theoretical perspective, I am not sure what the tangible benefits of this approach are. For example, does using Riemannian optimization allow the model to converge faster than the alternatives? The evaluation doesn't show a dramatic advantage to RO-SGNS; the 1% difference on the word similarity benchmarks is within the range of hyperparameter effects (see \"Improving Distributional Similarity with Lessons Learned from Word Embeddings\", (Levy et al., 2015)). The theoretical connection to Riemannian optimization is nice though, and it might be useful for understanding related methods in the future."}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "The paper is mostly clearly written. The observation made in the paper that word-embedding models based on optimizing skip-gram negative sampling objective function can be formulated as a low-rank matrix estimation problem, and solved using manifold optimization techniques, is sound. However, this observation by itself is not new and has come up in various other contexts such as matrix completion. As such the reviewers do not see sufficient novelty in the algorithmic aspects of the paper, and empirical evaluation on the specific problem of learning word embeddings does not show striking enough gains relative to standard SGD methods. The authors are encouraged to explore complimentary algorithmic angles and benefits that their approach provides for this specific class of applications.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"TITLE": "Not convincing ", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "The paper considers Grassmannian SGD to optimize the skip gram negative sampling (SGNS) objective for learning better word embeddings. It is not clear why the proposed optimization approach has any advantage over the existing vanilla SGD-based approach - neither approach comes with theoretical guarantees - the empirical comparisons show marginal improvements. Furthermore, the key idea here - that of projector splitting algorithm - has been applied on numerous occasions to machine learning problems - see references by Vandereycken on matrix completion and by Sepulchre on matrix factorization. \n\nThe computational cost of the two approaches is not carefully discussed. For instance, how expensive is the SVD in (7)? One can always perform an efficient low-rank update to the SVD - therefore, a rank one update requires O(nd) operations. What is the computational cost of each iteration of the proposed approach? \n\n", "IS_META_REVIEW": false, "RECOMMENDATION": 4, "DATE": "28 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "still somewhat confused", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "Dear authors,\n\nThe authors' response clarified some of my confusion. But I still have the following question:\n\n-- The response said a first contribution is a different formulation: you divide the word embedding learning into two steps, step 1 looks for a low-rank X (by Riemannian optimization), step 2 factorizes X into two matrices (W, C). You are claiming that your model outperforms previous approaches that directly optimizes over (W, C). But since the end result (the factors) is the same, can the authors provide some intuition and justification why the proposed method works better?\n\nAs far as I can see, though parameterized differently, the first step of your method and previous methods (SGD) are both optimizing over low-rank matrices. Admittedly, Riemannian optimization avoids the rotational degree of freedom (the invertible matrix S you are mentioning in sec 2.3), but I am not 100% certain at this point this is the source of your gain; learning curves of objectives would help to see if Riemannian optimization is indeed more effective. \n\n-- Another detail I could not easily find is the following. You said a disadvantage of other approaches is that their factors W and C do not directly reflect similarity. Did you try to multiply the factors W and C from other optimizers and then factorize the product using the method in section 2.3, and use the new W for your downstream tasks? I am not sure if this would cause much difference in the performance.\n\nOverall, I think it is always interesting to apply advanced optimization techniques to machine learning problems. The current paper would be stronger from the machine learning perspective, if more thorough comparison and discussion (as mentioned above) are provided. On the other hand, my expertise is not in NLP and I leave it to other reviewers to decide the significance in experimental results.\n\n", "IS_META_REVIEW": false, "RECOMMENDATION": 5, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 3}, {"TITLE": "Elegant method, not sure about the practical benefits", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "This paper presents a principled optimization method for SGNS (word2vec).\n\nWhile the proposed method is elegant from a theoretical perspective, I am not sure what the tangible benefits of this approach are. For example, does using Riemannian optimization allow the model to converge faster than the alternatives? The evaluation doesn't show a dramatic advantage to RO-SGNS; the 1% difference on the word similarity benchmarks is within the range of hyperparameter effects (see \"Improving Distributional Similarity with Lessons Learned from Word Embeddings\", (Levy et al., 2015)). The theoretical connection to Riemannian optimization is nice though, and it might be useful for understanding related methods in the future.\n", "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "14 Dec 2016", "REVIEWER_CONFIDENCE": 3}, {"DATE": "02 Dec 2016", "TITLE": "main contribution?", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3"}, {"DATE": "02 Dec 2016", "TITLE": "How fast is the new algorithm?", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2"}, {"IS_META_REVIEW": true, "comments": "This paper presents a principled optimization method for SGNS (word2vec).\n\nWhile the proposed method is elegant from a theoretical perspective, I am not sure what the tangible benefits of this approach are. For example, does using Riemannian optimization allow the model to converge faster than the alternatives? The evaluation doesn't show a dramatic advantage to RO-SGNS; the 1% difference on the word similarity benchmarks is within the range of hyperparameter effects (see \"Improving Distributional Similarity with Lessons Learned from Word Embeddings\", (Levy et al., 2015)). The theoretical connection to Riemannian optimization is nice though, and it might be useful for understanding related methods in the future."}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "The paper is mostly clearly written. The observation made in the paper that word-embedding models based on optimizing skip-gram negative sampling objective function can be formulated as a low-rank matrix estimation problem, and solved using manifold optimization techniques, is sound. However, this observation by itself is not new and has come up in various other contexts such as matrix completion. As such the reviewers do not see sufficient novelty in the algorithmic aspects of the paper, and empirical evaluation on the specific problem of learning word embeddings does not show striking enough gains relative to standard SGD methods. The authors are encouraged to explore complimentary algorithmic angles and benefits that their approach provides for this specific class of applications.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"TITLE": "Not convincing ", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "The paper considers Grassmannian SGD to optimize the skip gram negative sampling (SGNS) objective for learning better word embeddings. It is not clear why the proposed optimization approach has any advantage over the existing vanilla SGD-based approach - neither approach comes with theoretical guarantees - the empirical comparisons show marginal improvements. Furthermore, the key idea here - that of projector splitting algorithm - has been applied on numerous occasions to machine learning problems - see references by Vandereycken on matrix completion and by Sepulchre on matrix factorization. \n\nThe computational cost of the two approaches is not carefully discussed. For instance, how expensive is the SVD in (7)? One can always perform an efficient low-rank update to the SVD - therefore, a rank one update requires O(nd) operations. What is the computational cost of each iteration of the proposed approach? \n\n", "IS_META_REVIEW": false, "RECOMMENDATION": 4, "DATE": "28 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "still somewhat confused", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "Dear authors,\n\nThe authors' response clarified some of my confusion. But I still have the following question:\n\n-- The response said a first contribution is a different formulation: you divide the word embedding learning into two steps, step 1 looks for a low-rank X (by Riemannian optimization), step 2 factorizes X into two matrices (W, C). You are claiming that your model outperforms previous approaches that directly optimizes over (W, C). But since the end result (the factors) is the same, can the authors provide some intuition and justification why the proposed method works better?\n\nAs far as I can see, though parameterized differently, the first step of your method and previous methods (SGD) are both optimizing over low-rank matrices. Admittedly, Riemannian optimization avoids the rotational degree of freedom (the invertible matrix S you are mentioning in sec 2.3), but I am not 100% certain at this point this is the source of your gain; learning curves of objectives would help to see if Riemannian optimization is indeed more effective. \n\n-- Another detail I could not easily find is the following. You said a disadvantage of other approaches is that their factors W and C do not directly reflect similarity. Did you try to multiply the factors W and C from other optimizers and then factorize the product using the method in section 2.3, and use the new W for your downstream tasks? I am not sure if this would cause much difference in the performance.\n\nOverall, I think it is always interesting to apply advanced optimization techniques to machine learning problems. The current paper would be stronger from the machine learning perspective, if more thorough comparison and discussion (as mentioned above) are provided. On the other hand, my expertise is not in NLP and I leave it to other reviewers to decide the significance in experimental results.\n\n", "IS_META_REVIEW": false, "RECOMMENDATION": 5, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 3}, {"TITLE": "Elegant method, not sure about the practical benefits", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "This paper presents a principled optimization method for SGNS (word2vec).\n\nWhile the proposed method is elegant from a theoretical perspective, I am not sure what the tangible benefits of this approach are. For example, does using Riemannian optimization allow the model to converge faster than the alternatives? The evaluation doesn't show a dramatic advantage to RO-SGNS; the 1% difference on the word similarity benchmarks is within the range of hyperparameter effects (see \"Improving Distributional Similarity with Lessons Learned from Word Embeddings\", (Levy et al., 2015)). The theoretical connection to Riemannian optimization is nice though, and it might be useful for understanding related methods in the future.\n", "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "14 Dec 2016", "REVIEWER_CONFIDENCE": 3}, {"DATE": "02 Dec 2016", "TITLE": "main contribution?", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3"}, {"DATE": "02 Dec 2016", "TITLE": "How fast is the new algorithm?", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2"}]}
{"text": "1 INTRODUCTION\nNeural network language models, especially recurrent neural networks (RNN), are now standard tools for natural language processing. Amongst other things, they are used for translation Sutskever et al. (2014), language modelling Jozefowicz et al. (2016), and question answering Hewlett et al. (2016). In particular, the Long Short Term Memory (LSTM) Hochreiter & Schmidhuber (1997) architecture has become a basic building block of neural NLP. Although LSTM\u2019s are regularly used in state of the art systems, their operation is not well understood. Besides the basic desire from a scientific viewpoint to clarify their workings, it is often the case that it is important to understand why a machine learning algorithm made a particular choice. Moreover, LSTM\u2019s are computationally intensive compared to discrete models with lookup tables and pattern matching.\nIn this work, we describe a novel method for visualizing the importance of specific inputs for determining the output of an LSTM. We then demonstrate that, by searching for phrases which are consistently important, the importance scores can be used to extract simple phrase patterns consisting of one to five words from a trained LSTM. The phrase extraction is first done in a general document classification framework on two different sentiment analysis datasets. We then demonstrate that it can also be specialized to more complex models by applying it to WikiMovies, a recently introduced question answer dataset. To concretely validate the extracted patterns, we use them as input to a rules-based classifier which approximates the performance of the original LSTM.\n2 RELATED WORK\nThere are two lines of related work on visualizing LSTMs. First, Hendrik et al. (2016) and Karpathy et al. (2016) analyse the movement of the raw gate activations over a sequence. Karpathy et al. (2016) is able to identify co-ordinates of ct that correspond to semantically meaningful attributes such as whether the text is in quotes and how far along the sentence a word is. However, most of the cell co-ordinates are harder to interpret, and in particular, it is often not obvious from their activations which inputs are important for specific outputs.\n\u2217Work started during an internship at Facebook AI Research\nAnother approach that has emerged in the literature Alikaniotis et al. (2016) Denil et al. (2015) Bansal et al. (2016) is for each word in the document, looking at the norm of the derivative of the loss function with respect to the embedding parameters for that word. This bridges the gap between high-dimensional cell state and low-dimensional outputs. These techniques are general- they are applicable to visualizing the importance of sets of input coordinates to output coordinates of any differentiable function. In this work, we describe techniques that are designed around the structure of LSTM\u2019s, and show that they can give better results in that setting.\nA recent line of work Li et al. (2016) Hewlett et al. (2016) Rajpurkar et al. (2016) Miller et al. (2016) has focused on neural network techniques for extracting answers directly from documents. Previous work had focused on Knowledge Bases (KBs), and techniques to map questions to logical forms suitable for querying them. Although they are effective within their domain, KBs are inevitably incomplete, and are thus an unsatisfactory solution to the general problem of question-answering. Wikipedia, in contrast, has enough information to answer a far broader array of questions, but is not as easy to query. Originally introduced in Miller et al. (2016), the WikiMovies dataset consists of questions about movies paired with Wikipedia articles.\n3 WORD IMPORTANCE SCORES IN LSTMS\nWe present a novel decomposition of the output of an LSTM into a product of factors, where each term in the product can be interpreted as the contribution of a particular word. Thus, we can assign importance scores to words according to their contribution to the LSTM\u2019s prediction\n3.1 LONG SHORT TERM MEMORY NETWORKS\nOver the past few years, LSTMs have become an important part of neural NLP systems. Given a sequence of word embeddings x1, ..., xT \u2208 Rd, an LSTM processes one word at a time, keeping track of cell and state vectors (c1, h1), ..., (cT , hT ) which contain information in the sentence up to word i. ht and ct are computed as a function of xt, ct\u22121 using the below updates\nft = \u03c3(Wfxt + Vfht\u22121 + bf ) (1) it = \u03c3(Wixt + Viht\u22121 + bi) (2) ot = \u03c3(Woxt + Voht\u22121 + bo) (3) c\u0303t = tanh(Wcxt + Vcht\u22121 + bc) (4) ct = ftct\u22121 + itc\u0303t (5) ht = ot tanh(ct) (6)\nAs initial values, we define c0 = h0 = 0. After processing the full sequence, a probability distribution over C classes is specified by p, with\npi = SoftMax(WhT ) = eWihT\u2211C j=1 e Wjht (7)\nwhere Wi is the i\u2019th row of the matrix W\n3.2 DECOMPOSING THE OUTPUT OF A LSTM\nWe now show that we can decompose the numerator of pi in Equation 7 into a product of factors, and interpret those factors as the contribution of individual words to the predicted probability of class i. Define \u03b2i,j = exp (Wi(oT (tanh(cj)\u2212 tanh(cj\u22121))) , (8) so that\nexp(WihT ) = exp  T\u2211 j=1 Wi(oT (tanh(cj)\u2212 tanh(cj\u22121))  = T\u220f j=1 \u03b2i,j .\nAs tanh(cj)\u2212 tanh(cj\u22121) can be viewed as the update resulting from word j, so \u03b2i,j can be interpreted as the multiplicative contribution to pi by word j.\n3.3 AN ADDITIVE DECOMPOSITION OF THE LSTM CELL\nWe will show below that the \u03b2i,j capture some notion of the importance of a word to the LSTM\u2019s output. However, these terms fail to account for how the information contributed by word j is affected by the LSTM\u2019s forget gates between words j and T . Consequently, we empirically found that the importance scores from this approach often yield a considerable amount of false positives. A more nuanced approach is obtained by considering the additive decomposition of cT in equation (9), where each term ej can be interpreted as the contribution to the cell state cT by word j. By iterating the equation ct = ftct\u22121 + itc\u0303t, we get that\ncT = T\u2211 i=1 ( T\u220f j=i+1 fj)iic\u0303i = T\u2211 i=1 ei,T (9)\nThis suggests a natural definition of an alternative score to the \u03b2i,j , corresponding to augmenting the cj terms with products of forget gates to reflect the upstream changes made to cj after initially processing word j.\nexp(WihT ) = T\u220f j=1 exp\n( Wi(oT (tanh(\nj\u2211 k=1 ek,T )\u2212 tanh( j\u22121\u2211 k=1 ek,T )))\n) (10)\n= T\u220f j=1 exp Wi(oT (tanh(( t\u220f k=j+1 fk)cj)\u2212 tanh(( t\u220f k=j fk)cj\u22121)))  (11) =\nT\u220f j=1 \u03b3i,j (12)\n4 PHRASE EXTRACTION FOR DOCUMENT CLASSIFICATION\nWe now introduce a technique for using our variable importance scores to extract phrases from a trained LSTM. To do so, we search for phrases which consistently provide a large contribution to the prediction of a particular class relative to other classes. The utility of these patterns is validated by using them as input for a rules based classifier. For simplicity, we focus on the binary classification case.\n4.1 PHRASE EXTRACTION\nA phrase can be reasonably described as predictive if, whenever it occurs, it causes a document to both be labelled as a particular class, and not be labelled as any other. As our importance scores introduced above correspond to the contribution of particular words to class predictions, they can be used to score potential patterns by looking at a pattern\u2019s average contribution to the prediction of a given class relative to other classes. More precisely, given a collection of D documents {{xi,j}Ndi=1}Dj=1, for a given phrase w1, ..., wk we can compute scores S1, S2 for classes 1 and 2, as well as a combined score S and class C as\nS1(w1, ..., wk) = Averagej,b\n{\u220fk l=1 \u03b21,b+l,j |xb+i,j = wi, i = 1, ..., k } Averagej,b {\u220fk l=1 \u03b22,b+l,j |xb+i,j = wi, i = 1, ..., k\n} (13) S2(w1, .., wk) = 1\nS1(w1, ..., wk) (14)\nS(w1, ..., wk) = max i (Si(w1, ..., wk)) (15)\nC(w1, ..., wk) = argmaxi(Si(w1, ..., wk)) (16)\nwhere \u03b2i,j,k denotes \u03b2i,j applied to document k.\nThe numerator of S1 denotes the average contribution of the phrase to the prediction of class 1 across all occurrences of the phrase. The denominator denotes the same statistic, but for class 2. Thus, if\nS1 is high, then w1, ..., wk is a strong signal for class 1, and likewise for S2. We propose to use S as a score function in order to search for high scoring, representative, phrases which provide insight into the trained LSTM, and C to denote the class corresponding to a phrase.\nIn practice, the number of phrases is too large to feasibly compute the score of them all. Thus, we approximate a brute force search through a two step procedure. First, we construct a list of candidate phrases by searching for strings of consecutive words j with importance scores \u03b2i,j > c for any i and some threshold c; in the experiments below we use c = 1.1. Then, we score and rank the set of candidate phrases, which is much smaller than the set of all phrases.\n4.2 RULES BASED CLASSIFIER\nThe extracted patterns from Section 4.1 can be used to construct a simple, rules-based classifier which approximates the output of the original LSTM. Given a document and a list of patterns sorted by descending score given by S, the classifier sequentially searches for each pattern within the document using simple string matching. Once it finds a pattern, the classifier returns the associated class given by C, ignoring the lower ranked patterns. The resulting classifier is interpretable, and despite its simplicity, retains much of the accuracy of the LSTM used to build it.\n5 EXPERIMENTS\nWe now present the results of our experiments.\n5.1 TRAINING DETAILS\nWe implemented all models in Torch using default hyperparameters for weight initializations. For WikiMovies, all documents and questions were pre-processed so that multiple word entities were concatenated into a single word. For a given question, relevant articles were found by first extracting from the question the rarest entity, then returning a list of Wikipedia articles containing any of those words. We use the pre-defined splits into train, validation and test sets, containing 96k, 10k and 10k questions, respectively. The word and hidden representations of the LSTM were both set to dimension 200 for WikiMovies, 300 and 512 for Yelp, and 300 and 150 for Stanford Sentiment Treebank. All models were optimized using Adam Kingma & Ba (2015) with the default learning rate of 0.001 using early stopping on the validation set. For rule extraction using gradient scores, the product in the reward function is replaced by a sum. In both datasets, we found that normalizing the gradient scores by the largest gradient improved results.\n5.2 SENTIMENT ANALYSIS\nWe first applied the document classification framework to two different sentiment analysis datasets. Originally introduced in Zhang et al. (2015), the Yelp review polarity dataset was obtained from the Yelp Dataset Challenge and has train and test sets of size 560,000 and 38,000. The task is binary prediction for whether the review is positive (four or five stars) or negative (one or two stars). The reviews are relatively long, with an average length of 160.1 words. We also used the binary classification task from the Stanford Sentiment Treebank (SST) Socher et al. (2013), which has less data with train/dev/test sizes of 6920/872/1821, and is done at a sentence level, so has much shorter document lengths.\nWe report results in Table 1 for seven different models. We report state of the art results from prior work using convolutional neural networks; Kim (2014) for SST and Zhang et al. (2015) for Yelp. We also report our LSTM baselines, which are competitive with state of the art, along with the three different pattern matching models described above. For SST, we also report prior results using bag of words features with Naive Bayes.\nThe additive cell decomposition pattern equals or outperforms the cell-difference patterns, which handily beat the gradient results. This coincides with our empirical observations regarding the information contained within the importance measures, and validates our introduced measure. The differences between measures become more pronounced in Yelp, as the longer document sizes provide more opportunities for false positives.\nAlthough our pattern matching algorithms underperform other methods, we emphasize that pure performance is not our goal, nor would we expect more from such a simple model. Rather, the fact that our method provides reasonable accuracy is one piece of evidence, in addition to the qualitative evidence given later, that our word importance scores and extracted patterns contain useful information for understanding the actions of a LSTM.\n5.3 WIKIMOVIES\nAlthough document classification comprises a sizeable portion of current research in natural language processing, much recent work focuses on more complex problems and models. In this section, we examine WikiMovies, a recently introduced question answer dataset, and show that with some simple modifications our approach can be adapted to this problem.\n5.3.1 DATASET\nWikiMovies is a dataset consisting of more than 100,000 questions about movies, paired with relevant Wikipedia articles. It was constructed using the pre-existing dataset MovieLens, paired with templates extracted from the SimpleQuestions dataset Bordes et al. (2015), a open-domain question answering dataset based on Freebase. They then selected a set of Wikipedia articles about movies by identifying a set of movies from OMDb that had an associated article by title match, and kept the title and first section for each article.\nFor a given question, the task is to read through the relevant articles and extract the answer, which is contained somewhere within the text. The dataset also provides a list of 43k entities containing all possible answers.\n5.3.2 LSTMS FOR WIKIMOVIES\nWe propose a simplified version of recent work Li et al. (2016). Given a pair of question xq1, ..., x q N and document xd1, ..., x d T , we first compute an embedding for the question using a LSTM. Then, for each word t in the document, we augment the word embedding xt with the computed question embedding. This is equivalent to adding an additional term which is linear in the question embedding into the gate equations 3-6, allowing the patterns an LSTM absorbs to be directly conditioned upon the question at hand.\nhqt = LSTM(x q t ) (17)\nht = LSTM(xdt \u2016h q N ) (18)\nHaving run the above model over the document while conditioning on a question, we are given contextual representations h1, ..., hT of the words in the document. For each entity t in the document\nwe use pt to conduct a binary prediction for whether or not the entity is the answer. At test time, we return the entity with the highest probability as the answer.\npt = SoftMax(Wht) (19)\n5.3.3 PHRASE EXTRACTION\nWe now introduce some simple modifications that were useful in adapting our pattern extraction framework to this specific task. First, in order to define the set of classifications problems to search over, we treat each entity t within each document as a separate binary classification task with corresponding predictor pt. Given this set of classification problems, rather than search over the space of all possible phrases, we restrict ourselves to those ending at the entity in question. We also distinguish patterns starting at the beginning of the document with those that do not and introduce an entity character into our pattern vocabulary, which can be matched by any entity. Template examples can be seen below, in Table 4. Once we have extracted a list of patterns, in the rules-based classifier we only search for positive examples, and return as the answer the entity matched to the highest ranked positive pattern.\n5.3.4 RESULTS\nWe report results on six different models in Tables 2 and 3. We show the results from Miller et al. (2016), which fit a key-value memory network (KV-MemNN) on representations from information extraction (IE) and raw text (Doc). Next, we report the results of the LSTM described in Section 5.3.2. Finally, we show the results of using three variants of the pattern matching algorithm described in Section 5.3.3: using patterns extracted using the additive decomposition (cell decomposition), difference in cells approaches (cell-difference) and gradient importance scores (gradient), as discussed in Section 2. Performance is reported using the accuracy of the top hit over all possible answers (all entities), i.e. the hits@1 metric.\nAs shown in Table 2, our LSTM model surpasses the prior state of the art by nearly 4%. Moreover, our automatic pattern matching model approximates the LSTM with less than 6% error, which is surprisingly small for such a simple model, and falls within 2% of the prior state of the art. Similarly to sentiment analysis, we observe a clear ordering of the results across question categories, with our cell decomposition scores providing the best performance, followed by the cell difference and gradient scores.\n6 DISCUSSION\n6.1 LEARNED PATTERNS\nWe present extracted patterns for both sentiment tasks, and some WikiMovies question categories in Table 4. These patterns are qualitatively sensible, providing further validation of our approach. The increased size of the Yelp dataset allowed for longer phrases to be extracted relative to SST.\nCategory Top Patterns Yelp Polarity Positive definitely come back again., love love love this\nplace, great food and great service., highly recommended!, will definitely be coming back, overall great experience, love everything about, hidden gem.\nYelp Polarity Negative worst customer service ever, horrible horrible horrible, won\u2019t be back, disappointed in this place, never go back there, not worth the money, not recommend this place SST Positive riveting documentary, is a real charmer, funny and touching, well worth your time, journey of the heart, emotional wallop, pleasure to watch, the whole family, cast is uniformly superb, comes from the heart, best films of the year, surprisingly funny, deeply satisfying SST Negative pretentious mess ..., plain bad, worst film of the year, disappointingly generic, fart jokes, banal dialogue, poorly executed, waste of time, a weak script, dullard, how bad it is, platitudes, never catches fire, tries too hard to be, bad acting, untalented artistes, derivative horror film, lackluster WikiMovies movie to writer film adaptation of Charles Dickens\u2019, film adapted from ENT, by journalist ENT, written by ENT WikiMovies movie to actor western film starring ENT, starring Ben Affleck, . The movie stars ENT, that stars ENT WikiMovies movie to language is a 2014 french, icelandic, finnish, russian, danish, bengali, dutch, original german, zulu,czech, estonian, mandarin, filipino, hungarian\nTable 4: Selected top patterns using cell decomposition scores, ENT denotes an entity placeholder\n6.2 APPROXIMATION ERROR BETWEEN LSTM AND PATTERN MATCHING\nAlthough our approach is able to extract sensible patterns and achieve reasonable performance, there is still an approximation gap between our algorithm and the LSTM. In Table 5 we present some examples of instances where the LSTM was able to correctly classify a sentence, and our algorithm was not, along with the pattern used by our algorithm. At first glance, the extracted patterns are sensible, as \u201dgets the job done\u201d or \u201dwitty dialogue\u201d are phrases you\u2019d expect to see in a positive review of a movie. However, when placed in the broader context of these particular reviews, they cease to be predictive. This demonstrates that, although our work is useful as a firstorder approximation, there are still additional relationships that an LSTM is able to learn from data.\n6.3 COMPARISON BETWEEN WORD IMPORTANCE MEASURES\nWhile the prediction accuracy of our rules-based classifier provides quantitative validation of the relative merits of our visualizations, the qualitative differences are also insightful. In Table 6, we provide a side-by-side comparison between the different measures. As discussed before, the difference in cells technique fails to account for how the updates resulting from word j are affected by the LSTM\u2019s forget gates between when the word is initially processed and the answer. Consequently, we empirically found that without the interluding forget gates to dampen cell movements, the variable importance scores were far noisier than in additive cell decomposition approach. Under the additive cell decomposition, it identifies the phrase \u2019it stars\u2019, as well as the actor\u2019s name Aqib Khan as being important, a sensible conclusion. Moreover, the vast majority of words are labelled with an importance score of 1, corresponding to irrelevant. On the other hand, the difference in cells approach yields widely changing importance scores, which are challenging to interpret. In terms of noise, the gradient measures seem to lie somewhere in the middle. These patterns are broadly consistent with what we have observed, and provide qualitative validation of our metrics.\n7 CONCLUSION\nIn this paper, we introduced a novel method for visualizing the importance of specific inputs in determining the output of an LSTM. By searching for phrases which consistently provide large contributions, we are able to distill trained, state of the art, LSTMs into an ordered set of representative phrases. We quantitatively validate the extracted phrases through their performance in a simple, rules-based classifier. Results are shown in a general document classification framework, then specialized to a more complex, recently introduced, question answer dataset. Our introduced measures provide superior predictive ability and cleaner visualizations relative to prior work. We believe that this represents an exciting new paradigm for analysing the behaviour of LSTM\u2019s.\nAdditive cell decomposition Difference in cell values Gradient\nwest is west is a 2010 british comedy - drama film , which is a sequel to the 1999 comedy \u201d\neast is east \u201d . it stars aqib khan\nwest is west is a 2010 british comedy -\ndrama film , which is a sequel to the 1999 comedy \u201d east is east \u201d . it starsaqib khan\nwest is west is a 2010 british comedy - drama film , which is a sequel to the 1999 comedy \u201d east is east \u201d. itstars aqib khan\nTable 6: Comparison of importance scores acquired by three different approaches, conditioning on the question \u201dthe film west is west starred which actors?\u201d. Bigger and darker means more important.\nACKNOWLEDGEMENTS\nThis research was partially funded by Air Force grant FA9550-14-1-0016. It was also supported by the Center for Science of Information (CSoI), an US NSF Science and Technology Center, under grant agreement CCF-0939370.\n8 APPENDIX - HEAT MAPS\nWe provide an example heat map using the cell decomposition metric for each class in both sentiment analysis datasets, and selected WikiMovie question categories\nDataset Category Heat Map\nYelp Polarity Positive we went here twice for breakfast . had the bananas foster waffles with fresh whipped cream , they were amazing ! ! perfect seat out side on the terrace\nYelp Polarity Negative call me spoiled ...this sushi is gross and the orange chicken , well it was so thin i don \u2019t think it had chicken in it. gosomewhereelse\nStanford Sentiment Positive Whether or not you \u2019re enlightened by any of Derrida \u2019s lectures on \u201c the other \u201d and \u201c the self ,\n\u201d Derrida is an undeniablyfascinatingandplayfulfellow\nStanford Sentiment Negative ... begins with promise , but runs aground after being snared in its own tangled plot\nPattern Question Heat Map\nMovie to Year What was the release year of another 48 hours?\nanother 48 hrs is a 1990\nMovie to Writer Which person wrote the movie last of the dogmen? last of the dogmen is a 1995 western adventure film written and directed by tab murphy\nMovie to Actor Who acted in the movie thunderbolt? thunderbolt ( ) ( \u201d piklik foh \u201d ) is a 1995 hong kong action filmstarring jackie chan\nMovie to Director Who directed bloody bloody bible camp? bloody bloody bible cam p is a 2012 american horror - comedy /s platter film . the film\nwas directed by vito trabucco\nMovie to Genre What genre is trespass in? trespassisa 1992 action Movie to Votes How would people rate the pool? though filmed in hindi , a language smith didn \u2019t know , the film earned\ngood\u2217 Movie to Rating How popular was les miserables? les mis rables is a 1935 american drama film starring fredric march and charles laughton\nbased upon thefamous Movie to Tags Describe rough magic?\nrough magic is a 1995 comedy film directed by clare peploe and starring bridget fonda , russell crowe\nMovie to Language What is the main language in fate?\nfate ( ) is a 2001 turkish\n", "cats": {"READ": 1.0, "DONTREAD": 0.0}, "reviews": [{"IS_META_REVIEW": true, "comments": "This paper proposes a novel method for extracting rule-based classifiers from trained LSTM models. The proposed method is applied to a factoid question-answering task, where it is demonstrated that the extracted rules perform comparatively to the original LSTM. The analysis of the extracted rules illustrate the features the LSTM model picks up on.\n\nAnalyzing and visualizing the computations carried out by RNNs in order to understand the functions they compute is an important direction of research. This sort of analysis will help us understand the pitfalls of RNNs, and how we can improve them. Although the approach taken is relatively inflexible - each rule is defined as an ordered sequence of words - the authors experiment with three different scores for picking salient words (state-difference, cell-difference and gradient) and their approach yields comparable performance, which suggests that the extracted rules mimic the RNN closely. The results are also somewhat surprising, since most of the rules consist only of two or three words.\n\nIt would have been interesting to try extend the approach on other natural language processing tasks, such as machine translation. Presumably the rules learned here will be quite different.\n\nOther comments:\n- Eq. (12) is over-parametrized with two vectors $P$ and $Q$. The same function can be computed with a single vector. This becomes clear when you divide both the numerator and denominator by $e^{P h_t}$.\n- Section 4.1. Is it correct that this section is focused on the forward LSTM? If so, please clarify it in the text.\n- In Eq. (13), define $c_0 = 0$.\n- Eq. (13) is exactly the same as Eq. (15). Is there a mistake?\n- In Table 1, third column should have word \"film\" highlighted.\n- \"are shown in 2\" -> \"are shown in Table 2\".\n- Since there are some problems representing numbers, it may help to replace each digit with the hashtag symbol #."}, {"DATE": "12 Feb 2017", "TITLE": "A typo in the published version?", "IS_META_REVIEW": false, "comments": "The equation between (8) - (9) seems to be incorrect, as the left hand side of which should be p_i.", "OTHER_KEYS": "(anonymous)"}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "Timely topic (interpretability of neural models for NLP), interesting approach, surprising results.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"DATE": "14 Jan 2017", "TITLE": "Updated paper", "IS_META_REVIEW": false, "comments": "In response to helpful comments from reviewers, we have just uploaded a revision. The main changes are as follows\n\n- In response to requests for extensions to other datasets, we now have results on 2 different binary sentiment analysis datasets - Stanford Sentiment Treebank and Yelp reviews\n\n- We introduced a simpler, more general approach for extracting rules from LSTMs trained on document classification, and demonstrate that with some easy modifications it can replace our prior, more complex, rule extraction mechanism on WikiMovies.\n\n- Our rules now take the form of simple phrases, rather than allowing for variable-sized gaps between words as before\n\n- Our LSTM baseline on WikiMovies is now SOTA by nearly 4%, and the automatically extracted patterns outperform the manual patterns in the earlier version.\n\n- We added a discussion on instances correctly classified by the LSTM, but incorrectly classified by our rules-based algorithm\n\n- For simplicity, we changed our WikiMovies baseline to a unidirectional LSTM, and removed bidirectional LSTMs from the paper. Extension of our new approach to bidirectional LSTMs would be straightforward, but we feel would add unneeded complexity to the presentation\n\nAll told, we feel that these algorithms and results are simpler, more powerful and more general than our prior work, and we look forward to discussing them.", "OTHER_KEYS": "W. James Murdoch"}, {"TITLE": "review", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "EDIT: the revisions made to this paper are very thorough and address many of my concerns, and the paper is also easier to understand. i recommend the latest version of this paper for acceptance and have increased my score.\n\nThis paper presents a way of interpreting LSTM models, which are notable for their opaqueness. In particular, the authors propose decomposing the LSTM's predictions for a QA task into importance scores for words, which are then used to generate patterns that are used to find answers with a simple matching algorithm. On the WikiMovies dataset, the extracted pattern matching method achieves accuracies competitive with a normal LSTM, which shows the power of the proposed approach. \n\nI really like the motivation of the paper, as interpreting LSTMs is definitely still a work-in-progress, and the high performance of the pattern matching was surprising. However, several details of the pattern extraction process are not very clear, and  the evaluation is conducted on a very specific task, where predictions are made at every word. As such, I recommend the paper in its current form as a weak accept but hope that the authors clarify their approach, as I believe the proposed method is potentially useful for NLP researchers.\n\nComments:\n- Please introduce in more detail the specific QA tasks you are applying your models on before section 3.3, as it's not clear at that point that the answer is an entity within the document.\n- 3.3: is the softmax predicting a 0/1 value (e.g., is this word the answer or not?)\n- 3.3: what are the P and Q vectors? do you just mean that you are transforming the hidden state into a 2-dimensional vector for binary prediction?\n- how does performance of the pattern matching change with different cutoff constant values?\n- 5.2: are there questions whose answers are not entities? \n- how could the proposed approach be used when predictions aren't made at every word? is there any extension for, say, sentence-level sentiment classification?\n", "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "19 Dec 2016 (modified: 24 Jan 2017)", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "Nice idea to improve understanding of LSTM models", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "This work proposes a pattern extraction method to both understand what a trained LSTM has learnt and to allow implementation of a hand-coded algorithm that performs similarly to the LSTM. Good results are shown on one dataset for one model architecture so it is unclear how well this approach will generalize, however, it seems it will be a useful way to understand and debug models.\n\nThe questions in WikiMovies seem to be generated from templates and so this pattern matching approach will likely work well. However, from the experiments it's not clear if this will extend to other types of Q&A tasks where the answer may be free form text and not be a substring in the document. Is the model required to produce a continuous span over the original document?\n\nThe approach also seems to have some deficiencies in how it handles word types such as numbers or entity names. This can be encoded in the embedding for the word but from the description of the algorithm, it seems that the approach requires an entity detector. Does this mean that the approach is unable to determine when it has reached an entity from the decomposition of the output of the LSTM? The results where 'manual pattern matching' where explicit year annotations are used, seem to show that the automatic method is unable to deal with word types.\n\nIt would also be good to see an attention model as a baseline in addition to the gradient-based baseline.\n\nMinor comments:\n- P and Q seem to be undefined.\n- Some references seem to be bad, e.g. in section 5.1: 'in 1' instead of 'in table 1'. Similarly above section 7: 'as shown in 3' and in section 7.1.\n- In the paragraph above section 6.3: 'adam' -> 'Adam'.", "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "17 Dec 2016 (modified: 21 Jan 2017)", "REVIEWER_CONFIDENCE": 3}, {"TITLE": "Review", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "This paper proposes a novel method for extracting rule-based classifiers from trained LSTM models. The proposed method is applied to a factoid question-answering task, where it is demonstrated that the extracted rules perform comparatively to the original LSTM. The analysis of the extracted rules illustrate the features the LSTM model picks up on.\n\nAnalyzing and visualizing the computations carried out by RNNs in order to understand the functions they compute is an important direction of research. This sort of analysis will help us understand the pitfalls of RNNs, and how we can improve them. Although the approach taken is relatively inflexible - each rule is defined as an ordered sequence of words - the authors experiment with three different scores for picking salient words (state-difference, cell-difference and gradient) and their approach yields comparable performance, which suggests that the extracted rules mimic the RNN closely. The results are also somewhat surprising, since most of the rules consist only of two or three words.\n\nIt would have been interesting to try extend the approach on other natural language processing tasks, such as machine translation. Presumably the rules learned here will be quite different.\n\nOther comments:\n- Eq. (12) is over-parametrized with two vectors $P$ and $Q$. The same function can be computed with a single vector. This becomes clear when you divide both the numerator and denominator by $e^{P h_t}$.\n- Section 4.1. Is it correct that this section is focused on the forward LSTM? If so, please clarify it in the text.\n- In Eq. (13), define $c_0 = 0$.\n- Eq. (13) is exactly the same as Eq. (15). Is there a mistake?\n- In Table 1, third column should have word \"film\" highlighted.\n- \"are shown in 2\" -> \"are shown in Table 2\".\n- Since there are some problems representing numbers, it may help to replace each digit with the hashtag symbol #.", "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "15 Dec 2016", "REVIEWER_CONFIDENCE": 3}, {"DATE": "01 Dec 2016", "TITLE": "More Complex Tasks", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2"}, {"IS_META_REVIEW": true, "comments": "This paper proposes a novel method for extracting rule-based classifiers from trained LSTM models. The proposed method is applied to a factoid question-answering task, where it is demonstrated that the extracted rules perform comparatively to the original LSTM. The analysis of the extracted rules illustrate the features the LSTM model picks up on.\n\nAnalyzing and visualizing the computations carried out by RNNs in order to understand the functions they compute is an important direction of research. This sort of analysis will help us understand the pitfalls of RNNs, and how we can improve them. Although the approach taken is relatively inflexible - each rule is defined as an ordered sequence of words - the authors experiment with three different scores for picking salient words (state-difference, cell-difference and gradient) and their approach yields comparable performance, which suggests that the extracted rules mimic the RNN closely. The results are also somewhat surprising, since most of the rules consist only of two or three words.\n\nIt would have been interesting to try extend the approach on other natural language processing tasks, such as machine translation. Presumably the rules learned here will be quite different.\n\nOther comments:\n- Eq. (12) is over-parametrized with two vectors $P$ and $Q$. The same function can be computed with a single vector. This becomes clear when you divide both the numerator and denominator by $e^{P h_t}$.\n- Section 4.1. Is it correct that this section is focused on the forward LSTM? If so, please clarify it in the text.\n- In Eq. (13), define $c_0 = 0$.\n- Eq. (13) is exactly the same as Eq. (15). Is there a mistake?\n- In Table 1, third column should have word \"film\" highlighted.\n- \"are shown in 2\" -> \"are shown in Table 2\".\n- Since there are some problems representing numbers, it may help to replace each digit with the hashtag symbol #."}, {"DATE": "12 Feb 2017", "TITLE": "A typo in the published version?", "IS_META_REVIEW": false, "comments": "The equation between (8) - (9) seems to be incorrect, as the left hand side of which should be p_i.", "OTHER_KEYS": "(anonymous)"}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "Timely topic (interpretability of neural models for NLP), interesting approach, surprising results.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"DATE": "14 Jan 2017", "TITLE": "Updated paper", "IS_META_REVIEW": false, "comments": "In response to helpful comments from reviewers, we have just uploaded a revision. The main changes are as follows\n\n- In response to requests for extensions to other datasets, we now have results on 2 different binary sentiment analysis datasets - Stanford Sentiment Treebank and Yelp reviews\n\n- We introduced a simpler, more general approach for extracting rules from LSTMs trained on document classification, and demonstrate that with some easy modifications it can replace our prior, more complex, rule extraction mechanism on WikiMovies.\n\n- Our rules now take the form of simple phrases, rather than allowing for variable-sized gaps between words as before\n\n- Our LSTM baseline on WikiMovies is now SOTA by nearly 4%, and the automatically extracted patterns outperform the manual patterns in the earlier version.\n\n- We added a discussion on instances correctly classified by the LSTM, but incorrectly classified by our rules-based algorithm\n\n- For simplicity, we changed our WikiMovies baseline to a unidirectional LSTM, and removed bidirectional LSTMs from the paper. Extension of our new approach to bidirectional LSTMs would be straightforward, but we feel would add unneeded complexity to the presentation\n\nAll told, we feel that these algorithms and results are simpler, more powerful and more general than our prior work, and we look forward to discussing them.", "OTHER_KEYS": "W. James Murdoch"}, {"TITLE": "review", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "EDIT: the revisions made to this paper are very thorough and address many of my concerns, and the paper is also easier to understand. i recommend the latest version of this paper for acceptance and have increased my score.\n\nThis paper presents a way of interpreting LSTM models, which are notable for their opaqueness. In particular, the authors propose decomposing the LSTM's predictions for a QA task into importance scores for words, which are then used to generate patterns that are used to find answers with a simple matching algorithm. On the WikiMovies dataset, the extracted pattern matching method achieves accuracies competitive with a normal LSTM, which shows the power of the proposed approach. \n\nI really like the motivation of the paper, as interpreting LSTMs is definitely still a work-in-progress, and the high performance of the pattern matching was surprising. However, several details of the pattern extraction process are not very clear, and  the evaluation is conducted on a very specific task, where predictions are made at every word. As such, I recommend the paper in its current form as a weak accept but hope that the authors clarify their approach, as I believe the proposed method is potentially useful for NLP researchers.\n\nComments:\n- Please introduce in more detail the specific QA tasks you are applying your models on before section 3.3, as it's not clear at that point that the answer is an entity within the document.\n- 3.3: is the softmax predicting a 0/1 value (e.g., is this word the answer or not?)\n- 3.3: what are the P and Q vectors? do you just mean that you are transforming the hidden state into a 2-dimensional vector for binary prediction?\n- how does performance of the pattern matching change with different cutoff constant values?\n- 5.2: are there questions whose answers are not entities? \n- how could the proposed approach be used when predictions aren't made at every word? is there any extension for, say, sentence-level sentiment classification?\n", "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "19 Dec 2016 (modified: 24 Jan 2017)", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "Nice idea to improve understanding of LSTM models", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "This work proposes a pattern extraction method to both understand what a trained LSTM has learnt and to allow implementation of a hand-coded algorithm that performs similarly to the LSTM. Good results are shown on one dataset for one model architecture so it is unclear how well this approach will generalize, however, it seems it will be a useful way to understand and debug models.\n\nThe questions in WikiMovies seem to be generated from templates and so this pattern matching approach will likely work well. However, from the experiments it's not clear if this will extend to other types of Q&A tasks where the answer may be free form text and not be a substring in the document. Is the model required to produce a continuous span over the original document?\n\nThe approach also seems to have some deficiencies in how it handles word types such as numbers or entity names. This can be encoded in the embedding for the word but from the description of the algorithm, it seems that the approach requires an entity detector. Does this mean that the approach is unable to determine when it has reached an entity from the decomposition of the output of the LSTM? The results where 'manual pattern matching' where explicit year annotations are used, seem to show that the automatic method is unable to deal with word types.\n\nIt would also be good to see an attention model as a baseline in addition to the gradient-based baseline.\n\nMinor comments:\n- P and Q seem to be undefined.\n- Some references seem to be bad, e.g. in section 5.1: 'in 1' instead of 'in table 1'. Similarly above section 7: 'as shown in 3' and in section 7.1.\n- In the paragraph above section 6.3: 'adam' -> 'Adam'.", "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "17 Dec 2016 (modified: 21 Jan 2017)", "REVIEWER_CONFIDENCE": 3}, {"TITLE": "Review", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "This paper proposes a novel method for extracting rule-based classifiers from trained LSTM models. The proposed method is applied to a factoid question-answering task, where it is demonstrated that the extracted rules perform comparatively to the original LSTM. The analysis of the extracted rules illustrate the features the LSTM model picks up on.\n\nAnalyzing and visualizing the computations carried out by RNNs in order to understand the functions they compute is an important direction of research. This sort of analysis will help us understand the pitfalls of RNNs, and how we can improve them. Although the approach taken is relatively inflexible - each rule is defined as an ordered sequence of words - the authors experiment with three different scores for picking salient words (state-difference, cell-difference and gradient) and their approach yields comparable performance, which suggests that the extracted rules mimic the RNN closely. The results are also somewhat surprising, since most of the rules consist only of two or three words.\n\nIt would have been interesting to try extend the approach on other natural language processing tasks, such as machine translation. Presumably the rules learned here will be quite different.\n\nOther comments:\n- Eq. (12) is over-parametrized with two vectors $P$ and $Q$. The same function can be computed with a single vector. This becomes clear when you divide both the numerator and denominator by $e^{P h_t}$.\n- Section 4.1. Is it correct that this section is focused on the forward LSTM? If so, please clarify it in the text.\n- In Eq. (13), define $c_0 = 0$.\n- Eq. (13) is exactly the same as Eq. (15). Is there a mistake?\n- In Table 1, third column should have word \"film\" highlighted.\n- \"are shown in 2\" -> \"are shown in Table 2\".\n- Since there are some problems representing numbers, it may help to replace each digit with the hashtag symbol #.", "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "15 Dec 2016", "REVIEWER_CONFIDENCE": 3}, {"DATE": "01 Dec 2016", "TITLE": "More Complex Tasks", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2"}]}
{"text": "We propose the Gaussian attention model for content-based neural memory access. With the proposed attention model, a neural network has the additional degree of freedom to control the focus of its attention from a laser sharp attention to a broad attention. It is applicable whenever we can assume that the distance in the latent space reflects some notion of semantics. We use the proposed attention model as a scoring function for the embedding of a knowledge base into a continuous vector space and then train a model that performs question answering about the entities in the knowledge base. The proposed attention model can handle both the propagation of uncertainty when following a series of relations and also the conjunction of conditions in a natural way. On a dataset of soccer players who participated in the FIFA World Cup 2014, we demonstrate that our model can handle both path queries and conjunctive queries well.\n1 INTRODUCTION\nThere is a growing interest in incorporating external memory into neural networks. For example, memory networks (Weston et al., 2014; Sukhbaatar et al., 2015) are equipped with static memory slots that are content or location addressable. Neural Turing machines (Graves et al., 2014) implement memory slots that can be read and written as in Turing machines (Turing, 1938) but through differentiable attention mechanism.\nEach memory slot in these models stores a vector corresponding to a continuous representation of the memory content. In order to recall a piece of information stored in memory, attention is typically employed. Attention mechanism introduced by Bahdanau et al. (2014) uses a network that outputs a discrete probability mass over memory items. A memory read can be implemented as a weighted sum of the memory vectors in which the weights are given by the attention network. Reading out a single item can be realized as a special case in which the output of the attention network is peaked at the desired item. The attention network may depend on the current context as well as the memory item itself. The attention model is called location-based and content-based, if it depends on the location in the memory and the stored memory vector, respectively.\nKnowledge bases, such as WordNet and Freebase, can also be stored in memory either through an explicit knowledge base embedding (Bordes et al., 2011; Nickel et al., 2011; Socher et al., 2013) or through a feedforward network (Bordes et al., 2015).\nWhen we embed entities from a knowledge base in a continuous vector space, if the capacity of the embedding model is appropriately controlled, we expect semantically similar entities to be close to each other, which will allow the model to generalize to unseen facts. However the notion of proximity may strongly depend on the type of a relation. For example, Benjamin Franklin was an engineer but also a politician. We would need different metrics to capture his proximity to other engineers and politicians of his time.\nIn this paper, we propose a new attention model for content-based addressing. Our model scores each item vitem in the memory by the (logarithm of) multivariate Gaussian likelihood as follows:\nscore(vitem) = log \u03c6(vitem|\u00b5context,\u03a3context)\n= \u22121 2 (vitem \u2212 \u00b5context)\u03a3 \u22121 context(vitem \u2212 \u00b5context) + const. (1)\nwhere context denotes all the variables that the attention depends on. For example, \u201cAmerican engineers in the 18th century\u201d or \u201cAmerican politicians in the 18th century\u201d would be two contexts that include Benjamin Franklin but the two attentions would have very different shapes.\nCompared to the (normalized) inner product used in previous work (Sukhbaatar et al., 2015; Graves et al., 2014) for content-based addressing, the Gaussian model has the additional control of the spread of the attention over items in the memory. As we show in Figure 1, we can view the conventional inner-product-based attention and the proposed Gaussian attention as addressing by an affine energy function and a quadratic energy function, respectively. By making the addressing mechanism more complex, we may represent many entities in a relatively low dimensional embedding space. Since knowledge bases are typically extremely sparse, it is more likely that we can afford to have a more complex attention model than a large embedding dimension.\nWe apply the proposed Gaussian attention model to question answering based on knowledge bases. At the high-level, the goal of the task is to learn the mapping from a question about objects in the knowledge base in natural language to a probability distribution over the entities. We use the scoring function (1) for both embedding the entities as vectors, and extracting the conditions mentioned in the question and taking a conjunction of them to score each candidate answer to the question.\nThe ability to compactly represent a set of objects makes the Gaussian attention model well suited for representing the uncertainty in a multiple-answer question (e.g., \u201cwho are the children of Abraham Lincoln?\u201d). Moreover, traversal over the knowledge graph (see Guu et al., 2015) can be naturally handled by a series of Gaussian convolutions, which generalizes the addition of vectors. In fact, we model each relation as a Gaussian with mean and variance parameters. Thus a traversal on a relation corresponds to a translation in the mean and addition of the variances.\nThe proposed question answering model is able to handle not only the case where the answer to a question is associated with an atomic fact, which is called simple Q&A (Bordes et al., 2015), but also questions that require composition of relations (path queries in Guu et al. (2015)) and conjunction of queries. An example flow of how our model deals with a question \u201cWho plays forward for Borussia Dortmund?\u201d is shown in Figure 2 in Section 3.\nThis paper is structured as follows. In Section 2, we describe how the Gaussian scoring function (1) can be used to embed the entities in a knowledge base into a continuous vector space. We call our model TransGaussian because of its similarity to the TransE model proposed by Bordes et al. (2013). Then in Section 3, we describe our question answering model. In Section 4, we carry out experiments on WorldCup2014 dataset we collected. The dataset is relatively small but it allows us to evaluate not only simple questions but also path queries and conjunction of queries. The proposed TransGaussian embedding with the question answering model achieves significantly higher accuracy than the vanilla TransE embedding or TransE trained with compositional relations Guu et al. (2015) combined with the same question answering model.\n2 KNOWLEDGE BASE EMBEDDING\nIn this section, we describe the proposed TransGaussian model based on the Gaussian attention model (1). While it is possible to train a network that computes the embedding in a single pass (Bordes et al., 2015) or over multiple passes (Li et al., 2015), it is more efficient to offload the embedding as a separate step for question answering based on a large static knowledge base.\n2.1 THE TRANSGAUSSIAN MODEL\nLet E be the set of entities and R be the set of relations. A knowledge base is a collection of triplets (s, r, o), where we call s \u2208 E , r \u2208 R, and o \u2208 E , the subject, the relation, and the object of the triplet, respectively. Each triplet encodes a fact. For example, (Albert Einstein,has profession,theoretical physicist). All the triplets given in a knowledge base are assumed to be true. However generally speaking a triplet may be true or false. Thus knowledge base embedding aims at training a model that predict if a triplet is true or not given some parameterization of the entities and relations (Bordes et al., 2011; 2013; Nickel et al., 2011; Socher et al., 2013; Wang et al., 2014).\nIn this paper, we associate a vector vs \u2208 Rd with each entity s \u2208 E , and we associate each relation r \u2208 R with two parameters, \u03b4r \u2208 Rd and a positive definite symmetric matrix \u03a3r \u2208 Rd\u00d7d++ . Given subject s and relation r, we can compute the score of an object o to be in triplet (s, r, o) using the Gaussian attention model as (1) with\nscore(s, r, o) = log \u03c6(vo|\u00b5context,\u03a3context), (2) where \u00b5context = vs + \u03b4r, \u03a3context = \u03a3r. Note that if \u03a3r is fixed to the identity matrix, we are modeling the relation of subject vs and object vo as a translation \u03b4r, which is equivalent to the TransE model (Bordes et al., 2013). We allow the covariance \u03a3r to depend on the relation to handle one-to-many relations (e.g., profession has person relation) and capture the shape of the distribution of the set of objects that can be in the triplet. We call our model TransGaussian because of its similarity to TransE (Bordes et al., 2013).\nParameterization For computational efficiency, we will restrict the covariance matrix \u03a3r to be diagonal in this paper. Furthermore, in order to ensure that \u03a3r is strictly positive definite, we employ the exponential linear unit (ELU, Clevert et al., 2015) and parameterize \u03a3r as follows:\n\u03a3r = diag\n( ELU(mr,1)+1+\n. . . ELU(mr,d)+1+ ) where mr,j (j = 1, . . . , d) are the unconstrained parameters that are optimized during training and is a small positive value that ensure the positivity of the variance during numerical computation. The ELU is defined as\nELU(x) = { x, x \u2265 0, exp (x)\u2212 1, x < 0.\nRanking loss Suppose we have a set of triplets T = {(si, ri, oi)}Ni=1 from the knowledge base. Let N (s, r) be the set of incorrect objects to be in the triplet (s, r, \u00b7). Our objective function uses the ranking loss to measure the margin between the scores of true answers and those of false answers and it can be written as follows:\nmin {ve:e\u2208E},\n{\u03b4r,Mr,:r\u2208R\u0304}\n1\nN \u2211 (s,r,o)\u2208T Et\u2032\u223cN (s,r) [ [\u00b5\u2212 score(s, r, o) + score(s, r, t\u2032)]+ ]\n+ \u03bb \u2211 e\u2208E \u2016ve\u201622 + \u2211 r\u2208R\u0304 ( \u2016\u03b4r\u201622 + \u2016M r\u20162F ) , (3) where, N = |T |, \u00b5 is the margin parameter and M r denotes the diagonal matrix with mr,j , j = 1, . . . , d on the diagonal; the function [\u00b7]+ is defined as [x]+ = max(0, x). Here, we treat an inverse\nrelation as a separate relation and denote by R\u0304 = R\u222aR\u22121 the set of all the relations including both relations in R and their inverse relations; a relation r\u0303 is the inverse relation of r if (s, r\u0303, o) implies (o, r, s) and vice versa. Moreover, Et\u2032\u223cN (s,r) denotes the expectation with respect to the uniform distribution over the set of incorrect objects, which we approximate with 10 random samples in the experiments. Finally, the last terms are `2 regularization terms for the embedding parameters.\n2.2 COMPOSITIONAL RELATIONS\nGuu et al. (2015) has recently shown that training TransE with compositional relations can make it competitive to more complex models, although TransE is much simpler compared to for example, neural tensor networks (NTN, Socher et al. (2013)) and TransH Wang et al. (2014). Here, a compositional relation is a relation that is composed as a series of relations in R, for example, grand father of can be composed as first applying the parent of relation and then the father of relation, which can be seen as a traversal over a path on the knowledge graph.\nTransGaussian model can naturally handle and propagate the uncertainty over such a chain of relations by convolving the Gaussian distributions along the path. That is, the score of an entity o to be in the \u03c4 -step relation r1/r2/ \u00b7 \u00b7 \u00b7 /r\u03c4 with subject s, which we denote by the triplet (s, r1/r2/ \u00b7 \u00b7 \u00b7 /r\u03c4 , o), is given as\nscore(s, r1/r2/ \u00b7 \u00b7 \u00b7 /r\u03c4 , o) = log \u03c6(vo|\u00b5context,\u03a3context), (4) with \u00b5context = vs + \u2211\u03c4 t=1 \u03b4rt , \u03a3context = \u2211\u03c4 t=1 \u03a3rt , where the covariance associated with each relation is parameterized in the same way as in the previous subsection. Training with compositional relations Let P = {( si, ri1/ri2/ \u00b7 \u00b7 \u00b7 /rili , oi )}N \u2032 i=1\nbe a set of randomly sampled paths from the knowledge graph. Here relation rik in a path can be a relation in R or an inverse relation in R\u22121. With the scoring function (4), the generalized training objective for compositional relations can be written identically to (3) except for replacing T with T \u222a P and replacing N with N \u2032 = |T \u222a P|.\n3 QUESTION ANSWERING\nGiven a set of question-answer pairs, in which the question is phrased in natural language and the answer is an entity in the knowledge base, our goal is to train a model that learns the mapping from the question to the correct entity. Our question answering model consists of three steps, entity recognition, relation composition, and conjunction. We first identify a list of entities mentioned in the question (which is assumed to be provided by an oracle in this paper). If the question is \u201cWho plays Forward for Borussia Dortmund?\u201d then the list would be [Forward, Borussia Dortmund]. The next step is to predict the path of relations on the knowledgegraph starting from each entity in the list extracted in the first step. In the above example, this will be (smooth versions of) /Forward/position played by/ and /Borussia Dortmund/has player/ predicted as series of Gaussian convolutions. In general, we can have multiple relations appearing in each path. Finally, we take a product of all the Gaussian attentions and renormalize it, which is equivalent to Bayes\u2019 rule with independent observations (paths) and a noninformative prior.\n3.1 ENTITY RECOGNITION\nWe assume that there is an oracle that provides a list containing all the entities mentioned in the question, because (1) a domain specific entity recognizer can be developed efficiently (Williams et al., 2015) and (2) generally entity recognition is a challenging task and it is beyond the scope of this paper to show whether there is any benefit in training our question answering model jointly with a entity recognizer. We assume that the number of extracted entities can be different for each question.\n3.2 RELATION COMPOSITION\nWe train a long short-term memory (LSTM, Hochreiter & Schmidhuber, 1997) network that emits an output ht for each token in the input sequence. Then we compute the attention over the hidden\nstates for each recognized entity e as\npt,e = softmax (f(ve,ht)) (t = 1, . . . , T ),\nwhere ve is the vector associated with the entity e. We use a two-layer perceptron for f in our experiments, which can be written as follows:\nf(ve,ht) = u > f ReLU (W f,vve +W f,hht + b1) + b2,\nwhereW f,v\u2208RL\u00d7d,W f,h\u2208RL\u00d7H , b1 \u2208 RL, uf \u2208 RL, b2 \u2208 R are parameters. Here ReLU(x)= max(0, x) is the rectified linear unit. Finally, softmax denotes softmax over the T tokens.\nNext, we use the weights pt,e to compute the weighted sum over the hidden states ht as oe = \u2211T\nt=1 pt,eht. (5)\nThen we compute the weights \u03b1r,e over all the relations as \u03b1r,e = ReLU ( w>r oe ) (\u2200r \u2208 R \u222a R\u22121). Here the rectified linear unit is used to ensure the positivity of the weights. Note however that the weights should not be normalized, because we may want to use the same relation more than once in the same path. Making the weights positive also has the effect of making the attention sparse and interpretable because there is no cancellation.\nFor each extracted entity e, we view the extracted entity and the answer of the question to be the subject and the object in some triplet (e, p, o), respectively, where the path p is inferred from the question as the weights \u03b1r,e as we described above. Accordingly, the score for each candidate answer o can be expressed using (1) as:\nscoree(vo) = log \u03c6(vo|\u00b5e,\u03b1,KB,\u03a3e,\u03b1,KB) (6) with \u00b5e,\u03b1,KB = ve + \u2211 r\u2208R\u0304 \u03b1r,e\u03b4r, \u03a3e,\u03b1,KB = \u2211 r\u2208R\u0304 \u03b1 2 r,e\u03a3r, where ve is the vector associated with entity e and R\u0304 = R\u222aR\u22121 denotes the set of relations including the inverse relations.\n3.3 CONJUNCTION\nLet E(q) be the set of entities recognized in the question q. The final step of our model is to take the conjunction of the Gaussian attentions derived in the previous step. This step is simply carried out by multiplying the Gaussian attentions as follows:\nscore(vo|E(q),\u0398) = log \u220f\ne\u2208E(q)\n\u03c6(vo|\u00b5e,\u03b1,KB,\u03a3e,\u03b1,KB)\n= \u22121 2 \u2211 e\u2208E(q) ( vo \u2212 \u00b5e,\u03b1,KB )> \u03a3\u22121e,\u03b1,KB(vo \u2212 \u00b5e,\u03b1,KB) + const., (7)\nwhich is again a (logarithm of) Gaussian scoring function, where \u00b5e,\u03b1,KB and \u03a3e,\u03b1,KB are the mean and the covariance of the Gaussian attention given in (6). Here \u0398 denotes all the parameters of the question-answering model.\n3.4 TRAINING THE QUESTION ANSWERING MODEL\nSuppose we have a knowledge base (E ,R, T ) and a trained TransGaussian model( {ve}e\u2208E , {(\u03b4r,\u03a3r)}r\u2208R\u0304 ) , where R\u0304 is the set of all relations including the inverse relations. During training time, we assume the training set is a supervised question-answer pairs {(qi, E(qi), ai) : i = 1, 2, . . . ,m}. Here, qi is a question formulated in natural language, E(qi) \u2282 E is a set of knowledge base entities that appears in the question, and ai \u2208 E is the answer to the question. For example, on a knowledge base of soccer players, a valid training sample could be\n(\u201cWho plays forward for Borussia Dortmund?\u201d,[Forward,Borussia Dortmund], Marco Reus).\nNote that the answer to a question is not necessarily unique and we allow ai to be any of the true answers in the knowledge base. During test time, our model is shown (qi, E(qi)) and the task is to find ai. We denote the set of answers to qi by A(qi).\nTo train our question-answering model, we minimize the objective function\n1\nm m\u2211 i=1 ( E t\u2032\u223cN (qi) [ [\u00b5\u2212 score(vai |E(qi),\u0398) + score(vt\u2032 |E(qi),\u0398)]+ ] + \u03bd \u2211 e\u2208E(qi) \u2211 r\u2208R\u0304 |\u03b1r,e| ) + \u03bb\u2016\u0398\u201622\nwhere Et\u2032\u223cN (qi) is expectation with respect to a uniform distribution over of all incorrect answers to qi, which we approximate with 10 random samples. We assume that the number of relations implied in a question is small compared to the total number of relations in the knowledge base. Hence the coefficients \u03b1r,e computed for each question qi are regularized by their `1 norms.\n4 EXPERIMENTS\nAs a demonstration of the proposed framework, we perform question and answering on a dataset of soccer players. In this work, we consider two types of questions. A path query is a question that contains only one named entity from the knowledge base and its answer can be found from the knowledge graph by walking down a path consisting of a few relations. A conjunctive query is a question that contains more than one entities and the answer is given as the conjunction of all path queries starting from each entity. Furthermore, we experimented on a knowledge base completion task with TransGaussian embeddings to test its capability of generalization to unseen fact. Since knowledge base completion is not the main focus of this work, we include the results in the Appendix.\n4.1 WORLDCUP2014 DATASET\nWe build a knowledge base of football players that participated in FIFA World Cup 2014 1. The original dataset consists of players\u2019 information such as nationality, positions on the field and ages etc. We picked a few attributes and constructed 1127 entities and 6 atomic relations. The entities include 736 players, 297 professional soccer clubs, 51 countries, 39 numbers and 4 positions. And the six atomic relations are\n1The original dataset can be found at https://datahub.io/dataset/fifa-world-cup-2014-all-players.\nplays in club: PLAYER\u2192 CLUB, plays position: PLAYER\u2192 POSITION, is aged: PLAYER\u2192 NUMBER, wears number 2: PLAYER\u2192 NUMBER, plays for country: PLAYER\u2192 COUNTRY, is in country: CLUB\u2192 COUNTRY,\nwhere PLAYER, CLUB, NUMBER, etc, denote the type of entities that can appear as the left or right argument for each relation. Some relations share the same type as the right argument, e.g., plays for country and is in country.\nGiven the entities and relations, we transformed the dataset into a set of 3977 triplets. A list of sample triplets can be found in the Appendix. Based on these triplets, we created two sets of question answering tasks which we call path query and conjunctive query respectively. The answer of every question is always an entity in the knowledge base and a question can involve one or two triplets. The questions are generated as follows.\nPath queries. Among the paths on the knowledge graph, there are some natural composition of relations, e.g., plays in country (PLAYER \u2192 COUNTRY) can be decomposed as the composition of plays in club (PLAYER\u2192 CLUB) and is in country (CLUB\u2192 COUNTRY). In addition to the atomic relations, we manually picked a few meaningful compositions of relations and formed query templates, which takes the form \u201cfind e \u2208 E , such that (s, p, e) is true\u201d, where s is the subject and p can be an atomic relation or a path of relations. To formulate a set of path-based question-answer pairs, we manually created one or more question templates for every query template (see Table 5) Then, for a particular instantiation of a query template with subject and object entities, we randomly select a question template to generate a question given the subject; the object entity becomes the answer of the question. See Table 6 for the list of composed relations, sample questions, and answers. Note that all atomic relations in this dataset are many-to-one while these composed relations can be one-to-many or many-to-many as well.\nConjunctive queries. To generate question-and-answer pairs of conjunctive queries, we first picked three pairs of relations and used them to create query templates of the form \u201cFind e \u2208 E , such that both (s1, r1, e) and (s2, r2, e) are true.\u201d (see Table 5). For a pair of relations r1 and r2, we enumerated all pairs of entities s1, s2 that can be their subjects and formulated the corresponding query in natural language using question templates as in the same way as path queries. See Table 7 for a list of sample questions and answers.\nAs a result, we created 8003 question-and-answer pairs of path queries and 2208 pairs of conjunctive queries which are partitioned into train / validation / test subsets. We refer to Table 1 for more statistics about the dataset. Templates for generating the questions are list in Table 5.\n4.2 EXPERIMENTAL SETUP\nTo perform question and answering under our proposed framework, we first train the TransGaussian model on WorldCup2014 dataset. In addition to the atomic triplets, we randomly sampled 50000 paths with length 1 or 2 from the knowledge graph and trained a TransGaussian model compositionally as described in Set 2.2. An inverse relation is treated as a separate relation. Following the naming convention from Guu et al. (2015), we denote this trained embedding by TransGaussian (COMP). We found that the learned embedding possess some interesting properties. Some dimensions of the embedding space dedicate to represent a particular relation. Players are clustered by their attributes when entities\u2019 embeddings are projected to the corresponding lower dimensional subspaces. We elaborate and illustrate such properties in the Appendix.\nBaseline methods We also trained a TransGaussian model only on the atomic triplets and denote such a model by TransGaussian (SINGLE). Since no inverse relation was involved when TransGaussian (SINGLE) was trained, to use this embedding in question answering tasks, we represent the inverse relations as follows: for each relation r with mean \u03b4r and variance \u03a3r, we model its inverse r\u22121 as a Gaussian attention with mean \u2212\u03b4r and variance equal to \u03a3r. We also trained TransE models on WorldCup2014 dataset by using the code released by the authors of Guu et al. (2015). Likewise, we use TransE (SINGLE) to denote the model trained with atomic triplets only and use TransE (COMP) to denote the model trained with the union of triplets and paths. Note that TransE can be considered as a special case of TransGaussian where the variance matrix is the identity and hence, the scoring formula Eq. (7) is applicable to TransE as well.\nTraining configurations For all models, dimension of entity embeddings was set to 30. The hidden size of LSTM was set to 80. Word embeddings were trained jointly with the question answering model and dimension of word embedding was set to 40. We employed Adam (Kingma & Ba, 2014) as the optimizer. All parameters were tuned on the validation set. Under the same setting, we experimented with two cases: first, we trained models for path queries and conjunctive queries separately; Furthermore, we trained a single model that addresses both types queries. We present the results of the latter case in the next subsection while the results of the former are included in the Appendix.\nEvaluation metrics During test time, our model receives a question in natural language and a list of knowledge base entities contained in the question. Then it predicts the mean and variance of a Gaussian attention formulated in Eq. (7) which is expected to capture the distribution of all positive answers. We rank all entities in the knowledge base by their scores under this Gaussian attention. Next, for each entity which is a correct answer, we check its rank relative to all incorrect answers and call this rank the filtered rank. For example, if a correct entity is ranked above all negative answers except for one, it has filtered rank two. We compute this rank for all true answers and report mean filtered rank and H@1 which is the percentage of true answers that have filtered rank 1.\n4.3 EXPERIMENTAL RESULTS\nWe present the results of joint learning in Table 2. These results show that TransGaussian works better than TransE in general. In fact, TransGaussian (COMP) achieved the best performance in almost all aspects. Most notably, it achieved the highest H@1 rates on challenging questions such as \u201cwhere is the club that edin dzeko plays for?\u201d (#11, composition of two relations) and \u201cwho are the defenders on german national team?\u201d (#14, conjunction of two queries).\nThe same table shows that TransGaussian benefits remarkably from compositional training. For example, compositional training improved TransGaussian\u2019s H@1 rate by near 60% in queries on players from a given countries (#8) and queries on players who play a particular position (#9). It also boosted TransGaussian\u2019s performance on all conjunctive quries (#13\u2013#15) significantly.\nTo understand TransGaussian (COMP)\u2019s weak performance on answering queries on the professional football club located in a given country (#10) and queries on professional football club that has players from a particular country (#12), we tested its capability of modeling the composed relation by feeding the correct relations and subjects during test time. It turns out that these two relations were not modeled well by TransGaussian (COMP) embedding, which limits its performance in question answering. (See Table 8 in the Appendix for quantitative evaluations.) The same limit was found in the other three embeddings as well.\nNote that all the models compared in Table 2 uses the proposed Gaussian attention model because TransE is the special case of TransGaussian where the variance is fixed to one. Thus the main differences are whether the variance is learned and whether the embedding was trained compositionally. Finally, we refer to Table 9 and 10 in the Appendix for experimental results of models trained on path and conjunctive queries separately.\n5 RELATED WORK\nThe work of Vilnis & McCallum (2014) is similar to our Gaussian attention model. They discuss many advantages of the Gaussian embedding; for example, it is arguably a better way of handling asymmetric relations and entailment. However the work was presented in the word2vec (Mikolov et al., 2013)-style word embedding setting and the Gaussian embedding was used to capture the diversity in the meaning of a word. Our Gaussian attention model extends their work to a more general setting in which any memory item can be addressed through a concept represented as a Gaussian distribution over the memory items.\nBordes et al. (2014; 2015) proposed a question-answering model that embeds both questions and their answers to a common continuous vector space. Their method in Bordes et al. (2015) can combine multiple knowledge bases and even generalize to a knowledge base that was not used during training. However their method is limited to the simple question answering setting in which the answer of each question associated with a triplet in the knowledge base. In contrast, our method can handle both composition of relations and conjunction of conditions, which are both naturally enabled by the proposed Gaussian attention model.\nNeelakantan et al. (2015a) proposed a method that combines relations to deal with compositional relations for knowledge base completion. Their key technical contribution is to use recurrent neural networks (RNNs) to encode a chain of relations. When we restrict ourselves to path queries, question answering can be seen as a sequence transduction task (Graves, 2012; Sutskever et al., 2014) in which the input is text and the output is a series of relations. If we use RNNs as a decoder, our model would be able to handle non-commutative composition of relations, which the current weighted convolution cannot handle well. Another interesting connection to our work is that they take the maximum of the inner-product scores (see also Weston et al., 2013; Neelakantan et al., 2015b), which are computed along multiple paths connecting a pair of entities. Representing a set as a collection of vectors and taking the maximum over the inner-product scores is a natural way to represent a set of memory items. The Gaussian attention model we propose in this paper, however, has the advantage of differentiability and composability.\n6 CONCLUSION\nIn this paper, we have proposed the Gaussian attention model which can be used in a variety of contexts where we can assume that the distance between the memory items in the latent space is compatible with some notion of semantics. We have shown that the proposed Gaussian scoring function can be used for knowledge base embedding achieving competitive accuracy. We have also shown that our embedding model can naturally propagate uncertainty when we compose relations together. Our embedding model also benefits from compositional training proposed by Guu et al. (2015). Furthermore, we have demonstrated the power of the Gaussian attention model in a challenging question answering problem which involves both composition of relations and conjunction of queries. Future work includes experiments on natural question answering datasets and end-to-end training including the entity extractor.\nACKNOWLEDGMENTS\nThe authors would like to thank Daniel Tarlow, Nate Kushman, and Kevin Gimpel for valuable discussions.\nA WORDCUP2014 DATASET\nB TRANSGAUSSIAN EMBEDDING OF WORLDCUP2014\nWe trained our TransGaussian model on triplets and paths from WorldCup2014 dataset and illustrated the embeddings in Fig 3 and 4. Recall that we modeled every relation as a Gaussian with diagonal covariance matrix. Fig 3 shows the learned variance parameters of different relations. Each row corresponds to the variances of one relation. Columns are permuted to reveal the block structure. From this figure, we can see that every relation has a small variance in two or more dimensions. This implies that the coordinates of the embedding space are partitioned into semantically coherent clusters each of which represent a particular attribute of a player (or a football club). To verify this further, we picked the two coordinates in which a relation (e.g. plays position) has the least variance and projected the embedding of all valid subjects and objects (e.g. players and positions) of the relation to this 2 dimensional subspace. See Fig. 4. The relation between the subjects and the objects are simply translation in the projection when the corresponding subspace is two dimensional (e.g., plays position relation in Fig. 4 (a)). The same is true for other relations that requires larger dimension but it is more challenging to visualize in two dimensions. For relations that have a large number of unique objects, we only plotted for the eight objects with the most subjects for clarity of illustration.\nFurthermore, in order to elucidate whether we are limited by the capacity of the TransGaussian embedding or the ability to decode question expressed in natural language, we evaluated the test question-answer pairs using the TransGaussian embedding composed according to the ground-truth relations and entities. The results were evaluated with the same metrics as in Sec. 4.3. This estimation is conducted for TransE embeddings as well. See Table 8 for the results. Compared to Table 2, the accuracy of TransGaussian (COMP) is higher on the atomic relations and path queries but lower on conjunctive queries. This is natural because when the query is simple there is not much room for the question-answering network to improve upon just combining the relations according to the ground truth relations, whereas when the query is complex the network could combine the embedding in a more creative way to overcome its limitation. In fact, the two queries (#10 and #12) that TransGaussian (COMP) did not perform well in Table 2 pertain to a single relation is in country\u22121 (#10) and a composition of two relations plays for country\u22121 / plays in club (#12). The performance of the two queries were low even when the ground truth\nrelations were given, which indicates that the TransGaussian embedding rather than the questionanswering network is the limiting factor.\nC KNOWLEDGE BASE COMPLETION\nKnowledge base completion has been a common task for testing knowledge base models on their ability of generalizing to unseen facts. Here, we apply our TransGaussian model to a knowledge completion task and show that it has competitive performance.\nWe tested on the subset of WordNet released by Guu et al. (2015). The atomic triplets in this dataset was originally created by Socher et al. (2013) and Guu et al. (2015) added path queries that were randomly sampled from the knowledge graph. We build our TransGaussian model by training on these triplets and paths and tested our model on the same link prediction task as done by Socher et al. (2013); Guu et al. (2015).\nAs done by Guu et al. (2015), we trained TransGaussian (SINGLE) with atomic triplets only and trained TransGaussian (COMP) with the union of atomic triplets and paths. We did not incorporate\nword embedding in this task and each entity is assigned its individual vector. Without getting parameters tuned too much, TransGaussian (COMP) obtained accuracy comparable to TransE (COMP). See Table 11.\n", "cats": {"READ": 1.0, "DONTREAD": 0.0}, "reviews": [{"IS_META_REVIEW": true, "comments": "SUMMARY.\n\nThe paper propose a new scoring function for knowledge base embedding.\nThe scoring function called TransGaussian is an novel take on (or a generalization of) the well-known TransE scoring function.\nThe proposed function is tested on two tasks knowledge-base completion and question answering.\n\n----------\n\nOVERALL JUDGMENT\nWhile I think this proposed work is very interesting and it is an idea worth to explore further, the presentation and the experimental section of the paper have some problems.\nRegarding the presentation, as far as I understand this is not an attention model as intended standardly in the literature.\nPlus, it has hardly anything to share with memory networks/neural Turing machines, the parallel that the authors try to make is not very convincing.\nRegarding the experimental section, for a fair comparison the authors should test their model on standard benchmarks, reporting state-of-the-art models.\nFinally, the paper lack of discussion of results and insights on the behavior of the proposed model.\n\n\n----------\n\nDETAILED COMMENTS\n\n\nIn section 2.2 when the authors calculate \\mu_{context} do not they loose the order of relations? And if it is so, does it make any sense?"}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "Three knowledgable reviewers recommend rejection. While they agree that the paper has interesting aspects, they suggest a more convincing evaluation. The authors did not address some of the reviewer's concerns. The AC strongly encourages the authors to improve their paper and resubmit it to a future conference.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"TITLE": "review", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "The contribution of this paper can be summarized as:\n\n1, A TransGaussian model (in a similar idea of TransE) which models the subject / object embeddings in a parameterization of Gaussian distribution.  The model can be naturally adapted to path queries like the formulation of (Guu et al, 2015).\n2. Along with the entity / relation representations trained by TransGaussian, an LSTM + attention model is built on natural language questions, aiming at learning a distribution (not normalized though) over relations for question answering.\n3. Experiments on a generated WorldCup2014 dataset, focusing on path queries and conjunctive queries.\n\nOverall, I think the Gaussian parameterization exhibits some nice properties, and could be suitable to KB completion and question answering. However, some details and the main experimental results are not convincing enough to me.  The paper writing also needs to be improved. More comments below:\n\n[Major comments]\n\n- My main concern is that that evaluation results are NOT strong. Either knowledge base completion or KB-based question answering, there are many existing and competitive benchmarks (e.g., FB15k / WebQuestions). Experimenting with such a tiny WordCup2014 dataset is not convincing.  Moreover, the questions are just generated by a few templates, which is far from NL questions. I am not even not sure why we need to apply an LSTM in such scenario. The paper would be much stronger if you can demonstrate its effectiveness on the above benchmarks. \n\n- Conjunctive queries:  the current model assumes that all the detected entities in the question could be aligned to one or more relations and we can take conjunctions in the end. This assumption might be not always correct, so it is more necessary to justify this on real QA datasets.\n\n- The model is named as  \u201cGaussian attention\u201d and I kind of think it is not very closely related to well-known attention mechanism, but more related to KB embedding literature.\n\n[Minor comments]\n- I find Figure 2 a bit confusing. The first row of orange blocks denote KB relations, and the second row of those denote every single word of the NL question. Maybe make it clearer?\n\n- Besides \u201centity recognition\u201d, usually we still need an \u201centity linker\u201d component which links the text mention to the KB entity. \n", "IS_META_REVIEW": false, "RECOMMENDATION": 5, "DATE": "21 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "Review", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "This paper presents extensions to previous work using embeddings for modeling Knowledge Bases and performing Q&A on them, centered around the use of multivariate gaussian likelihood instead of inner products to score attention. This is supposed to allow more control on the attention by dealing with its spread.\n\nThis is a dense paper centered around a quite complicated model. With the supplementary material, this makes a 16p paper. It might be clearer to make 2 separate papers: one on KB completion and another one on Q&A.\n\nI like the idea of controlling the spread of the attention. This makes sense. However, I do not feel that this paper is convincing enough to justify its use compared to usual inner products.\n\nFor several reasons:\n- These should be more ablation experiments to separate the different pieces of the model and study their influence separately. The only interesting point in that sense is Table 8 in Appendix B. We need more of this. \n- In particular, a canonical experiments comparing Gaussian interaction vs inner product would be very useful. \n- Experiments on existing benchmarks (for KB completion, or QA) would help. I agree with the authors that it is difficult to find the perfect benchmark, so it is a good idea to propose a new one (WorldCup2014). But this should come in addition to experiments on existing data.\n- Table 11 of Appendix C (page 16) that compares TransE and TransGaussian for the task of link prediction on WordNet can be seen as fixing the two points above (simple setting on existing benchmark). Unfortunately, TransGaussian does not perform well compared to simpler TransE. This, along with the poor results of TransGaussian (SINGLE) of Table 2, indicate that training TransGaussian seems pretty complex, and hence question the actual validity of this architecture.\n\n", "IS_META_REVIEW": false, "RECOMMENDATION": 4, "DATE": "19 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "No Title", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "SUMMARY.\n\nThe paper propose a new scoring function for knowledge base embedding.\nThe scoring function called TransGaussian is an novel take on (or a generalization of) the well-known TransE scoring function.\nThe proposed function is tested on two tasks knowledge-base completion and question answering.\n\n----------\n\nOVERALL JUDGMENT\nWhile I think this proposed work is very interesting and it is an idea worth to explore further, the presentation and the experimental section of the paper have some problems.\nRegarding the presentation, as far as I understand this is not an attention model as intended standardly in the literature.\nPlus, it has hardly anything to share with memory networks/neural Turing machines, the parallel that the authors try to make is not very convincing.\nRegarding the experimental section, for a fair comparison the authors should test their model on standard benchmarks, reporting state-of-the-art models.\nFinally, the paper lack of discussion of results and insights on the behavior of the proposed model.\n\n\n----------\n\nDETAILED COMMENTS\n\n\nIn section 2.2 when the authors calculate \\mu_{context} do not they loose the order of relations? And if it is so, does it make any sense?\n", "IS_META_REVIEW": false, "RECOMMENDATION": 4, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 3}, {"DATE": "03 Dec 2016", "TITLE": "Questions", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1"}, {"DATE": "30 Nov 2016", "TITLE": "Some questions", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3"}, {"IS_META_REVIEW": true, "comments": "SUMMARY.\n\nThe paper propose a new scoring function for knowledge base embedding.\nThe scoring function called TransGaussian is an novel take on (or a generalization of) the well-known TransE scoring function.\nThe proposed function is tested on two tasks knowledge-base completion and question answering.\n\n----------\n\nOVERALL JUDGMENT\nWhile I think this proposed work is very interesting and it is an idea worth to explore further, the presentation and the experimental section of the paper have some problems.\nRegarding the presentation, as far as I understand this is not an attention model as intended standardly in the literature.\nPlus, it has hardly anything to share with memory networks/neural Turing machines, the parallel that the authors try to make is not very convincing.\nRegarding the experimental section, for a fair comparison the authors should test their model on standard benchmarks, reporting state-of-the-art models.\nFinally, the paper lack of discussion of results and insights on the behavior of the proposed model.\n\n\n----------\n\nDETAILED COMMENTS\n\n\nIn section 2.2 when the authors calculate \\mu_{context} do not they loose the order of relations? And if it is so, does it make any sense?"}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "Three knowledgable reviewers recommend rejection. While they agree that the paper has interesting aspects, they suggest a more convincing evaluation. The authors did not address some of the reviewer's concerns. The AC strongly encourages the authors to improve their paper and resubmit it to a future conference.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"TITLE": "review", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "The contribution of this paper can be summarized as:\n\n1, A TransGaussian model (in a similar idea of TransE) which models the subject / object embeddings in a parameterization of Gaussian distribution.  The model can be naturally adapted to path queries like the formulation of (Guu et al, 2015).\n2. Along with the entity / relation representations trained by TransGaussian, an LSTM + attention model is built on natural language questions, aiming at learning a distribution (not normalized though) over relations for question answering.\n3. Experiments on a generated WorldCup2014 dataset, focusing on path queries and conjunctive queries.\n\nOverall, I think the Gaussian parameterization exhibits some nice properties, and could be suitable to KB completion and question answering. However, some details and the main experimental results are not convincing enough to me.  The paper writing also needs to be improved. More comments below:\n\n[Major comments]\n\n- My main concern is that that evaluation results are NOT strong. Either knowledge base completion or KB-based question answering, there are many existing and competitive benchmarks (e.g., FB15k / WebQuestions). Experimenting with such a tiny WordCup2014 dataset is not convincing.  Moreover, the questions are just generated by a few templates, which is far from NL questions. I am not even not sure why we need to apply an LSTM in such scenario. The paper would be much stronger if you can demonstrate its effectiveness on the above benchmarks. \n\n- Conjunctive queries:  the current model assumes that all the detected entities in the question could be aligned to one or more relations and we can take conjunctions in the end. This assumption might be not always correct, so it is more necessary to justify this on real QA datasets.\n\n- The model is named as  \u201cGaussian attention\u201d and I kind of think it is not very closely related to well-known attention mechanism, but more related to KB embedding literature.\n\n[Minor comments]\n- I find Figure 2 a bit confusing. The first row of orange blocks denote KB relations, and the second row of those denote every single word of the NL question. Maybe make it clearer?\n\n- Besides \u201centity recognition\u201d, usually we still need an \u201centity linker\u201d component which links the text mention to the KB entity. \n", "IS_META_REVIEW": false, "RECOMMENDATION": 5, "DATE": "21 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "Review", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "This paper presents extensions to previous work using embeddings for modeling Knowledge Bases and performing Q&A on them, centered around the use of multivariate gaussian likelihood instead of inner products to score attention. This is supposed to allow more control on the attention by dealing with its spread.\n\nThis is a dense paper centered around a quite complicated model. With the supplementary material, this makes a 16p paper. It might be clearer to make 2 separate papers: one on KB completion and another one on Q&A.\n\nI like the idea of controlling the spread of the attention. This makes sense. However, I do not feel that this paper is convincing enough to justify its use compared to usual inner products.\n\nFor several reasons:\n- These should be more ablation experiments to separate the different pieces of the model and study their influence separately. The only interesting point in that sense is Table 8 in Appendix B. We need more of this. \n- In particular, a canonical experiments comparing Gaussian interaction vs inner product would be very useful. \n- Experiments on existing benchmarks (for KB completion, or QA) would help. I agree with the authors that it is difficult to find the perfect benchmark, so it is a good idea to propose a new one (WorldCup2014). But this should come in addition to experiments on existing data.\n- Table 11 of Appendix C (page 16) that compares TransE and TransGaussian for the task of link prediction on WordNet can be seen as fixing the two points above (simple setting on existing benchmark). Unfortunately, TransGaussian does not perform well compared to simpler TransE. This, along with the poor results of TransGaussian (SINGLE) of Table 2, indicate that training TransGaussian seems pretty complex, and hence question the actual validity of this architecture.\n\n", "IS_META_REVIEW": false, "RECOMMENDATION": 4, "DATE": "19 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "No Title", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "SUMMARY.\n\nThe paper propose a new scoring function for knowledge base embedding.\nThe scoring function called TransGaussian is an novel take on (or a generalization of) the well-known TransE scoring function.\nThe proposed function is tested on two tasks knowledge-base completion and question answering.\n\n----------\n\nOVERALL JUDGMENT\nWhile I think this proposed work is very interesting and it is an idea worth to explore further, the presentation and the experimental section of the paper have some problems.\nRegarding the presentation, as far as I understand this is not an attention model as intended standardly in the literature.\nPlus, it has hardly anything to share with memory networks/neural Turing machines, the parallel that the authors try to make is not very convincing.\nRegarding the experimental section, for a fair comparison the authors should test their model on standard benchmarks, reporting state-of-the-art models.\nFinally, the paper lack of discussion of results and insights on the behavior of the proposed model.\n\n\n----------\n\nDETAILED COMMENTS\n\n\nIn section 2.2 when the authors calculate \\mu_{context} do not they loose the order of relations? And if it is so, does it make any sense?\n", "IS_META_REVIEW": false, "RECOMMENDATION": 4, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 3}, {"DATE": "03 Dec 2016", "TITLE": "Questions", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1"}, {"DATE": "30 Nov 2016", "TITLE": "Some questions", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3"}]}
{"text": "RECTIFIED FACTOR NETWORKS FOR BICLUSTERING\n1 INTRODUCTION\nBiclustering is widely-used in statistics (A. Kasim & Talloen, 2016), and recently it also became popular in the machine learning community (O\u00b4 Connor & Feizi, 2014; Lee et al., 2015; Kolar et al., 2011), e.g., for analyzing large dyadic data given in matrix form, where one dimension are the samples and the other the features. A matrix entry is a feature value for the according sample. A bicluster is a pair of a sample set and a feature set for which the samples are similar to each other on the features and vice versa. Biclustering simultaneously clusters rows and columns of a matrix. In particular, it clusters row elements that are similar to each other on a subset of column elements. In contrast to standard clustering, the samples of a bicluster are only similar to each other on a subset of features. Furthermore, a sample may belong to different biclusters or to no bicluster at all. Thus, biclusters can overlap in both dimensions. For example, in drug design biclusters are compounds which activate the same gene module and thereby indicate a side effect. In this example different chemical compounds are added to a cell line and the gene expression is measured (Verbist et al., 2015). If multiple pathways are active in a sample, it belongs to different biclusters and may\nhave different side effects. In e-commerce often matrices of costumers times products are available, where an entry indicates whether a customer bought the product or not. Biclusters are costumers which buy the same subset of products. In a collaboration with the internet retailer Zalando the biclusters revealed outfits which were created by customers which selected certain clothes for a particular outfit.\nFABIA (factor analysis for bicluster acquisition, (Hochreiter et al., 2010)) evolved into one of the most successful biclustering methods. A detailed comparison has shown FABIA\u2019s superiority over existing biclustering methods both on simulated data and real-world gene expression data (Hochreiter et al., 2010). In particular FABIA outperformed non-negative matrix factorization with sparseness constraints and state-of-the-art biclustering methods. It has been applied to genomics, where it identified in gene expression data task-relevant biological modules (Xiong et al., 2014). In the large drug design project QSTAR, FABIA was used to extract biclusters from a data matrix that contains bioactivity measurements across compounds (Verbist et al., 2015). Due to its successes, FABIA has become part of the standard microarray data processing pipeline at the pharmaceutical company Janssen Pharmaceuticals. FABIA has been applied to genetics, where it has been used to identify DNA regions that are identical by descent in different individuals. These individuals inherited an IBD region from a common ancestor (Hochreiter, 2013; Povysil & Hochreiter, 2014). FABIA is a generative model that enforces sparse codes (Hochreiter et al., 2010) and, thereby, detects biclusters. Sparseness of code units and parameters is essential for FABIA to find biclusters, since only few samples and few features belong to a bicluster. Each FABIA bicluster is represented by two membership vectors: one for the samples and one for the features. These membership vectors are both sparse since only few samples and only few features belong to the bicluster.\nHowever, FABIA has shortcomings, too. A disadvantage of FABIA is that it is only feasible with about 20 code units (the biclusters) because of the high computational complexity which depends cubically on the number of biclusters, i.e. the code units. If less code units were used, only the large and common input structures would be detected, thereby, occluding the small and rare ones. Another shortcoming of FABIA is that units are insufficiently decorrelated and, therefore, multiple units may encode the same event or part of it. A third shortcoming of FABIA is that the membership vectors do not have exact zero entries, that is the membership is continuous and a threshold have to be determined. This threshold is difficult to adjust. A forth shortcoming is that biclusters can have large positive but also large negative members of samples (that is positive or negative code values). In this case it is not clear whether the positive pattern or the negative pattern has been recognized.\nRectified Factor Networks (RFNs; (Clevert et al., 2015)) RFNs overcome the shortcomings of FABIA. The first shortcoming of only few code units is avoided by extending FABIA to thousands of code units. RFNs introduce rectified units to FABIA\u2019s posterior distribution and, thereby, allow for fast computations on GPUs. They are the first methods which apply rectification to the posterior distribution of factor analysis and matrix factorization, though rectification it is well established in Deep Learning by rectified linear units (ReLUs). RFNs transfer the methods for rectification from the neural network field to latent variable models. Addressing the second shortcoming of FABIA, RFNs achieve decorrelation by increasing the sparsity of the code units using dropout from field of Deep Learning. RFNs also address the third FABIA shortcoming, since the rectified posterior means yield exact zero values. Therefore, memberships to biclusters are readily obtained by values that are not zero. Since RFNs only have non-negative code units, the problem of separating the negative from the positive pattern disappears.\n2 IDENTIFYING BICLUSTERS BY RECTIFIED FACTOR NETWORKS\n2.1 RECTIFIED FACTOR NETWORKS\nWe propose to use the recently introduced Rectified Factor Networks (RFNs; (Clevert et al., 2015)) for biclustering to overcome the drawbacks of the FABIA model. The factor analysis model and the construction of a bicluster matrix are depicted in Fig. 1. RFNs efficiently construct very sparse, nonlinear, high-dimensional representations of the input. RFN models identify rare and small events in the input, have a low interference between code units, have a small reconstruction error, and explain the data covariance structure.\nRFN learning is a generalized alternating minimization algorithm derived from the posterior regularization method which enforces non-negative and normalized posterior means. These posterior means are the code of the input data. The RFN code can be computed very efficiently. For nonGaussian priors, the computation of the posterior mean of a new input requires either to numerically solve an integral or to iteratively update variational parameters. In contrast, for Gaussian priors the posterior mean is the product between the input and a matrix that is independent of the input. RFNs use a rectified Gaussian posterior, therefore, they have the speed of Gaussian posteriors but lead to sparse codes via rectification. RFNs are implemented on GPUs.\nThe RFN model is a factor analysis model\nv = Wh + \u270f , (1)\nwhich extracts the covariance structure of the data. The prior h \u21e0 N (0, I) of the hidden units (factors) h 2 Rl and the noise \u270f \u21e0 N (0, ) of visible units (observations) v 2 Rm are independent. The model parameters are the weight (factor loading) matrix W 2 Rm\u21e5l and the noise covariance matrix 2 Rm\u21e5m. RFN models are selected via the posterior regularization method (Ganchev et al., 2010). For data {v} = {v1, . . . ,vn}, it maximizes the objective F :\nF = 1 n\nnX\ni=1\nlog p(vi) 1\nn\nnX\ni=1\nDKL(Q(hi | vi) k p(hi | vi)), (2)\nwhere DKL is the Kullback-Leibler distance. Maximizing F achieves two goals simultaneously: (1) extracting desired structures and information from the data as imposed by the generative model and (2) ensuring sparse codes via Q from the set of rectified Gaussians.\nFor Gaussian posterior distributions, and mean-centered data {v} = {v1, . . . ,vn}, the posterior p(hi | vi) is Gaussian with mean vector (\u00b5p)i and covariance matrix \u2303p:\n(\u00b5p)i = I + W T 1W 1 W T 1 vi , \u2303p = I + W T 1W 1 . (3)\nFor rectified Gaussian posterior distributions, \u2303p remains as in the Gaussian case, but minimizing the second DKL of Eq. (2) leads to constrained optimization problem (see Clevert et al. (2015))\nmin\n\u00b5i\n1\nn\nnX\ni=1\n(\u00b5i (\u00b5p)i)T \u2303 1p (\u00b5i (\u00b5p)i)\ns.t. 8i : \u00b5i 0 , 8j : 1\nn\nnX\ni=1\n\u00b52ij = 1 , (4)\nwhere \u201c \u201d is component-wise. In the E-step of the generalized alternating minimization algorithm (Ganchev et al., 2010), which is used for RFN model selection, we only perform a step of the gradient projection algorithm (Bertsekas, 1976; Kelley, 1999), in particular a step of the projected Newton method for solving Eq. (4) (Clevert et al., 2015). Therefore, RFN model selection is extremely efficient but still guarantees the correct solution.\n2.2 RFN BICLUSTERING\nFor a RFN model, each code unit represents a bicluster, where samples, for which the code unit is active, belong to the bicluster. On the other hand features that activates the code unit belong to the bicluster, too. The vector of activations of a unit across all samples is the sample membership vector. The weight vector which activates the unit is the feature membership vector. The un-constraint posterior mean vector is computed by multiplying the input with a matrix according to Eq. (3). The constraint posterior of a code unit is obtained by multiplying the input by a vector and subsequently rectifying and normalizing the code unit (Clevert et al., 2015).\nTo keep feature membership vector sparse, we introduce a Laplace prior on the parameters. Therefore only few features contribute to activating a code unit, that is, only few features belong to a bicluster. Sparse weights Wi are achieved by a component-wise independent Laplace prior for the weights:\np(Wi) = \u21e3\n1p 2\n\u2318n nY\nk=1\ne p 2 |Wki| (5)\nThe weight update for RFN (Laplace prior on the weights) is\nW = W + \u2318 U S 1 W\n\u21b5 sign(W ) . (6)\nWhereby the sparseness of the weight matrix can be controlled by the hyper-parameter \u21b5 and U and S are defined as U = 1n Pn i=1 vi\u00b5 T i and S = 1 n Pn i=1 \u00b5i\u00b5 T i +\u2303, respectively. In order to enforce more sparseness of the sample membership vectors, we introduce dropout of code units. Dropout means that during training some code units are set to zero at the same time as they get rectified. Dropout avoids co-adaptation of code units and reduces correlation of code units \u2014 a problem of FABIA which is solved.\nRFN biclustering does not require a threshold for determining sample memberships to a bicluster since rectification sets code units to zero. Further crosstalk between biclusters via mixing up negative and positive memberships is avoided, therefore spurious biclusters do less often appear.\n3 EXPERIMENTS\nIn this section, we will present numerical results on multiple synthetic and real data sets to verify the performance of our RFN biclustering algorithm, and compare it with various other biclustering methods.\n3.1 METHODS COMPARED\nTo assess the performance of rectified factor networks (RFNs) as unsupervised biclustering methods, we compare the following 14 biclustering methods:\n(1) RFN: rectified factor networks (Clevert et al., 2015), (2) FABIA: factor analysis with Laplace prior on the hidden units (Hochreiter et al., 2010; Hochreiter, 2013), (3) FABIAS: factor analysis with sparseness projection (Hochreiter et al., 2010), (4) MFSC: matrix factorization with sparseness constraints (Hoyer, 2004), (5) plaid: plaid model (Lazzeroni & Owen, 2002; T. Chekouo & Raffelsberger, 2015), (6) ISA: iterative signature algorithm (Ihmels et al., 2004), (7) OPSM: orderpreserving sub-matrices (Ben-Dor et al., 2003), (8) SAMBA: statistical-algorithmic method for bicluster analysis (Tanay et al., 2002), (9) xMOTIF: conserved motifs (Murali & Kasif, 2003), (10) Bimax: divide-and-conquer algorithm (Prelic et al., 2006), (11) CC: Cheng-Church -biclusters\n(Cheng & Church, 2000), (12) plaid t: improved plaid model (Turner et al., 2003), (13) FLOC: flexible overlapped biclustering, a generalization of CC (Yang et al., 2005), and (14) spec: spectral biclustering (Kluger et al., 2003).\nFor a fair comparison, the parameters of the methods were optimized on auxiliary toy data sets. If more than one setting was close to the optimum, all near optimal parameter settings were tested. In the following, these variants are denoted as method variant (e.g. plaid ss). For RFN we used the following parameter setting: 13 hidden units, a dropout rate of 0.1, 500 iterations with a learning rate of 0.1, and set the parameter \u21b5 (controlling the sparseness on the weights) to 0.01.\n3.2 SIMULATED DATA SETS WITH KNOWN BICLUSTERS\nIn the following subsections, we describe the data generation process and results for synthetically generated data according to either a multiplicative or additive model structure.\n3.2.1 DATA WITH MULTIPLICATIVE BICLUSTERS\nWe assumed n = 1000 genes and l = 100 samples and implanted p = 10 multiplicative biclusters. The bicluster datasets with p biclusters are generated by following model:\nX = pX\ni=1\ni z T i + \u2325 , (7)\nwhere \u2325 2 Rn\u21e5l is additive noise; i 2 Rn and zi 2 Rl are the bicluster membership vectors for the i-th bicluster. The i\u2019s are generated by (i) randomly choosing the number N i of genes in bicluster i from {10, . . . , 210}, (ii) choosing N i genes randomly from {1, . . . , 1000}, (iii) setting i components not in bicluster i to N (0, 0.22) random values, and (iv) setting i components that are in bicluster i to N (\u00b13, 1) random values, where the sign is chosen randomly for each gene. The zi\u2019s are generated by (i) randomly choosing the number Nzi of samples in bicluster i from {5, . . . , 25}, (ii) choosing Nzi samples randomly from {1, . . . , 100}, (iii) setting zi components not in bicluster i to N (0, 0.22) random values, and (iv) setting zi components that are in bicluster i to N (2, 1) random values. Finally, we draw the \u2325 entries (additive noise on all entries) according to N (0, 32) and compute the data X according to Eq. (7). Using these settings, noisy biclusters of random sizes between 10\u21e55 and 210\u21e525 (genes\u21e5samples) are generated. In all experiments, rows (genes) were standardized to mean 0 and variance 1.\n3.2.2 DATA WITH ADDITIVE BICLUSTERS\nIn this experiment we generated biclustering data where biclusters stem from an additive two-way ANOVA model:\nX = pX\ni=1\n\u2713i ( i zTi ) + \u2325 , \u2713ikj = \u00b5i + \u21b5ik + ij , (8)\nwhere is the element-wise product of matrices and both i and zi are binary indicator vectors which indicate the rows and columns belonging to bicluster i. The i-th bicluster is described by an ANOVA model with mean \u00b5i, k-th row effect \u21b5ik (first factor of the ANOVA model), and jth column effect ij (second factor of the ANOVA model). The ANOVA model does not have interaction effects. While the ANOVA model is described for the whole data matrix, only the effects on rows and columns belonging to the bicluster are used in data generation. Noise and bicluster sizes are generated as in previous Subsection 3.2.1.\nData was generated for three different signal-to-noise ratios which are determined by distribution from which \u00b5i is chosen: A1 (low signal) N (0, 22), A2 (moderate signal) N (\u00b12, 0.52), and A3 (high signal) N (\u00b14, 0.52), where the sign of the mean is randomly chosen. The row effects \u21b5ki are chosen from N (0.5, 0.22) and the column effects ij are chosen from N (1, 0.52).\n3.2.3 RESULTS ON SIMULATED DATA SETS\nFor method evaluation, we use the previously introduced biclustering consensus score for two sets of biclusters (Hochreiter et al., 2010), which is computed as follows:\nStep (3) penalizes different numbers of biclusters in the sets. The highest consensus score is 1 and only obtained for identical sets of biclusters.\nTable 1 shows the biclustering results for these data sets. RFN significantly outperformed all other methods (t-test and McNemar test of correct elements in biclusters).\n3.3 GENE EXPRESSION DATA SETS\nIn this experiment, we test the biclustering methods on gene expression data sets, where the biclusters are gene modules. The genes that are in a particular gene module belong to the according bicluster and samples for which the gene module is activated belong to the bicluster. We consider three gene expression data sets which have been provided by the Broad Institute and were previously clustered by Hoshida et al. (2007) using additional data sets. Our goal was to study how well biclustering methods are able to recover these clusters without any additional information. (A) The \u201cbreast cancer\u201d data set (van\u2019t Veer et al., 2002) was aimed at a predictive gene signature for the outcome of a breast cancer therapy. We removed the outlier array S54 which leads to a data set with 97 samples and 1213 genes. In Hoshida et al. (2007), three biologically meaningful subclasses were found that should be re-identified. (B) The \u201cmultiple tissue types\u201d data set (Su et al., 2002) are gene expression profiles from human cancer samples from diverse tissues and cell lines. The data set contains 102 samples with 5565 genes. Biclustering should be able to re-identify the tissue types. (C) The \u201cdiffuse large-B-cell lymphoma (DLBCL)\u201d data set (Rosenwald et al., 2002) was aimed at predicting the survival after chemotherapy. It contains 180 samples and 661 genes. The three classes found by Hoshida et al. (2007) should be re-identified.\nFor methods assuming a fixed number of biclusters, we chose five biclusters \u2014 slightly higher than the number of known clusters to avoid biases towards prior knowledge about the number of actual clusters. Besides the number of hidden units (biclusters) we used the same parameters as described in Sec. 3.1. The performance was assessed by comparing known classes of samples in the data sets with the sample sets identified by biclustering using the consensus score defined in Subsection 3.2.3 \u2014 here the score is evaluated for sample clusters instead of biclusters. The biclustering results are summarized in Table 2. RFN biclustering yielded in two out of three datasets significantly better results than all other methods and was on second place for the third dataset (significantly according to a McNemar test of correct samples in clusters).\n3.4 1000 GENOMES DATA SETS\nIn this experiment, we used RFN for detecting DNA segments that are identical by descent (IBD). A DNA segment is IBD in two or more individuals, if they have inherited it from a common ancestor, that is, the segment has the same ancestral origin in these individuals. Biclustering is well-suited to detect such IBD segments in a genotype matrix (Hochreiter, 2013; Povysil & Hochreiter, 2014), which has individuals as row elements and genomic structural variations (SNVs) as column elements. Entries in the genotype matrix usually count how often the minor allele of a particular SNV is present in a particular individual. Individuals that share an IBD segment are similar to each other because they also share minor alleles of SNVs (tagSNVs) within the IBD segment. Individuals that share an IBD segment represent a bicluster.\nFor our IBD-analysis we used the next generation sequencing data from the 1000 Genomes Phase 3. This data set consists of low-coverage whole genome sequences from 2,504 individuals of the main continental population groups (Africans (AFR), Asians (ASN), Europeans (EUR), and Admixed Americans (AMR)). Individuals that showed cryptic first degree relatedness to others were removed, so that the final data set consisted of 2,493 individuals. Furthermore, we also included archaic human and human ancestor genomes, in order to gain insights into the genetic relationships between humans, Neandertals and Denisovans. The common ancestor genome was reconstructed from human, chimpanzee, gorilla, orang-utan, macaque, and marmoset genomes. RFN IBD detec-\ntion is based on low frequency and rare variants, therefore we removed common and private variants prior to the analysis. Afterwards, all chromosomes were divided into intervals of 10,000 variants with adjacent intervals overlapping by 5,000 variants\nIn the data of the 1000 Genomes Project, we found IBD-based indications of interbreeding between ancestors of humans and other ancient hominins within Africa (see Fig. 2 as an example of an IBD segment that matches the Neandertal genome).\n4 CONCLUSION\nWe have introduced rectified factor networks (RFNs) for biclustering and benchmarked it with 13 other biclustering methods on artificial and real-world data sets.\nOn 400 benchmark data sets with artificially implanted biclusters, RFN significantly outperformed all other biclustering competitors including FABIA. On three gene expression data sets with previously verified ground-truth, RFN biclustering yielded twice significantly better results than all other methods and was once the second best performing method. On data of the 1000 Genomes Project, RFN could identify IBD segments which support the hypothesis that interbreeding between ancestors of humans and other ancient hominins already have taken place in Africa.\nRFN biclustering is geared to large data sets, sparse coding, many coding units, and distinct membership assignment. Thereby RFN biclustering overcomes the shortcomings of FABIA and has the potential to become the new state of the art biclustering algorithm.\nAcknowledgment. We thank the NVIDIA Corporation for supporting this research with several Titan X GPUs.\n", "cats": {"READ": 1.0, "DONTREAD": 0.0}, "reviews": [{"IS_META_REVIEW": true, "comments": "This paper applies RFN for biclustering to overcome the drawbacks in FABIA. The proposed method performs best among 14 biclustering methods, However, my first concern is that from the methodological point of view, the novelty of the proposed method seems small. The authors replied to the same question which another reviewer gave, but the replies were not so convincing. \n\nThis paper was actually difficult for me to follow. For instance, in Figure 1, a bicluster matrix is constructed as an outer product of $h$ and $w$. $h$ is a hidden unit, but what is $w$? I could not find any definition. Furthermore, I could not know how $h$ is estimated in this method. Therefore, I do NOT understand how this method performs biclustering. \n\nTotally, I am not sure that this paper is suitable for publication. \n\nProns:\nEmpirical performance is good.\n\nCons:\nNovelty of the proposed method\nSome description in the paper is unclear."}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "The reviewers pointed out several issues with the paper, and all recommended rejection.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"TITLE": "Interesting work but poorly presented", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "The paper presents a repurposing of rectified factor networks proposed\nearlier by the same authors to biclustering. The method seems\npotentially quite interesting but the paper has serious problems in\nthe presentation.\n\n\nQuality:\n\nThe method relies mainly on techniques presented in a NIPS 2015 paper\nby (mostly) the same authors. The experimental procedure should be\nclarified further. The results (especially Table 2) seem to depend\ncritically upon the sparsity of the reported clusters, but the authors\ndo not explain in sufficient detail how the sparsity hyperparameter is\ndetermined.\n\n\nClarity:\n\nThe style of writing is terrible and completely unacceptable as a\nscientific publication. The text looks more like an industry white\npaper or advertisement, not an objective scientific paper. A complete\nrewrite would be needed before the paper can be considered for\npublication. Specifically, all references to companies using your\nmethods must be deleted.\n\nAdditionally, Table 1 is essentially unreadable. I would recommend\nusing a figure or cleaning up the table by removing all engineering\nnotation and reporting numbers per 1000 so that e.g. \"0.475 +/- 9e-4\"\nwould become \"475 +/- 0.9\". In general figures would be preferred as a\nprimary means for presenting the results in text while tables can be\nincluded as supplementary information.\n\n\nOriginality:\n\nThe novelty of the work appears limited: the method is mostly based on\na NIPS 2015 paper by the same authors. The experimental evaluation\nappears at least partially novel, but for example the IBD detection is\nvery similar to Hochreiter (2013) but without any comparison.\n\n\nSignificance:\n\nThe authors' strongest claim is based on strong empirical performance\nin their own benchmark problems. It is however unclear how useful this\nwould be to others as there is no code available and the details of\nthe implementation are less than complete. Furthermore, the method\ndepends on many specific tuning parameters whose tuning method is not\nfully defined, leaving it unclear how to guarantee the generalisation\nof the good performance.\n", "IS_META_REVIEW": false, "RECOMMENDATION": 4, "DATE": "15 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "Interesting paper, but the exact model being used is difficult to understand. ", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "Clarity: The novel contribution of the paper --- Section 2.2 --- was very difficult to understand. The notation seemed inconsistent (particularly the use of l, p, and m), and I am still not confident that I understand the model being used.\n\nOriginality: The novelty comes from applying the RFN model (including the ReLU non-linearity and dropout training) to the problem of biclustering. It sounds like a good idea. \n\nSignificance: The proposed algorithm appears to be a useful tool for unsupervised data modelling, and the authors make a convincing argument that it is significant. (I.E. The previous state-of-the-art, FABIA, is widely used and this method both outperforms and addresses some of the practical difficulties with that method.)\n\nQuality: The experiments are high-quality. \n\nComments:\n1) The introduction claims that this method is much faster than FABIA because the use of rectified units allow it to be run on GPUs. It is not clear to me how this works. How many biclusters can be supported with this method? It looks like the number of biclusters used for this method in the experiments is only 3-5?\n2) The introduction claims that using dropout during training increases sparsity in the bicluster assignments. This seems like a reasonable hypothesis, but this claim should be supported with a better argument or experiments.\n3) How is the model deep? The model isn't deep just because it uses a relu and dropout.", "IS_META_REVIEW": false, "RECOMMENDATION": 5, "DATE": "12 Dec 2016", "REVIEWER_CONFIDENCE": 2}, {"TITLE": "Review on the paper", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "This paper applies RFN for biclustering to overcome the drawbacks in FABIA. The proposed method performs best among 14 biclustering methods, However, my first concern is that from the methodological point of view, the novelty of the proposed method seems small. The authors replied to the same question which another reviewer gave, but the replies were not so convincing. \n\nThis paper was actually difficult for me to follow. For instance, in Figure 1, a bicluster matrix is constructed as an outer product of $h$ and $w$. $h$ is a hidden unit, but what is $w$? I could not find any definition. Furthermore, I could not know how $h$ is estimated in this method. Therefore, I do NOT understand how this method performs biclustering. \n\nTotally, I am not sure that this paper is suitable for publication. \n\nProns:\nEmpirical performance is good.\n\nCons:\nNovelty of the proposed method\nSome description in the paper is unclear.\n\n", "IS_META_REVIEW": false, "RECOMMENDATION": 5, "DATE": "08 Dec 2016", "REVIEWER_CONFIDENCE": 2}, {"DATE": "02 Dec 2016", "TITLE": "Relationship with the previous NIPS paper", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1"}, {"IS_META_REVIEW": true, "comments": "This paper applies RFN for biclustering to overcome the drawbacks in FABIA. The proposed method performs best among 14 biclustering methods, However, my first concern is that from the methodological point of view, the novelty of the proposed method seems small. The authors replied to the same question which another reviewer gave, but the replies were not so convincing. \n\nThis paper was actually difficult for me to follow. For instance, in Figure 1, a bicluster matrix is constructed as an outer product of $h$ and $w$. $h$ is a hidden unit, but what is $w$? I could not find any definition. Furthermore, I could not know how $h$ is estimated in this method. Therefore, I do NOT understand how this method performs biclustering. \n\nTotally, I am not sure that this paper is suitable for publication. \n\nProns:\nEmpirical performance is good.\n\nCons:\nNovelty of the proposed method\nSome description in the paper is unclear."}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "The reviewers pointed out several issues with the paper, and all recommended rejection.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"TITLE": "Interesting work but poorly presented", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "The paper presents a repurposing of rectified factor networks proposed\nearlier by the same authors to biclustering. The method seems\npotentially quite interesting but the paper has serious problems in\nthe presentation.\n\n\nQuality:\n\nThe method relies mainly on techniques presented in a NIPS 2015 paper\nby (mostly) the same authors. The experimental procedure should be\nclarified further. The results (especially Table 2) seem to depend\ncritically upon the sparsity of the reported clusters, but the authors\ndo not explain in sufficient detail how the sparsity hyperparameter is\ndetermined.\n\n\nClarity:\n\nThe style of writing is terrible and completely unacceptable as a\nscientific publication. The text looks more like an industry white\npaper or advertisement, not an objective scientific paper. A complete\nrewrite would be needed before the paper can be considered for\npublication. Specifically, all references to companies using your\nmethods must be deleted.\n\nAdditionally, Table 1 is essentially unreadable. I would recommend\nusing a figure or cleaning up the table by removing all engineering\nnotation and reporting numbers per 1000 so that e.g. \"0.475 +/- 9e-4\"\nwould become \"475 +/- 0.9\". In general figures would be preferred as a\nprimary means for presenting the results in text while tables can be\nincluded as supplementary information.\n\n\nOriginality:\n\nThe novelty of the work appears limited: the method is mostly based on\na NIPS 2015 paper by the same authors. The experimental evaluation\nappears at least partially novel, but for example the IBD detection is\nvery similar to Hochreiter (2013) but without any comparison.\n\n\nSignificance:\n\nThe authors' strongest claim is based on strong empirical performance\nin their own benchmark problems. It is however unclear how useful this\nwould be to others as there is no code available and the details of\nthe implementation are less than complete. Furthermore, the method\ndepends on many specific tuning parameters whose tuning method is not\nfully defined, leaving it unclear how to guarantee the generalisation\nof the good performance.\n", "IS_META_REVIEW": false, "RECOMMENDATION": 4, "DATE": "15 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "Interesting paper, but the exact model being used is difficult to understand. ", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "Clarity: The novel contribution of the paper --- Section 2.2 --- was very difficult to understand. The notation seemed inconsistent (particularly the use of l, p, and m), and I am still not confident that I understand the model being used.\n\nOriginality: The novelty comes from applying the RFN model (including the ReLU non-linearity and dropout training) to the problem of biclustering. It sounds like a good idea. \n\nSignificance: The proposed algorithm appears to be a useful tool for unsupervised data modelling, and the authors make a convincing argument that it is significant. (I.E. The previous state-of-the-art, FABIA, is widely used and this method both outperforms and addresses some of the practical difficulties with that method.)\n\nQuality: The experiments are high-quality. \n\nComments:\n1) The introduction claims that this method is much faster than FABIA because the use of rectified units allow it to be run on GPUs. It is not clear to me how this works. How many biclusters can be supported with this method? It looks like the number of biclusters used for this method in the experiments is only 3-5?\n2) The introduction claims that using dropout during training increases sparsity in the bicluster assignments. This seems like a reasonable hypothesis, but this claim should be supported with a better argument or experiments.\n3) How is the model deep? The model isn't deep just because it uses a relu and dropout.", "IS_META_REVIEW": false, "RECOMMENDATION": 5, "DATE": "12 Dec 2016", "REVIEWER_CONFIDENCE": 2}, {"TITLE": "Review on the paper", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "This paper applies RFN for biclustering to overcome the drawbacks in FABIA. The proposed method performs best among 14 biclustering methods, However, my first concern is that from the methodological point of view, the novelty of the proposed method seems small. The authors replied to the same question which another reviewer gave, but the replies were not so convincing. \n\nThis paper was actually difficult for me to follow. For instance, in Figure 1, a bicluster matrix is constructed as an outer product of $h$ and $w$. $h$ is a hidden unit, but what is $w$? I could not find any definition. Furthermore, I could not know how $h$ is estimated in this method. Therefore, I do NOT understand how this method performs biclustering. \n\nTotally, I am not sure that this paper is suitable for publication. \n\nProns:\nEmpirical performance is good.\n\nCons:\nNovelty of the proposed method\nSome description in the paper is unclear.\n\n", "IS_META_REVIEW": false, "RECOMMENDATION": 5, "DATE": "08 Dec 2016", "REVIEWER_CONFIDENCE": 2}, {"DATE": "02 Dec 2016", "TITLE": "Relationship with the previous NIPS paper", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1"}]}
{"text": "1 INTRODUCTION\nThe idea of exploiting the large amounts of data captured in electronic medical records for both clinical care and secondary research holds great promise, but its potential is weakened by errors and omissions in those records (Safran et al., 2007; de Lusignan & van Weel, 2006). Among many other problems, accurately capturing the list of medications currently taken by a given patient is extremely challenging (Velo & Minuz, 2009). In one study, over 50% of electronic medication lists contained omissions (Caglar et al., 2011), and in another, 25% of all medications taken by patients were not recorded (Kaboli et al., 2004). Even medication lists provided by the patients themselves contain multiple errors and omissions (Green et al., 2010) .\nMany efforts have been made to ensure the correctness of medication lists, most of them involving improved communication between patients and providers (Keogh et al., 2016), but these efforts have not yet been successful, and incorrect or incomplete medication documentation continues to be a source of error in computational medical research. In this work we attempt to identify likely errors and omissions in the record, predicting the set of active medications from the sequence of most recent disease-based billing codes in the record. Predictions from such a model could be used either in manual medication reconciliation (a common process undertaken to correct the medication record) or to provide a prior to other models, such as an NLP model attempting to extract medication use from the narrative clinical text.\nGiven the sequential nature of clinical data, we suspected that recurrent neural networks would be a good architecture for making these predictions. In this work we investigate this potential, comparing the performance of recurrent networks to that of similarly-configured feed forward networks.\nThe input for each case is a sequence of ICD-9 billing codes (Section 2.1), for which the model produces a single, multi-label prediction of the therapeutic classes (Section 3.1) of medications taken by the patient during the period of time covered by the billing code sequence.\nThis work is designed to test how well the complete set of medications a patient is actively taking at a given moment can be predicted by the sequence of diagnostic billing codes leading up to that moment, in the context of non-trivial label noise. It also explores whether sequence-oriented recursive neural nets can do a better job of that prediction than standard feed-forward networks.\n2 BACKGROUND\n2.1 MEDICAL BILLING CODES\nEach time a patient has billable contact with the healthcare system, one or more date-stamped billing codes are attached to the patient record, indicating the medical conditions that are associated (or suspected to be associated) with the reason for the visit. While these codes are notoriously unreliable because they are only used for billing and not actual clinical practice (O\u2019Malley et al., 2005), they are nevertheless useful in a research context (Bastarache & Denny, 2011; Denny et al., 2010), especially if they are used probabilistically (Lasko, 2014). In our institution, codes from the International Classification of Diseases, Ninth Revision (ICD-9) have historically been used, although we have recently transitioned to the tenth revision (ICD-10). For this project, we used ICD-9 codes.\nThe ICD-9 hierarchy consists of 21 chapters roughly corresponding to a single organ system or pathologic class (Appendix B). Leaf-level codes in that tree represent single diseases or disease subtypes. For this project, we used a subset of the two thousand most common leaf-level codes as our input data.\n2.2 RECURRENT NEURAL NETWORKS AND VARIATIONS\nMost of the ICLR community are very familiar with recurrent neural networks and their variations, but we include a conceptual description of them here for readers coming from other fields. More thorough descriptions are available elsewhere (Graves, 2012; Olah, 2015).\nA recurrent neural network is a variation in which the output of one node on input xt loops around to become an input to another node on input xt+1, allowing information to be preserved as it iterates over an input data sequence (Figure 1). They were introduced in the 1980s (Rumelhart et al., 1986), but achieved explosive popularity only recently, after the development of methods to more reliably capture long-term dependencies, which significantly improved their performance on sequence-tosequence mapping (Hochreiter & Schmidhuber, 1997; Sutskever et al., 2014).\nThe basic RNN unit has a simple internal structure (Figure 2a). Output from the previous iteration ht\u22121 and the next input in a sequence xt are both fed to the network on the next iteration. The Long Short-Term Memory configuration (LSTM) introduces new, more complex internal structure (Figure 2b) consisting of four neural network layers and a cell state (ct), which is carried from one iteration to another. The additional layers form forget, input and output gates, which allow for the information to be forgotten (reset) or passed on to varying degrees.\nThe LSTM model and its variations are commonly used in applications where sequence and temporal data are involved, such as in image captioning (Vinyals et al., 2014), language translation (Sutskever et al., 2014), and speech recognition (Graves et al., 2013). In many cases LSTM models define the state of the art, such as with a recent conversational speech recognizer that (slightly) outperforms professional transcriptionists (Xiong et al., 2016).\nA recent variation on the LSTM architecture is the Gated Recurrent Unit (GRU) (Cho et al., 2014), which introduces a single update gate in place of input and forget gates (Figure 2c). GRUs perform as well as or better than LSTMs in many cases (Chung et al., 2014; Jozefowicz et al., 2015), and have the additional advantage of a simpler structure.\nIn this work we try both an LSTM and a GRU on our learning problem.\n2.3 RELATED WORK\nLittle research in the computational medical domain has used recurrent neural networks. The earliest example we are aware of is the use of an LSTM model that produced reasonable accuracy\n(micro-AUC 0.86) in a 128-dimensional multi-label prediction of diagnoses from regularly sampled, continuously-monitored, real-valued physiologic variables in an Intensive Care Unit setting. This was an interesting initial application, but it turned out to be only 0.001 better than the baseline classifier, which was a multi-layer perceptron with expert-designed features (Lipton et al., 2016). Given the dataset size (10,401 patient records) the lack of improvement may have been due to insufficient data to power accurate feature learning in the recurrent network.\nVery recent work, contemporary with ours, used a GRU model with a semantic embedding in 32,787 patient records to predict the development of heart failure 3 - 6 months in the future, from medication orders and billing codes in an 18-month window. The model achieved respectable accuracy (0.88 AUC), and demonstrated a meaningful 0.05 AUC improvement over a deep feedforward network (Choi et al., 2016b).\nOther recent work from the same group used a GRU model in a multi-label context to predict the medications, billing codes, and time of the next patient visit from a sequence of that same information for previous visits, using 263,706 patient records. It achieved a recall@30 of 72.4 for the task, an improvement of 20 over a single-hidden-layer MLP with 2000 units (Choi et al., 2016a). This is an example of using one of the strengths of a recurrent network - predicting the next element in a sequence. It contrasts with our work that exploits a different strength of recurrent networks - predicting a sequence or class that is semantically distinct from but parallel to the elements of the input sequence.\nThe closest work to ours from a medical domain perspective is a series of collaborative filter models (including co-occurrence counting, k-nearest neighbors, and logistic regression) that predict missing medications using a leave-one-drug-out evaluation design, with predictions based on the rest of the medications, ICD-9 billing codes, and demographic data. The models were trained and tested on data from 419 patients in three different clinics, with accuracy varying by clinic, as expected, but not appreciably by model. Most models ranked the missing drug in the top 10 results between 40 and 50% of the time, and ranked the therapeutic class of the drug in the top 10 results between 50 and 65% of the time.\nMany aspects of our work can be found in these prior efforts, but none addresses our particular problem in the same way. Our work is unique in its learning problem of identifying all drugs a patient is likely to be taking, based only on the billing codes in the record. Like most others cited, we use recurrent neural networks in a multi-label predictive context, but in contrast to them we compare\nto the most similar non-recurrent model we can construct, in order to evaluate the contribution of the temporal sequence information to the solution. Finally, we use one to four orders of magnitude more data (3.3 million instances, see Section 3.1) than these prior efforts, which we hope will give us a more realistic assessment of the various deep architectures we use on our problem.\n3 EXPERIMENTS\n3.1 DATA\nOur source database was the deidentified mirror of Vanderbilt\u2019s Electronic Medical Record, which contains billing codes, medication histories, laboratory test results, narrative text and medical imaging data for over 2 million patients, reaching back nearly 30 years (Roden et al., 2008). We obtained IRB approval to use this data in this research.\nFor this experiment we filtered all records in our database to include only the top 1,000 most common medications and the top m = 2000 most common billing codes, which cover 99.5% of all medication occurrences and 85.1% of all billing code occurrences. We then included all records from the filtered data that had at least one medication occurrence and at least ten billing code occurrences. This resulted in 610,076 complete patient records, which we divided 80/5/15 into training, validation, and final test sets.\nA data instance d = {E, T, y} consisted of a sequence E = {e1, . . . , en}, of one-hot billing code vectors ei \u2208 {0, 1}m and their associated times T = {t1, . . . , tn}, ti \u2208 R as input, and a multi-label vector y \u2208 {0, 1}k of medication classes as the output target. The most recent n = 100 billing codes to a selected reference time point in a given patient record were collected into the input sequence E, and their occurrence times into T , zero padding if necessary. All medications that occurred during the time span of T were then collected into the output vector y. Practice patterns change over time, so simply taking the most recent 100 codes for each patient could produce a biased result. To avoid this, we chose random reference points, stratified by medication. In other words, the reference points were randomly chosen from the occurrences of each medication in the entire dataset, up to 10,000 points per medication. This resulted in 3.3 million data instances, an average of 5.4 instances per patient record. Each patient\u2019s data was included in at most one of the training, validation, or test sets.\nBecause there are often many approximately equivalent medication choices for a given therapeutic purpose, we converted medication names to their therapeutic class (beta blocker, immunosuppressant, corticosteroid, etc.) as a synonym reduction step. This step also aggregated generic with brand names, as well as different formulations of the same active ingredient. For this task we used the Anatomical Chemical Classification System (ATC)1, which is a multi-level ontology of medications, organized by both anatomic and therapeutic class. The top level is a broad categorization of medications (Appendix B), the bottom (fifth) level is individual medications, and we used the third level, which contains 287 therapeutic classes of the approximately appropriate abstraction level for our purpose. We used a publicly available mapping2 to translate between our medication names and ATC codes, with manual mapping for the minority of medications that had no mapping entry. Our set of medications used k = 182 third-level ATC codes, rendering our output label a 182-elementlong multi-label vector, in which an element is set yi = 1 if a medication in that class appeared in the set of medications identified for that instance, yi = 0 otherwise. Some medications mapped to more than one class, and we set yi = 1 for all of them.\nOur medication data was collected from structured order entry records and extracted using NLP (Xu et al., 2010) from mentions in the narrative text of a patient record that included the medication name, dose, route and frequency. As discussed above, we assumed (and our results demonstrate) that the medication data is incomplete, and our hope was that a model learned from a sufficiently large dataset will be robust to the missing data.\nThis configuration represents the input billing codes in a sequence, but the output medications as a multi-label vector. This is because ICD-9 codes are represented sequentially in our source data, but medications are not. They are represented as a list that changes over time in the record. The\n1http://www.whocc.no/atc/structure and principles/ 2https://www.nlm.nih.gov/research/umls/rxnorm/\nusual goal of clinicians is to verify the list of medications at each visit, and if omissions or additions are indicated by the patient, to change the list to reflect that. But in the time-constrained reality of clinical practice, this reconciliation happens sporadically, and many clinicians are hesitant to change an entry on the medication list for which they were not the original prescriber, so the timing of the changes in the documentation do not reflect the timing of changes in reality. Therefore we are reduced to predicting a single multi-label vector, representing the medications that the patient probably took during the span of time represented by the input codes. (We actually did attempt some full sequence-to-sequence mappings, with various orderings of the medication sequences, but we did not achieve any promising results in that direction.)\n3.2 CLASSIFIERS\nOur main technical goal was to test the performance of recurrent neural networks on this sequencecentric prediction problem. To evaluate the specific gains provided by the recurrent architectures, we compare performance against a fully connected feed-forward network configured as similarly as possible to the recurrent networks, and (as baselines) a random forest and a constant-prevalence model. We discuss the specific configurations of these classifiers in this section.\n3.2.1 RECURRENT NEURAL NETWORKS\nWe tested both LSTMs and GRUs in this experiment. We configured both architectures to first compute a semantic embedding xi \u2208 Rb of each input ei vector, before appending the times ti (Figure 3) and feeding the result to three layers of recurrent units. The final output from the last pass of recurrent unit is as a multi-label prediction for each candidate medication.\nThe optimal hyperparameters for the model were selected in the randomized parameter optimization (Bergstra & Bengio, 2012), with the embedding dimension b = 32, number of layers, and number of nodes optimized by a few trials of human-guided search. Other optimized parameters included the fraction of dropout (between layers, input gates and recurrent connections), and L1 and L2 regularization coefficients (final values are presented in Appendix A).\nBoth models were implemented using Keras (Chollet, 2015) and trained for 300 iterations using cross-entropy under the Adadelta optimizer (Zeiler, 2012).\n3.2.2 FULLY CONNECTED NEURAL NETWORK\nThe fully connected network used as similar an architecture as possible to the recurrent networks, in an attempt to isolate the gain achieved from the recurrence property. Specifically, we used the same architecture for embedding and timestamp appending (Figure 3).\nHyperparameters were optimized using random search over the number of layers, number of nodes, dropout, activation function between layers, L1 and L2 regularization coefficients (Appendix A). (Surprisingly, the optimizer chose tanh over ReLU as the optimal activation function.)\nThe models were also implemented using Keras, and were trained using cross-entropy for 500 iterations under the Adadelta optimizer.\n3.2.3 RANDOM FOREST\nBecause the random forest model is not easily structured to operate on sequences, we represented the input data as either binary occurrence vectors v \u2208 {0, 1}m, or bag-of-codes vectors w \u2208 Nm (counts of each code value in the sequence) rather than as sequences of codes with associated times. No embedding was used, because random forest code was not able to cope with the large size of the data in the (dense) embedded space.\nEven in the (sparse) original space, the full dataset was too large for the random forest code, so we implemented it as an ensemble of ten independent forests, each trained on one tenth of the training data, and their average score used for test predictions.\nModels were implemented using scikit-learn (Pedregosa et al., 2011) with parameters optimized under random search (Appendix A).\nWhile other models could reasonably serve as a baseline for this work, we chose a random forest because they tend to perform well on widely varying datasets (Ferna\u0301ndez-Delgado et al., 2014), they are efficient to train and test, and they don\u2019t require a huge effort to optimize (in order to produce a fair comparison).\n3.3 CONSTANT-PREVALENCE MODEL\nThis minimum baseline model simply predicts the prevalence of each label for all instances. For example, if there were three possible medications, with prevalences of 0.3, 0.9, and 0.2, then the prediction of this model would be a constant [0.3, 0.9, 0.2] for each instance. We include this model in order to mitigate the fact that while all of our evaluation measures are suitable for comparing models on the same data, some are not well suited for external comparison because they depend, for example, on the prevalence of positive labels (Section 3.4). By including this model we can at least establish a true minimum baseline for reference.\n3.4 EVALUATION\nOur main evaluation focused on the models, although we also performed a separate evaluation of the embedding.\n3.4.1 MODELS\nThere are several possibilities for evaluation in a multi-label classification context (Sechidis et al., 2011; Zhang & Zhou, 2014). We chose micro-averaged area under the ROC curve (AUC) and label ranking loss as the primary methods of evaluation, because they treat each instance with equal weight, regardless of the nature of the positive labels for that instance. In other words, we wanted primary measures that did not give a scoring advantage to instances with either very many or very few positive labels, or that included very rare or very prevalent labels. Additionally, both of these measures appeal to us as intuitive extensions of the usual binary AUC, when seen from the perspective of a single instance. However, because these two measures don\u2019t reflect all aspects of multi-label prediction performance, we also include macro-averaged AUC, label ranking average precision and coverage error measures.\nMicro-averaged AUC considers each of the multiple label predictions in each instance as either true or false, and then computes the binary AUC as if they all belonged to the same 2-class problem (Zhang & Zhou, 2014). In other words, micro-averaged AUC A\u00b5 is:\nA\u00b5 = \u2223\u2223(x, x\u2032, l, l\u2032) : f(x, l) \u2265 f(x\u2032, l\u2032), (x, l),\u2208 S, (x\u2032, l\u2032) \u2208 S\u0304\u2223\u2223\u2223\u2223S\u2223\u2223\u2223\u2223S\u0304\u2223\u2223 , (1) where S = {(x, l) : l \u2208 Y } is the set of (instance, label) pairs with a positive label, and Y = {yi : yi = 1, i = 1 . . . k} is the set of positive labels for input x. Label ranking loss LR gives the average fraction of all possible (positive, negative) label pairs for each instance in which the negative label has a higher score than the positive label (Tsoumakas et al., 2010):\nLR = 1\nN N\u2211 j=1\n1\n|Y (j)||Y (j)| \u2223\u2223\u2223{(l, l\u2032) : r(j)(l) > r(j)(l\u2032), (l, l\u2032) \u2208 Y (j) \u00d7 Y (j) }\u2223\u2223\u2223 (2) where the superscript (j) refers to the jth test instance (of N total instances) and r(l) is the predicted rank of a label l.\nMacro-averaged AUC can be thought of as averaging the AUC performance of several one-vs-all classifiers, one model for each label. It treats each model equally, regardless of the prevalence of positive labels for that model. This gives a score of 0.5 to the constant-prevalence model, at the cost of weighting instances differently in order to achieve that. This is in contrast to micro-averaged AUC, which can be thought of as averaging across instances rather than labels. It weighs each instance equally, at the cost of a 0.5 score no longer being the random-guessing baseline.\nLabel ranking average precision gives the mean fraction of correct positive labels among all positive labels with lower scores for each label. The coverage error function calculates the mean number of labels on the ranked list that are needed to cover all the positive labels of the sample. Both of these depend on the prevalence of positive labels in a test instance.\n3.4.2 EMBEDDING\nWe evaluated the embedding based on how strongly related in a clinical semantic sense the nearest neighbor to each code is (in the embedding space). A licensed physician manually annotated the list of all 2000 codes with its match category m \u2208 {strongly related,loosely related,unrelated}, and we computed the empirical marginal probability P (m) of each category, the empirical conditional probability P (m|d) of the match category given the nearest neighbor (Manhattan) distance d and the empirical marginal probability P (d). For comparison, we computed P (m) under 100 random code pairings.\n4 RESULTS AND DISCUSSION\nThe GRU model had the top performance by all measures, although the LSTM was a close second (Table 1), a performance pattern consistent with previous reports (Chung et al., 2014). The deep neural net performance was about 0.01 worse in both measures, suggesting that the recurrent models were able to use the sequence information, but only to a small advantage over the most similar nontemporal architecture. However, we note that both RNNs\u2019 performance peaked at the top end of our tractable range for model size, while the feed-forward network peaked using a model about one third that size (Appendix A). Experimenting with the architecture, we found that increasing the number of nodes or layers for the feed-forward network increased training time but not performance. This suggests that the RNN performance was limited by the hardware available, and increasing the size of the model may further increase performance, and that the feed-forward network was limited by something else.\nBoth random forest models were weaker than the deep neural net, as might be expected from the need to resort to binary and bag-of-codes representations of the input data.\nA natural question is what performance is good enough for clinical use. While there is little clinical experience with multi-label classifiers, we would generally expect clinicians using a binary classifier in an advisory role to find an AUC & 0.9 to be useful, and AUC & 0.95 to be very useful. An AUC difference of 0.01, and perhaps 0.005 are potentially noticeable in clinical use.\nThis 0.9/0.01 rule of thumb may loosely translate to our AUC variants, but it can directly translate to Label Ranking Loss LR (2). If we think of a single output prediction y\u0302 \u2208 [0, 1]k as a set of predictions for k binary labels, then 1\u2212 AUC for that set of predictions is equivalent to LR for the original instance y\u0302. Therefore, values of LR . 0.1 may be clinically useful, and LR . 0.05 may be very useful.\nSubjectively examining performance on 20 randomly selected cases, we find very good detailed predictions, but also evidence of both missing medications and missing billing codes. An example of a good set of detailed predictions is from a complex patient suffering from multiple myeloma (a type of cancer) with various complications. This patient was taking 26 medications, 24 of which had moderate to high probability predictions (Figure 4). (We have found by eyeball that a prediction cutoff of 0.2 gives a reasonable balance between sensitivity and specificity for our model.) In the other direction, only two of the high-prediction classes were not actually being taken, but those classes, along with several of the other moderately-predicted classes, are commonly used for cancer and are clinically reasonable for the case. (Details of this and the two cases below are in Appendix C).\nA good example of missing medications is a case in which the record has multiple billing codes for both osteoporosis (which is very commonly treated with medication) and postablative hypothyroidism (a deliberately induced condition that is always treated with medication), but no medications of the appropriate classes were in the record. The GRU model predicted both of these classes, which the patient was almost surely taking.\nA good example of either missing billing codes or discontinued medications that remain documented as active is a case in which the record has at least five years of data consisting only of codes for Parkinson\u2019s disease, but which lists medications for high cholesterol, hypertension, and other heart disease. The GRU model predicted a reasonable set of medications for Parkinson\u2019s disease and its complications, but did not predict the other medications that are not suggested by the record.\nGiven how easy it was to find cases with apparently missing codes and medications, we conclude that there is indeed a substantial amount of label noise in our data, and we therefore interpret our models\u2019 performance as lower bounds on the actual performance. We are encouraged that this kind of a model may actually be useful for identifying missing medications in the record, but of course a more thorough validation, and possibly a more accurate model, would be necessary before using in a clinical scenario. A definitive experiment would use off-line research, including reconciling information from various electronic and human sources to establish the ground truth of which medications were being taken on a particular day, but such efforts are labor intensive and expensive, and can only be conducted on a very small scale.\nAn interesting byproduct of these models is the semantic embedding of ICD-9 codes used in the recurrent networks (Figure 5). Transforming input to a semantic embedding is a common pre-\nprocessing step to improve performance, but clearly the semantic understanding it provides to an algorithm can be useful beyond the immediate learning problem (Mikolov et al., 2013). Investigating the embedding learned in this experiment shows some generalizable potential, but it also reveals the need for further refinement before it can be truly useful. Specifically, while it\u2019s easy to find tight groups of ICD-9 codes that are strongly clinically related in our embedding, we also find groups for which we cannot see a meaningful clinical relationship.\nFor example, we see two groups of codes relating to kidney failure and diabetes mellitus, two classes of very prevalent disease (Figure 5, insets). In other iterations with different parameter settings, the kidney failure codes were even embedded in a sequence reflecting the natural progression of the disease, with the code for dialysis (an intensive treatment for end-stage kidney failure) embedded at the appropriate place. Interestingly, these were not the parameter settings that optimized overall prediction performance. In other settings, such as our performance-optimal setting, the sequence is close to the natural progression of the disease, but not quite identical. Nevertheless, this is an exciting result that suggests great potential.\nFurther evaluation of the embedding found that 49% of codes were strongly related semantically to their nearest neighbor, 10% were loosely related, and 41% unrelated. This fraction of strongly related nearest neighbors was lower than we had hoped, but much higher than expected by chance (Figure 6), and it definitely improved classification performance. Furthermore, it was obvious by inspection that in general, codes closer in the embedding were more semantically related than distant codes, but interestingly, the distance to the nearest such neighbor showed the opposite relationship \u2014 nearest neighbors that were very close were less likely to be semantically related than nearest neighbors that were far, and this trend is roughly linear across the full range of d (Figure 6). So the sparser the points are in the embedded space, the more semantically related they are to their nearest neighbor, but the causal direction of that effect and the technical reason for it are beyond the scope of this initial work.\nFor this prediction problem, we settled on predicting the medications that occurred in the record during the same time span as the billing codes used. Originally, we intended to predict only the medications listed on the day of the reference point, but that turned out to greatly exacerbate the missing medication problem. After trying medications that fell on the reference day only, the week prior to the reference day, and the six months prior, our best performance both subjectively and objectively was achieved using the full time range of the input data.\nWhile the performance of the recurrent networks was quite good, we believe it could be improved by including additional input data, such as laboratory test results, demographics, and perhaps vital\nsigns. We also suspect that if we can devise a way to convert our medication data into reliablyordered sequences, we can more fully exploit the strengths of recurrent networks for medication prediction. We look forward to trying these and other variations in future work.\nACKNOWLEDGMENTS\nThis work was funded by grants from the Edward Mallinckrodt, Jr. Foundation and the National Institutes of Health R21LM011664 and R01EB020666. Clinical data was provided by the Vanderbilt Synthetic Derivative, which is supported by institutional funding and by the Vanderbilt CTSA grant ULTR000445.\nAPPENDIX A.\nThis appendix lists the optimized parameters for the different models. Except where noted, parameters were optimized under random search.\nRecurrent Neural Network Models: (parameters marked with an asterisk were optimized with human-guided search.)\nParameter Model\nGRU LSTM\nDropout for input gates 0.1 0.25 Dropout for recurrent connections 0.75 0.75 L1 applied to the input weights matrices 0 0 L1 applied to the recurrent weights matrices 0 0 L2 applied to the input weights matrices 0.0001 0.0001 L2 applied to the recurrent weights matrices 0.0001 0.001 L2 applied to the output layer\u2019s weights matrices 0.0001 0.001 Dropout before the output layer 0.5 0.5 *Number of recurrent layers 3 3 *Number of nodes in recurrent units 400 400\nFeed Forward Neural Network Model:\nParameter Value\nDropout before the output layer 0.1 Dropout between feed-forward layers 0.1 Number of feed-forward layers 3 Activation function between feed-forward layers tanh Number of nodes in feed-forward layers 128\nRandom Forest Model (binary input):\nParameter Value\nNumber of estimators 800 Ratio of features to consider when looking for the best split 0.4666 Minimum number of samples required to split an internal node 87 Minimum number of samples required to be at a leaf node 3 The function to measure the quality of a split entropy\nAPPENDIX B.\nThis appendix lists the top level classes for International Statistical Classification of Diseases and Related Health Problems, Ninth Revision (ICD-9) and Anatomical Chemical Classification System (ATC).\nICD-9 chapters.\nCode range Description\n001-139 Infectious and parasitic diseases 140-239 Neoplasms 240-279 Endocrine, nutritional and metabolic diseases, and immunity disorders 280-289 Diseases of the blood and blood-forming organs 290-319 Mental disorders 320-359 Diseases of the nervous system 360-389 Diseases of the sense organs 390-459 Diseases of the circulatory system 460-519 Diseases of the respiratory system 520-579 Diseases of the digestive system 580-629 Diseases of the genitourinary system 630-679 Complications of pregnancy, childbirth, and the puerperium 680-709 Diseases of the skin and subcutaneous tissue 710-739 Diseases of the musculoskeletal system and connective tissue 740-759 Congenital anomalies 760-779 Certain conditions originating in the perinatal period 780-799 Symptoms, signs, and ill-defined conditions 800-999 Injury and poisoning\nV01-V91 Supplementary - factors influencing health status and contact with health services E000-E999 Supplementary - external causes of injury and poisoning\nTop level groups ATC codes and their corresponding colors used in Figure 4 and Appendix C.\nCode Contents Color\nA Alimentary tract and metabolism B Blood and blood forming organs C Cardiovascular system D Dermatologicals G Genito-urinary system and sex hormones H Systemic hormonal preparations, excluding sex hormones and insulins J Antiinfectives for systemic use L Antineoplastic and immunomodulating agents M Musculo-skeletal system N Nervous system P Antiparasitic products, insecticides and repellents R Respiratory system S Sensory organs V Various\nAPPENDIX C.\nThis appendix presents results from three illustrative cases from the dozen cases randomly selected for individual evaluation.\nCASE 1.\nICD-9 code Code description Time estimate (ago)\n203.00 Multiple myeloma, without mention of having achieved remission 4.8 months ago 273.1 Monoclonal paraproteinemia 4.8 months ago 285.9 Anemia, unspecified 4.8 months ago 276.50 Volume depletion, unspecified 4.8 months ago 733.00 Osteoporosis, unspecified 4.8 months ago 203.00 Multiple myeloma, without mention of having achieved remission 4.8 months ago 203.00 Multiple myeloma, without mention of having achieved remission 2.9 months ago 203.01 Multiple myeloma, in remission 2.9 months ago 273.1 Monoclonal paraproteinemia 2.9 months ago 273.1 Monoclonal paraproteinemia 1.6 months ago 279.3 Unspecified immunity deficiency 1.6 months ago 203.00 Multiple myeloma, without mention of having achieved remission 1.6 months ago 781.2 Abnormality of gait 3.7 weeks ago 203.00 Multiple myeloma, without mention of having achieved remission 3.7 weeks ago 401.9 Unspecified essential hypertension 3.7 weeks ago V12.54 Personal history of transient ischemic attack (TIA), and cerebral infarction without residual deficits 3.7 weeks ago 794.31 Nonspecific abnormal electrocardiogram [ECG] [EKG] 3.7 weeks ago 786.09 Other respiratory abnormalities 3.7 weeks ago 273.1 Monoclonal paraproteinemia 3.7 weeks ago 203.00 Multiple myeloma, without mention of having achieved remission 3.6 weeks ago V58.69 Long-term (current) use of other medications 3.6 weeks ago 794.31 Nonspecific abnormal electrocardiogram [ECG] [EKG] 3.4 weeks ago 203.00 Multiple myeloma, without mention of having achieved remission 4 days ago V42.82 Peripheral stem cells replaced by transplant 4 days ago 203.01 Multiple myeloma, in remission 3 days ago\n38.97 Central venous catheter placement with guidance 3 days ago V42.82 Peripheral stem cells replaced by transplant 3 days ago V58.81 Fitting and adjustment of vascular catheter 3 days ago 203.00 Multiple myeloma, without mention of having achieved remission 3 days ago V42.82 Peripheral stem cells replaced by transplant 2 days ago 203.01 Multiple myeloma, in remission 2 days ago 203.00 Multiple myeloma, without mention of having achieved remission 1 day ago V42.82 Peripheral stem cells replaced by transplant 1 day ago 203.00 Multiple myeloma, without mention of having achieved remission now V42.82 Peripheral stem cells replaced by transplant now\nMedication predictions for a complicated patient. Each vertical bar represents the prediction for a single medication class, with the height of the bar representing the confidence of the prediction. Black labels above arrows indicate ATC therapeutic classes for medications the patient was actually taking. Colors and letters below the axis indicate high-level therapeutic class groups.\nPredicted vs. actual medication classes for the patient in Case 1. The four-character sequence in the first and fourth columns is the ATC code for the medication therapeutic class, and an asterisk in the first column indicates that the predicted medication is in the actual medication list. Probabilities listed are the model predictions for the listed therapeutic class. In the predicted medications column, all predictions with probability at least 0.2 are listed.\nTop predictions Prob. True labels Prob.\nS03B* Corticosteroids 97.01% S03B Corticosteroids 97.01% S01C* Antiinflammatory agents and antiinfectives in combi-\nnation 95.54% S01C Antiinflammatory agents and antiinfectives in combination 95.54%\nS02B* Corticosteroids 95.54% S02B Corticosteroids 95.54% L01A Alkylating agents 94.00% D07X Corticosteroids, other combinations 93.37% D07X* Corticosteroids, other combinations 93.37% H02A Corticosteroids for systemic use, plain 91.06% H02A* Corticosteroids for systemic use, plain 91.06% D07A Corticosteroids, plain 90.83% D07A* Corticosteroids, plain 90.83% S01B Antiinflammatory agents 90.79% S01B* Antiinflammatory agents 90.79% D10A Anti-acne preparations for topical use 88.56% D10A* Anti-acne preparations for topical use 88.56% C05A Agents for treatment of hemorrhoids and anal fissures\nfor topical use 88.52%\nC05A* Agents for treatment of hemorrhoids and anal fissures for topical use 88.52% R01A Decongestants and other nasal preparations for topical use 87.02% A04A Antiemetics and antinauseants 87.95% J05A Direct acting antivirals 86.83% R01A* Decongestants and other nasal preparations for topi-\ncal use 87.02% A01A Stomatological preparations 86.11%\nJ05A* Direct acting antivirals 86.8% N02A Opioids 84.86% A01A* Stomatological preparations 86.11% B05C Irrigating solutions 82.56% N02A* Opioids 84.86% A12C Other mineral supplements 79.50% B05C* Irrigating solutions 82.56% B05X I.V. solution additives 74.84% A12C* Other mineral supplements 79.50% L04A Immunosuppressants 68.76% B05X* I.v. solution additives 74.84% N02B Other analgesics and antipyretics 57.24% L04A* Immunosuppressants 68.76% S01A Antiinfectives 54.59% N05A Antipsychotics 58.64% J01D Other beta-lactam antibacterials 43.40% N02B* Other analgesics and antipyretics 57.24% C03C High-ceiling diuretics 39.88% S01A* Antiinfectives 54.59% J01M Quinolone antibacterials 29.78% L03A Immunostimulants 45.96% C07A Beta blocking agents 27.08% A02B Drugs for peptic ulcer and gastro-oesophageal reflux\ndisease 44.56%\nJ01D* Other beta-lactam antibacterials 43.40% N03A Antiepileptics 20.00% C03C* High-ceiling diuretics 39.88% J01X Other antibacterials 5.88% B01A Antithrombotic agents 37.80% M03B Muscle relaxants, centrally acting agents 5.09% V03A All other therapeutic products 34.18% R06A Antihistamines for systemic use 31.78% A06A Drugs for constipation 31.57% J01M* Quinolone antibacterials 29.78% N05B Anxiolytics 29.42% D04A Antipruritics, incl. antihistamines, anesthetics, etc. 27.62% C07A* Beta blocking agents 27.08% L01X Other antineoplastic agents 24.72% R05C Expectorants, excl. combinations with cough sup-\npressants 20.43%\nN03A* Antiepileptics 20.00%\nCASE 2.\nICD-9 code Code description Time estimate (ago)\n735.4 Other hammer toe (acquired) 2.4 years ago 729.5 Pain in limb 2.4 years ago 244.1 Other postablative hypothyroidism 1.5 years ago 285.9 Anemia, unspecified 1.5 years ago 244.1 Other postablative hypothyroidism 1.2 years ago 244.1 Other postablative hypothyroidism 11.5 months ago\n733.00 Osteoporosis, unspecified 11.5 months ago 733.01 Senile osteoporosis 7.7 months ago 268.9 Unspecified vitamin D deficiency 7.7 months ago 729.5 Pain in limb 7.7 months ago 174.9 Malignant neoplasm of breast (female), unspecified 7.7 months ago 722.52 Degeneration of lumbar or lumbosacral intervertebral disc 7.7 months ago 279.3 Unspecified immunity deficiency 7.7 months ago 733.01 Senile osteoporosis 6.4 months ago 733.01 Senile osteoporosis 6.2 months ago 244.1 Other postablative hypothyroidism 6.0 months ago 401.1 Benign essential hypertension 6.0 months ago V58.69 Long-term (current) use of other medications 1.9 weeks ago 733.01 Senile osteoporosis now 244.1 Other postablative hypothyroidism now V58.69 Long-term (current) use of other medications now\nPredicted vs. actual medication classes for Case 2. Table structure as in case 1.\nTop predictions Prob. True labels Prob.\nM05B Drugs affecting bone structure and mineralization 88.18% A11C Vitamin a and d, incl. combinations of the two 39.42% H03A Thyroid preparations 84.82% N06A Antidepressants 20.88% H05A Parathyroid hormones and analogues 66.33% C10A Lipid modifying agents, plain 17.05% A11C* Vitamin a and d, incl. combinations of the two 39.42% N03A Antiepileptics 15.61% N02B Other analgesics and antipyretics 37.58% C09C Angiotensin ii antagonists, plain 10.38% A01A Stomatological preparations 23.05% L02B Hormone antagonists and related agents 4.22% A12A Calcium 21.59% N06A* Antidepressants 20.88% C07A Beta blocking agents 20.81%\nMedication predictions for a simpler patient. Note that the high-prediction medications are clinically reasonable given the billing codes in the sequence. Figure representation as in case 1.\nCASE 3.\nICD-9 code Code description Time estimate (ago)\n332.0 Paralysis agitans 5.0 years ago 332.0 Paralysis agitans 4.7 years ago 332.0 Paralysis agitans 4.5 years ago 332.0 Paralysis agitans 4.0 years ago 332.0 Paralysis agitans 3.5 years ago 332.0 Paralysis agitans 3.0 years ago 332.0 Paralysis agitans 2.7 years ago 332.0 Paralysis agitans 2.4 years ago 332.0 Paralysis agitans 2.0 years ago 332.0 Paralysis agitans 1.7 years ago 332.0 Paralysis agitans 1.0 years ago 332.0 Paralysis agitans 9.9 months ago 332.0 Paralysis agitans 4.1 months ago 332.0 Paralysis agitans now\nPredicted vs. actual medication classes for Case 3. Table structure as in case 1.\nTop predictions Prob. True labels Prob.\nN04B Dopaminergic agents 97.66% C10A Lipid modifying agents, plain 13.90% N03A Antiepileptics 34.01% C09A Ace inhibitors, plain 9.21% N02B Other analgesics and antipyretics 32.81% C01E Other cardiac preparations 5.56% N06A Antidepressants 26.10% C02C Antiadrenergic agents, peripherally acting 0.72% N02A Opioids 20.33% G03B Androgens 0.32%\nA14A Anabolic steroids 0.08%\nMedication predictions for a patient with only one ICD-9 code, repeated many times over five years. The medications listed under true labels are not indicated for paralysis agitans (Parkinson\u2019s disease), but the patient was surely taking them for reasons not documented in the ICD-9 sequence. The model predicted mostly reasonable medications for a patient with Parkinson\u2019s disease, especially Dopaminergic agents, which is the primary treatment for the disease. Figure representation as in case 1, above.\n", "cats": {"READ": 1.0, "DONTREAD": 0.0}, "reviews": [{"IS_META_REVIEW": true, "comments": "This is a well-conducted and well-written study on the prediction of medication from diagnostic codes. The authors compared GRUs, LSTMs, feed-forward networks and random forests (making a case for why random forests should be used, instead of SVMs) and analysed the predictions and embeddings.\n\nThe authors also did address the questions of the reviewers.\n\nMy only negative point is that this work might be more relevant for a data science or medical venue rather than at ICLR."}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "This paper applies RNNs to predict medications from billing costs. While this paper does not have technical novelty, it is well done and well organized. It demonstrates a creative use of recent models in a very important domain, and I think many people in our community are interested and inspired by well-done applications that branch to socially important domains. Moreover, I think an advantage to accepting it at ICLR is that it gives our \"expert\" stamp of approval -- I see a lot of questionable / badly applied / antiquated machine learning methods in domain conferences, so I think it would be helpful for those domains to have examples of application papers that are considered sound.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"TITLE": "Thorough empirical investigation of an interesting and (to my knowledge) novel application area", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "This is a well written, organized, and presented paper that I enjoyed reading.  I commend the authors on their attention to the narrative and the explanations.  While it did not present any new methodology or architecture, it instead addressed an important application of predicting the medications a patient is using, given the record of billing codes.  The dataset they use is impressive and useful and, frankly, more interesting than the typical toy datasets in machine learning.  That said, the investigation of those results was not as deep as I thought it should have been in an empirical/applications paper.  Despite their focus on the application, I was encouraged to see the authors use cutting edge choices (eg Keras, adadelta, etc) in their architecture.  A few points of criticism:\n\n-The numerical results are in my view too brief.  Fig 4 is anecdotal, Fig 5 is essentially a negative result (tSNE is only in some places interpretable), so that leaves Table 1.  I recognize there is only one dataset, but this does not offer a vast amount of empirical evidence and analysis that one might expect out of a paper with no major algorithmic/theoretical advances.  To be clear I don't think this is disqualifying or deeply concerning; I simply found it a bit underwhelming.\n\n- To be constructive, re the results I would recommend removing Fig 5 and replacing that with some more meaningful analysis of performance.  I found Fig 5 to be mostly uninformative, other than as a negative result, which I think can be stated in a sentence rather than in a large figure.\n\n- There is a bit of jargon used and expertise required that may not be familiar to the typical ICLR reader.  I saw that another reviewer suggested perhaps ICLR is not the right venue for this work.  While I certainly see the reviewer's point that a medical or healthcare venue may be more suitable, I do want to cast my vote of keeping this paper here... our community benefits from more thoughtful and in depth applications. Instead I think this can be addressed by tightening up those points of jargon and making the results more easy to evaluate by an ICLR reader (that is, as it stands now researchers without medical experience have to take your results after Table 1 on faith, rather than getting to apply their well-trained quantitative eye). \n\nOverall, a nice paper.", "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "17 Dec 2016", "REVIEWER_CONFIDENCE": 5}, {"TITLE": "Strong application work, very important problem", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "In light of the detailed author responses and further updates to the manuscript, I am raising my score to an 8 and reiterating my support for this paper. I think it will be among the strongest non-traditional applied deep learning work at ICLR and will receive a great deal of interest and attention from attendees.\n\n-----\n\nThis paper describes modern deep learning approach to the problem of predicting the medications taken by a patient during a period of time based solely upon the sequence of ICD-9 codes assigned to the patient during that same time period. This problem is formulated as a multilabel sequence classification (in contrast to language modeling, which is multiclass classification). They propose to use standard LSTM and GRU architectures with embedding layers to handle the sparse categorical inputs, similar to that described in related work by Choi, et al. In experiments using a cohort of ~610K patient records, they find that RNN models outperform strong baselines including an MLP and a random forest, as well as a common sense baseline. The differences in performance between the recurrent models and the MLP appear to be large enough to be significant, given the size of the test set.\n\nStrengths:\n- Very important problem. As the authors point out, two the value propositions of EHRs -- which have been widely adopted throughout the US due to a combination of legislation and billions of dollars in incentives from the federal government -- included more accurate records and fewer medication mistakes. These two benefits have largely failed to materialize. This seems like a major opportunity for data mining and machine learning.\n- Paper is well-written with lucid introduction and motivation, thorough discussion of related work, clear description of experiments and metrics, and interesting qualitative analysis of results.\n- Empirical results are solid with a strong win for RNNs over convincing baselines. This is in contrast to some recent related papers, including Lipton & Kale et al, ICLR 2016, where the gap between the RNN and MLP was relatively small, and Choi et al, MLHC 2016, which omitted many obvious baselines.\n- Discussion is thorough and thoughtful. The authors are right about the kidney code embedding results: this is a very promising result.\n\nWeaknesses:\n- The authors make several unintuitive decisions related to data preprocessing and experimental design, foremost among them the choice NOT to use full patient sequences but instead only truncated patient sequences that each ends at randomly chosen time point. This does not necessarily invalidate their results, but it is somewhat unnatural and the explanation is difficult to follow, reducing the paper's potential impact. It is also reduces the RNN's potential advantage.\n- The chosen metrics seem appropriate, but non-experts may have trouble interpreting the absolute and relative performances (beyond the superficial, e.g., RNN score 0.01 more than NN!). The authors should invest some space in explaining (1) what level of performance -- for each metric -- would be necessary for the model to be useful in a real clinical setting and (2) whether the gaps between the various models are \"significant\" (even in an informal sense).\n- The paper proposes nothing novel in terms of methods, which is a serious weakness for a methods conference like ICLR. I think it is strong enough empirically (and sufficiently interesting in application) to warrant acceptance regardless, but there may be things the authors can do to make it more competitive. For example, one potential hypothesis is that higher capacity models are more prone to overfitting noisy targets. Is there some way to investigate this, perhaps by looking at the kinds of errors each model makes?\n\nI have a final comment: as a piece of clinical work, the paper has a huge weakness: the lack of ground truth labels for missing medications. Models are both trained and tested on data with noisy labels. For training, the authors are right that this shouldn't be a huge problem, provided the label noise is random (even class conditional isn't too big of a problem). For testing, though, this seems like it could skew metrics. Further, the assumption that the label noise is not systemic seems very unlikely given that these data are recorded by human clinicians. The cases shown in Appendix C lend some credence to this assertion: for Case 1, 7/26 actual medications received probabilities < 0.5. My hunch is that clinical reviewers would view the paper with great skepticism. The authors will need to get creative about evaluation -- or invest a lot of time/money in labeling data -- to really prove that this works.\n\nFor what it is worth, I hope that this paper is accepted as I think it will be of great interest to the ICLR community. However, I am borderline about whether I'd be willing to fight for its acceptance. If the authors can address the reviewers' critiques -- and in particular, dive into the question of overfitting the imperfect labels and provide some insights -- I might be willing to raise my score and lobby for acceptance.", "IS_META_REVIEW": false, "RECOMMENDATION": 8, "DATE": "17 Dec 2016 (modified: 21 Jan 2017)", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "Good medical application paper for a medical or data science venue", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "This is a well-conducted and well-written study on the prediction of medication from diagnostic codes. The authors compared GRUs, LSTMs, feed-forward networks and random forests (making a case for why random forests should be used, instead of SVMs) and analysed the predictions and embeddings.\n\nThe authors also did address the questions of the reviewers.\n\nMy only negative point is that this work might be more relevant for a data science or medical venue rather than at ICLR.", "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "16 Dec 2016 (modified: 28 Jan 2017)", "REVIEWER_CONFIDENCE": 3}, {"DATE": "03 Dec 2016", "TITLE": "Task setup details a little unclear", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1"}, {"DATE": "03 Dec 2016", "TITLE": "Baselines", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3"}, {"DATE": "02 Dec 2016", "TITLE": "distinction vs Choi et al 2016b", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2"}, {"IS_META_REVIEW": true, "comments": "This is a well-conducted and well-written study on the prediction of medication from diagnostic codes. The authors compared GRUs, LSTMs, feed-forward networks and random forests (making a case for why random forests should be used, instead of SVMs) and analysed the predictions and embeddings.\n\nThe authors also did address the questions of the reviewers.\n\nMy only negative point is that this work might be more relevant for a data science or medical venue rather than at ICLR."}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "This paper applies RNNs to predict medications from billing costs. While this paper does not have technical novelty, it is well done and well organized. It demonstrates a creative use of recent models in a very important domain, and I think many people in our community are interested and inspired by well-done applications that branch to socially important domains. Moreover, I think an advantage to accepting it at ICLR is that it gives our \"expert\" stamp of approval -- I see a lot of questionable / badly applied / antiquated machine learning methods in domain conferences, so I think it would be helpful for those domains to have examples of application papers that are considered sound.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"TITLE": "Thorough empirical investigation of an interesting and (to my knowledge) novel application area", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "This is a well written, organized, and presented paper that I enjoyed reading.  I commend the authors on their attention to the narrative and the explanations.  While it did not present any new methodology or architecture, it instead addressed an important application of predicting the medications a patient is using, given the record of billing codes.  The dataset they use is impressive and useful and, frankly, more interesting than the typical toy datasets in machine learning.  That said, the investigation of those results was not as deep as I thought it should have been in an empirical/applications paper.  Despite their focus on the application, I was encouraged to see the authors use cutting edge choices (eg Keras, adadelta, etc) in their architecture.  A few points of criticism:\n\n-The numerical results are in my view too brief.  Fig 4 is anecdotal, Fig 5 is essentially a negative result (tSNE is only in some places interpretable), so that leaves Table 1.  I recognize there is only one dataset, but this does not offer a vast amount of empirical evidence and analysis that one might expect out of a paper with no major algorithmic/theoretical advances.  To be clear I don't think this is disqualifying or deeply concerning; I simply found it a bit underwhelming.\n\n- To be constructive, re the results I would recommend removing Fig 5 and replacing that with some more meaningful analysis of performance.  I found Fig 5 to be mostly uninformative, other than as a negative result, which I think can be stated in a sentence rather than in a large figure.\n\n- There is a bit of jargon used and expertise required that may not be familiar to the typical ICLR reader.  I saw that another reviewer suggested perhaps ICLR is not the right venue for this work.  While I certainly see the reviewer's point that a medical or healthcare venue may be more suitable, I do want to cast my vote of keeping this paper here... our community benefits from more thoughtful and in depth applications. Instead I think this can be addressed by tightening up those points of jargon and making the results more easy to evaluate by an ICLR reader (that is, as it stands now researchers without medical experience have to take your results after Table 1 on faith, rather than getting to apply their well-trained quantitative eye). \n\nOverall, a nice paper.", "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "17 Dec 2016", "REVIEWER_CONFIDENCE": 5}, {"TITLE": "Strong application work, very important problem", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "In light of the detailed author responses and further updates to the manuscript, I am raising my score to an 8 and reiterating my support for this paper. I think it will be among the strongest non-traditional applied deep learning work at ICLR and will receive a great deal of interest and attention from attendees.\n\n-----\n\nThis paper describes modern deep learning approach to the problem of predicting the medications taken by a patient during a period of time based solely upon the sequence of ICD-9 codes assigned to the patient during that same time period. This problem is formulated as a multilabel sequence classification (in contrast to language modeling, which is multiclass classification). They propose to use standard LSTM and GRU architectures with embedding layers to handle the sparse categorical inputs, similar to that described in related work by Choi, et al. In experiments using a cohort of ~610K patient records, they find that RNN models outperform strong baselines including an MLP and a random forest, as well as a common sense baseline. The differences in performance between the recurrent models and the MLP appear to be large enough to be significant, given the size of the test set.\n\nStrengths:\n- Very important problem. As the authors point out, two the value propositions of EHRs -- which have been widely adopted throughout the US due to a combination of legislation and billions of dollars in incentives from the federal government -- included more accurate records and fewer medication mistakes. These two benefits have largely failed to materialize. This seems like a major opportunity for data mining and machine learning.\n- Paper is well-written with lucid introduction and motivation, thorough discussion of related work, clear description of experiments and metrics, and interesting qualitative analysis of results.\n- Empirical results are solid with a strong win for RNNs over convincing baselines. This is in contrast to some recent related papers, including Lipton & Kale et al, ICLR 2016, where the gap between the RNN and MLP was relatively small, and Choi et al, MLHC 2016, which omitted many obvious baselines.\n- Discussion is thorough and thoughtful. The authors are right about the kidney code embedding results: this is a very promising result.\n\nWeaknesses:\n- The authors make several unintuitive decisions related to data preprocessing and experimental design, foremost among them the choice NOT to use full patient sequences but instead only truncated patient sequences that each ends at randomly chosen time point. This does not necessarily invalidate their results, but it is somewhat unnatural and the explanation is difficult to follow, reducing the paper's potential impact. It is also reduces the RNN's potential advantage.\n- The chosen metrics seem appropriate, but non-experts may have trouble interpreting the absolute and relative performances (beyond the superficial, e.g., RNN score 0.01 more than NN!). The authors should invest some space in explaining (1) what level of performance -- for each metric -- would be necessary for the model to be useful in a real clinical setting and (2) whether the gaps between the various models are \"significant\" (even in an informal sense).\n- The paper proposes nothing novel in terms of methods, which is a serious weakness for a methods conference like ICLR. I think it is strong enough empirically (and sufficiently interesting in application) to warrant acceptance regardless, but there may be things the authors can do to make it more competitive. For example, one potential hypothesis is that higher capacity models are more prone to overfitting noisy targets. Is there some way to investigate this, perhaps by looking at the kinds of errors each model makes?\n\nI have a final comment: as a piece of clinical work, the paper has a huge weakness: the lack of ground truth labels for missing medications. Models are both trained and tested on data with noisy labels. For training, the authors are right that this shouldn't be a huge problem, provided the label noise is random (even class conditional isn't too big of a problem). For testing, though, this seems like it could skew metrics. Further, the assumption that the label noise is not systemic seems very unlikely given that these data are recorded by human clinicians. The cases shown in Appendix C lend some credence to this assertion: for Case 1, 7/26 actual medications received probabilities < 0.5. My hunch is that clinical reviewers would view the paper with great skepticism. The authors will need to get creative about evaluation -- or invest a lot of time/money in labeling data -- to really prove that this works.\n\nFor what it is worth, I hope that this paper is accepted as I think it will be of great interest to the ICLR community. However, I am borderline about whether I'd be willing to fight for its acceptance. If the authors can address the reviewers' critiques -- and in particular, dive into the question of overfitting the imperfect labels and provide some insights -- I might be willing to raise my score and lobby for acceptance.", "IS_META_REVIEW": false, "RECOMMENDATION": 8, "DATE": "17 Dec 2016 (modified: 21 Jan 2017)", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "Good medical application paper for a medical or data science venue", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "This is a well-conducted and well-written study on the prediction of medication from diagnostic codes. The authors compared GRUs, LSTMs, feed-forward networks and random forests (making a case for why random forests should be used, instead of SVMs) and analysed the predictions and embeddings.\n\nThe authors also did address the questions of the reviewers.\n\nMy only negative point is that this work might be more relevant for a data science or medical venue rather than at ICLR.", "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "16 Dec 2016 (modified: 28 Jan 2017)", "REVIEWER_CONFIDENCE": 3}, {"DATE": "03 Dec 2016", "TITLE": "Task setup details a little unclear", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1"}, {"DATE": "03 Dec 2016", "TITLE": "Baselines", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3"}, {"DATE": "02 Dec 2016", "TITLE": "distinction vs Choi et al 2016b", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2"}]}
{"text": "1 INTRODUCTION\nOnline commerce has been a great impact on our life over the past decade. We focus on an online market for fashion related items1. Finding similar fashion-product images for a given image query is a classical problem in an application to computer vision, however, still challenging due to the absence of an absolute definition of the similarity between arbitrary fashion items.\nDeep learning technology has given great success in computer vision tasks such as efficient feature representation (Razavian et al., 2014; Babenko et al., 2014), classification (He et al., 2016a; Szegedy et al., 2016b), detection (Ren et al., 2015; Zhang et al., 2016), and segmentation (Long et al., 2015). Furthermore, image to caption generation (Vinyals et al., 2015; Xu et al., 2015) and visual question answering (VQA) (Antol et al., 2015) are emerging research fields combining vision, language (Mikolov et al., 2010), sequence to sequence (Sutskever et al., 2014), long-term memory (Xiong et al., 2016) based modelling technologies.\nThese computer vision researches mainly concern about general object recognition. However, in our fashion-product search domain, we need to build a very specialised model which can mimic human's perception of fashion-product similarity. To this end, we start by brainstorming about what makes two fashion items are similar or dissimilar. Fashion-specialist and merchandisers are also involved. We then compose fashion-attribute dataset for our fashion-product images. Table 1 explains a part of our fashion-attributes. Conventionally, each of the columns in Table 1 can be modelled as a multi-class classification. Therefore, our fashion-attributes naturally is modelled as a multi-label classification.\n\u2217This work was done by the author at SK Planet. 1In our e-commerce platform, 11st (http://english.11st.co.kr/html/en/main.html), al-\nmost a half of user-queries are related to the fashion styles, and clothes.\nMulti-label classification has a long history in the machine learning field. To address this problem, a straightforward idea is to split such multi-labels into a set of multi-class classification problems. In our fashion-attributes, there are more than 90 attributes. Consequently, we need to build more than 90 classifiers for each attribute. It is worth noting that, for example, collar attribute can represent the upper-garments, but it is absent to represent bottom-garments such as skirts or pants, which means some attributes are conditioned on other attributes. This is the reason that the learning tree structure of the attributes dependency can be more efficient (Zhang & Zhang, 2010; Fu et al., 2012; Gibaja & Ventura, 2015).\nRecently, recurrent neural networks (RNN) are very commonly used in automatic speech recognition (ASR) (Graves et al., 2013; Graves & Jaitly, 2014), language modelling (Mikolov et al., 2010), word dependency parsing (Mirowski & Vlachos, 2015), machine translation (Cho et al., 2014), and dialog modelling (Henderson et al., 2014; Serban et al., 2016). To preserve long-term dependency in hidden context, Long-Short Term Memory (LSTM) (Hochreiter & Schmidhuber, 1997) and its variants (Zaremba et al., 2014; Cooijmans et al., 2016) are breakthroughs in such fields. We use this LSTM to learn fashion-attribute dependency structure implicitly. By using the LSTM, our attribute recognition problem is regarded to as a sequence classification. There is a similar work in Wang et al. (2016), however, we do not use the VGG16 network (Simonyan & Zisserman, 2014) as an image encoder but use our own encoder. To the best of our knowledge, it is the first work applying LSTM into a multi-label classification task in the commercial fashion-product search domain.\nThe remaining of this paper is organized as follows. In Sec. 2, We describe details about our fashion-attribute dataset. Sec. 3 describes the proposed fashion-product search system in detail. Sec. 4 explains empirical results given image queries. Finally, we draw our conclusion in Sec. 5.\n2 BUILDING THE FASHION-ATTRIBUTE DATASET\nWe start by building large-scale fashion-attribute dataset in the last year. We employ maximum 100 man-months and take almost one year for completion. There are 19 fashion-categories and more than 90 attributes for representing a specific fashion-style. For example, top garments have the Tshirts, blouse, bag etc. The T-shirts category has the collar, sleeve-length, gender, etc. The gender attribute has binary classes (i.e. female and male). Sleeve-length attribute has multiple classes (i.e. long, a half, sleeveless etc.). Theoretically, the combination of our attributes can represent thousands of unique fashion-styles. A part of our attributes are in Table 1. ROIs for each fashion item in an image are also included in this dataset. Finally, we collect 1 million images in total. This internal dataset is to be used for training our fashion-attribute recognition model and fashion-product ROI detector respectively.\n3 FASHION-PRODUCT SEARCH SYSTEM\nIn this section, we describe the details of our system. The whole pipeline is illustrated in Fig. 3. As a conventional information retrieval system, our system has offline and online phase. In offline process, we take both an image and its textual meta-information as the inputs. The reason we take additional textual meta-information is that, for example, in Fig. 1a dominant fashion item in the image is a white dress however, our merchandiser enrolled it to sell the brown cardigan as described\nin its meta-information. In Fig. 1b, there is no way of finding which fashion item is to be sold without referring the textual meta-information seller typed manually. Therefore, knowing intension (i.e. what to sell) for our merchandisers is very important in practice. To catch up with these intension, we extract fashion-category information from the textual meta. The extracted fashion-category information is fed to the fashion-attribute recognition model. The fashion-attribute recognition model predicts a set of fashion-attributes for the given image. (see Fig. 2) These fashion-attributes are used as keys in the inverted indexing scheme. On the next stage, our fashion-product ROI detector finds where the fashion-category item is in the image. (see Fig. 8) We extract colour and appearance features for the detected ROI. These visual features are stored in a postings list. In these processes, it is worth noting that, as shown in Fig. 8, our system can generate different results in the fashion-attribute recognition and the ROI detection for the same image by guiding the fashioncategory information. In online process, there is two options for processing a user-query. We can\ntake a guided information, what the user wants to find, or the fashion-attribute recognition model automatically finds what fashion-category item is the most likely to be queried. This is up to the user's choice. For the given image by the user, the fashion-attribute recognition model generates fashion-attributes, and the results are fed into the fashion-product ROI detector. We extract colour and appearance features in the ROI resulting from the detector. We access to the inverted index addressed by the generated a set of fashion-attributes, and then get a postings list for each fashionattribute. We perform nearest-neighbor retrieval in the postings lists so that the search complexity is reduced drastically while preserving the semantic similarity. To reduce memory capacity and speed up this nearest-neighbor retrieval process once more, our features are binarized and CPU depen-\ndent intrinsic instruction (i.e. assembly popcnt instruction2) is used for computing the hamming distance.\n3.1 VISION ENCODER NETWORK\nWe build our own vision encoder network (ResCeption) which is based on inception-v3 architecture (Szegedy et al., 2016b). To improve both speed of convergence and generalization, we introduce a shortcut path (He et al., 2016a;b) for each data-flow stream (except streams containing one convolutional layer at most) in all inception-v3 modules. Denote input of l-th layer , xl \u2208 R , output of the l-th layer, xl+1, a l-th layer is a function, H : xl 7\u2192 xl+1 and a loss function, L(\u03b8;xL). Then forward and back(ward)propagation is derived such that\nxl+1 = H(xl) + xl (1) \u2202xl+1\n\u2202xl = \u2202H(xl) \u2202xl + 1 (2)\nImposing gradients from the loss function to l-th layer to Eq. (2),\n\u2202L \u2202xl := \u2202L \u2202xL . . . \u2202xl+2 \u2202xl+1 \u2202xl+1 \u2202xl\n= \u2202L \u2202xL\n( 1+ \u00b7 \u00b7 \u00b7+ \u2202H(x L\u22122)\n\u2202xl + \u2202H(xL\u22121) \u2202xl ) =\n\u2202L \u2202xL\n( 1+ l\u2211 i=L\u22121 \u2202H(xi) \u2202xl ) . (3)\nAs in the Eq. (3), the error signal, \u2202L \u2202xL\n, goes down to the l-th layer directly through the shortcut path, and then the gradient signals from (L \u2212 1)-th layer to l-th layer are added consecutively (i.e.\u2211l i=L\u22121 \u2202H(xi) \u2202xl\n). Consequently, all of terms in Eq. (3) are aggregated by the additive operation instead of the multiplicative operation except initial error from the loss (i.e. \u2202L\n\u2202xL ). It prevents\nfrom vanishing or exploding gradient problem. Fig. 4 depicts network architecture for shortcut\n2http://www.gregbugaj.com/?tag=assembly (accessed at Aug. 2016)\npaths in an inception-v3 module. We use projection shortcuts throughout the original inception-v3 modules due to the dimension constraint.3 To demonstrate the effectiveness of the shortcut paths in the inception modules, we reproduce ILSVRC2012 classification benchmark (Russakovsky et al., 2015) for inception-v3 and our ResCeption network. As in Fig. 5a, we verify that residual shortcut paths are beneficial for fast training and slight better generalization.4 The whole of the training curve is shown in Fig. 5b. The best validation error is reached at 23.37% and 6.17% at top-1 and top-5, respectively. That is a competitive result.5 To demonstrate the representation power of our ResCeption, we employ the transfer learning strategy for applying the pre-trained ResCeption as an image encoder to generate captions. In this experiment, we verify our ResCeption encoder outperforms the existing VGG16 network6 on MS-COCO challenge benchmark (Chen et al., 2015). The best validation CIDEr-D score (Vedantam et al., 2015) for c5 is 0.923 (see Fig. 5c) and test CIDEr-D score for c40 is 0.937.7\n3.2 MULTI-LABEL LEARNING AS SEQUENCE PREDICTION BY USING THE RNN\nThe traditional multi-class classification associates an instance x with a single label a from previously defined a finite set of labels A. The multi-label classification task associates several finite sets of labels An \u2282 A. The most well known method in the multi-label literature are the binary relevance method (BM) and the label combination method (CM). There are drawbacks in both BM\n3If the input and output dimension of the main-branch is not the same, projection shortcut should be used instead of identity shortcut.\n4This is almost the same finding from Szegedy et al. (2016a) but our work was done independently. 5http://image-net.org/challenges/LSVRC/2015/results 6https://github.com/torch/torch7/wiki/ModelZoo 7We submitted our final result with beam search on MS-COCO evaluation server and found out the beam\nsearch improves final CIDEr-D for c40 score by 0.02.\nand CM. The BM ignores label correlations that exist in the training data. The CM directly takes into account label correlations, however, a disadvantage is its worst-case time complexity (Read et al., 2009). To tackle these drawbacks, we introduce to use the RNN. Suppose we have random variables a \u2208 An, An \u2282 A. The objective of the RNN is to maximise the joint probability, p(at, at\u22121, at\u22122, . . . a0), where t is a sequence (time) index. This joint probability is factorized as a product of conditional probabilities recursively,\np(at, at\u22121, . . . a0) = p(a0)p(a1|a0)\ufe38 \ufe37\ufe37 \ufe38 p(a0,a1) p(a2|a1, a0)\n\ufe38 \ufe37\ufe37 \ufe38 p(a0,a1,a2)\n\u00b7 \u00b7 \u00b7\n\ufe38 \ufe37\ufe37 \ufe38 p(a0,a1,a2,... )\n(4)\n= p(a0) \u220fT t=1 p(at|at\u22121, . . . , a0).\nFollowing the Eq. 4, we can handle multi-label classification as sequence classification which is illustrated in Fig. 6. There are many label dependencies among our fashion-attributes. Direct modelling of such label dependencies in the training data using the RNN is our key idea. We use the ResCeption as a vision encoder \u03b8I , LSTM and softmax regression as our sequence classifier \u03b8seq, and negative log-likelihood (NLL) as the loss function. We backpropagage gradient signal from the sequence classifier to vision encoder.8 Empirical results of our ResCeption-LSTM based attribute recognition are in Fig. 2. Many fashion-category dependent attributes such as sweetpants, fading, zipper-lock, mini, and tailored-collar are recognized quite well. Fashion-category independent attributes (e.g., male, female) are also recognizable. It is worth noting we do not model the fashionattribute dependance tree at all. We demonstrate the RNN learns attribute dependency structure implicitly. We evaluate our attribute recognition model on the fashion-attribute dataset. We split this dataset into 721544, 40000, and 40000 images for training, validating, and testing. We employ the early-stopping strategy to preventing over-fitting using the validation set. We measure precision and recall for a set of ground-truth attributes and a set of predicted attributes for each image. The quantitative results are in Table 2.\n8Our attribute recognition model is parameterized as \u03b8 = [\u03b8I ; \u03b8seq]. In our case, updating \u03b8I as well as \u03b8seq in the gradient descent step helps for much better performance.\n3.3 Guided ATTRIBUTE-SEQUENCE GENERATION\nOur prediction model of the fashion-attribute recognition is based on the sequence generation process in the RNN (Graves, 2013). The attribute-sequence generation process is illustrated in Fig. 7. First, we predict a probability of the first attribute for a given internal representation of the image i.e. p\u03b8seq(a0|g\u03b8I (I)), and then sample from the estimated probability of the attribute, a0 \u223c p\u03b8seq(a0|g\u03b8I (I)). The sampled symbol is fed to as the next input to compute p\u03b8seq(a1|a0, g\u03b8I (I)). This sequential process is repeated recursively until a sampled result is reached at the special endof-sequence (EOS) symbol. In case that we generate a set of attributes for a guided fashion-category, we do not sample from the previously estimated probability, but select the guided fashion-category, and then we feed into it as the next input deterministically. It is the key to considering for each seller's intention. Results for the guided attribute-sequence generation is shown in Fig. 8.\n3.4 Guided ROI DETECTION\nOur fashion-product ROI detection is based on the Faster R-CNN (Ren et al., 2015). In the conventional multi-class Faster R-CNN detection pipeline, one takes an image and outputs a tuple of (ROI coordinate, object-class, class-score). In our ROI detection pipeline, we take additional information, guided fashion-category from the ResCeption-LSTM based attribute-sequence generator. Our fashion-product ROI detector finds where the guided fashion-category item is in a given image. Jing et al. (2015) also uses a similar idea, but they train several detectors for each category independently so that their works do not scale well. We train a detector for all fashion-categories jointly. Our detector produces ROIs for all of the fashion-categories at once. In post-processing, we reject ROIs that their object-classes are not matched to the guided fashion-category. We demonstrate that the guided fashion-category information contributes to higher performance in terms of mean average precision (mAP) on the fashion-attribute dataset. We measure the mAP for the intersection-of-union (IoU) between ground-truth ROIs and predicted ROIs. (see Table 3) That is due to the fact that our guided fashion-category information reduces the false positive rate. In our fashion-product search pipeline, the colour and appearance features are extracted in the detected ROIs.\n3.5 VISUAL FEATURE EXTRACTION\nTo extract appearance feature for a given ROI, we use pre-trained GoogleNet (Szegedy et al., 2015). In this network, both inception4 and inception5 layer's activation maps are used. We evaluate this feature on two similar image retrieval benchmarks, i.e. Holidays (Jegou et al., 2008) and UK-benchmark (UKB) (Niste\u0301r & Stewe\u0301nius, 2006). In this experiment, we do not use any postprocessing method or fine-tuning at all. The mAP on Holidays is 0.783, and the precision@4 and recall@4 on UKB is 0.907 and 0.908 respectively. These scores are competitive against several deep feature representation methods (Razavian et al., 2014; Babenko et al., 2014). Examples of queries and resulting nearest-neighbors are in Fig. 9. On the next step, we binarize this appearance feature by simply thresholding at 0. The reason we take this simple thresholding to generate the hash code is twofold. The neural activation feature map at a higher layer is a sparse and distributed code in nature. Furthermore, the bias term in a linear layer (e.g., convolutional layer) compensates for\naligning zero-centering of the output feature space weakly. Therefore, we believe that a code from a well-trained neural model, itself, can be a good feature even to be binarized. In our experiment, such simple thresholding degrades mAP by 0.02 on the Holidays dataset, but this method makes it possible to scaling up in the retrieval. In addition to the appearance feature, we extract colour feature using the simple (bins) colour histogram in HSV space, and distance between a query and a reference image is computed by using the weighted combination of the two distances from the colour and the appearance feature.\n4 EMPIRICAL RESULTS\nTo evaluate empirical results of the proposed fashion-product search system, we select 3 million fashion-product images in our e-commerce platform at random. These images are mutually exclusive to the fashion-attribute dataset. We have again selected images from the web used for the queries. All of the reference images pass through the offline process as described in Sec. 3, and resulting inverted indexing database is loaded into main-memory (RAM) by our daemon system. We send the pre-selected queries to the daemon system with the RESTful API. The daemon system then performs the online process and returns nearest-neighbor images correspond to the queries. In this scenario, there are three options to get similar fashion-product images. Option 1 is that the fashion-attribute recognition model automatically selects fashion-category, the most likely to be queried in the given image. Option 2 is that a user manually selects a fashion-category given a query image. (see Fig. 10) Option 3 is that a user draw a rectangle to be queried by hand like Jing et al. (2015). (see Fig. 11) By the recognized fashion-attributes, the retrieved results reflect the user's main needs, e.g. gender, season, utility as well as the fashion-style, that could be lacking when using visual feature representation only.\n5 CONCLUSIONS\nToday's deep learning technology has given great impact on various research fields. Such a success story is about to be applied to many industries. Following this trend, we traced the start-of-the art computer vision and language modelling research and then, used these technologies to create value for our customers especially in the e-commerce platform. We expect active discussion on that how to apply many existing research works into the e-commerce industry.\n", "cats": {"READ": 0.0, "DONTREAD": 1.0}, "reviews": [{"IS_META_REVIEW": true, "comments": "The manuscript is a bit scattered and hard to follow. There is technical depth but the paper doesn't do a good job explaining what shortcoming the proposed methods are overcoming and what baselines they are outperforming. \n\nThe writing could be improved. There are numerous grammatical errors.\n\nThe experiments in 3.1 are interesting, but you need to be clearer about the relationship of your ResCeption method to the state-of-the-art. The use of extensive footnotes on page 5 is a bit odd. \"That is a competitive result\" is vague. A footnote links to \""}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "Three knowledgable reviewers recommend rejection. The main concern is missing related work on fashion product search, and thus also baselines. The authors did not post a rebuttal to address the concerns. The AC agrees with the reviewers' recommendation.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"TITLE": "interesting exploration but several major concerns", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "The paper presents a large-scale visual search system for finding product images given a fashion item. The exploration is interesting and the paper does a nice job of discussing the challenges of operating in this domain. The proposed approach addresses several of the challenges. \n\nHowever, there are several concerns.\n\n1) The main concern is that there are no comparisons or even mentions of the work done by Tamara Berg\u2019s group on fashion recognition and fashion attributes, e.g., \n-  \u201cAutomatic Attribute Discovery and Characterization from Noisy Web Data\u201d ECCV 2010 \n- \u201cWhere to Buy It: Matching Street Clothing Photos in Online Shops\u201d ICCV 2015,\n- \u201cRetrieving Similar Styles to Parse Clothing, TPAMI 2014,\netc\nIt is difficult to show the contribution and novelty of this work without discussing and comparing with this extensive prior art.\n\n2) There are not enough details about the attribute dataset and the collection process. What is the source of the images? Are these clean product images or real-world images? How is the annotation done? What instructions are the annotators given? What annotations are being collected? I understand data statistics for example may be proprietary, but these kinds of qualitative details are important to understand the contributions of the paper. How can others compare to this work?\n\n3) There are some missing baselines. How do the results in Table 2 compare to simpler methods, e.g., the BM or CM methods described in the text?\n\nWhile the paper presents an interesting exploration, all these concerns would need to be addressed before the paper can be ready for publication.", "IS_META_REVIEW": false, "RECOMMENDATION": 4, "DATE": "20 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "Good practical visual search system but lack novelty", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "This paper introduces a pratical large-scale visual search system for a fashion site. It uses RNN to recognize multi-label attributes and uses state-of-art faster RCNN to extract features inside those region-of-interest (ROI). The technical contribution of this paper is not clear. Most of the approaches used are standard state-of-art methods and there are not a lot of novelties in applying those methods. For multi-label recognition task, there are other available methods, e.g. using binary models, changing cross-entropy loss function, etc. There aren't any comparison between the RNN method and other simple baselines. The order of the sequential RNN prediction is not clear either. It seems that the attributes form a tree hierarchy and that is used as the order of sequence.\n\nThe paper is not well written either. Most results are reported in the internal dataset and the authors won't release the dataset. \n", "IS_META_REVIEW": false, "RECOMMENDATION": 3, "DATE": "19 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "Contribution not clear enough; concerns about data set itself", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "The manuscript is a bit scattered and hard to follow. There is technical depth but the paper doesn't do a good job explaining what shortcoming the proposed methods are overcoming and what baselines they are outperforming. \n\nThe writing could be improved. There are numerous grammatical errors.\n\nThe experiments in 3.1 are interesting, but you need to be clearer about the relationship of your ResCeption method to the state-of-the-art. The use of extensive footnotes on page 5 is a bit odd. \"That is a competitive result\" is vague. A footnote links to \"", "IS_META_REVIEW": false, "RECOMMENDATION": 3, "DATE": "17 Dec 2016", "REVIEWER_CONFIDENCE": 3}, {"DATE": "11 Dec 2016", "TITLE": "Non-recurrent basline?", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1"}, {"DATE": "01 Dec 2016", "TITLE": "Clarifications, relationship to prior work, and baselines", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3"}, {"DATE": "19 Nov 2016", "TITLE": "Revision for minor typographical error", "IS_META_REVIEW": false, "comments": "We fixed minor typographical error in author's name and Section. 4. etc.\nOur policy restricts to reveal much more details about the internal dataset and results of the end-user satisfaction measure, however, we did our best to introduce how our idea is to be used for multi-label learning in an application to computer vision, especially e-commerce industry. ", "OTHER_KEYS": "Taewan Kim"}, {"IS_META_REVIEW": true, "comments": "The manuscript is a bit scattered and hard to follow. There is technical depth but the paper doesn't do a good job explaining what shortcoming the proposed methods are overcoming and what baselines they are outperforming. \n\nThe writing could be improved. There are numerous grammatical errors.\n\nThe experiments in 3.1 are interesting, but you need to be clearer about the relationship of your ResCeption method to the state-of-the-art. The use of extensive footnotes on page 5 is a bit odd. \"That is a competitive result\" is vague. A footnote links to \""}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "Three knowledgable reviewers recommend rejection. The main concern is missing related work on fashion product search, and thus also baselines. The authors did not post a rebuttal to address the concerns. The AC agrees with the reviewers' recommendation.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"TITLE": "interesting exploration but several major concerns", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "The paper presents a large-scale visual search system for finding product images given a fashion item. The exploration is interesting and the paper does a nice job of discussing the challenges of operating in this domain. The proposed approach addresses several of the challenges. \n\nHowever, there are several concerns.\n\n1) The main concern is that there are no comparisons or even mentions of the work done by Tamara Berg\u2019s group on fashion recognition and fashion attributes, e.g., \n-  \u201cAutomatic Attribute Discovery and Characterization from Noisy Web Data\u201d ECCV 2010 \n- \u201cWhere to Buy It: Matching Street Clothing Photos in Online Shops\u201d ICCV 2015,\n- \u201cRetrieving Similar Styles to Parse Clothing, TPAMI 2014,\netc\nIt is difficult to show the contribution and novelty of this work without discussing and comparing with this extensive prior art.\n\n2) There are not enough details about the attribute dataset and the collection process. What is the source of the images? Are these clean product images or real-world images? How is the annotation done? What instructions are the annotators given? What annotations are being collected? I understand data statistics for example may be proprietary, but these kinds of qualitative details are important to understand the contributions of the paper. How can others compare to this work?\n\n3) There are some missing baselines. How do the results in Table 2 compare to simpler methods, e.g., the BM or CM methods described in the text?\n\nWhile the paper presents an interesting exploration, all these concerns would need to be addressed before the paper can be ready for publication.", "IS_META_REVIEW": false, "RECOMMENDATION": 4, "DATE": "20 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "Good practical visual search system but lack novelty", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "This paper introduces a pratical large-scale visual search system for a fashion site. It uses RNN to recognize multi-label attributes and uses state-of-art faster RCNN to extract features inside those region-of-interest (ROI). The technical contribution of this paper is not clear. Most of the approaches used are standard state-of-art methods and there are not a lot of novelties in applying those methods. For multi-label recognition task, there are other available methods, e.g. using binary models, changing cross-entropy loss function, etc. There aren't any comparison between the RNN method and other simple baselines. The order of the sequential RNN prediction is not clear either. It seems that the attributes form a tree hierarchy and that is used as the order of sequence.\n\nThe paper is not well written either. Most results are reported in the internal dataset and the authors won't release the dataset. \n", "IS_META_REVIEW": false, "RECOMMENDATION": 3, "DATE": "19 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "Contribution not clear enough; concerns about data set itself", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "The manuscript is a bit scattered and hard to follow. There is technical depth but the paper doesn't do a good job explaining what shortcoming the proposed methods are overcoming and what baselines they are outperforming. \n\nThe writing could be improved. There are numerous grammatical errors.\n\nThe experiments in 3.1 are interesting, but you need to be clearer about the relationship of your ResCeption method to the state-of-the-art. The use of extensive footnotes on page 5 is a bit odd. \"That is a competitive result\" is vague. A footnote links to \"", "IS_META_REVIEW": false, "RECOMMENDATION": 3, "DATE": "17 Dec 2016", "REVIEWER_CONFIDENCE": 3}, {"DATE": "11 Dec 2016", "TITLE": "Non-recurrent basline?", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1"}, {"DATE": "01 Dec 2016", "TITLE": "Clarifications, relationship to prior work, and baselines", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3"}, {"DATE": "19 Nov 2016", "TITLE": "Revision for minor typographical error", "IS_META_REVIEW": false, "comments": "We fixed minor typographical error in author's name and Section. 4. etc.\nOur policy restricts to reveal much more details about the internal dataset and results of the end-user satisfaction measure, however, we did our best to introduce how our idea is to be used for multi-label learning in an application to computer vision, especially e-commerce industry. ", "OTHER_KEYS": "Taewan Kim"}]}
{"text": "1 INTRODUCTION\n(a) (b)\nFigure 1: Slot car racing \u2013 the agent has learn how to drive any of the cars as far as possible (left), based on its raw observations (right).\nIn many reinforcement learning problems, the agent has to solve a variety of different tasks to fulfill its overall goal. A common approach to this problem is to learn a single policy for the whole problem, and leave the decomposition of the problem into subtasks to the learner. In many cases, this approach is successful (Mnih et al., 2015; Zahavy et al., 2016), but it comes at the expense of requiring large amounts of training data. Alternatively, multiple policies dedicated to different subtasks can be learned. This, however, requires prior knowledge about how the overal problem decomposes into subtasks. More-\nover, it can run into the same issue of requiring large amounts of data, because the subtasks might overlap and thus afford shared computation to solve them.\nA common approach to address overlapping problems is multi-task learning (Caruana, 1997): by learning a single policy with different subgoals, knowledge between the different tasks can be transferred. This not only allows to learn a compact representation more efficiently, but also improves the agent\u2019s performance on all the individual subtasks (Rusu et al., 2016).\nMulti-task learning, however, faces two problems: it requires the decomposition of the overall problem into subtasks to be given. Moreover, it is not applicable if the subtasks are unrelated, and are better solved without sharing computation. In this case, the single-policy approach results in an agent that does not perform well on any of the individual tasks (Stulp et al., 2014) or that unlearns\n1The first two authors contributed equally to this work.\nthe successful strategy for one subtasks once it switches to another one, an issue known as catastrophic forgetting (McCloskey & Cohen, 1989).\nIn this work, we address the problem of identifying and isolating individual unrelated subtasks, and learning multiple separate policies in an unsupervised way. To that end, we present MT-LRP, an algorithm for learning state representations for multiple tasks by learning with robotic priors. MT-LRP is able to acquire different low-dimensional state representations for multiple tasks in an unsupervised fashion. Importantly, MT-LRP does not require knowledge about which task is executed at a given time or about the number of tasks involved. The representations learned with MT-LRP enable the use of standard reinforcement learning methods to compute effective policies from few data.\nAs explained before, our approach is orthogonal to the classical multi-task learning approach, and constitutes a problem of its own right due to the issues of underperformance and catastrophic forgetting. Therefore, we disregard the shared knowledge problem in this paper. However, any complete reinforcement learning system will need to combine both flavors of multi-task learning, for related and unrelated tasks, and future work will have to address the two problems together.\nMT-LRP is implemented as two neural networks, coupled by a gating mechanism (Sigaud et al., 2015; Droniou et al., 2015) as illustrated in Figure 2. The first network, \u03c7 , detects which task is being executed and selects the corresponding state representation. The second network, \u03d5 , learns task-specific state representations. The networks are trained simultaneously using the robotic priors learning objective (Jonschkowski & Brock, 2015), exploiting physics-based prior knowledge about how states, actions, and rewards relate to each other. Both networks learn from raw sensor data, without supervision and solely based on the robot\u2019s experiences.\nIn a simulated experimental scenario, we show that MT-LRP is able to learn multiple state representations and task detectors from raw observations and that these representations allow to learn better policies from fewer data when compared with other methods. Moreover, we analyze the contribution to this result of each the method\u2019s individual components.\n2 RELATED WORK\nMT-LRP combines three ideas into a novel approach for task discovery and state representation learning: 1) extracting state representations for each task with robotic priors (Jonschkowski & Brock, 2015); 2) discovering discrete tasks and corresponding actions/policies in a RL context (Stulp et al., 2014; Ho\u0308fer & Brock, 2016); 3) using gated networks to implement a \u201cmixture of experts\u201d (Jacobs et al., 1991; Droniou et al., 2015).\nState Representation Learning: Learning from raw observations is considered a holy grail in reinforcement learning (RL). Deep RL has had major success in this, using model-free (Mnih et al., 2015) but also by combining model-free and model-based RL (Levine et al., 2015). These approaches apply end-to-end learning to get from raw input to value functions and policies. A different approach is to explicitly learn state representations using unsupervised learning, e.g. using auto-encoders (Lange et al., 2012). Recently, Watter et al. (2015) extended this idea to learn state representations jointly with dynamic models and apply optimal control to compute a policy. We use learning with robotic priors (Jonschkowski & Brock, 2015), a state representation learning method\nthat exploits information about temporal structure, actions, and rewards. We go beyond previous work by not only learning single state representations, but learning multiple state representations given raw data from multiple tasks.\nOptions and Parameterized Skills: A common approach to factorizing a RL problem into subtasks are macro-actions, often called options (Sutton et al., 1999; Hengst, 2002). The main difference with our approach is that options are used to hierarchically decompose one high-level task into subtasks (and learn sub-policies for these subtasks), whereas we learn task-specific state representations for different high-level tasks. However, options bear resemblance on a technical level, since they are often implemented by a high-level \u201cselection\u201d policy that parametrizes low-level policies (Daniel et al., 2012; Kupcsik et al., 2013; Stulp et al., 2014). Continuous versions of options, referred to as parametrized skills, have been proposed, too (Da Silva et al., 2012; Deisenroth et al., 2014; DoshiVelez & Konidaris, 2016). However, in all the work above, the state representation is given. To the best of our knowledge, state representation learning has not yet been considered in the context of RL with options or parameterized skills.\nGated Networks for Mixtures of Experts and Submanifold Learning: Gated networks are networks that contain gating connections, in which the outputs of at least two neurons are multiplied (Sigaud et al., 2015). This allows a gating neuron g to prohibit (or limit) the flow of information from one neuron x to another neuron y, similar to how transistors function. An early example of gated networks is the mixture of experts approach (Jacobs et al., 1991; Jacobs & Jordan, 1993; Haruno et al., 2001), where separate networks in a modular neural network specialize in predicting subsets of training examples from a database. Our contribution is to extend mixtures of experts by state representation learning (e.g. from raw images) and to the more difficult RL (rather than supervised learning) context. Our gated network architecture is similar to the one proposed by Droniou et al. (2015). Their network simultaneously learns discrete classes jointly with continuous class variations (called submanifolds) in an unsupervised way, e.g., discrete digit classes and shape variations within each class. We use a similar architecture, but in a different way: rather than learning discrete classes, we learn discrete tasks; class-specific submanifolds correspond to task-specific state representations; and finally, we consider a RL rather than an unsupervised learning context.\nAs mentioned in the introduction, our work is orthogonal to multi-task learning (Caruana, 1997) which has been extensively studied in recent reinforcement learning literature, too (Parisotto et al., 2016). Our approach can be trivially combined with multi-task learning by by prepending the gate and state extraction modules with a subnetwork that shares knowledge across tasks. Another interesting multi-task approach is policy distillation (Rusu et al., 2016). This method combines different policies for multiple tasks into a single network, which enables to share information between tasks and to learn a compact network that can even outperform the individual policies.\n3 BACKGROUND: STATE REPRESENTATION LEARNING FOR REINFORCEMENT LEARNING\nWe formulate MT-LRP in a reinforcement learning (RL) setting using a Markov decision process (MDP) (S,A,T,R,\u03b3): Based on the current state s \u2208 S, the agent chooses and executes an action a \u2208 A, obtains a new state s\u2032 \u2208 S (according to the transition function T ) and collects a reward r \u2208 R. The agent\u2019s goal is to learn a policy \u03c0 : S\u2192 A that maximizes the expected return IE(\u2211\u221et=0 \u03b3 trt), with rt being the reward collected at time t and 0 < \u03b3 \u2264 1 the discount factor. We consider an episodic setting with episodes of finite length, a continuous state space S and a discrete action space A.\nIn this work, we assume that the agent cannot directly observe the state s but only has access to observations o \u2208 O, which are usually high-dimensional and contain task-irrelevant distractors. This requires us to extract the state from the observations by learning an observation-state-mapping \u03d5 : O\u2192 S, and use the resulting state representation S to solve the RL problem (assuming that a Markov state can be extracted from a single observation). To learn the state representation, we apply learning with robotic priors (Jonschkowski & Brock (2015), from now on referred to as LRP). This method learns \u03d5 from a set of temporally ordered experiences D = {(ot ,at ,rt)}dt=1 by optimizing the following loss:\nLRP(D,\u03d5) = \u03c9tLtemp.(D,\u03d5)+\u03c9pLprop.(D,\u03d5)+\u03c9cLcaus.(D,\u03d5)+\u03c9rLrep.(D,\u03d5). (1)\nThis loss consists of four terms, each expressing a different prior about suitable state representations for robot RL. We optimize it using gradient descent, assuming \u03d5 to be differentiable. We now explain the four robotic prior loss terms in Eq. (1).\nTemporal Coherence enforces states to change gradually over time (Wiskott & Sejnowski, 2002): Ltemp.(D,\u03d5) = IE [ \u2016\u2206st\u20162 ] ,\nwhere \u2206st = st+1\u2212 st denotes the state change. (To increase readability we replace \u03d5(o) by s.) Proportionality expresses the prior that the same action should change the state by the same magnitude, irrespective of time and the location in the state space:\nLprop.(D,\u03d5) = IE [ (\u2016\u2206st2\u2016\u2212\u2016\u2206st1\u2016) 2 \u2223\u2223\u2223 at1 = at2].\nCausality enforces two states st1 ,st2 to be dissimilar if executing the same action in st1 generates a different reward than in st2 .\nLcaus.(D,\u03d5) = IE [ e\u2212\u2016st2\u2212st1\u2016 2 \u2223\u2223\u2223 at1 = at2 ,rt1+1 6= rt2+1].\nRepeatability requires actions to have repeatable effects by enforcing that the same action produces a similar state change in similar states:\nLrep.(D, \u03d5\u0302) = IE [ e\u2212\u2016st2\u2212st1\u2016 2\u2016\u2206st2 \u2212\u2206st1\u2016 2 \u2223\u2223\u2223 at1 = at2].\nAdditionally, the method enforces simplicity by requiring s to be low-dimensional.\nNote that learning with robotic priors only makes use of the actions a, rewards r, and temporal information t during optimization, but not at test time for computing \u03d5(o) = s. Using a, r and t in this way is an instance of the learning with side information paradigm (Jonschkowski et al., 2015).\n4 MULTI-TASK STATE REPRESENTATIONS: MT-LRP\nNow consider a scenario in which an agent is learning multiple distinct tasks. For each task \u03c4 \u2208 {1, . . . ,T}, the agent now requires a task-specific policy \u03c0\u03c4 : S\u03c4 \u2192 A. We approach the problem by learning a task-specific state representation \u03d5\u03c4 : O\u2192 S\u03c4 for each policy, and a task detector \u03c7 which determines the task, given the current observation. We will consider a probabilistic task-detector \u03c7 : O\u2192 [0,1]T that assigns a probability to each task being active. In order to solve the full multi-task RL problem, we must learn \u03c7, {\u03d5\u03c4}\u03c4\u2208{1,...,T} and {\u03c0\u03c4}\u03c4\u2208{1,...,T}. We propose to address this problem by MT-LRP, a method that jointly learns \u03c7 and {\u03d5\u03c4}\u03c4\u2208{1,...,T} from raw observations, actions, and rewards. MT-LRP then uses the state representations {\u03d5\u03c4} to learn task-specific policies {\u03c0\u03c4}\u03c4\u2208{1,...,T} (using standard RL methods), and switches between them using the task detector \u03c7 . To solve the joint learning problem, MT-LRP generalizes LRP (Jonschkowski & Brock, 2015) in the following regards: (i) we replace the linear observation-statemapping from the original method with a gated neural network, where the gates act as task detectors that switch between different task-specific observation-state-mappings; (ii) we extend the list of robotic priors by the prior of task coherence, which allows us to train multiple task-specific state representations without any specification (or labels) of tasks and states.\n4.1 GATED NEURAL NETWORK ARCHITECTURE\nWe use a gated neural network architecture as shown schematically in Fig. 2. The key idea is that both the task detector \u03c7 as well as the state representation \u03d5 are computed from raw inputs. However, the output of the task detector gates the output of the state representation. Effectively, this means the output of \u03c7(o) decides which task-specific state representation \u03d5\u03c4 is passed further to the policy, which is also gated by the output of \u03c7(o).\nFormally, \u03c7(o) = \u03c3(\u03c7pre(o)) is composed of a function \u03c7pre with T -dimensional output and a softmax \u03c3(z) = e z j\n\u2211k ezk . The softmax ensures that \u03c7 computes a proper probability distribution over tasks.\nThe probabilities are then used to gate \u03d5 . To do this, we decompose \u03d5 into a pre-gating function\n\u03d5pre that extracts features shared across all tasks (i.e. \u201dmulti-task\u201d in the sense of Caruana (1997), unless set to the identity), and a T \u00d7M\u00d7N gating tensor G that encodes the T (linear) observationstate mappings (M = dim(s) and N is the output dimension of \u03d5pre). The value of the state\u2019s i-th dimension si computes as the expectation of the dot product of gating tensor and \u03d5pre(o) over the task probabilities \u03c7(o):\nsi = \u03d5i(o) = T\n\u2211 k=1 \u03c7k(o) \u3008Gk,i,:,\u03d5pre(o)\u3009. (2)\n4.2 LEARNING OBJECTIVE\nTo train the network, we extend the robotic prior loss LRP (Eq. 1), by a task-coherence prior L\u03c4 :\nL= LRP(D,\u03d5)+\u03c9\u03c4L\u03c4(D,\u03c7), (3)\nwhere \u03c9\u03c4 is a scalar weight balancing the influence of the additional loss term. Task coherence is the assumption that a task only changes between training episodes, not within the same episode. It does not presuppose any knowledge about the number of tasks or the task presented in an episode, but it exploits the fact that task switching weakly correlates with training episodes. Moreover, this assumption only needs to hold during training: since \u03c7 operates directly on the observation o, it can in principle switch the task at every point in time during execution. Task-coherence applies directly to the output of the task detector, \u03c7(o), and consists of two terms:\nLcon+sep\u03c4 = Lcon\u03c4 +L sep \u03c4 . (4)\nThe first term enforces task consistency during an episode: Lcon\u03c4 = IE [ H(\u03c7(ot1),\u03c7(ot2)) \u2223\u2223\u2223 episodet1 = episodet2], (5) where H denotes the cross-entropy H(p,q) = \u2212\u2211x p(x) logq(x). It can be viewed as a measure of dissimilarity between probability distributions p and q. We use it to penalize \u03c7 if it assigns different task distributions to inputs ot1 , ot2 that belong to the same episode. Note that task-consistency can be viewed as a temporal coherence prior on the task level (Wiskott & Sejnowski, 2002).\nThe second term expresses task separation and encourages \u03c7 to assign tasks to different episodes: Lsep\u03c4 = IE [ e\u2212H(\u03c7(ot1 ),\u03c7(ot2 )) \u2223\u2223\u2223 episodet1 6= episodet2]. (6) This loss is complementary to task consistency, as it penalizes \u03c7 if it assigns similar task distributions to ot1 , ot2 from different episodes. Note that L sep \u03c4 will in general not become zero. The reason is that the number of episodes usually exceeds the number of tasks, and therefore two observations from different episodes sometimes do belong to the same task. We will evaluate the contribution of each of the two terms to learning success in Section 5.2.\n5 EXPERIMENTS\nWe evaluate MT-LRP in two scenarios. In the multi-task slot-car racing scenario (inspired by Lange et al. (2012)), we apply MT-LRP to a linearly solvable problem, allowing us to easily inspect what and how MT-LRP learns. In slot-car racing, the agent controls one of multiple cars (Figure 1), with the goal of traversing the circuit as fast as possible without leaving the track due to speeding in curves. However, the agent does not know a priori which car it controls, and only receives the raw visual signal as input. Additionally, uncontrolled cars driving at random velocity, act as visual distractors. We turn this scenario into a multi-task problem in which the agent must learn to control each car, where controlling the different cars corresponds to separate tasks. We will now provide the technical details of our experimental set-up.\n5.1 EXPERIMENTAL SET-UP: SLOT-CAR RACING\nThe agent controls the velocity of one car (see Fig. 1), receives a reward proportional to the car\u2019s velocity, chosen from [0.01, 0.02, . . . , 0.1], and a negative reward of \u221210 if the car goes too fast\nin curves. The velocity is subject to Gaussian noise (zero mean, standard deviation 10%) of the commanded velocity. All cars move on independent lanes and do not influence each other. The agent observes the scenario by getting a downscaled 16x16 RGB top-down view (dimension N = 16\u00d716\u00d73 = 768) of the car circuit (Fig. 1(b)). In our experiments, there are two or three cars on the track, and the agent controls a different one in every episode. To recognize the task, the agent must be able to extract a visual cue from the observation which correlates with the task. We study two types of visual cues: Static Visual Cue: The arrangement of cars stays the same in all episodes and a static visual cue (a picture of the controlled car) in the top-left image corner indicates which car is currently controlled. Dynamic Visual Cue: The agent always controls the same car (with a certain color), but in each task the car is located on a different lane (as in Fig. 1(b)).\nData Collection and Learning Procedure: The agent collects 40 episodes per task, each episode consisting of 100 steps. To select an action in each step, the agent performs \u03b5-greedy exploration by picking a random action with probability \u03b5 = 0.3 and the best action according to its current policy otherwise. The agent computes a policy after every \u03c4 episodes, by first learning the observation-state mapping \u03d5 (state representation) and then computing policies \u03c01, . . . ,\u03c0\u03c4 (based on the outcomes of the learned \u03c7 and \u03d5). To monitor the agent\u2019s learning progress, we measure the average reward the agent attains on T test episodes, i.e. one test episode of length 100 per task (using the greedy policy), amounting to 8000 experiences in total. To collect sufficient statistics, the whole experiment is repeated 10 times. Policy Learning: We consider the model-free setting with continuous states S, discrete actions A and solve it using nearest-neighbor Q-learning kNN-TD-RL (Mart\u0131\u0301n H et al., 2009) with k = 10. More recent approaches to model-free RL would be equally applicable (Mnih et al., 2015). Learning Strategies and Baselines: We compare five strategies. We run a) MT-LRP with 5 gate units (two/three more than necessary), state dimensionality M = 2 and using Lcon+sep\u03c4 as taskcoherence prior. We compare MT-LRP to several state representation methods; for each method we evaluate different M and report only the best performing M: a) robotic priors without gated network, LRP (M = 4), b) principal components analysis (PCA) on the observations (M = 20) and c) raw observations (M = 768). Additionally, we evaluate d) a lower baseline in the form of a randomly moving agent and e) an upper baseline by applying RL on the known 2D-position of the slot car under control (M = 2). We use the same RL algorithm for all methods. To learn the state representations with robotic priors, we base our implementation on Theano and lasagne, using the Adam optimizer with learning rate 0.005, batch size 100, Glorot\u2019s weight initialization and \u03c9t = 1,\u03c9p = 5,\u03c9c = 1,\u03c9r = 5,\u03c9\u03c4 = 10. Moreover, we apply an L1 regularization of 0.001 on \u03d5 . Additionally, we analyze the contribution of task coherence priors by applying MT-LRP to the full set of 8000 experiences a) without task-coherence, b) with task consistency Lcon\u03c4 only c) with task separation Lcon\u03c4 only) and d) without task consistency and separation L con+sep \u03c4 .\n5.2 RESULTS\nWe will now present the three main results of our experiments: (i) we show that MT-LRP enables the agent to extract better representations for RL; (ii) we provide insight in how the learner detects the task and encodes the state representations; and finally, (iii) we show the contribution of each of the task-coherence loss terms.\nMT-LRP Extracts Better State Representations for RL Figure 3 shows the learning curves for RL based on state representations learned by the different methods in the two-slot-car scenario (static visual cue on the left, dynamic on the right). No method reaches the performance of the upper baseline, mainly due to aliasing errors resulting from the low image resolution.\nThe random baseline ranges around an average reward of \u221284.9 with standard error 0.72 and was omitted from the Figure. The state representation learning baselines without robotic priors perform poorly because they are unable to identify the taskirrelevant distractions. MT-LRP gets very close to the performance of the upper baseline, especially for very low amounts of training data (d < 2500), whereas LRP does not even attain this level of performance for the full training set d = 8000 in the static task. The gap between MT-LRP and LRP increases even more if we add another car (Figure 5) because LRP can only learn one state representation for all three tasks. Including the three slot cars in this representation results in distractions for the RL method. However, in the dynamic-visual-cue scenario LRP-4 performs on par with MT-LRP. Surprisingly, running LRP with only two dimensions suffices to achieve the performance of MT-LRP. We\nwill explain this phenomenon below. To conclude, MT-LRP allows to learn as good or better policies than the baselines in all slot-car scenarios.\nMT-LRP Detects All Tasks and Learns Good State Representations To gain more insight into what is learned, we analyze the state representations extracted by MT-LRP and LRP. Figure 4 shows the state representation learned by MT-LRP for the static-visual-cue scenario. Each point in the figure corresponds to one observation, markers indicate the task and colors the most active gate unit. We see that the first gate unit (blue) is always active for task 1 (circle), and the second gate unit for task 2. This shows that the task is detected with high accuracy. The task detector \u03c7 is also highly cer-\ntain which is reflected in the fact that its entropy evaluated on the data is close to zero. Moreover, the states reflect the circular structure of the slot car racing track. We thus conclude that MT-LRP has learned to identify the tasks and to represent the position of each car on the track.\nThe RL experiments raised the question why LRP manages to solve the dynamic, but not the static-visual-cue scenario as well as MT-LRP. We hypothesize that, for the dynamic cue, LRP is able to extract the position of the car on regardless of which lane it is in using a single linear mapping. Figure 6 confirms this hypothesis: LRP filters for the car\u2019s color (blue) along the track and assigns increasing weights to these pixels which results in the extraction of its position. It also assigns constant weights along the track in the red channel using the lane change of the two cars as an offset. This results in a mapping to two circles similar to Fig. 4, where the state encodes both the position and the task. Such a mapping can be expressed by a linear\nfunction precisely because the features that are relevant for one task do not reappear in another task (e.g. a blue slot car in track 1 does not appear in the task where the blue car is in track 2).\nHowever, there exists no equivalent linear mapping for the static-visual-cue variant of the slotcar problem, because cars that are relevant for one task are also present in every other task.\nWe can generalize from this insight as follows. A single linear observation-state-mapping is sufficient for multiple tasks if the state representation for every task can be extracted by a linear function using only features that stay constant for all other tasks. If this is the case, than there is no need for decoupling the extraction of task and state.\nTask-Consistency is Critical for Learning Performance To understand the influence of\nthe different task-coherence prior variants, we compared their performance in Figure 7. We see that relying solely on the robotic priors gives poor results, mainly because the gate units are not used properly: more than one gate unit is activated per task (\u03c7 has high entropy). Adding the taskseparation prior forces the network to use as many gates as possible (5 in our case), leading to bad state representations. Interestingly, using task consistency only gives roughly the same result as using task consistency and task separation.\nDiscussion The experiments showed that MT-LRP is able to solve the representation and reinforcement learning tasks better than the baselines. Important questions for future work concern: the necessity and influence of the task-separation loss, in particular for short episode lengths and if the number of expected tasks exceeds the number of actual tasks; and transferring knowledge by adding a shared neural network layers before gating.\n6 CONCLUSION\nWe have presented MT-LRP, a method for multi-task state representation learning with robotic priors. The method learns in an unsupervised fashion, solely based on the robots own observations, actions, and rewards. Our experiments confirmed that MT-LRP is effective in simultaneously identifying tasks and learning task-specific state representations. This capability is beneficial for scaling reinforcement learning to realistic scenarios that require dedicated skills for different tasks.\nACKNOWLEDGMENTS\nWe gratefully acknowledge the funding provided by the German Research Foundation (DFG, Exploration Challenge, BR 2248/3-1), the Alexander von Humboldt foundation through an Alexander von Humboldt professorship (funded by the German Federal Ministry of Education and Research). Additionally, Antonin Raffin was supported by an Erasmus+ grant.\n", "cats": {"READ": 1.0, "DONTREAD": 0.0}, "reviews": [{"IS_META_REVIEW": true, "comments": "The paper presents a method to learn a low-dimensional state representations from raw obervation for multi-task setting. In contrast to classic multi-task learning setting where a joint representation is usually learned by exploring the transferable information among different tasks, the method aims to identify individual task and solve them separately. To this end, the authors extend the learning with robotic priors approach by extending the loss function with additional term for task coherence, i.e., a task only changes representation between training episodes. The method has been evaluated on two tasks, multi-task slot-car racing and mobile navigation to prove its efficacy.\n\nthere were several unclear issues:\n\n1. The first question is that if the method is only appealing on the scenario like the slot-car racing, otherwise it should be benchmarked with mutli-task learning. While the author made the argument in the related work, the proposed method is orthogonal to multi-task learning they did admit both explore shared knowledge between tasks. What's the advantage and disadvantage for the proposed method for general mutiple task setting, in particular over the multi-task learning?\nThe reply of the authors was not fully satisfactory. The argument did not support the lack of comparison to multi-task joint-learning. It seems they don't plan to include any comparison neither. I think it's important for the fundamental motivation for the work, without such comparison, the method seems to be purely an alternative to multi-task joint-learning without any(or much) practical advantage.\n\n2.Following up to the previous question, please clarify the results on the mobile navigation scenario. It's not clear how the plot on the right indicates MT-LRP identifies all tasks as the author claimed and and seems very weak to support the method, in particular compared to the multi-slot car-racing driving experiment, there is too little results to make sound argument (almost no comparison to alternative methods, i.e. no baseline method, is that true for the problem).\nThe explanation of the authors did provide more details and more explicit information. \n\n3. The proposed gated neural network architecture seems to be a soft gated structure(correct me if I am wrong), a possible baseline would be a hard gated unit, how would this affect the conclusion. This is particularly interesting as the authors reflect on the constraint that the representation should stay consistent during the training.\nThe author simply stated again what they did for the modeling without counter the comparison to hard-gating, but it's probably less an issue compared to Question 1.\n\nIn summary, while there are remaining concerns about lacking comparisons, the is a weak tendency towards accepting the submission."}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "The authors propose to explore an important problem -- learning state representations for reinforcement learning. However, the experimental evaluation raised a number of concerns among the reviewers. The method is tested on only a single relatively easy domain, with some arbitrarily choices justified in unconvincing ways (e.g. computational limitations as a reason to not fix aliasing). The evaluation also compares to extremely weak baselines. In the end, there is insufficient evidence to convincingly show that the method actually works, and since the contribution is entirely empirical (as noted by the reviewers in regard to some arbitrary parameter choices), the unconvincing evaluation makes this method unsuitable for publication at this time. The authors would be encouraged to evaluate the method rigorously on a wide range of realistic tasks.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"DATE": "13 Jan 2017", "TITLE": "Reply to Reviewer Comments", "IS_META_REVIEW": false, "comments": "We would like to thank all reviewers for their thorough and helpful comments!\n\n1) Before we turn to the individual questions raised the reviewers, we would like to address the main issue that all reviewers raised, namely the relationship of our method to multi-task learning:\n\n\u201cThe authors state that the proposed method is orthogonal to multi-task learning though the end goal of learning to solve multiple tasks is the same.\u201d (AnonReviewer1)\n\u201cThe argument did not support the lack of comparison to multi-task joint-learning.\u201d (AnonReviewer2)\n\u201cLimiting the multi-task learning to be different to individual tasks rather than sharing and transferring knowledge between tasks\u201d (AnonReviewer 3)\n\nThe intro has been rewritten to clarify our motivation and how our work compares to multi-task learning. We completely agree that successful RL will require multi-task learning to share knowledge that generalizes over multiple tasks. \n\nBut there are sets of tasks that require multiple dedicated skills without sharing knowledge. For instance, in a video game, an agent have to achieve several subgoals (fight an enemy, avoid obstacles,...), each of these can be seen as individual task. Learning multiple, (sub-)policies dedicated to *different* tasks is a problem of its own right, as it faces significant theoretical issues, such as \u201ccatastrophic forgetting\u201d. We have elaborated on this argument in the introduction.\n\nSince there is few work approaching this problem in RL, our paper studies the question of how to learn fully independent policies for different tasks. We fully agree that future work will need to combine learning shared and separate representations but we regard our work on the  independent-policy multi-task RL problem as a contribution in itself. \n\nWe now reply to the individual comments raised by the reviewers.\n\n----\n\nAnonReviewer1\n\n1) \u201cReferences to other multi-task learning works, e.g. policy distillation and actor-mimic (both ICLR \u201916), may be appropriate as well.\u201d\n\nThank you for the pointers, we have integrated the two suggested papers in the related work of the paper.\n\n2) \u201cThe approach does not necessarily share information across tasks for better learning, and requires learning a different policy for each task\u201d\n\nThis is the very idea of the method proposed, we updated the introduction to clarify the reasons we focused on this approach.\n\n3) \u201cIn the second navigation scenario, only the state representation is qualitatively shown, not the resulting control policy nor any other learned state representations for comparison. Since the multi-task state representation learning approach is only useful if you can also learn control better, the paper should also evaluate on control, with the same comparisons as in the first experiment. Without this evaluation, the experiment is incomplete.\u201d\n\nWe agree and as mentioned before this was a preliminary and incomplete experiment, and we decided to remove it from the paper. \n\n4) \u201cLastly, to be on par with publications at a venue like ICLR, the method should be evaluated more thoroughly, on a wider range of set-ups [...]\u201d\n\nWe agree that it is beneficial to apply a method to a wider range of tasks. Yet, we chose to invest into rigorously evaluating the performance of the method on the chosen task, and provide a thorough argument why and how the method works. We believe that it will scale to a wider range of tasks, but we will have to address this in future work.\n\n5) \u201cCould this approach be combined with other state representation learning approaches? e.g. approaches that use an autoencoder.\u201d\n\nYes, in principle it would be possible to use other state representation learning objectives.  Note, however, that in the slot car racing scenario a PCA/auto-encoder loss will not perform as well as LRP, as it has will try to explain all variations in the observation, in particular the second slot car. This has been shown in our previous work (Jonschkowski & Brock 2015) and is also reflected in the performance of PCA in the slot-car experiment.\n\n6) \u201dOne additional useful comparison would be to evaluate performance in the single-task setting (e.g. only controlling the red car), as an upper bound on how well the policy should be able to perform. Does the learned multi-task policy reach the same level of performance? This upper bound will be tighter than the \u201cknown car position\u201d baseline (which is also useful in its own right).\u201d\n\nThank you for this suggestion; in our experiment, however, the performance of the car in the single-task setting is identical to the performance we see in the multi-task setting. The reason is that the task detector module has a very high accuracy (greater than 99%) for the slot car tasks, and in consequence, a separate policy for each slot car is learned.\n\n7) \u201cDoes the \u201cobservations\u201d baseline eventually reach the performance of the LRP approach? It would be useful to know if this approach simply speeds up learning (significantly) or if it enables better performance.\u201d\n\nOur experiments and previous work (Jonschkowski & Brock 2015) suggest that it will eventually reach the same performance with enough data, but for now, even in our largest experiments, we did not see it happening.\n\n8) \u201cIf there are aliasing issues with the images, why not just use higher resolution images?\u201d\n\nMainly computational reasons: we wanted to evaluate a wide variety of parameter settings and study their influence on our algorithm, yet we did not have the computational power to do this exhaustively on higher resolutions.\n\n---\n\nAnonReviewer2\n\n1) \u201cThe author simply stated again what they did for the modeling without counter the comparison to hard-gating, but it's probably less an issue compared to Question 1.\u201d\n\nWe are sorry that our answer in the pre-review phase did not address your question. We were trying to explain that our method is technically a soft gating, but effectively learns to perform hard gating. We are not sure how whether and how using a hard would influence the conclusion of the paper, and we are not aware of a way to implement a differentiable hard gate (if it is not differentiable, we cannot train it using backpropagation).\n\n---\n\nAnonReviewer3\n\n2) \u201cParameters choice is arbitrary (w parameters)\u201d\nThe weights w for the different methods are chosen as described in Jonschkowski & Brock 2015, by monitoring the gradient on a small part of the training set. The goal is to have gradients of the same magnitude for the different terms in the loss, so only relative weighting matters. Small changes to the parameters do not affect the method and there is no need for careful tuning.\n\n3) \u201cThe experiments could have been conducted using a standardized simulation tool such as OpenAI Gym to make it easy to compare.\u201d\nWe agree that evaluating experiments on a standardized tool such as OpenAI gym is a great idea. We want to point out, though, that the slot car racing task considered in the paper is a well-known task that has been evaluated in previous work, too, e.g. (Lange et al., 2012). Moreover, it is the simplest task that has the properties we are interested in this paper (non-overlapping tasks).\nBut we agree that that open simulation tools such as OpenAI gym are great and we will apply our method to these tasks in future work.", "OTHER_KEYS": "Sebastian H\u00f6fer"}, {"TITLE": "Unsupervised Learning of State Representations for Multiple Tasks", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "This paper is about learning unsupervised state representations using multi-task reinforcement learning.  The authors propose a novel approach combining gated neural networks with multitask learning with robotics priors. They evaluated their approach on two simulated datasets and showed promising results. The paper is clearly written and is theoretically sound.\n\nPositives:\n+ Gating to enable learning a joint representation\n+ Multi-task learning extended from a single task in prior work\n+ Combining multiple types of losses to learn a strong representation (Coherence, Proportionality, Causality, Repeatability, Consistency and Separation)\n\nNegatives:\n- Parameters choice is arbitrary (w parameters)\n- Limiting the multi-task learning to be different to individual tasks rather than sharing and transferring knowledge between tasks\n- The experiments could have been conducted using a standardized simulation tool such as OpenAI Gym to make it easy to compare.\n\nI would recommend that the authors consider a more standardized way of picking the model parameters and evaluate on a more standard and high-dimensional datasets.", "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "Review", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "This paper builds upon the method of Jonschkowski & Brock to learn state representations for multiple tasks, rather than a single task. The research direction of learning representations for multiple tasks is an interesting one, and largely unexplored. The approach in the paper is to learn a different representation for each task, and a different policy for each task, where the task is detected automatically and built into the neural network.\n\nThe authors state that the proposed method is orthogonal to multi-task learning, though the end goal of learning to solve multiple tasks is the same. It would be interesting and helpful to see more discussion on this point in the paper, as discussed in the pre-review question phase. References to other multi-task learning works, e.g. policy distillation and actor-mimic (both ICLR \u201916), may be appropriate as well.\n\nThe method proposes to jointly learn a task classifier with a state representation learner, by using a differentiable gating mechanism to control the flow of information. The paper proposes a task coherence prior for this gating mechanism to ensure that the learned task classifier is temporally coherent. Introducing this structure is what enables the method to improve performance over the standard, non-multitask approach.\n\nThe evaluation involves two toy experimental scenarios. The first involves controlling one of two cars to drive around a track. In this task, detecting the \u201ctask\u201d is very easy, and the learned state representation is linear in the observation. The paper evaluates the performance of the policies learned with the proposed approach, and shows sufficient comparisons to demonstrate the usefulness of the approach over a standard non-multitask set-up.\n\nIn the second navigation scenario, only the state representation is qualitatively shown, not the resulting control policy nor any other learned state representations for comparison. Since the multi-task state representation learning approach is only useful if you can also learn control better, the paper should also evaluate on control, with the same comparisons as in the first experiment. Without this evaluation, the experiment is incomplete.\n\nLastly, to be on par with publications at a venue like ICLR, the method should be evaluated more thoroughly, on a wider range of set-ups, to demonstrate the generality of the approach and show that the method applies to more complex tasks. While in theory, the method should scale, the experiments do not demonstrate that it can handle more realistic scenarios, such as scaling beyond MNIST-level images, to 3D or real images, or higher-dimensional control tasks. Evaluating the method in this more complex scenario is important, because unexpected issues can come up when trying to scale. If scaling-up is straight-forward, then running this experiment (and including it in the paper) should be straight-forward.\n\nIn summary, here are the pros and cons of this paper:\nCons\n- The approach does not necessarily share information across tasks for better learning, and requires learning a different policy for each task\n- Only one experimental set-up that evaluates learned policy with multi-task state representation\n- No experiments on more realistic scenarios, such 3D environments or high-dimensional control problems\nPros: \n- This approach enables using the same network for multiple tasks, which is often not true for transfer and multi-task learning approaches\n- Novel way to learn a single policy for multiple tasks, including a task coherence prior which ensures that the task classification is meaningful\n- Experimentally validated on two toy tasks. One task shows improvement over baseline approaches\n\nThus, my rating would be higher if the paper included an evaluation of the control policy for navigation and included another more challenging and compelling scenario.\n\n\nLastly, here are some minor comments/questions on how I think the paper could be improved, but are not as important as the above:\n\nApproach:\nCould this approach be combined with other state representation learning approaches? e.g. approaches that use an autoencoder.\n\nExperiments:\nOne additional useful comparison would be to evaluate performance in the single-task setting (e.g. only controlling the red car), as an upper bound on how well the policy should be able to perform. Does the learned multi-task policy reach the same level of performance? This upper bound will be tighter than the \u201cknown car position\u201d baseline (which is also useful in its own right).\n\nDoes the \u201cobservations\u201d baseline eventually reach the performance of the LRP approach? It would be useful to know if this approach simply speeds up learning (significantly) or if it enables better performance.\n\nIf there are aliasing issues with the images, why not just use higher resolution images?", "IS_META_REVIEW": false, "RECOMMENDATION": 5, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "Unsupervised Learning of State Representations for Multiple Tasks", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "The paper presents a method to learn a low-dimensional state representations from raw obervation for multi-task setting. In contrast to classic multi-task learning setting where a joint representation is usually learned by exploring the transferable information among different tasks, the method aims to identify individual task and solve them separately. To this end, the authors extend the learning with robotic priors approach by extending the loss function with additional term for task coherence, i.e., a task only changes representation between training episodes. The method has been evaluated on two tasks, multi-task slot-car racing and mobile navigation to prove its efficacy.\n\nthere were several unclear issues:\n\n1. The first question is that if the method is only appealing on the scenario like the slot-car racing, otherwise it should be benchmarked with mutli-task learning. While the author made the argument in the related work, the proposed method is orthogonal to multi-task learning they did admit both explore shared knowledge between tasks. What's the advantage and disadvantage for the proposed method for general mutiple task setting, in particular over the multi-task learning?\nThe reply of the authors was not fully satisfactory. The argument did not support the lack of comparison to multi-task joint-learning. It seems they don't plan to include any comparison neither. I think it's important for the fundamental motivation for the work, without such comparison, the method seems to be purely an alternative to multi-task joint-learning without any(or much) practical advantage.\n\n2.Following up to the previous question, please clarify the results on the mobile navigation scenario. It's not clear how the plot on the right indicates MT-LRP identifies all tasks as the author claimed and and seems very weak to support the method, in particular compared to the multi-slot car-racing driving experiment, there is too little results to make sound argument (almost no comparison to alternative methods, i.e. no baseline method, is that true for the problem).\nThe explanation of the authors did provide more details and more explicit information. \n\n3. The proposed gated neural network architecture seems to be a soft gated structure(correct me if I am wrong), a possible baseline would be a hard gated unit, how would this affect the conclusion. This is particularly interesting as the authors reflect on the constraint that the representation should stay consistent during the training.\nThe author simply stated again what they did for the modeling without counter the comparison to hard-gating, but it's probably less an issue compared to Question 1.\n\nIn summary, while there are remaining concerns about lacking comparisons, the is a weak tendency towards accepting the submission.", "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 3}, {"DATE": "02 Dec 2016", "TITLE": "pre-review questions", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2"}, {"DATE": "02 Dec 2016", "TITLE": "pre-review questions", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1"}, {"IS_META_REVIEW": true, "comments": "The paper presents a method to learn a low-dimensional state representations from raw obervation for multi-task setting. In contrast to classic multi-task learning setting where a joint representation is usually learned by exploring the transferable information among different tasks, the method aims to identify individual task and solve them separately. To this end, the authors extend the learning with robotic priors approach by extending the loss function with additional term for task coherence, i.e., a task only changes representation between training episodes. The method has been evaluated on two tasks, multi-task slot-car racing and mobile navigation to prove its efficacy.\n\nthere were several unclear issues:\n\n1. The first question is that if the method is only appealing on the scenario like the slot-car racing, otherwise it should be benchmarked with mutli-task learning. While the author made the argument in the related work, the proposed method is orthogonal to multi-task learning they did admit both explore shared knowledge between tasks. What's the advantage and disadvantage for the proposed method for general mutiple task setting, in particular over the multi-task learning?\nThe reply of the authors was not fully satisfactory. The argument did not support the lack of comparison to multi-task joint-learning. It seems they don't plan to include any comparison neither. I think it's important for the fundamental motivation for the work, without such comparison, the method seems to be purely an alternative to multi-task joint-learning without any(or much) practical advantage.\n\n2.Following up to the previous question, please clarify the results on the mobile navigation scenario. It's not clear how the plot on the right indicates MT-LRP identifies all tasks as the author claimed and and seems very weak to support the method, in particular compared to the multi-slot car-racing driving experiment, there is too little results to make sound argument (almost no comparison to alternative methods, i.e. no baseline method, is that true for the problem).\nThe explanation of the authors did provide more details and more explicit information. \n\n3. The proposed gated neural network architecture seems to be a soft gated structure(correct me if I am wrong), a possible baseline would be a hard gated unit, how would this affect the conclusion. This is particularly interesting as the authors reflect on the constraint that the representation should stay consistent during the training.\nThe author simply stated again what they did for the modeling without counter the comparison to hard-gating, but it's probably less an issue compared to Question 1.\n\nIn summary, while there are remaining concerns about lacking comparisons, the is a weak tendency towards accepting the submission."}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "The authors propose to explore an important problem -- learning state representations for reinforcement learning. However, the experimental evaluation raised a number of concerns among the reviewers. The method is tested on only a single relatively easy domain, with some arbitrarily choices justified in unconvincing ways (e.g. computational limitations as a reason to not fix aliasing). The evaluation also compares to extremely weak baselines. In the end, there is insufficient evidence to convincingly show that the method actually works, and since the contribution is entirely empirical (as noted by the reviewers in regard to some arbitrary parameter choices), the unconvincing evaluation makes this method unsuitable for publication at this time. The authors would be encouraged to evaluate the method rigorously on a wide range of realistic tasks.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"DATE": "13 Jan 2017", "TITLE": "Reply to Reviewer Comments", "IS_META_REVIEW": false, "comments": "We would like to thank all reviewers for their thorough and helpful comments!\n\n1) Before we turn to the individual questions raised the reviewers, we would like to address the main issue that all reviewers raised, namely the relationship of our method to multi-task learning:\n\n\u201cThe authors state that the proposed method is orthogonal to multi-task learning though the end goal of learning to solve multiple tasks is the same.\u201d (AnonReviewer1)\n\u201cThe argument did not support the lack of comparison to multi-task joint-learning.\u201d (AnonReviewer2)\n\u201cLimiting the multi-task learning to be different to individual tasks rather than sharing and transferring knowledge between tasks\u201d (AnonReviewer 3)\n\nThe intro has been rewritten to clarify our motivation and how our work compares to multi-task learning. We completely agree that successful RL will require multi-task learning to share knowledge that generalizes over multiple tasks. \n\nBut there are sets of tasks that require multiple dedicated skills without sharing knowledge. For instance, in a video game, an agent have to achieve several subgoals (fight an enemy, avoid obstacles,...), each of these can be seen as individual task. Learning multiple, (sub-)policies dedicated to *different* tasks is a problem of its own right, as it faces significant theoretical issues, such as \u201ccatastrophic forgetting\u201d. We have elaborated on this argument in the introduction.\n\nSince there is few work approaching this problem in RL, our paper studies the question of how to learn fully independent policies for different tasks. We fully agree that future work will need to combine learning shared and separate representations but we regard our work on the  independent-policy multi-task RL problem as a contribution in itself. \n\nWe now reply to the individual comments raised by the reviewers.\n\n----\n\nAnonReviewer1\n\n1) \u201cReferences to other multi-task learning works, e.g. policy distillation and actor-mimic (both ICLR \u201916), may be appropriate as well.\u201d\n\nThank you for the pointers, we have integrated the two suggested papers in the related work of the paper.\n\n2) \u201cThe approach does not necessarily share information across tasks for better learning, and requires learning a different policy for each task\u201d\n\nThis is the very idea of the method proposed, we updated the introduction to clarify the reasons we focused on this approach.\n\n3) \u201cIn the second navigation scenario, only the state representation is qualitatively shown, not the resulting control policy nor any other learned state representations for comparison. Since the multi-task state representation learning approach is only useful if you can also learn control better, the paper should also evaluate on control, with the same comparisons as in the first experiment. Without this evaluation, the experiment is incomplete.\u201d\n\nWe agree and as mentioned before this was a preliminary and incomplete experiment, and we decided to remove it from the paper. \n\n4) \u201cLastly, to be on par with publications at a venue like ICLR, the method should be evaluated more thoroughly, on a wider range of set-ups [...]\u201d\n\nWe agree that it is beneficial to apply a method to a wider range of tasks. Yet, we chose to invest into rigorously evaluating the performance of the method on the chosen task, and provide a thorough argument why and how the method works. We believe that it will scale to a wider range of tasks, but we will have to address this in future work.\n\n5) \u201cCould this approach be combined with other state representation learning approaches? e.g. approaches that use an autoencoder.\u201d\n\nYes, in principle it would be possible to use other state representation learning objectives.  Note, however, that in the slot car racing scenario a PCA/auto-encoder loss will not perform as well as LRP, as it has will try to explain all variations in the observation, in particular the second slot car. This has been shown in our previous work (Jonschkowski & Brock 2015) and is also reflected in the performance of PCA in the slot-car experiment.\n\n6) \u201dOne additional useful comparison would be to evaluate performance in the single-task setting (e.g. only controlling the red car), as an upper bound on how well the policy should be able to perform. Does the learned multi-task policy reach the same level of performance? This upper bound will be tighter than the \u201cknown car position\u201d baseline (which is also useful in its own right).\u201d\n\nThank you for this suggestion; in our experiment, however, the performance of the car in the single-task setting is identical to the performance we see in the multi-task setting. The reason is that the task detector module has a very high accuracy (greater than 99%) for the slot car tasks, and in consequence, a separate policy for each slot car is learned.\n\n7) \u201cDoes the \u201cobservations\u201d baseline eventually reach the performance of the LRP approach? It would be useful to know if this approach simply speeds up learning (significantly) or if it enables better performance.\u201d\n\nOur experiments and previous work (Jonschkowski & Brock 2015) suggest that it will eventually reach the same performance with enough data, but for now, even in our largest experiments, we did not see it happening.\n\n8) \u201cIf there are aliasing issues with the images, why not just use higher resolution images?\u201d\n\nMainly computational reasons: we wanted to evaluate a wide variety of parameter settings and study their influence on our algorithm, yet we did not have the computational power to do this exhaustively on higher resolutions.\n\n---\n\nAnonReviewer2\n\n1) \u201cThe author simply stated again what they did for the modeling without counter the comparison to hard-gating, but it's probably less an issue compared to Question 1.\u201d\n\nWe are sorry that our answer in the pre-review phase did not address your question. We were trying to explain that our method is technically a soft gating, but effectively learns to perform hard gating. We are not sure how whether and how using a hard would influence the conclusion of the paper, and we are not aware of a way to implement a differentiable hard gate (if it is not differentiable, we cannot train it using backpropagation).\n\n---\n\nAnonReviewer3\n\n2) \u201cParameters choice is arbitrary (w parameters)\u201d\nThe weights w for the different methods are chosen as described in Jonschkowski & Brock 2015, by monitoring the gradient on a small part of the training set. The goal is to have gradients of the same magnitude for the different terms in the loss, so only relative weighting matters. Small changes to the parameters do not affect the method and there is no need for careful tuning.\n\n3) \u201cThe experiments could have been conducted using a standardized simulation tool such as OpenAI Gym to make it easy to compare.\u201d\nWe agree that evaluating experiments on a standardized tool such as OpenAI gym is a great idea. We want to point out, though, that the slot car racing task considered in the paper is a well-known task that has been evaluated in previous work, too, e.g. (Lange et al., 2012). Moreover, it is the simplest task that has the properties we are interested in this paper (non-overlapping tasks).\nBut we agree that that open simulation tools such as OpenAI gym are great and we will apply our method to these tasks in future work.", "OTHER_KEYS": "Sebastian H\u00f6fer"}, {"TITLE": "Unsupervised Learning of State Representations for Multiple Tasks", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "This paper is about learning unsupervised state representations using multi-task reinforcement learning.  The authors propose a novel approach combining gated neural networks with multitask learning with robotics priors. They evaluated their approach on two simulated datasets and showed promising results. The paper is clearly written and is theoretically sound.\n\nPositives:\n+ Gating to enable learning a joint representation\n+ Multi-task learning extended from a single task in prior work\n+ Combining multiple types of losses to learn a strong representation (Coherence, Proportionality, Causality, Repeatability, Consistency and Separation)\n\nNegatives:\n- Parameters choice is arbitrary (w parameters)\n- Limiting the multi-task learning to be different to individual tasks rather than sharing and transferring knowledge between tasks\n- The experiments could have been conducted using a standardized simulation tool such as OpenAI Gym to make it easy to compare.\n\nI would recommend that the authors consider a more standardized way of picking the model parameters and evaluate on a more standard and high-dimensional datasets.", "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "Review", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "This paper builds upon the method of Jonschkowski & Brock to learn state representations for multiple tasks, rather than a single task. The research direction of learning representations for multiple tasks is an interesting one, and largely unexplored. The approach in the paper is to learn a different representation for each task, and a different policy for each task, where the task is detected automatically and built into the neural network.\n\nThe authors state that the proposed method is orthogonal to multi-task learning, though the end goal of learning to solve multiple tasks is the same. It would be interesting and helpful to see more discussion on this point in the paper, as discussed in the pre-review question phase. References to other multi-task learning works, e.g. policy distillation and actor-mimic (both ICLR \u201916), may be appropriate as well.\n\nThe method proposes to jointly learn a task classifier with a state representation learner, by using a differentiable gating mechanism to control the flow of information. The paper proposes a task coherence prior for this gating mechanism to ensure that the learned task classifier is temporally coherent. Introducing this structure is what enables the method to improve performance over the standard, non-multitask approach.\n\nThe evaluation involves two toy experimental scenarios. The first involves controlling one of two cars to drive around a track. In this task, detecting the \u201ctask\u201d is very easy, and the learned state representation is linear in the observation. The paper evaluates the performance of the policies learned with the proposed approach, and shows sufficient comparisons to demonstrate the usefulness of the approach over a standard non-multitask set-up.\n\nIn the second navigation scenario, only the state representation is qualitatively shown, not the resulting control policy nor any other learned state representations for comparison. Since the multi-task state representation learning approach is only useful if you can also learn control better, the paper should also evaluate on control, with the same comparisons as in the first experiment. Without this evaluation, the experiment is incomplete.\n\nLastly, to be on par with publications at a venue like ICLR, the method should be evaluated more thoroughly, on a wider range of set-ups, to demonstrate the generality of the approach and show that the method applies to more complex tasks. While in theory, the method should scale, the experiments do not demonstrate that it can handle more realistic scenarios, such as scaling beyond MNIST-level images, to 3D or real images, or higher-dimensional control tasks. Evaluating the method in this more complex scenario is important, because unexpected issues can come up when trying to scale. If scaling-up is straight-forward, then running this experiment (and including it in the paper) should be straight-forward.\n\nIn summary, here are the pros and cons of this paper:\nCons\n- The approach does not necessarily share information across tasks for better learning, and requires learning a different policy for each task\n- Only one experimental set-up that evaluates learned policy with multi-task state representation\n- No experiments on more realistic scenarios, such 3D environments or high-dimensional control problems\nPros: \n- This approach enables using the same network for multiple tasks, which is often not true for transfer and multi-task learning approaches\n- Novel way to learn a single policy for multiple tasks, including a task coherence prior which ensures that the task classification is meaningful\n- Experimentally validated on two toy tasks. One task shows improvement over baseline approaches\n\nThus, my rating would be higher if the paper included an evaluation of the control policy for navigation and included another more challenging and compelling scenario.\n\n\nLastly, here are some minor comments/questions on how I think the paper could be improved, but are not as important as the above:\n\nApproach:\nCould this approach be combined with other state representation learning approaches? e.g. approaches that use an autoencoder.\n\nExperiments:\nOne additional useful comparison would be to evaluate performance in the single-task setting (e.g. only controlling the red car), as an upper bound on how well the policy should be able to perform. Does the learned multi-task policy reach the same level of performance? This upper bound will be tighter than the \u201cknown car position\u201d baseline (which is also useful in its own right).\n\nDoes the \u201cobservations\u201d baseline eventually reach the performance of the LRP approach? It would be useful to know if this approach simply speeds up learning (significantly) or if it enables better performance.\n\nIf there are aliasing issues with the images, why not just use higher resolution images?", "IS_META_REVIEW": false, "RECOMMENDATION": 5, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "Unsupervised Learning of State Representations for Multiple Tasks", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "The paper presents a method to learn a low-dimensional state representations from raw obervation for multi-task setting. In contrast to classic multi-task learning setting where a joint representation is usually learned by exploring the transferable information among different tasks, the method aims to identify individual task and solve them separately. To this end, the authors extend the learning with robotic priors approach by extending the loss function with additional term for task coherence, i.e., a task only changes representation between training episodes. The method has been evaluated on two tasks, multi-task slot-car racing and mobile navigation to prove its efficacy.\n\nthere were several unclear issues:\n\n1. The first question is that if the method is only appealing on the scenario like the slot-car racing, otherwise it should be benchmarked with mutli-task learning. While the author made the argument in the related work, the proposed method is orthogonal to multi-task learning they did admit both explore shared knowledge between tasks. What's the advantage and disadvantage for the proposed method for general mutiple task setting, in particular over the multi-task learning?\nThe reply of the authors was not fully satisfactory. The argument did not support the lack of comparison to multi-task joint-learning. It seems they don't plan to include any comparison neither. I think it's important for the fundamental motivation for the work, without such comparison, the method seems to be purely an alternative to multi-task joint-learning without any(or much) practical advantage.\n\n2.Following up to the previous question, please clarify the results on the mobile navigation scenario. It's not clear how the plot on the right indicates MT-LRP identifies all tasks as the author claimed and and seems very weak to support the method, in particular compared to the multi-slot car-racing driving experiment, there is too little results to make sound argument (almost no comparison to alternative methods, i.e. no baseline method, is that true for the problem).\nThe explanation of the authors did provide more details and more explicit information. \n\n3. The proposed gated neural network architecture seems to be a soft gated structure(correct me if I am wrong), a possible baseline would be a hard gated unit, how would this affect the conclusion. This is particularly interesting as the authors reflect on the constraint that the representation should stay consistent during the training.\nThe author simply stated again what they did for the modeling without counter the comparison to hard-gating, but it's probably less an issue compared to Question 1.\n\nIn summary, while there are remaining concerns about lacking comparisons, the is a weak tendency towards accepting the submission.", "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 3}, {"DATE": "02 Dec 2016", "TITLE": "pre-review questions", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2"}, {"DATE": "02 Dec 2016", "TITLE": "pre-review questions", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1"}]}
{"text": "AN EMPIRICAL ANALYSIS OF DEEP NETWORK LOSS SURFACES\n1 INTRODUCTION\nDeep neural networks are trained by optimizing an extremely high-dimensional loss function with respect to the weights of the network\u2019s linear layers. The objective function minimized is some measure of the error of the network\u2019s predictions based on these weights compared to training data. This loss function is non-convex and has many local minima. These loss functions are usually minimized using first-order gradient descent (Robbins & Monro, 1951; Polyak, 1964) algorithms such as stochastic gradient descent (SGD) (Bottou, 1991). The success of deep learning critically depends on how well we can minimize this loss function, both in terms of the quality of the local minima found and the time to find them. Understanding the geometry of this loss function and how well optimization algorithms can find good local minima is thus of vital importance.\nSeveral works have theoretically analyzed and characterized the geometry of deep network loss functions. However, to make these analyses tractible, they have relied on simplifications of the network structures, including that the networks are linear (Saxe et al., 2014), or assuming the path and variable independence of the neural networks (Choromanska et al., 2015). Orthogonally, the performance of various gradient descent algorithms has been theoretically characterized (Nesterov, 1983). Again, these analyses make simplifying assumptions, in particular that the loss function is strictly convex, i.e. there is only a single local minimum.\nIn this work, we empirically investigated the geometry of the real loss functions for state-of-the-art networks and data sets. In addition, we investigated how popular optimization algorithms interact with these real loss surfaces. To do this, we plotted low-dimensional projections of the loss function in subspaces chosen to investigate properties of the local minima selected by different algorithms. We chose these subspaces to address the following questions:\n\u2022 What types of changes to the optimization procedure result in different local minima? \u2022 Do different optimization algorithms find qualitatively different types of local minima?\n2 RELATED WORK\n2.1 LOSS SURFACES\nThere have been several attempts to understand the loss surfaces of deep neural networks. Some have studied the critical points of the deep linear neural networks (Baldi, 1989; Baldi & Hornik,\n\u2217Work done during an internship at Janelia Research Campus\n1989; Baldi & Lu, 2012). Others further investigated the learning dynamics of the deep linear neural networks (Saxe et al., 2014). More recently, several others have attempted to study the loss surfaces of deep non-linear neural networks (Choromanska et al., 2015; Kawaguchi, 2016; Soudry & Carmon, 2016).\nOne approach is to analogize the states of neurons as the magnetics dipoles used in spherical spinglass Ising models from statistical physics (Parisi, 2016; Fyodorov & Williams, 2007; Bray & Dean, 2007). Choromanska et al. (2015) attempted to understand the loss function of neural networks through studying the random Gaussian error functions of Ising models. Recent results (Kawaguchi, 2016; Soudry & Carmon, 2016) have provided cursory evidence in agreement with the theory provided by Choromanska et al. (2015) in that they found that that there are no \u201cpoor\u201d local minima in neural networks still with strong assumptions.\nThere is some potential disconnect between these theoretical results and what is found in practice due to several strong assumptions such as the activation of the hidden units and output being independent of the previous hidden units and input data. The work of Dauphin et al. (2014) empirically investigated properties of the critical points of neural network loss functions and demonstrated that their critical points behave similarly to the critical points of random Gaussian error functions in high dimensional space. We will expose further evidence along this trajectory.\n2.2 OPTIMIZATION\nIn practice, the local minima of deep network loss functions are for the most part decent. This implies that we probably do not need to take many precautions to avoid bad local minima in practice. If all local minima are decent, then the task of finding a decent local minimum quickly is reduced to the task of finding any local minimum quickly. From an optimization perspective this implies that solely focusing on designing fast methods are of key importance for training deep networks.\nIn the literature the common method for measuring performance of optimization methods is to analyze them on nice convex quadratic functions (Polyak, 1964; Broyden, 1970; Nesterov, 1983; Martens, 2010; Erdogdu & Montanari, 2015) even though the problems are applied to non-convex problems. For non-convex problems, however, if two methods converge to different local minima their performance will be dictated on how those methods solve those two convex subproblems. It is challenging to show that one method will beat another without knowledge of the sort of convex subproblems, which is generally not known apriori. What we will explore is whether indeed are some characteristics that can found experimentally. If so, perhaps one could validate where these analytical results are valid or even improve methods for training neural networks.\n2.2.1 LEARNING PHASES\nOne of the interesting empirical observation is that we often observe is that the incremental improvement of optimization methods decreases rapidly even in non-convex problems. This behavior has been discussed as a \u201ctransient\u201d phase followed by a \u201cminimization\u201d phase (Sutskever et al., 2013)\nwhere the former finds the neighborhood of a decent local minima and the latter finds the local minima within that neighborhood. The existence of these phases implies that if certain methods are better at different phases one could create novel methods that schedule when to apply each method.\n3 EXPERIMENTAL SETUP AND TOOLS\n3.1 NETWORK ARCHITECTURES AND DATA SETS\nWe conducted experiments on three state-of-the-art neural network architectures. Network-inNetwork (NIN) (Lin et al., 2014) and the VGG(Simonyan & Zisserman, 2015) network are feedforward convolutional networks developed for image classification, and have excellent performance on the Imagenet (Russakovsky et al., 2014) and CIFAR10 (Krizhevsky, 2009) data sets. The long short-term memory network (LSTM) (Hochreiter & Schmidhuber, 1997) is a recurrent neural network that has been successful in tasks that take variable-length sequences as input and/or produce variable-length sequences as output, such as speech recognition and image caption generation. These are large networks currently used in many machine vision and learning tasks, and the loss functions minimized by each are highly non-convex.\nAll results using the feed-forward convolutional networks (NIN and VGG) are on the CIFAR10 image classification data set, while the LSTM was tested on the Penn Treebank next-word prediction data set.\n3.2 OPTIMIZATION METHODS\nWe analyzed the performance of five popular gradient-descent optimization methods for these learning frameworks: Stochastic gradient descent (SGD) (Robbins & Monro, 1951), stochastic gradient descent with momentum (SGDM), RMSprop (Tieleman & Hinton, 2012), Adadelta (Zeiler et al., 2011), and ADAM (Kingma & Ba, 2014). These are all first-order gradient descent algorithms that estimate the gradients based on randomly-grouped minibatches of training examples. One of the major differences between these algorithms is how they select the weight-update step-size at each iteration, with SGD and SGDM using fixed schedules, and RMSprop, Adadelta, and ADAM using adaptive, per-parameter step-sizes. Details are provided in Section A.2.\nIn addition to these five existing optimization methods, we compare to a new gradient descent method we developed based on the family of Runge Kutta integrators. In our experiments, we tested a second-order Runge-Kutta integrator in combination with SGD (RK2) and in combination with ADAM (ADAM&RK2). Details are provided in Section A.3).\n3.3 ANALYSIS METHODS\nSeveral of our empirical analyses are based on the technique of Goodfellow et al. (Goodfellow et al., 2015). They visualize the loss function by projecting it down to one carefully chosen dimension. They plot the value of the loss function along a set of samples along this dimension. The projection space is chosen based on important weight configurations, thus they plot the value of the loss function at linear interpolations between two weight configurations. They perform two such analyses: one in which they interpolate between the initialization weights and the final learned weights, and one in which they interpolate between two sets of final weights, each learned from different initializations.\nIn this work, we use a similar visualization technique, but choose different low-dimensional subspaces for the projection of the loss function. These subspaces are based on the initial weights as well as the final weights learned using the different optimization algorithms and combinations of them, and are chosen to answer a variety of questions about the loss function and how the different optimization algorithms interact with this loss function. In contrast, Goodfellow et al. only looked at SGDM. In addition, we explore the use of two-dimensional projections of the loss function, allowing us to better visualize the space between local minima. We do this via barycentric and bilinar interpolation for triplets and quartets of points respectively (details in Section A.1).\nWe refer to the critical points found using these variants of SGD, for which the gradient is approximately 0, as local minima. Our evidence that these are local minima as opposed to saddle points is\nsimilar to that presented in Goodfellow et al. (Goodfellow et al., 2015). If we interpolate beyond the critical point, in this one-dimensional projection, the loss increases (Fig. 10).\n3.4 TECHNICAL DETAILS\nWe used the VGG and NIN implementations from https://github.com/szagoruyko/cifar.torch.git.\nThe batch size was set to 128 and the number of epochs was set to 200. The learning rate was chosen from the discrete range between [0.2, 0.1, 0.05, 0.01] for SGD and [0.002, 0.001, 0.0005, 0.0001] for adaptive learning methods. We doubled the learning rates when we ran our augmented versions with Runge-Kutta because they required two stochastic gradient computations per epoch. We used batchnormalization and dropout to regularize our networks. All experiments were run on a 6-core Intel(R) Xeon(R) CPU @ 2.40GHz with a TITAN X.\n4 EXPERIMENTAL RESULTS\n4.1 DIFFERENT OPTIMIZATION METHODS FIND DIFFERENT LOCAL MINIMA\nWe trained the neural networks described above using each optimization method starting from the same initial weights and with the same minibatching. We computed the value of the loss function for weight vectors interpolated between the initial weights, the final weights for one algorithm, and the final weights for a second algorithm for several pairings of algorithms. The results are shown in the lower triangle of Table 1.\nFor every pair of optimization algorithms, we observe that the training loss between the final weights for different algorithms shows a sharp increase along the interpolated path. This suggests that each optimization algorithm found a different critical point, despite starting at the same initialization. We investigated the space between other triplets and quadruples of weight vectors (Figure 2 and 3), and even in these projections of the loss function, we still see that the local minima returned by different algorithms are separated by high loss weight parameters.\nDeep networks are overparameterized. For example, if we switch all corresponding weights for a pair of nodes in our network, we will obtain effectively the same network, with both the original and permuted networks outputting the same prediction for a given input. To ensure that the weight vectors returned by the different algorithms were functionally different, we compared the outpts of the networks on each example in a validation data set:\ndist(\u03b81, \u03b82) = \u221a\u221a\u221a\u221a 1 Ntest Ntest\u2211 i=1 \u2016F (xi, \u03b81)\u2212 F (xi, \u03b82)\u20162,\nwhere \u03b81 and \u03b82 are the weights learned by two different optimization algorithms, xi is the input for a validation example, and F (x, \u03b8) is the output of the network for weights \u03b8 on input x.\nWe found that, for all pairs of algorithms, the average distance between the outputs of the networks (Equation 4.1) was approximately 0.16, corresponding to a label disagreement of about 8% (upper triangle of Table 1). Given the generalization error of these networks (approximately 11%, Figure 4), the maximum disagreement we could see was 22%. Thus, these networks disagreed on a large fraction of these test examples \u2013 over 13 rd. Thus, the local minima found by different algorithms correspond to effectively different networks, not trivial reparameterizations of the same one.\n4.2 DIFFERENT OPTIMIZATION ALGORITHMS FIND DIFFERENT TYPES OF LOCAL MINIMA\nNext, we investigated whether the local minima found by the different optimization algorithms had distinguishing properties. To do this, we trained the networks with each optimization algorithm using different initial parameters. We then compared differences between runs of the same algorithm but different initializations to differences between different algorithms.\nAs shown in Figure 4(a), in terms of training accuracy, we do see some stereotypy for the optima found by different algorithms, with SGD finding local minima with the lowest training accuracy and ADAM, Rmsprop, and Adadelta finding local minima with the highest training accuracy. However, this could be attributed to SGD\u2019s asymtotically slow convergence near local minima due to the gradient diminishing near extrema. Despite this limitation, Figure 4(b) shows that the generalization accuracy of these different local minima on validation data was not significantly different between algorithms. We also did not see a relationship between the weight initialization and the validation accuracy. Thus, while these algorithms fall into different local minima, they are not different in terms of their final quality.\nWe visualized the loss surface around each of the local minima for the multiple runs. To do this, we plotted the value of the loss function between the initial and final weights for each algorithm (Figure 5(a,c)) for each run of the algorithm from a different initialization. In addition, we plotted\nthe value of the loss function between the final weights for selected pairs of algorithms for each run (Figure 5(b,d)). We see that the surfaces look strikingly similar for different runs of the same algorithm, but characteristically different for different algorithms. Thus, we found evidence that the different algorithms land in qualitatively different types of local minima.\nIn particular, we see in Figure 5(a,c) that the size of the basins around the local minima found by ADAM and ADAM&RK2 are larger than those found by SGD and RK2, i.e. that the training loss is small for a wider range of \u03b1 values. This is a relative measure, and the magnitude of the change in the weight vector is \u2206\u03b1\u2016\u03b81 \u2212 \u03b80\u2016 for a change of size \u2206\u03b1, where \u03b80 is the initial weight vector \u03b81 is the result found by a given optimization algorithm. In Figure 6, we repeat this analysis, instead showing the loss as a function of the absolute distance in parameter space:\n\u03b8(\u03bb) = \u03b81 + \u03bb \u03b80 \u2212 \u03b81 \u2016\u03b80 \u2212 \u03b81\u2016\n(1)\nWe again see that the size of the basin around the local minima varies by optimization algorithm. Note that we evaluate the loss for weight vectors beyond the initial configuration, which had a loss of 2.4.\n4.3 ANALYZING LEARNING AFTER \u201cTRANSIENT PERIOD\u201d\nRecall that, during optimization, it has been observed that there is a short \u201ctransient\u201d phase when the loss decreases rapidly and a \u201cminimization\u201d phase in which the loss decreases slowly (Section 2.2.1 and Figure 1). In this set of experiments, we investigated the effects of switching from one type of optimization method to another at various points during training, in particular at late stages of training when it is thought that a local minimum has been chosen and is only being localized. We switched from one optimization method to another 25%, 50%, and 75% of the way through training. The results are plotted in Figure 7d. We emphasize that we are not switching methods to improve performance, but rather to investigate the shape of the loss function in regions explored during the \u201cminimization\u201d phase of optimization.\nWe found that, regardless of how late we switch optimization algorithms, as shown in the rightmost column of Figure 7, the local minima found were all different. This directly disagrees with the notion that the local minimum has effectively been chosen before the \u201cminimization\u201d phase, but instead that which local minimum is found is still in flux this late in optimization. It appears that this switch from one local minimum to another happens almost immediately after the optimization method switches, with the training accuracy jumping to the characteristic accuracy for the given method within a few epochs (Figure 7, left column). Interestingly, we also see the distance between the initial and current weight vectors changes drastically after switching from one optimization\nmethod to another, and that this distance is characteristic per algorithm (Figure 7, middle column). While distance increases with training epoch for any single optimization method, it actually starts to decrease when switching from ADAM to SGD.\n4.4 EFFECTS OF BATCH-NORMALIZATION\nTo understand how batch normalization affects the types of local minima found, we performed a set of experiments comparing loss surfaces near local minima found with and without batch normal-\nization for each of the optimization methods. We visualized the surface near these local minima by interpolating between the initial weights and the final weights as well as between pairs of final weights found with different algorithms.\nWe observed clear qualitative differences between optimization with (Figure 5) and without (Figure 8) batch normalization. We see that, without batch normalization, the quality of local minimum found by a given algorithm is much more dependent on the initialization. In addition, the surfaces between different local minima are more complex in appearance: with batch normalization we see sharp unimodal jumps in performance but without batch normalization we obtain wide bumpy shapes that aren\u2019t necessarily unimodal.\nThe neural networks are typically initialized with very small parameter values (Glorot & Bengio, 2010; He et al., 2015). Instead, we trained NIN with exotic intializations such as initial parameters drawn from N (\u221210.0, 0.01) or N (\u22121.0, 1.0) and observe the loss surface behaviours. The details of results are discussed in Appendix A.5.\n5 CONCLUSIONS\nIn this work, we performed a series of empirical analyses to understand the geometry of the loss functions corresponding to deep neural networks, and how different optimization methods minimize this loss to answer the two questions posed in the introduction.\nWhat types of changes to the optimization procedure result in different local minima?\nWe found that every type of change to the optimization procedure we tested resulted in a different local minimum. Different local minima were found using the different optimization algorithms from the same initialization (Section 4.1). Even switching the optimization algorithm to another very late in optimization \u2013 during the slow \u201cmimimization\u201d portion of learning \u2013 resulted in a different local minimum (Section 4.3). The quality of the local minima found, in terms of training and generalization error, is similar. These different local minima were not equivalent, and made mistakes on different test examples (Section 4.1). Thus, they were not trivially different local minima, as would occur if nodes in internal layers of the network were permuted. We observed that the quality of these local minima was only consistently good when we used batch normalization for regularization. Without batch normalization, the quality of the critical points found depended on the initialization, and some solutions found were not as good as others. Our observations are in contrast to the conclusions of Goodfellow et al., i.e. that local minima are not a problem in deep learning because, in the region of the loss function explored by SGD algorithms, the loss function is well-behaved (Goodfellow et al., 2015). Instead, our observations are more consistent with the explanation that the local minima found by popular SGD optimization methods are almost all good (Choromanska et al., 2015; Kawaguchi, 2016; Soudry & Carmon, 2016).\nDo different optimization algorithms find qualitatively different types of local minima?\nInterestingly, we found that, while the local minima found by the same optimization algorithm from different initializations were different, the shape of the loss function around these local minima was strikingly similar, and was a characteristic of the optimization algorithm. In particular, we found that the size of the basin around ADAM-based optimization was larger than that around vanilla SGD (Section 4.2). A large basin is related to a large margin, as small changes in the weight vector will not affect the training error, and perhaps could have some implications for generalization error. In our experiments, however, we did not observe better generalization error for ADAM than SGD. Questions for potential future research are why the shapes of the loss functions around different local minima found by the same algorithm are so similar, and what the practical implications of this are.\nA SUPPLEMANTARY MATERIALS\nA.1 3D VISUALIZATION\nGoodfellow et al. (2015) introduced the idea of visualizing 1D subspace of the loss surface between the parameters. Here, we propose to visualize loss surface in 3D space through interpolating over three and four vertices.\nLinear Interpolation Given two parameters \u03b81 and \u03b82, \u03b8i = \u03b1\u03b81 + (1\u2212 \u03b1)\u03b82, \u2200\u03b1 \u2208 [0, 1]. (2)\nBilinear Interpolation Given four parameters \u03b80, \u03b81, \u03b82, and \u03b83, \u03c6i = \u03b1\u03b81 + (1\u2212 \u03b1)\u03b82 (3) \u03d5i = \u03b1\u03b83 + (1\u2212 \u03b1)\u03b84 (4) \u03b8j = \u03b2\u03c6i + (1\u2212 \u03b2)\u03d5i (5)\nfor all \u03b1 \u2208 [0, 1] and \u03b2 \u2208 [0, 1].\nBarycentric Interpolation Given four parameters \u03b80, \u03b81, and \u03b82, let d1 = \u03b81 \u2212 \u03b80 and d2 = \u03b82 \u2212 \u03b80. Then, the formulation of the interpolation is\n\u03c6i = \u03b1d1 + \u03b80 (6) \u03d5i = \u03b1d2 + \u03b80 (7) \u03b8j = \u03b2\u03c6i + (1\u2212 \u03b2)\u03d5i (8)\nfor all \u03b1 \u2208 [0, 1] and \u03b2 \u2208 [0, 1].\nA.2 OPTIMIZATION METHODS\nA.2.1 STOCHASTIC GRADIENT DESCENT\nIn many deep learning applications both the number of parameters and quantity of input data points can be quite large. This makes the full evaluation of U(\u03b8) be prohibitively expensive. A standard technique for aleviating computational loadis to apply an stochastic approximation to the gradient Robbins & Monro (1951). More precisely, one approximates U by a subset of n data points, denoted by {\u03c3j}Nj=1 at each timestep:\nUn(\u03b8) = 1\nn n\u2211 j=1 `(\u03b8,x\u03c3j ) ' 1 N N\u2211 i=1 `(\u03b8,xi) = U(\u03b8) (9)\nOf course this approximation also carries over to the gradient, which is of vital importance to optimization techniques:\n\u2207Un(\u03b8) = 1 n n\u2211 j=1 \u2207`(\u03b8,x\u03c3j ) ' \u2207U(\u03b8) (10)\nThis method is what is commonly called Stochastic Gradient Descent or SGD. So long as the data is distributed nicely the approximation error of Un should be sufficiently small such that not only will SGD still behave like normal GD , but it\u2019s wall clock time for to converge should be significantly lower as well.\nUsually one uses the stochastic gradient rather than the true gradient, but the inherent noisiness must be kept in mind. In what follows we will always mean the stochastic gradient.\nA.2.2 MOMENTUM\nIn order to aleviate both noise in the input data as well as noise from stochasticity used in computing quantities one often maintains history of previous evaluations. In order to only require one extra variable one usually stores variables of the form\nE[F ]t = \u03b1Ft + \u03b2E[F ]t\u22121. (11)\nwhere Ft is some value changing over time and E[F ]t is the averaged quantity.\nAn easy scheme to apply this method to is to compute a rolling weighted average of gradients such as E[g]t = (1\u2212 \u03b1)gt + \u03b1E[g]t\u22121 but there will be other uses in the future.\nA.2.3 PERTINENT METHODS\nWith the aforementioned tools there are a variety of methods that can be constructed. We choose to view these algorithms as implementations of Explicit Euler on a variety of different vector fields to remove the ambiguity between \u03b7 and gt. We therefore can define a method by the vector field Xt that explicit Euler is applied to with a single \u03b7 that is never changed.\nSGD with Momentum (SGDM) By simply applying momentum to gt one obtains this stabilized stochastic version of gradient descent:\nXt = \u2212E[g]t. (12) This is the most fundamental method that is used in practice and the basis for everything that follows.\nAdagrad Adagrad rescalesXt by summing up the sqaures of all previous gradients in a coefficientwise fashion:\nXt = \u2212 gt\u221a\u2211t\ni=1 g 2 i +\n. (13)\nHere is simply set to some small positive value to prevent division-by-zero. In the future we will neglect this term in denominators because it is always necessary.\nThe concept is to accentuate variations in gt, but because the denominator is monotonically nondecreasing over time this method is doomed to retard its own progress over time. The denominator can also be seen as a form of momentum where \u03b1 and \u03b2 are both set to 1.\nRmsprop A simple generalization of ADAGrad is to simply allow for \u03b1 and \u03b2 to be changed from 1. In particular one usually chooses a \u03b2 less than 1, and presumably \u03b1 = 1\u2212 \u03b2. Thus one arrives at a method where the effects of the distance history are diminished:\nXt =\u2212 gt\u221a E[g2]t . (14)\nAdadelta Adadelta adds another term to RMSprop in order to guarantee that the magnitude of X is balanced with gt Zeiler et al. (2011). More precisely it maintains\nXt\u221a E[X 2t ] = \u2212 gt\u221a E[g2t ]\n(15)\nwhich results in the following vector field: Xt =\u2212 \u221a E[X 2t ]\u221a E[g2t ] gt. (16)\nand \u03b7 is set to 1.\nADAM By applying momentum to both gt and g2t one arrives at what is called ADAM. This is often considered a combination of SGDM + RMSprop,\nXt = ct E[g]t\u221a E[g2]t . (17)\nct =\n\u221a 1\u2212\u03b2t2\n1\u2212\u03b2t1 is the initialization bias correction term with \u03b21, \u03b22 \u2208 [0, 1) being the \u03b2 parameters\nused in momentum for g and g2 respectively. Initialization bias is caused by the history of the momentum variable being initially set to zero.\nA.3 RUNGE KUTTA\nRunge-Kutta methods Butcher (1963) are a broad class of numerical integrators categorized by their truncation error. Because the ordinary differential equations Runge-Kutta methods solve generalize gradient descent, our augmentation is quite straightforward. Although our method applies to all explicit Runge-Kutta methods we will only describe second order methods for simplicity.\nThe general form of second-order explicit Runge-Kutta on a time-independent vector field is\n\u03b8t+1 = \u03b8t + (a1k1 + a2k2)h (18) k1 = X (\u03b8t) (19) k2 = X (\u03b8t + q1hk1) (20)\nwhere a1, a2, and q1 are parameters that define a given Runge-Kutta method. Table 3 refers to the parameters used for the different Runge-Kutta variants we use in our experiments.\nA.3.1 AUGMENTING OPTIMIZATION WITH RUNGE KUTTA\nFor a given timestep, explicit integrators can be seen as a morphism over vector fields X \u2192 X\u0304 h. For a gradient gt = \u2207\u03b8U we can solve a modified RK2 gradient g\u0304t in the following fashion:\n\u03b8t+1 = \u03b8t + g\u0304th = Advect rk2 g (\u03b8, h) (21)\nrearranged with respect to g\u0304t\ng\u0304t = Advectrk2g (\u03b8, h)\u2212 \u03b8t\nh (22)\n= \u03b8t + (a1k1 + a2k2)h\u2212 \u03b8t\nh (23)\n= (a1k1 + a2k2). (24) If we simply substitute the gradient gt with g\u0304t one obtains an RK2-augmented optimization technique.\nA.4 EXPERIMENTS WITH RUNGE-KUTTA INTEGRATOR\nThe results in Figure 9 illustrates that, with the exception of the Midpoint method, stochastic RungeKutta methods outperform SGD. \u201cSGD x2\u201d is the stochastic gradient descent with twice of the learning rate of \u201cSGD\u201d. From the figure, we observe that the Runge-Kutta methods perform even better with half the number of gradient computed by SGD. The reason is because SGD has the accumulated truncated error of O(h) while second-order Runge-Kutta methods have the accumulated truncated error of O(h2).\nUnfortunately, ADAM outperforms ADAM+RK2 methods. We speculate that this is because the way how ADAM\u2019s renormalization of input gradients in conjunction with momentum eliminates the value added by using our RK-based descent directions.\nA.5 EFFECTS OF BATCH-NORMALIZATION AND EXTREME INITIALIZATIONS\nThe neural networks are typically initialized with very small parameter values (Glorot & Bengio, 2010; He et al., 2015). Instead, we trained NIN with exotic intializations such as initial parameters drawn from N (\u221210.0, 0.01) or N (\u22121.0, 1.0) and observe the loss surface behaviours. The results are shown in Figure 11. We can see that NIN without BN does not train at all with any of these initializations. Swirszcz et al. (2016) mentioned that bad performance of neural networks trained with these initializations are due to finding a bad local minima. However, we see that loss surface region around these initializations are plateau 1 rather than a bad local minima as shown in Figure 11b. On\n1We used same initializations as (Swirszcz et al., 2016) but we trained different neural networks with SGD on a different dataset. We used NIN and CIFAR10 and Swirszcz et al. (2016) used smaller neural network and MNIST.\nthe other hand, NIN with BN does train slowly over time but finds a local minima. This implies that BN redeems the ill-posed loss surface (plateau region). Nevertheless, the local minima it found was not good as when the parameters were initialized with small values. However, it is not totally clear whether this is due to difficulty of training or due to falling in a bad local minima.\nA.6 SWITCHING OPTIMIZATION METHODS\n", "cats": {"READ": 1.0, "DONTREAD": 0.0}, "reviews": [{"IS_META_REVIEW": true, "comments": "This paper provides an extensive analysis of the error loss function for different optimization methods. The presentation is well done and informative. The experimental procedure is clarified sufficiently well. Theoretical evaluations like this are crucial for a wide range of applications and help to better understand and improve the convergence behavior for a given system.\n\nPros:\n\n- Important analysis\n- Good visualizations\n\nCons:\n\n- The paper describes mostly the observation that the optima vary for different methods, however doesn't attempt to explain why it happens and how to solve it (aside from batch-norm)\n- Some fonts are very small (e.g. Fig. 5)"}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "The paper proposes an empirical investigation of the energy landscape of deep neural networks using several stochastic optimization algorithms.\n \n The extensive experiments conducted by the authors are interesting and inspiring. However, several reviewers expressed major concerns pointing out the limitations of a experimental investigation on real-world datasets. The paper would benefit from additional experiments on simulated datasets. This would allow to complement the experimental analysis. Furthermore, theoretical analysis and analytical derivations, consistent with the simulated datasets, could shed light on the experimental results and allow to make more precise claims. \n \n A revised version, following the reviewers' suggestions, will result in a stronger submission to a future venue.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"TITLE": "From an interested reader: many issues to be ironed out", "OTHER_KEYS": "George Philipp", "comments": "First of all, I would like to thank the authors for putting this much work into a necessary but somewhat tedious topic. While I think the paper is somewhat below the standard of a conference paper (see detailed comments below), I would definitely love to see a version of this paper published with some of the issues ironed out. I also agree with many of the points raised by other reviewers and will not repeat them here.\n\nMajor points:\n\n-- \"As we saw in the previous section, the minima of deep network loss functions are for the most part decent.\"\n\nAll you said in the previous section was that theory shows that there are no bad minima under \"strong assumptions\". There is no practical proof that minima do not vary in quality.\n\n-- \"This implies that we probably do not need to take many precautions to avoid bad minima in practice. If all minima are decent, then the task of finding a \"decent minima quickly\" is reduced to the task of finding any minima quickly.\"\n\nFirst of all, as one of the reviewers pointed out, we are never guaranteed in practice to actually reach a local minimum. We could always hit a region of the objective function where the algorithm makes essentially no further progress. The final error level, in practice, actually does depend significantly on many factors such as (i) optimization algorithm (ii) learning rate schedule (iii) initialization of weights (iv) presence of unsupervised pretraining (v) whether neurons are added or eliminated during training etc. etc. Therefore, the task of optimizing neural networks is far from being \"reduced to finding any minima quickly\".\n\n-- Figure 1\n \nI don't like Figure 1, because it suggests to me that you diagnosed exactly where the transition between the two phases happened, which I don't think you did.\n \nAlso, the concept of having a fast-decaying error followed by a slow-decaying error is simple enough for readers to understand without a dedicated graph. Minor point on presentation: The red brace is positioned lower in the figure than the blue brace and the braces don't join up horizontally. Please be more careful.\n\n-- Misuse of the transient phase / minimization phase concept\n\nIn section 4.3, you talk about the transient and minimization phase of optimization. However, you have no way of diagnosing when or if your algorithm reaches the minimization phase. You seem to think that the minimization phase is simply the part of the optimization process where the error decreases slowly. AFAIK, this is not the case. The minimization phase is where the optimization algorithm enters the vicinity of the local minimum that can be approximated by the second-order Taylor expansion. For this to even occur, one would have to verify, for example, that the learning rate is small enough. You change the algorithm after 25%, 50% and 75% of training, but these points seem arbitrary. What is the minimization phase was reached at 99%, or 10 epochs after you decided to stop training?\n\n-- Only 1 dataset\n \nYou run most experiments on only 1 dataset (CIFAR). Please replicate with at least one more dataset.\n \n-- Many figures are unclear\n \nFor each figure, the following information are relevant: network used; dataset used; learning rate used; batch norm yes / no; whether figure shows train, test, or validation error. It should be easy for the reader to ascertain this information for all figures, not just for some.\n \n-- You say at the beginning of section 4.1 that each algorithm finds a different minimum as if this is a significant finding. However, this is obvious because the updates taken by these algorithms vary wildly. Keep in mind that there is an exponentially large number of minima. The probability of different algorithms choosing the same minimum is essentially zero because of their sheer number. The same would be true if you even shift the learning rate slightly or use a different random seed for minibatch generation etc. etc.\n  \n-- Lack of confidence intervals\n\nThe value of Figures 1, 2, 3 and 6 is limited is because it is unclear how these plots would change if the random seed were changed. We only get information for a single weight initialization and a single minibatch sequence. While figures 5 and 7 can be used to try and infer what confidence intervals around plots in figures 1, 2, 3 and 6 might look like, I think those confidence intervals should still be shown for at least a subset of the configurations presented.\n\n-- Lack of information regarding learning rate\n\nThere is big question mark left open regarding how all your results would change if different learning rates were used. You don't even tell us how you chose the learning rates from the intervals you gave in section 3.4.\n\n-- Lack of information regarding the absolute distance of interpolated points\n\nIn most figures, you interpolate between two or three trained weight configuration. However, you do not say how far the interpolated points are apart. This is highly significant, because if points are close together and there is a big \"hump\" between them, it means that those points are more \"brittle\" than if they are far apart and there is a big \"hump\" between them.\n \nMinor points:\n \n-- LSTM is not a fixed network architecture like NiN or VGG, but a layer type. LSTM would be equivalent to CNN. Also, the VGG paper has multiple versions of VGG. You should specify which one you used.\n  \n-- The font size for the legends in the upper triangle of Table 1 is too small. You can't just write \"best viewed in zoom\" in the table caption and pretend that somehow fixes the problem. Personally, I prefer no legend over an unreadable legend.", "IS_META_REVIEW": false, "RECOMMENDATION": 4, "DATE": "23 Jan 2017", "REVIEWER_CONFIDENCE": 3}, {"TITLE": "Interesting paper, but the message is not clear", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "The paper is dedicated to better understanding the optimization landscape in deep learning, in particular when explored with different optimization algorithms, and thus it also characterizes the behavior of these algorithms. It heavily re-uses the approach of Goodfellow et al. (2015). I find it hard to understand the contributions of the paper, for example: is it surprising that different algorithms reach different solutions when starting from the same initialization? It would be useful if the authors build such basic intuition in the paper. I also did not receive a clear answer to the question I posed to reviewers regarding clarifying how does the findings of the paper can contribute to future works on optimization in deep learning. And this is what I find fundamentally missing. So for example, there are probably plenty of ways to modify approach of Goodfellow et al. (2015), and similar works, and come up with interesting visualization methods for deep learning - but the question is: how is this helpful in terms of designing better algorithms, gaining more intuition how the optimization surface looks like in general, etc.? This is an interesting paper, though I am fairly confident it is a better fit for the journal than this conference. \n\nIt would be interesting and instructive, even for sanity check, to plot the eigenspectra of the solutions recovered by the algorithms to see the order of critical points recovered.", "IS_META_REVIEW": false, "RECOMMENDATION": 4, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "No Title", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "I appreciate the work but I do not think the paper is clear enough. \nMoreover, the authors say \"local minimia\" ~70 times but do not show (except for Figure 11?) that the solutions found are not necessarily local minima. \nThe authors do not talk about that fact that slices of a non-convex problem can look like the ones they show. \nIt is well-known that the first-order methods may just fail to deal with certain non-convex ill-conditioned problems even in low-dimensional noiseless cases, the place/solution where they fail to make progress is not necessarily a local minimum. \nSome sentences like the one given below suggest that the study is too superficial:  \n\"One of the interesting empirical observation is that we often observe is that the incremental improvement\nof optimization methods decreases rapidly even in non-convex problems.\"", "IS_META_REVIEW": false, "RECOMMENDATION": 4, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "A good evaluation of optimization methods", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "This paper provides an extensive analysis of the error loss function for different optimization methods. The presentation is well done and informative. The experimental procedure is clarified sufficiently well. Theoretical evaluations like this are crucial for a wide range of applications and help to better understand and improve the convergence behavior for a given system.\n\nPros:\n\n- Important analysis\n- Good visualizations\n\nCons:\n\n- The paper describes mostly the observation that the optima vary for different methods, however doesn't attempt to explain why it happens and how to solve it (aside from batch-norm)\n- Some fonts are very small (e.g. Fig. 5)\n", "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "15 Dec 2016 (modified: 26 Jan 2017)", "REVIEWER_CONFIDENCE": 4}, {"DATE": "03 Dec 2016", "TITLE": "conclusion", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2"}, {"DATE": "02 Dec 2016", "TITLE": "local minima", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3"}, {"DATE": "29 Nov 2016", "TITLE": "Clarifications", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1"}, {"IS_META_REVIEW": true, "comments": "This paper provides an extensive analysis of the error loss function for different optimization methods. The presentation is well done and informative. The experimental procedure is clarified sufficiently well. Theoretical evaluations like this are crucial for a wide range of applications and help to better understand and improve the convergence behavior for a given system.\n\nPros:\n\n- Important analysis\n- Good visualizations\n\nCons:\n\n- The paper describes mostly the observation that the optima vary for different methods, however doesn't attempt to explain why it happens and how to solve it (aside from batch-norm)\n- Some fonts are very small (e.g. Fig. 5)"}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "The paper proposes an empirical investigation of the energy landscape of deep neural networks using several stochastic optimization algorithms.\n \n The extensive experiments conducted by the authors are interesting and inspiring. However, several reviewers expressed major concerns pointing out the limitations of a experimental investigation on real-world datasets. The paper would benefit from additional experiments on simulated datasets. This would allow to complement the experimental analysis. Furthermore, theoretical analysis and analytical derivations, consistent with the simulated datasets, could shed light on the experimental results and allow to make more precise claims. \n \n A revised version, following the reviewers' suggestions, will result in a stronger submission to a future venue.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"TITLE": "From an interested reader: many issues to be ironed out", "OTHER_KEYS": "George Philipp", "comments": "First of all, I would like to thank the authors for putting this much work into a necessary but somewhat tedious topic. While I think the paper is somewhat below the standard of a conference paper (see detailed comments below), I would definitely love to see a version of this paper published with some of the issues ironed out. I also agree with many of the points raised by other reviewers and will not repeat them here.\n\nMajor points:\n\n-- \"As we saw in the previous section, the minima of deep network loss functions are for the most part decent.\"\n\nAll you said in the previous section was that theory shows that there are no bad minima under \"strong assumptions\". There is no practical proof that minima do not vary in quality.\n\n-- \"This implies that we probably do not need to take many precautions to avoid bad minima in practice. If all minima are decent, then the task of finding a \"decent minima quickly\" is reduced to the task of finding any minima quickly.\"\n\nFirst of all, as one of the reviewers pointed out, we are never guaranteed in practice to actually reach a local minimum. We could always hit a region of the objective function where the algorithm makes essentially no further progress. The final error level, in practice, actually does depend significantly on many factors such as (i) optimization algorithm (ii) learning rate schedule (iii) initialization of weights (iv) presence of unsupervised pretraining (v) whether neurons are added or eliminated during training etc. etc. Therefore, the task of optimizing neural networks is far from being \"reduced to finding any minima quickly\".\n\n-- Figure 1\n \nI don't like Figure 1, because it suggests to me that you diagnosed exactly where the transition between the two phases happened, which I don't think you did.\n \nAlso, the concept of having a fast-decaying error followed by a slow-decaying error is simple enough for readers to understand without a dedicated graph. Minor point on presentation: The red brace is positioned lower in the figure than the blue brace and the braces don't join up horizontally. Please be more careful.\n\n-- Misuse of the transient phase / minimization phase concept\n\nIn section 4.3, you talk about the transient and minimization phase of optimization. However, you have no way of diagnosing when or if your algorithm reaches the minimization phase. You seem to think that the minimization phase is simply the part of the optimization process where the error decreases slowly. AFAIK, this is not the case. The minimization phase is where the optimization algorithm enters the vicinity of the local minimum that can be approximated by the second-order Taylor expansion. For this to even occur, one would have to verify, for example, that the learning rate is small enough. You change the algorithm after 25%, 50% and 75% of training, but these points seem arbitrary. What is the minimization phase was reached at 99%, or 10 epochs after you decided to stop training?\n\n-- Only 1 dataset\n \nYou run most experiments on only 1 dataset (CIFAR). Please replicate with at least one more dataset.\n \n-- Many figures are unclear\n \nFor each figure, the following information are relevant: network used; dataset used; learning rate used; batch norm yes / no; whether figure shows train, test, or validation error. It should be easy for the reader to ascertain this information for all figures, not just for some.\n \n-- You say at the beginning of section 4.1 that each algorithm finds a different minimum as if this is a significant finding. However, this is obvious because the updates taken by these algorithms vary wildly. Keep in mind that there is an exponentially large number of minima. The probability of different algorithms choosing the same minimum is essentially zero because of their sheer number. The same would be true if you even shift the learning rate slightly or use a different random seed for minibatch generation etc. etc.\n  \n-- Lack of confidence intervals\n\nThe value of Figures 1, 2, 3 and 6 is limited is because it is unclear how these plots would change if the random seed were changed. We only get information for a single weight initialization and a single minibatch sequence. While figures 5 and 7 can be used to try and infer what confidence intervals around plots in figures 1, 2, 3 and 6 might look like, I think those confidence intervals should still be shown for at least a subset of the configurations presented.\n\n-- Lack of information regarding learning rate\n\nThere is big question mark left open regarding how all your results would change if different learning rates were used. You don't even tell us how you chose the learning rates from the intervals you gave in section 3.4.\n\n-- Lack of information regarding the absolute distance of interpolated points\n\nIn most figures, you interpolate between two or three trained weight configuration. However, you do not say how far the interpolated points are apart. This is highly significant, because if points are close together and there is a big \"hump\" between them, it means that those points are more \"brittle\" than if they are far apart and there is a big \"hump\" between them.\n \nMinor points:\n \n-- LSTM is not a fixed network architecture like NiN or VGG, but a layer type. LSTM would be equivalent to CNN. Also, the VGG paper has multiple versions of VGG. You should specify which one you used.\n  \n-- The font size for the legends in the upper triangle of Table 1 is too small. You can't just write \"best viewed in zoom\" in the table caption and pretend that somehow fixes the problem. Personally, I prefer no legend over an unreadable legend.", "IS_META_REVIEW": false, "RECOMMENDATION": 4, "DATE": "23 Jan 2017", "REVIEWER_CONFIDENCE": 3}, {"TITLE": "Interesting paper, but the message is not clear", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "The paper is dedicated to better understanding the optimization landscape in deep learning, in particular when explored with different optimization algorithms, and thus it also characterizes the behavior of these algorithms. It heavily re-uses the approach of Goodfellow et al. (2015). I find it hard to understand the contributions of the paper, for example: is it surprising that different algorithms reach different solutions when starting from the same initialization? It would be useful if the authors build such basic intuition in the paper. I also did not receive a clear answer to the question I posed to reviewers regarding clarifying how does the findings of the paper can contribute to future works on optimization in deep learning. And this is what I find fundamentally missing. So for example, there are probably plenty of ways to modify approach of Goodfellow et al. (2015), and similar works, and come up with interesting visualization methods for deep learning - but the question is: how is this helpful in terms of designing better algorithms, gaining more intuition how the optimization surface looks like in general, etc.? This is an interesting paper, though I am fairly confident it is a better fit for the journal than this conference. \n\nIt would be interesting and instructive, even for sanity check, to plot the eigenspectra of the solutions recovered by the algorithms to see the order of critical points recovered.", "IS_META_REVIEW": false, "RECOMMENDATION": 4, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "No Title", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "I appreciate the work but I do not think the paper is clear enough. \nMoreover, the authors say \"local minimia\" ~70 times but do not show (except for Figure 11?) that the solutions found are not necessarily local minima. \nThe authors do not talk about that fact that slices of a non-convex problem can look like the ones they show. \nIt is well-known that the first-order methods may just fail to deal with certain non-convex ill-conditioned problems even in low-dimensional noiseless cases, the place/solution where they fail to make progress is not necessarily a local minimum. \nSome sentences like the one given below suggest that the study is too superficial:  \n\"One of the interesting empirical observation is that we often observe is that the incremental improvement\nof optimization methods decreases rapidly even in non-convex problems.\"", "IS_META_REVIEW": false, "RECOMMENDATION": 4, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "A good evaluation of optimization methods", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "This paper provides an extensive analysis of the error loss function for different optimization methods. The presentation is well done and informative. The experimental procedure is clarified sufficiently well. Theoretical evaluations like this are crucial for a wide range of applications and help to better understand and improve the convergence behavior for a given system.\n\nPros:\n\n- Important analysis\n- Good visualizations\n\nCons:\n\n- The paper describes mostly the observation that the optima vary for different methods, however doesn't attempt to explain why it happens and how to solve it (aside from batch-norm)\n- Some fonts are very small (e.g. Fig. 5)\n", "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "15 Dec 2016 (modified: 26 Jan 2017)", "REVIEWER_CONFIDENCE": 4}, {"DATE": "03 Dec 2016", "TITLE": "conclusion", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2"}, {"DATE": "02 Dec 2016", "TITLE": "local minima", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3"}, {"DATE": "29 Nov 2016", "TITLE": "Clarifications", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1"}]}
{"text": "WHAT DOES IT TAKE TO GENERATE NATURAL TEXTURES?\n1 INTRODUCTION\nDuring the last two years several different approaches towards natural image generation have been suggested, among them generative adversarial networks (Goodfellow et al., 2014; Chen et al., 2016), probabilistic generative models like the conditional PixelCNN (van den Oord et al., 2016b;a) or maximum entropy models that rely on the representations of deep neural networks (e.g. Gatys et al., 2015b; Johnson et al., 2016; Ulyanov et al., 2016). The latter approach has been particularly groundbreaking for artistic style transfer and natural texture generation (e.g. Gatys et al., 2015a;b) and has the potential to uncover the regularities that supervisedly trained deep neural networks infer from natural images.\nFor the sake of clarity and concreteness, this paper will focus on natural texture synthesis. Parametric texture models aim to uniquely describe each texture by a set of statistical measurements that are taken over the spatial extent of the image. Each image with the same spatial summary statistics should be perceived as the same texture. Consequently, synthesizing a texture corresponds to finding a new image that reproduces the summary statistics inferred from the reference texture. Starting from Nth-order joint histograms of the pixels by Julesz (1962), many different statistical measures have been proposed (see e.g. Heeger & Bergen, 1995; Portilla & Simoncelli, 2000). The quality of the synthesized textures is usually determined by human inspection; the synthesis is successful if a human observer cannot tell the reference texture from the synthesized ones.\nThe current state of the art in parametric texture modeling (Gatys et al., 2015a) employs the hierarchical image representation in a deep 19-layer convolutional network (Simonyan & Zisserman (2014); in the following referred to as VGG network) that was trained on object recognition in natural images(Russakovsky et al. (2015)). In this model textures are described by the raw correlations between feature activations in response to the texture image from a collection of network layers (see section 5 for details). Since its initial reception several papers explored which additional elements or constraints can further increase the perceptual quality of the generated textures (Berger & Memisevic, 2016; Liu et al., 2016; Aittala et al., 2016). In this work we go the opposite way and ask which elements of the original texture synthesis algorithm (Gatys et al., 2015a) are absolutely indispensable.\nIn particular two aspects have been deemed critical for natural texture synthesis: the hierarchical multi-layer representation of the textures, and the supervised training of the feature spaces. Here we show that neither aspect is imperative for texture modeling and that in fact a single convolutional layer with random features can synthesize textures that often rival the perceptual quality of Gatys et al. (2015a). This is in contrast to earlier reports (Gatys et al., 2015a) that suggested that networks with random weights fail to generate perceptually interesting images. We suggest that this discrepancy originates from a more elaborate tuning of the optimization procedure (see section 4).\nOur main contributions are:\n\u2022 We present a strong minimal baseline for parametric texture synthesis that solely relies on a single-layer network and random, data-independent filters. \u2022 We show that textures synthesized from the baseline are of high quality and often rival\nstate-of-the-art approaches, suggesting that the depth and the pre-training of multi-layer image representations are not as indispensable for natural image generation as has previously been thought. \u2022 We test and compare a wide range of single-layer architectures with different filter-sizes and\ndifferent types of filters (random, hand-crafted and unsupervisedly learnt filters) against the state-of-the-art texture model by Gatys et al. (2015a). \u2022 We utilize a quantitative texture quality measure based on the synthesis loss in the VGG-\nbased model (Gatys et al., 2015a) to replace the common-place evaluation of texture models through qualitative human inspection. \u2022 We discuss a formal generalization of maximum entropy models to account for the natural\nvariability of textures with limited spatial extent.\n2 CONVOLUTIONAL NEURAL NETWORK\nIf not mentioned otherwise, all our models employ single-layer CNNs with standard rectified linear units (ReLUs) and convolutions with stride one, no bias and padding (f \u2212 1)/2 where f is the filter-size. This choice ensures that the spatial dimension of the output feature maps is the same as the input. All networks except the last one employ filters of size 11\u00d7 11\u00d7 3 (filter width \u00d7 filter height \u00d7 no. of input channels), but the number of feature maps as well as the selection of the filters differ:\n\u2022 Fourier-363: Each color channel (R, G, B) is filtered separately by each element Bi \u2208 R11\u00d711 of the 2D Fourier basis (11\u00d711 = 121 feature maps/channel), yielding 3\u00b7121 = 363 feature maps in total. More concretely, each filter can be described as the tensor product Bi \u2297 ek where the elements of the unit-norm ek \u2208 R3 are all zero except one. \u2022 Fourier-3267: All color channels (R, G, B) are filtered simultaneously by each element Bi of the 2D Fourier basis but with different weighting terms wR, wG, wB \u2208 [1, 0,\u22121], yielding 3 \u00b7 3 \u00b7 3 \u00b7 121 = 3267 feature maps in total. More concretely, each filter can be described by the tensor product Bi \u2297 [wR, wG, wB ]. \u2022 Kmeans-363: We randomly sample and whiten 1e7 patches of size 11 \u00d7 11 from the\nImagenet dataset (Russakovsky et al., 2015), partition the patches into 363 clusters using k-means (Rubinstein et al., 2009), and use the cluster means as convolutional filters. \u2022 Kmeans-3267: Same as Kmeans-363 but with 3267 clusters. \u2022 Kmeans-NonWhite-363/3267: Same as Kmeans-363/3267 but without whitening of the\npatches. \u2022 Kmeans-Sample-363/3267: Same as Kmeans-363/3267, but patches are only sampled\nfrom the target texture. \u2022 PCA-363: We randomly sample 1e7 patches of size 11 \u00d7 11 from the Imagenet dataset\n(Russakovsky et al., 2015), vectorize each patch, perform PCA and use the set of principal axes as convolutional filters. \u2022 Random-363: Filters are drawn from a uniform distribution according to (Glorot & Bengio,\n2010), 363 feature maps in total. \u2022 Random-3267: Same as Random-363 but with 3267 feature maps.\nOriginal Fourier-363, 11x11 K-means-363, 11x11 K-samples-363, 11x11 K-NW-363, 11x11 Random-363, 11x11 PCA-363, 11x11\nGatys et al. Fourier-3267, 11x11 K-means-3267, 11x11 K-samples-3267, 11x11 K-NW-3267, 11x11 Random-3267, 11x11 Random multi-scale\nThe networks were implemented in Lasagne (Dieleman et al., 2015; Theano Development Team, 2016). We remove the DC component of the inputs by subtracting the mean intensity in each color channel (estimated over the Imagenet dataset (Russakovsky et al., 2015)).\n3 TEXTURE MODEL\nThe texture model closely follows (Gatys et al., 2015a). In essence, to characterise a given vectorised texture x \u2208 RM , we first pass x through the convolutional layer and compute the output activations. The output can be understood as a non-linear filter bank, and thus its activations form a set of filtered images (so-called feature maps). For N distinct feature maps, the rectified output activations can be\ndescribed by a matrix F \u2208 RN\u00d7M . To capture the stationary structure of the textures, we compute the covariances (or, more precisely, the Gramian matrix) G \u2208 RN\u00d7N between the feature activations F by averaging the outer product of the point-wise feature vectors,\nGij = 1\nM M\u2211 m=1 FimFjm. (1)\nWe will denote G(x) as the Gram matrix of the feature activations for the input x. To determine the relative distance between two textures x and y we compute the euclidean distance of the normalized Gram matrices,\nd(x,y) = 1\u221a\u2211\nm,n Gmn(x)2 \u221a\u2211 m,n Gmn(y)2 N\u2211 i,j=1 (Gij(x)\u2212Gij(y))2 . (2)\nTo compare with the distance in the raw pixel values, we compute\ndp(x,y) = 1\u221a\u2211 m x2m \u221a\u2211 m y2m N\u2211 i=1 (xi \u2212 yi)2 . (3)\n4 TEXTURE SYNTHESIS\nTo generate a new texture we start from a uniform noise image (in the range [0, 1]) and iteratively optimize it to match the Gram matrix of the reference texture. More precisely, let G(x) be the Gram matrix of the reference texture. The goal is to find a synthesised image x\u0303 such that the squared distance between G(x) and the Gram matrix G(x\u0303) of the synthesized image is minimized, i.e.\nx\u0303 = argmin y\u2208RM E(y), (4)\nE(y) = 1\u2211N\ni,j=1Gij(x) 2 N\u2211 i,j=1 ( Gij(x)\u2212Gij(y) )2 . (5)\nThe gradient \u2202E(y)/\u2202y of the reconstruction error with respect to the image can readily be computed using standard backpropagation, which we then use in conjunction with the L-BFGS-B algorithm (Jones et al., 2001\u2013) to solve (4). We leave all parameters of the optimization algorithm at their default value except for the maximum number of iterations (2000), and add a box constraints with range [0, 1]. In addition, we scale the loss and the gradients by a factor of 107 in order to avoid early stopping of the optimization algorithm.\n5 TEXTURE EVALUATION\nEvaluating the quality of the synthesized textures is traditionally performed by human inspection. Optimal texture synthesis should generate samples that humans perceive as being the same texture as the reference. The high quality of the synthesized textures by (Gatys et al., 2015a) suggests that the summary statistics from multiple layers of VGG can approximate the perceptual metric of humans. Even though the VGG texture representation is not perfect, this allows us to utilize these statistics as a more objective quantification of texture quality.\nFor all details of the VGG-based texture model see (Gatys et al., 2015a). Here we use the standard 19-layer VGG network (Simonyan & Zisserman, 2014) with pretrained weights and average- instead of max-pooling1. We compute a Gram matrix on the output of each convolutional layer that follows a pooling layer. Let G`(.) be the Gram matrix on the activations of the `-th layer and\nE`(y) = 1\u2211N\ni,j=1G ` ij(x) 2 N\u2211 i,j=1 ( G`ij(x)\u2212G`ij(y) )2 . (6)\nthe corresponding relative reconstruction cost. The total reconstruction cost is then defined as the average distance between the reference Gram matrices and the synthesized ones, i.e.\nE(y) = 1\n5 5\u2211 `=1 E`(y). (7)\nThis cost is reported on top of each synthesised texture in Figures 4. To visually evaluate samples from our single- and multi-scale model against the VGG-based model (Gatys et al., 2015a), we additionally synthesize textures from VGG by minimizing (7) using L-BFGS-B as in section 4.\n6 RESULTS\nIn Fig. 1 we show textures synthesised from two random single- and multi-scale models, as well as eight other non-random single-layer models for three different source images (top left). For\n1https://github.com/Lasagne/Recipes/blob/master/modelzoo/vgg19.py as accessed on 12.05.2016.\ncomparison, we also plot samples generated from the VGG model by Gatys et al. (Gatys et al., 2015a) (bottom left). There are roughly two groups of models: those with a small number of feature maps (363, top row), and those with a large number of feature maps (3267, bottom row). Only the multi-scale model employs 1024 feature maps. Within each group, we can differentiate models for which the filters are unsupervisedly trained on natural images (e.g. sparse coding filters from k-means), principally devised filter banks (e.g. 2D Fourier basis) and completely random filters (see sec. 2 for all details). All single-layer networks, except for multi-scale, feature 11\u00d7 11 filters. Remarkably, despite the small spatial size of the filters, all models capture much of the small- and mid-scale structure of the textures, in particular if the number of feature maps is large. Notably, the scale of these structures extends far beyond the receptive fields of the single units (see e.g. the pebble texture). We further observe that a larger number of feature maps generally increases the perceptual quality of the generated textures. Surprisingly, however, completely random filters perform on par or better then filters that have been trained on the statistics of natural images. This is particularly true for the multi-scale model that clearly outperforms the single-scale models on all textures. The captured structures in the multi-scale model are generally much larger and often reach the full size of the texture (see e.g. the wall).\nWhile the above results show that for natural texture synthesis one neither needs a hierarchical deep network architecture with spatial pooling nor filters that are adapted to the statistics of natural images, we now focus on the aspects that are crucial for high quality texture synthesis. First, we evaluate whether the success of the random multi-scale network arises from the combination of filters on multiple scales or whether it is simply the increased size of its largest receptive fields (55\u00d7 55 vs. 11\u00d7 11) that leads to the improvement compared to the single-scale model. Thus, to investigate the influence of the spatial extend of the filters and the importance of combining multiple filter sizes in one model, we generate textures from multiple single-scale models, where each model has the same number of random filters as the multi-scale model (1024) but only uses filters from a single scale of the multi-scale model (Fig. 2). We find that while 3\u00d7 3 filters mainly capture the marginal distribution of the color channels, larger filters like 11 \u00d7 11 model small- to mid-scale structures (like small stones) but miss more long-range structures (larger stones are not well separated). Very large filters like 55\u00d7 55, on the other hand, are capable of modeling long-range structures but then miss much of the small- to midscale statistics (like the texture of the stone). Therefore we conclude that the combination of different scales in the multi-scale network is important for good texture synthesis since it allows to simultaneously model small-, mid- and long-range correlations of the textures. Finally we note that a further indispensable component for good texture models are the non-linearities: textures synthesised the multi-scale model without ReLU (Fig. 2, right column) are unable to capture the statistical dependencies of the texture.\nThe perceptual quality of the textures generated from models with only a single layer and random filters is quite remarkable and surpasses parametric methods like Portilla & Simoncelli (2000) that have been state-of-the-art two years ago (before the use of DNNs). The multi-scale model often rivals the current state of the art (Gatys et al., 2015a) as we show in Fig. 4 where we compare samples synthesized from 20 different textures for the random single- and multi-scale model, as well as VGG. The multi-scale model generates very competitive samples in particular for textures with extremely regular structures across the whole image (e.g. for the brick wall, the grids or the scales). In part, this effect can be attributed to the more robust optimization of the single-layer model that is less prone to local minima then the optimization in deeper models. This can be seen by initializing the VGG-based synthesis with textures from the single-layer model, which consistently yields superior synthesis results (see Appendix A, Fig. 5). In addition, for a few textures such as the grid structures, the VGG-based loss is paradoxically lower for samples from the multi-scale model then for the VGG-based model (which directly optimized the VGG-based loss). This suggests that the naive synthesis performed here favors images that are perceptually similar to the reference texture and thus looses variability (see sec. 7 for further discussion). Nonetheless, samples from the single-layer model still exhibit large perceptual differences, see Fig. 3. The VGG-based loss (7) appears to generally be an acceptable approximation of the perceptual differences between the reference and the synthesized texture. Only for a few textures, especially those with very regular men-made structures (e.g. the wall or the grids), the VGG-based loss fails to capture the perceptual advantage of the multi-scale synthesis.\n7 DISCUSSION\nWe proposed a generative model of natural textures based on a single-layer convolutional neural network with completely random filters and showed that the model is able to qualitatively capture the perceptual differences between natural textures. Samples from the model often rival the current state-of-the-art (Gatys et al., 2015a) (Fig. 4, third vs fourth row), even though the latter relies on a high-performance deep neural network with features that are tuned to the statistics of natural images. Seen more broadly, this finding suggests that natural image generation does not necessarily depend on deep hierarchical representations or on the training of the feature maps. Instead, for texture synthesis, both aspects rather seem to serve as fine-tuning of the image representation.\nOne concern about the proposed single-layer multi-scale model is its computational inefficiency since it involves convolutions with spatially large filters (up to 55\u00d7 55). A more efficient way to achieve receptive fields of similar size would be to use a hierarchical multi-layer net. We conducted extensive experiments with various hierarchical architectures and while the synthesis is indeed significantly faster, the quality of the synthesized textures does not improve compared to a single-layer model. Thus for a minimal model of natural textures, deep hierarchical representations are not necessary but they can improve the efficiency of the texture synthesis.\nOur results clearly demonstrate that Gram matrices computed from the feature maps of convolutional neural networks generically lead to useful summary statistics for texture synthesis. The Gram matrix on the feature maps transforms the representations from the convolutional neural network into a stationary feature space that captures the pairwise correlations between different features. If the number of feature maps is large, then the local structures in the image are well preserved in the projected space and the overlaps of the convolutional filtering add additional constraints. At the same time, averaging out the spatial dimensions yields sufficient flexibility to generate entirely new textures that differ from the reference on a patch by patch level, but still share much of the small- and long-range statistics.\nThe success of shallow convolutional networks with random filters in reproducing the structure of the reference texture is remarkable and indicates that they can be useful for parametric texture synthesis. Besides reproducing the stationary correlation structure of the reference image (\"perceptual similarity\") another desideratum of a texture synthesis is to exhibit a large variety between different samples generated from the same given image (\"variability\"). Hence, synthesis algorithms need to balance perceptual similarity and variability. This balance is determined by a complex interplay between the choice of summary statistics and the optimization algorithm used. For example the stopping criterion of the optimization algorithm can be adjusted to trade perceptual similarity for larger variability.\nFinding the right balance between perceptual similarity and variability is challenging because we are currently lacking robust measures of these quantities. In this work we introduced VGG-loss as a measure of perceptual similarity, and, even though, it works much better than other common measures such as Structural Similarity Index (SSIM, Wang et al., 2004, see Appendix A, Figure 6) or Euclidean distance in the pixel space (not shown), it is still not perfect (Figure 4). Measuring variability is probably even more difficult: in principle it requires measuring the entropy of generated samples, which is intractable in a high-dimensional space. A different approach could be based on a psychophysical assessment of generated samples. For example, we could use an inpainting task (illustrated in Appendix A, Figure 7) to make human observers decide between actual texture patches and inpaited ones. Performance close to a chance-level would indicate that the texture model produces variable enough samples to capture the diversity of actual patches. The further exploration of variability measures lies, however, beyond the scope of this work.\nIn this paper we focused on maximizing perceptual similarity only, and it is worth pointing out that additional efforts will be necessary to find an optimal trade-off between perceptual similarity and variability. For the synthesis of textures from the random models considered here, the trade-off leans more towards perceptual similarity in comparison to Gatys et al. (2015a)(due to the simpler optimization) which also explains the superior performance on some samples. In fact, we found some anecdotal evidence (not shown) in deeper multi-layer random CNNs where the reference texture was exactly reconstructed during the synthesis. From a theoretical point of view this is likely a finite size effect which does not necessarily constitute a failure of the chosen summary statistics: for finite size images it is well possible that only the reference image can exactly reproduce all the summary\nstatistics. Therefore, in practice, the Gram matrices are not treated as hard constraints but as soft constraints only. More generally, we do not expect a perceptual distance metric to assign exactly zero to a random pair of patches from the same texture. Instead, we expect it to assign small values for pairs from the same texture, and large values for patches from different textures. Therefore, the selection of constraints is not sufficient to characterize a texture synthesis model but only determines the exact minima of the objective function (which are sought for by the synthesis). If we additionally consider images with small but non-zero distance to the reference statistics, then the set of equivalent textures increases substantially, and the precise composition of this set becomes critically dependent on the perceptual distance metric.\nMathematically, parametric texture synthesis models are described as ergodic random fields that have maximum entropy subject to certain constraints Zhu et al. (1997); Bruna & Mallat (2013); Zhu et al. (2000) (MaxEnt framework). Practical texture synthesis algorithms, however, always deal with finite size images. As discussed above, two finite-size patches from the same ergodic random field will almost never feature the exact same summary statistics. This additional uncertainty in estimating the constraints on finite length processes is not thoroughly accounted for by the MaxEnt framework (see discussion on its \u201cad hockeries\u201d by Jaynes (Jaynes (1982))). Thus, a critical difference of practical implementations of texture synthesis algorithms from the conceptual MaxEnt texture modeling framework is that they genuinely allow a small mismatch in the constraints. Accordingly, specifying the summary statistics is not sufficient but a comprehensive definition of a texture synthesis model should specify:\n1. A metric d(x,y) that determines the distance between any two arbitrary textures x,y.\n2. A bipartition Px of the image space that determines which images are considered perceptually equivalent to a reference texture x. A simple example for such a partition is the -environment U (y) := {y : d(y,x) < } and its complement.\nThis definition is relevant for both under- as well as over-constrained models, but its importance becomes particularly obvious for the latter. According to the Minimax entropy principle for texture modeling suggested by Zhu et al Zhu et al. (1997), as many constraints as possible should be used to reduce the (Kullback-Leibler) divergence between the true texture model and its estimate. However, for finite spatial size, the synthetic samples become exactly equivalent to the reference texture (up to shifts) in the limit of sufficiently many independent constraints. In contrast, if we explicitly allow for a small mismatch between the summary statistics of the reference image and the synthesized textures, then the set of possible textures does not constitute a low-dimensional manifold but rather a small volume within the pixel space. Alternatively, instead of introducing an -environment it is also possible to extent the MaxEnt framework to allow for variability in the summary statistics (Joan Bruna, personal communication). It will be interesting to compare in the future to what extent the difference between the two approaches can lead to differences in the perceptual appearance of the textures.\nTaken together we have shown that simple single-layer CNNs with random filters can serve as the basis for excellent texture synthesis models that outperform previous hand-crafted synthesis models and sometimes even rivals the current state-of-the-art. This finding repeals previous observations that suggested a critical role for the multi-layer representations in trained deep networks for natural texture generation. On the other hand, it is not enough to just use sufficiently many constraints as one would predict from the MaxEnt framework. Instead, for the design of good texture synthesis algorithms it will be crucial to find distance measures for which the -environment around the reference texture leads to perceptually satisfying results. In this way, building better texture synthesis models is inherently related to better quantitative models of human perception.\nA APPENDIX\n", "cats": {"READ": 1.0, "DONTREAD": 0.0}, "reviews": [{"IS_META_REVIEW": true, "comments": "The framework of Gatys et al. demonstrated that correlation statistics (empirical Gram matrices) of deep feature responses provide an excellent characterisation of visual textures. This paper investigates in detail which kind of deep or shallow networks may work well in this framework. One of the main findings is that that very shallow nets, consisting of a single filter bank with random weights, work surprisingly well, and for simple and regular textures may produce results which are visually superior to complex data-adapted filters such as the ones in networks like VGG-19. More broadly, the paper contains an interesting and informative discussion on the strength and limitations on such methods for texture synthesis.\n\nFigure 4 shows that the optimisation of images with respect to shallow filter banks may result in texture images that have a lower VGG-19 loss than optimising the VGG-19 objective directly. This is imputed to the difficulty of optimising the highly non-linear VGG-19 cost function, which is a reasonable explanation. In the new supplementary material, the authors show that better optimisation results can be obtained by initialising the VGG-19-based optimisation with the shallow network optimisation results, which is a useful complement to the original experiments.\n\nThe main limitation of the paper is that it does not systematically compare different methods against a quantifiable objective. It is trivial to define image statistics that would allow to simply generate an exact copy of any reference texture, hence with very good visual quality. Such trivial statistics would also be very shallow. The aim is instead to capture a texture distribution, and measuring how well a method meets this challenge remains an open problem. Hence, while the empirical results seem to confirm the intuition that simple statistics are good enough for texture synthesis both in terms of quality and diversity (when compared to more complex statistics), it is difficult to conclusively confirm that this is the case.\n\nThe authors indicate that diversity could be measured in terms of entropy. This is reasonable, but, as they acknowledge in their answers to questions, difficult to do in practice. Furthermore, this would still not account for the other aspect of the problem, namely visual quality. They also suggest to perform a psychophysical assessment, which may be the only practical way of addressing this problem, but deem that to be material for future work.\n\nOverall, since evaluation of image generation is such an hard problem, I think the paper still has sufficient strengths to warrant publication in ICLR. Still, some form of psychophysical assessment would be useful to confirm the intuitions that, at present, can only be obtained by inspecting the figure sin the paper and in the supplementary material."}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "The authors agreed that the paper presented a solid contribution and interesting (and somewhat surprising findings). The experiments are thorough and convincing, and while some reviewers raised concerns about a lack of comparisons of methods on a clear quantifiable objective, this is unfortunately a common issue in this field. Overall, this paper is a solid contribution with findings that would be interesting to the ICLR audience.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "TITLE": "An interesting work with uncertain conclusions needed to be further testified.", "comments": "This work proposed a simple but strong baseline for parametric texture synthesis. In empirical experiments, samples generated by the baseline composed by multi-scale and random filters sometime rival the VGG-based model which has multi-layer and pre-trained filters. The authors concluded that texture synthesis does not necessarily depend on deep hierarchical representations or the learned feature maps. \nThis work is indeed interesting and insightful. However, the conclusions are needed to be further testified (especially for deep hierarchical representations). Firstly, all of generated samples by both VGG and single layer model are not perfect and much worse than the results from non-parametric methods.  Besides VGG-based model seems to do better in inpainting task in Figure 7. Last but not least, would a hierarchical model (instead of lots of filters with different size) handle multi-scale more efficiently? ", "SOUNDNESS_CORRECTNESS": 5, "ORIGINALITY": 4, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "18 Dec 2016", "CLARITY": 4, "REVIEWER_CONFIDENCE": 4}, {"TITLE": "Important work which deserves ", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "This paper provides an interesting analysis of the conditions which enable generation of natural looking textures. The results is quite surprising, and analysis is quite thorough. \nI do think the evaluation methods require more work, but as other reviewers mentioned this could be an interesting line of work moving forwards and does not take too much from this current paper which, I think, should be accepted.", "IS_META_REVIEW": false, "RECOMMENDATION": 8, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 3}, {"TITLE": "A good paper with interesting empirical insights on texture synthesis", "MEANINGFUL_COMPARISON": 3, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "The framework of Gatys et al. demonstrated that correlation statistics (empirical Gram matrices) of deep feature responses provide an excellent characterisation of visual textures. This paper investigates in detail which kind of deep or shallow networks may work well in this framework. One of the main findings is that that very shallow nets, consisting of a single filter bank with random weights, work surprisingly well, and for simple and regular textures may produce results which are visually superior to complex data-adapted filters such as the ones in networks like VGG-19. More broadly, the paper contains an interesting and informative discussion on the strength and limitations on such methods for texture synthesis.\n\nFigure 4 shows that the optimisation of images with respect to shallow filter banks may result in texture images that have a lower VGG-19 loss than optimising the VGG-19 objective directly. This is imputed to the difficulty of optimising the highly non-linear VGG-19 cost function, which is a reasonable explanation. In the new supplementary material, the authors show that better optimisation results can be obtained by initialising the VGG-19-based optimisation with the shallow network optimisation results, which is a useful complement to the original experiments.\n\nThe main limitation of the paper is that it does not systematically compare different methods against a quantifiable objective. It is trivial to define image statistics that would allow to simply generate an exact copy of any reference texture, hence with very good visual quality. Such trivial statistics would also be very shallow. The aim is instead to capture a texture distribution, and measuring how well a method meets this challenge remains an open problem. Hence, while the empirical results seem to confirm the intuition that simple statistics are good enough for texture synthesis both in terms of quality and diversity (when compared to more complex statistics), it is difficult to conclusively confirm that this is the case.\n\nThe authors indicate that diversity could be measured in terms of entropy. This is reasonable, but, as they acknowledge in their answers to questions, difficult to do in practice. Furthermore, this would still not account for the other aspect of the problem, namely visual quality. They also suggest to perform a psychophysical assessment, which may be the only practical way of addressing this problem, but deem that to be material for future work.\n\nOverall, since evaluation of image generation is such an hard problem, I think the paper still has sufficient strengths to warrant publication in ICLR. Still, some form of psychophysical assessment would be useful to confirm the intuitions that, at present, can only be obtained by inspecting the figure sin the paper and in the supplementary material.\n\n\n\n", "SOUNDNESS_CORRECTNESS": 3, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "RECOMMENDATION": 8, "DATE": "14 Dec 2016", "REVIEWER_CONFIDENCE": 5}, {"DATE": "14 Dec 2016", "TITLE": "No further questions", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1"}, {"OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "TITLE": "The used method of texture evaluation seems to be suspicious. Why not try compare different networks on texture inpainting task?", "comments": "", "SOUNDNESS_CORRECTNESS": 5, "ORIGINALITY": 4, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "DATE": "02 Dec 2016", "CLARITY": 4}, {"TITLE": "Clarifications and discussion about evaluation methodology", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "MEANINGFUL_COMPARISON": 3, "comments": "", "SOUNDNESS_CORRECTNESS": 3, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "DATE": "25 Nov 2016"}, {"IS_META_REVIEW": true, "comments": "The framework of Gatys et al. demonstrated that correlation statistics (empirical Gram matrices) of deep feature responses provide an excellent characterisation of visual textures. This paper investigates in detail which kind of deep or shallow networks may work well in this framework. One of the main findings is that that very shallow nets, consisting of a single filter bank with random weights, work surprisingly well, and for simple and regular textures may produce results which are visually superior to complex data-adapted filters such as the ones in networks like VGG-19. More broadly, the paper contains an interesting and informative discussion on the strength and limitations on such methods for texture synthesis.\n\nFigure 4 shows that the optimisation of images with respect to shallow filter banks may result in texture images that have a lower VGG-19 loss than optimising the VGG-19 objective directly. This is imputed to the difficulty of optimising the highly non-linear VGG-19 cost function, which is a reasonable explanation. In the new supplementary material, the authors show that better optimisation results can be obtained by initialising the VGG-19-based optimisation with the shallow network optimisation results, which is a useful complement to the original experiments.\n\nThe main limitation of the paper is that it does not systematically compare different methods against a quantifiable objective. It is trivial to define image statistics that would allow to simply generate an exact copy of any reference texture, hence with very good visual quality. Such trivial statistics would also be very shallow. The aim is instead to capture a texture distribution, and measuring how well a method meets this challenge remains an open problem. Hence, while the empirical results seem to confirm the intuition that simple statistics are good enough for texture synthesis both in terms of quality and diversity (when compared to more complex statistics), it is difficult to conclusively confirm that this is the case.\n\nThe authors indicate that diversity could be measured in terms of entropy. This is reasonable, but, as they acknowledge in their answers to questions, difficult to do in practice. Furthermore, this would still not account for the other aspect of the problem, namely visual quality. They also suggest to perform a psychophysical assessment, which may be the only practical way of addressing this problem, but deem that to be material for future work.\n\nOverall, since evaluation of image generation is such an hard problem, I think the paper still has sufficient strengths to warrant publication in ICLR. Still, some form of psychophysical assessment would be useful to confirm the intuitions that, at present, can only be obtained by inspecting the figure sin the paper and in the supplementary material."}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "The authors agreed that the paper presented a solid contribution and interesting (and somewhat surprising findings). The experiments are thorough and convincing, and while some reviewers raised concerns about a lack of comparisons of methods on a clear quantifiable objective, this is unfortunately a common issue in this field. Overall, this paper is a solid contribution with findings that would be interesting to the ICLR audience.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "TITLE": "An interesting work with uncertain conclusions needed to be further testified.", "comments": "This work proposed a simple but strong baseline for parametric texture synthesis. In empirical experiments, samples generated by the baseline composed by multi-scale and random filters sometime rival the VGG-based model which has multi-layer and pre-trained filters. The authors concluded that texture synthesis does not necessarily depend on deep hierarchical representations or the learned feature maps. \nThis work is indeed interesting and insightful. However, the conclusions are needed to be further testified (especially for deep hierarchical representations). Firstly, all of generated samples by both VGG and single layer model are not perfect and much worse than the results from non-parametric methods.  Besides VGG-based model seems to do better in inpainting task in Figure 7. Last but not least, would a hierarchical model (instead of lots of filters with different size) handle multi-scale more efficiently? ", "SOUNDNESS_CORRECTNESS": 5, "ORIGINALITY": 4, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "18 Dec 2016", "CLARITY": 4, "REVIEWER_CONFIDENCE": 4}, {"TITLE": "Important work which deserves ", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "This paper provides an interesting analysis of the conditions which enable generation of natural looking textures. The results is quite surprising, and analysis is quite thorough. \nI do think the evaluation methods require more work, but as other reviewers mentioned this could be an interesting line of work moving forwards and does not take too much from this current paper which, I think, should be accepted.", "IS_META_REVIEW": false, "RECOMMENDATION": 8, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 3}, {"TITLE": "A good paper with interesting empirical insights on texture synthesis", "MEANINGFUL_COMPARISON": 3, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "The framework of Gatys et al. demonstrated that correlation statistics (empirical Gram matrices) of deep feature responses provide an excellent characterisation of visual textures. This paper investigates in detail which kind of deep or shallow networks may work well in this framework. One of the main findings is that that very shallow nets, consisting of a single filter bank with random weights, work surprisingly well, and for simple and regular textures may produce results which are visually superior to complex data-adapted filters such as the ones in networks like VGG-19. More broadly, the paper contains an interesting and informative discussion on the strength and limitations on such methods for texture synthesis.\n\nFigure 4 shows that the optimisation of images with respect to shallow filter banks may result in texture images that have a lower VGG-19 loss than optimising the VGG-19 objective directly. This is imputed to the difficulty of optimising the highly non-linear VGG-19 cost function, which is a reasonable explanation. In the new supplementary material, the authors show that better optimisation results can be obtained by initialising the VGG-19-based optimisation with the shallow network optimisation results, which is a useful complement to the original experiments.\n\nThe main limitation of the paper is that it does not systematically compare different methods against a quantifiable objective. It is trivial to define image statistics that would allow to simply generate an exact copy of any reference texture, hence with very good visual quality. Such trivial statistics would also be very shallow. The aim is instead to capture a texture distribution, and measuring how well a method meets this challenge remains an open problem. Hence, while the empirical results seem to confirm the intuition that simple statistics are good enough for texture synthesis both in terms of quality and diversity (when compared to more complex statistics), it is difficult to conclusively confirm that this is the case.\n\nThe authors indicate that diversity could be measured in terms of entropy. This is reasonable, but, as they acknowledge in their answers to questions, difficult to do in practice. Furthermore, this would still not account for the other aspect of the problem, namely visual quality. They also suggest to perform a psychophysical assessment, which may be the only practical way of addressing this problem, but deem that to be material for future work.\n\nOverall, since evaluation of image generation is such an hard problem, I think the paper still has sufficient strengths to warrant publication in ICLR. Still, some form of psychophysical assessment would be useful to confirm the intuitions that, at present, can only be obtained by inspecting the figure sin the paper and in the supplementary material.\n\n\n\n", "SOUNDNESS_CORRECTNESS": 3, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "RECOMMENDATION": 8, "DATE": "14 Dec 2016", "REVIEWER_CONFIDENCE": 5}, {"DATE": "14 Dec 2016", "TITLE": "No further questions", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1"}, {"OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "TITLE": "The used method of texture evaluation seems to be suspicious. Why not try compare different networks on texture inpainting task?", "comments": "", "SOUNDNESS_CORRECTNESS": 5, "ORIGINALITY": 4, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "DATE": "02 Dec 2016", "CLARITY": 4}, {"TITLE": "Clarifications and discussion about evaluation methodology", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "MEANINGFUL_COMPARISON": 3, "comments": "", "SOUNDNESS_CORRECTNESS": 3, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "DATE": "25 Nov 2016"}]}
{"text": "1 INTRODUCTION\nThe ability to navigate efficiently within an environment is fundamental to intelligent behavior. Whilst conventional robotics methods, such as Simultaneous Localisation and Mapping (SLAM), tackle navigation through an explicit focus on position inference and mapping (Dissanayake et al., 2001), here we follow recent work in deep reinforcement learning (Mnih et al., 2015; 2016) and propose that navigational abilities could emerge as the by-product of an agent learning a policy that maximizes reward. One advantage of an intrinsic, end-to-end approach is that actions are not divorced from representation, but rather learnt together, thus ensuring that task-relevant features are present in the representation. Learning to navigate from reinforcement learning in partially observable environments, however, poses several challenges.\nFirst, rewards are often sparsely distributed in the environment, where there may be only one goal location. Second, environments often comprise dynamic elements, requiring the agent to use memory at different timescales: rapid one-shot memory for the goal location, together with short term memory subserving temporal integration of velocity signals and visual observations, and longer term memory for constant aspects of the environment (e.g. boundaries, cues).\nTo improve statistical efficiency we bootstrap the reinforcement learning procedure by augmenting our loss with auxiliary tasks that provide denser training signals that support navigation-relevant representation learning. We consider two additional losses: the first one involves reconstruction of a low-dimensional depth map at each time step by predicting one input modality (the depth channel) from others (the colour channels). This auxiliary task concerns the 3D geometry of the environment, and is aimed to encourage the learning of representations that aid obstacle avoidance and short-term trajectory planning. The second task directly invokes loop closure from SLAM: the agent is trained to predict if the current location has been previously visited within a local trajectory. \u2217Denotes equal contribution 1A video illustrating the navigation agents is available at: https://youtu.be/lNoaTyMZsWI\nTo address the memory requirements of the task we rely on a stacked LSTM architecture (Graves et al., 2013; Pascanu et al., 2013). We evaluate our approach using five 3D maze environments and demonstrate the accelerated learning and increased performance of the proposed agent architecture. These environments feature complex geometry, random start position and orientation, dynamic goal locations, and long episodes that require thousands of agent steps (see Figure 1). We also provide detailed analysis of the trained agent to show that critical navigation skills are acquired. This is important as neither position inference nor mapping are directly part of the loss; therefore, raw performance on the goal finding task is not necessarily a good indication that these skills are acquired. In particular, we show that the proposed agent resolves ambiguous observations and quickly localizes itself in a complex maze, and that this localization capability is correlated with higher task reward.\n2 APPROACH\nWe rely on a end-to-end learning framework that incorporates multiple objectives. Firstly it tries to maximize cumulative reward using an actor-critic approach. Secondly it minimizes an auxiliary loss of inferring the depth map from the RGB observation. Finally, the agent is trained to detect loop closures as an additional auxiliary task that encourages implicit velocity integration.\nThe reinforcement learning problem is addressed with the Asynchronous Advantage Actor-Critic (A3C) algorithm (Mnih et al., 2016) that relies on learning both a policy \u03c0(at|st; \u03b8) and value function V (st; \u03b8V ) given a state observation st. Both the policy and value function share all intermediate representations, both being computed using a separate linear layer from the topmost layer of the model. The agent setup closely follows the work of (Mnih et al., 2016) and we refer to this work for the details (e.g. the use of a convolutional encoder followed by either an MLP or an LSTM, the use of action repetition, entropy regularization to prevent the policy saturation, etc.). These details can also be found in the Appendix B.\nThe baseline that we consider in this work is an A3C agent (Mnih et al., 2016) that receives only RGB input from the environment, using either a recurrent or a purely feed-forward model (see Figure 2a,b). The encoder for the RGB input (used in all other considered architectures) is a 3 layer convolutional network. To support the navigation capability of our approach, we also rely on the Nav A3C agent (Figure 2c) which employs a two-layer stacked LSTM after the convolutional encoder. We expand the observations of the agents to include agent-relative velocity, the action sampled from the stochastic policy and the immediate reward, from the previous time step. We opt to feed the velocity and previously selected action directly to the second recurrent layer, with the first layer only receiving the reward. We postulate that the first layer might be able to make associations between reward and visual observations that are provided as context to the second layer from which the policy is computed. Thus, the observation st may include an image xt \u2208 R3\u00d7W\u00d7H (where W and H are the width and\nheight of the image), the agent-relative lateral and rotational velocity vt \u2208 R6, the previous action at\u22121 \u2208 RNA , and the previous reward rt\u22121 \u2208 R. Figure 2d shows the augmentation of the Nav A3C with the different possible auxiliary losses. In particular we consider predicting depth from the convolutional layer (we will refer to this choice as D1), or from the top LSTM layer (D2) or predicting loop closure (L). The auxiliary losses are computed on the current frame via a single layer MLP. The agent is trained by applying a weighted sum of the gradients coming from A3C, the gradients from depth prediction (multiplied with \u03b2d1 , \u03b2d2 ) and the gradients from the loop closure (scaled by \u03b2l). More details of the online learning algorithm are given in Appendix B.\n2.1 DEPTH PREDICTION\nThe primary input to the agent is in the form of RGB images. However, depth information, covering the central field of view of the agent, might supply valuable information about the 3D structure of the environment. While depth could be directly used as an input, we argue that if presented as an additional loss it is actually more valuable to the learning process. In particular if the prediction loss shares representation with the policy, it could help build useful features for RL much faster, bootstrapping learning. Since we know from (Eigen et al., 2014) that a single frame can be enough to predict depth, we know this auxiliary task can be learnt. A comparison between having depth as input versus as an additional loss is given in Appendix C, which shows significant gain for depth as a loss.\nSince the role of the auxiliary loss is just to build up the representation of the model, we do not necessarily care about the specific performance obtained or nature of the prediction. We do care about the data efficiency aspect of the problem and also computational complexity. If the loss is to be useful for the main task, we should converge faster on it compared to solving the RL problem (using less data samples), and the additional computational cost should be minimal. To achieve this we use a low resolution variant of the depth map, reducing the screen resolution to 4x16 pixels2.\nWe explore two different variants for the loss. The first choice is to phrase it as a regression task, the most natural choice. While this formulation, combined with a higher depth resolution, extracts the most information, mean square error imposes a unimodal distribution (van den Oord et al., 2016). To address this possible issue, we also consider a classification loss, where depth at each position is discretised into 8 different bands. The bands are non-uniformally distributed such that we pay more attention to far-away objects (details in Appendix B). The motivation for the classification formulation is that while it greatly reduces the resolution of depth, it is more flexible from a learning perspective and can result in faster convergence (hence faster bootstrapping).\n2The image is cropped before being subsampled to lessen the floor and ceiling which have little relevant depth information.\n2.2 LOOP CLOSURE PREDICTION\nLoop closure, like depth, is valuable for a navigating agent, since can be used for efficient exploration and spatial reasoning. To produce the training targets, we detect loop closures based on the similarity of local position information during an episode, which is obtained by integrating 2D velocity over time. Specifically, in a trajectory noted {p0, p1, . . . , pT }, where pt is the position of the agent at time t, we define a loop closure label lt that is equal to 1 if the position pt of the agent is close to the position pt\u2032 at an earlier time t\u2032. In order to avoid trivial loop closures on consecutive points of the trajectory, we add an extra condition on an intermediary position pt\u2032\u2032 being far from pt. Thresholds \u03b71 and \u03b72 provide these two limits. Learning to predict the binary loop label is done by minimizing the Bernoulli loss Ll between lt and the output of a single-layer output from the hidden representation ht of the last hidden layer of the model, followed by a sigmoid activation.\n3 RELATED WORK\nThere is a rich literature on navigation, primarily in the robotics literature. However, here we focus on related work in deep RL. Deep Q-networks (DQN) have had breakthroughs in extremely challenging domains such as Atari (Mnih et al., 2015). Recent work has developed on-policy RL methods such as advantage actor-critic that use asynchronous training of multiple agents in parallel (Mnih et al., 2016). Recurrent networks have also been successfully incorporated to enable state disambiguation in partially observable environments (Koutnik et al., 2013; Hausknecht & Stone, 2015; Mnih et al., 2016; Narasimhan et al., 2015).\nDeep RL has recently been used in the navigation domain. Kulkarni et al. (2016) used a feedforward architecture to learn deep successor representations that enabled behavioral flexibility to reward changes in the MazeBase gridworld, and provided a means to detect bottlenecks in 3D VizDoom. Zhu et al. (2016) used a feedforward siamese actor-critic architecture incorporating a pretrained ResNet to support navigation to a target in a discretised 3D environment. Oh et al. (2016) investigated the performance of a variety of networks with external memory (Weston et al., 2014) on simple navigation tasks in the Minecraft 3D block world environment. Tessler et al. (2016) also used the Minecraft domain to show the benefit of combining feedforward deep-Q networks with the learning of resuable skill modules (cf options: (Sutton et al., 1999)) to transfer between navigation tasks. Tai & Liu (2016) trained a convnet DQN-based agent using depth channel inputs for obstacle avoidance in 3D environments. Barron et al. (2016) investigated how well a convnet can predict the depth channel from RGB in the Minecraft environment, but did not use depth for training the agent.\nAuxiliary tasks have often been used to facilitate representation learning (Suddarth & Kergosien, 1990). Recently, the incorporation of additional objectives, designed to augment representation learning through auxiliary reconstructive decoding pathways (Zhang et al., 2016; Rasmus et al., 2015; Zhao et al., 2015; Mirowski et al., 2010), has yielded benefits in large scale classification tasks. In deep RL settings, however, only two previous papers have examined the benefit of auxiliary tasks. Specifically, Li et al. (2016) consider a supervised loss for fitting a recurrent model on the hidden representations to predict the next observed state, in the context of imitation learning of sequences provided by experts, and Lample & Chaplot (2016) show that the performance of a DQN agent in a first-person shooter game in the VizDoom environment can be substantially enhanced by the addition of a supervised auxiliary task, whereby the convolutional network was trained on an enemy-detection task, with information about the presence of enemies, weapons, etc., provided by the game engine.\nIn contrast, our contribution addresses fundamental questions of how to learn an intrinsic representation of space, geometry, and movement while simultaneously maximising rewards through reinforcement learning. Our method is validated in challenging maze domains with random start and goal locations.\n4 EXPERIMENTS\nWe consider a set of first-person 3D mazes from the DeepMind Lab environment (Beattie et al., 2016) (see Fig. 1) that are visually rich, with additional observations available to the agent such as inertial\ninformation and local depth information.3 The action space is discrete, yet allows finegrained control, comprising 8 actions: the agent can rotate in small increments, accelerate forward or backward or sideways, or induce rotational acceleration while moving. Reward is achieved in these environments by reaching a goal from a random start location and orientation. If the goal is reached, the agent is respawned to a new start location and must return to the goal. The episode terminates when a fixed amount of time expires, affording the agent enough time to find the goal several times. There are sparse \u2018fruit\u2019 rewards which serve to encourage exploration. Apples are worth 1 point, strawberries 2 points and goals are 10 points. Videos of the agent solving the maze are linked in Appendix A.\nIn the static variant of the maze, the goal and fruit locations are fixed and only the agent\u2019s start location changes. In the dynamic (Random Goal) variant, the goal and fruits are randomly placed on every episode. Within an episode, the goal and apple locations stay fixed until the episode ends. This encourages an explore-exploit strategy, where the agent should initially explore the maze, then retain the goal location and quickly refind it after each respawn. For both variants (static and random goal) we consider a small and large map. The small mazes are 5\u00d7 10 and episodes last for 3600 timesteps, and the large mazes are 9\u00d7 15 with 10800 steps (see Figure 1). The RGB observation is 84\u00d7 84. The I-Maze environment (see Figure 1, right) is inspired by the classic T-maze used to investigate navigation in rodents (Olton et al., 1979): the layout remains fixed throughout, the agent spawns in the central corridor where there are apple rewards and has to locate the goal which is placed in the alcove of one of the four arms. Because the goal is hidden in the alcove, the optimal agent behaviour must rely on memory of the goal location in order to return to the goal using the most direct route. Goal location is constant within an episode but varies randomly across episodes.\nThe different agent architectures described in Section 2 are evaluated by training on the five mazes. Figure 3 shows learning curves (averaged over the 5 top performing agents). The agents are a feedforward model (FF A3C), a recurrent model (LSTM A3C), the stacked LSTM version with velocity, previous action and reward as input (Nav A3C), and Nav A3C with depth prediction from the convolution layer (Nav A3C+D1), Nav A3C with depth prediction from the last LSTM layer (Nav A3C+D2), Nav A3C with loop closure prediction (Nav A3C+L) as well as the Nav A3C with\n3The environments used in this paper are publicly available at https://github.com/deepmind/lab.\nall auxiliary losses considered together (Nav A3C+D1D2L). In each case we ran 64 experiments with randomly sampled hyper-parameters (for ranges and details please see the appendix). The mean over the top 5 runs as well as the top 5 curves are plotted. Expert human scores, established by a professional game player, are compared to these results. The Nav A3C+D2 agents reach human-level performance on Static 1 and 2, and attain about 91% and 59% of human scores on Random Goal 1 and 2.\nIn Mnih et al. (2015) reward clipping is used to stabilize learning, technique which we employed in this work as well. Unfortunately, for these particular tasks, this yields slightly suboptimal policies because the agent does not distinguish apples (1 point) from goals (10 points). Removing the reward clipping results in unstable behaviour for the base A3C agent (see Appendix C). However it seems that the auxiliary signal from depth prediction mediates this problem to some extent, resulting in stable learning dynamics (e.g. Figure 3f, Nav A3C+D1 vs Nav A3C*+D1). We clearly indicate whether reward clipping is used by adding an asterisk to the agent name.\nFigure 3f also explores the difference between the two formulations of depth prediction, as a regression task or a classification task. We can see that the regression agent (Nav A3C*+D1[MSE]) performs worse than one that does classification (Nav A3C*+D1). This result extends to other maps, and we therefore only use the classification formulation in all our other results4. Also we see that predicting depth from the last LSTM layer (hence providing structure to the recurrent layer, not just the convolutional ones) performs better.\nWe note some particular results from these learning curves. In Figure 3 (a and b), consider the feedforward A3C model (red curve) versus the LSTM version (pink curve). Even though navigation seems to intrinsically require memory, as single observations could often be ambiguous, the feedforward model achieves competitive performance on static mazes. This suggest that there might be good strategies that do not involve temporal memory and give good results, namely a reactive policy held by the weights of the encoder, or learning a wall-following strategy. This motivates the dynamic environments that encourage the use of memory and more general navigation strategies.\nFigure 3 also shows the advantage of adding velocity, reward and action as an input, as well as the impact of using a two layer LSTM (orange curve vs red and pink). Though this agent (Nav A3C) is better than the simple architectures, it is still relatively slow to train on all of the mazes. We believe that this is mainly due to the slower, data inefficient learning that is generally seen in pure RL approaches. Supporting this we see that adding the auxiliary prediction targets of depth and loop closure (Nav A3C+D1D2L, black curve) speeds up learning dramatically on most of the mazes (see Table 1: AUC metric). It has the strongest effect on the static mazes because of the accelerated learning, but also gives a substantial and lasting performance increase on the random goal mazes.\nAlthough we place more value on the task performance than on the auxiliary losses, we report the results from the loop closure prediction task. Over 100 test episodes of 2250 steps each, within a large maze (random goal 2), the Nav A3C*+D1L agent demonstrated very successful loop detection, reaching an F-1 score of 0.83. A sample trajectory can be seen in Figure 4 (right).\n4An exception is the Nav A3C*+D1L agent on the I-maze (Figure 3c), which uses depth regression and reward clipping. While it does worse, we include it because some analysis is based on this agent.\n5 ANALYSIS\n5.1 POSITION DECODING\nIn order to evaluate the internal representation of location within the agent (either in the hidden units ht of the last LSTM, or, in the case of the FF A3C agent, in the features ft on the last layer of the conv-net), we train a position decoder that takes that representation as input, consisting of a linear classifier with multinomial probability distribution over the discretized maze locations. Small mazes (5\u00d7 10) have 50 locations, large mazes (9\u00d7 15) have 135 locations, and the I-maze has 77 locations. Note that we do not backpropagate the gradients from the position decoder through the rest of the network. The position decoder can only see the representation exposed by the model, not change it.\nAn example of position decoding by the Nav A3C+D2 agent is shown in Figure 6, where the initial uncertainty in position is improved to near perfect position prediction as more observations are acquired by the agent. We observe that position entropy spikes after a respawn, then decreases once the agent acquires certainty about its location. Additionally, videos of the agent\u2019s position decoding are linked in Appendix A. In these complex mazes, where localization is important for the purpose of reaching the goal, it seems that position accuracy and final score are correlated, as shown in Table 1. A pure feed-forward architecture still achieves 64.3% accuracy in a static maze with static goal, suggesting that the encoder memorizes the position in the weights and that this small maze is solvable by all the agents, with sufficient training time. In Random Goal 1, it is Nav A3C+D2 that achieves the best position decoding performance (85.5% accuracy), whereas the FF A3C and the LSTM A3C architectures are at approximately 50%.\nIn the I-maze, the opposite branches of the maze are nearly identical, with the exception of very sparse visual cues. We observe that once the goal is first found, the Nav A3C*+D1L agent is capable of directly returning to the correct branch in order to achieve the maximal score. However, the linear position decoder for this agent is only 68.5% accurate, whereas it is 87.8% in the plain LSTM A3C agent. We hypothesize that the symmetry of the I-maze will induce a symmetric policy that need not be sensitive to the exact position of the agent (see analysis below).\nA desired property of navigation agents in our Random Goal tasks is to be able to first find the goal, and reliably return to the goal via an efficient route after subsequent re-spawns. The latency column in Table 1 shows that the Nav A3C+D2 agents achieve the lowest latency to goal once the goal has been discovered (the first number shows the time in seconds to find the goal the first time, and the second number is the average time for subsequent finds). Figure 5 shows clearly how the agent finds the goal, and directly returns to that goal for the rest of the episode. For Random Goal 2, none of the agents achieve lower latency after initial goal acquisition; this is presumably due to the larger, more challenging environment.\n5.2 STACKED LSTM GOAL ANALYSIS\nFigure 7(a) shows shows the trajectories traversed by an agent for each of the four goal locations. After an initial exploratory phase to find the goal, the agent consistently returns to the goal location. We visualize the agent\u2019s policy by applying tSNE dimension reduction (Maaten & Hinton, 2008) to the cell activations at each step of the agent for each of the four goal locations. Whilst clusters corresponding to each of the four goal locations are clearly distinct in the LSTM A3C agent, there are 2 main clusters in the Nav A3C agent \u2013 with trajectories to diagonally opposite arms of the maze represented similarly. Given that the action sequence to opposite arms is equivalent (e.g. straight, turn left twice for top left and bottom right goal locations), this suggests that the Nav A3C policy-dictating LSTM maintains an efficient representation of 2 sub-policies (i.e. rather than 4 independent policies) \u2013 with critical information about the currently relevant goal provided by the additional LSTM.\n5.3 INVESTIGATING DIFFERENT COMBINATIONS OF AUXILIARY TASKS\nOur results suggest that depth prediction from the policy LSTM yields optimal results. However, several other auxiliary tasks have been concurrently introduced in (Jaderberg et al., 2017), and thus we provide a comparison of reward prediction against depth prediction. Following that paper, we implemented two additional agent architectures, one performing reward prediction from the convnet using a replay buffer, called Nav A3C*+R, and one combining reward prediction from the convnet and depth prediction from the LSTM (Nav A3C+RD2). Table 2 suggests that reward prediction (Nav A3C*+R) improves upon the plain stacked LSTM architecture (Nav A3C*) but not as much as depth prediction from the policy LSTM (Nav A3C+D2). Combining reward prediction and depth prediction (Nav A3C+RD2) yields comparable results to depth prediction alone (Nav A3C+D2); normalised average AUC values are respectively 0.995 vs. 0.981. Future work will explore other auxiliary tasks.\n6 CONCLUSION\nWe proposed a deep RL method, augmented with memory and auxiliary learning targets, for training agents to navigate within large and visually rich environments that include frequently changing start and goal locations. Our results and analysis highlight the utility of un/self-supervised auxiliary objectives, namely depth prediction and loop closure, in providing richer training signals that bootstrap learning and enhance data efficiency. Further, we examine the behavior of trained agents, their ability to localise, and their network activity dynamics, in order to analyse their navigational abilities.\nOur approach of augmenting deep RL with auxiliary objectives allows end-end learning and may encourage the development of more general navigation strategies. Notably, our work with auxiliary losses is related to (Jaderberg et al., 2017) which independently looks at data efficiency when exploiting auxiliary losses. One difference between the two works is that our auxiliary losses are online (for the current frame) and do not rely on any form of replay. Also the explored losses are very different in nature. Finally our focus is on the navigation domain and understanding if navigation emerges as a bi-product of solving an RL problem, while Jaderberg et al. (2017) is concerned with data efficiency for any RL-task.\nWhilst our best performing agents are relatively successful at navigation, their abilities would be stretched if larger demands were placed on rapid memory (e.g. in procedurally generated mazes), due to the limited capacity of the stacked LSTM in this regard. It will be important in the future to combine visually complex environments with architectures that make use of external memory (Graves et al., 2016; Weston et al., 2014; Olton et al., 1979) to enhance the navigational abilities of agents. Further, whilst this work has focused on investigating the benefits of auxiliary tasks for developing the ability to navigate through end-to-end deep reinforcement learning, it would be interesting for future work to compare these techniques with SLAM-based approaches.\nACKNOWLEDGEMENTS\nWe would like to thank Alexander Pritzel, Thomas Degris and Joseph Modayil for useful discussions, Charles Beattie, Julian Schrittwieser, Marcus Wainwright, and Stig Petersen for environment design and development, and Amir Sadik and Sarah York for expert human game testing.\nSupplementary Material\nA VIDEOS OF TRAINED NAVIGATION AGENTS\nWe show the behaviour of Nav A3C*+D1L agent in 5 videos, corresponding to the 5 navigation environments: I-maze5, (small) static maze6, (large) static maze7, (small) random goal maze8 and (large) random goal maze9. Each video shows a high-resolution video (the actual inputs to the agent are down-sampled to 84\u00d784 RGB images), the value function over time (with fruit reward and goal acquisitions), the layout of the mazes with consecutive trajectories of the agent marked in different colours and the output of the trained position decoder, overlayed on top of the maze layout.\nB NETWORK ARCHITECTURE AND TRAINING\nB.1 THE ONLINE MULTI-LEARNER ALGORITHM FOR MULTI-TASK LEARNING\nWe introduce a class of neural network-based agents that have modular structures and that are trained on multiple tasks, with inputs coming from different modalities (vision, depth, past rewards and past actions). Implementing our agent architecture is simplified by its modular nature. Essentially, we construct multiple networks, one per task, using shared building blocks, and optimise these networks jointly. Some modules, such as the conv-net used for perceiving visual inputs, or the LSTMs used for learning the navigation policy, are shared among multiple tasks, while other modules, such as depth predictor gd or loop closure predictor gl, are task-specific. The navigation network that outputs the policy and the value function is trained using reinforcement learning, while the depth prediction and loop closure prediction networks are trained using self-supervised learning.\nWithin each thread of the asynchronous training environment, the agent plays on its own episode of the game environment, and therefore sees observation and reward pairs {(st, rt)} and takes actions that are different from those experienced by agents from the other, parallel threads. Within a thread, the multiple tasks (navigation, depth and loop closure prediction) can be trained at their own schedule, and they add gradients to the shared parameter vector as they arrive. Within each thread, we use a flag-based system to subordinate gradient updates to the A3C reinforcement learning procedure.\nB.2 NETWORK AND TRAINING DETAILS\nFor all the experiments we use an encoder model with 2 convolutional layers followed by a fully connected layer, or recurrent layer(s), from which we predict the policy and value function. The architecture is similar to the one in (Mnih et al., 2016). The convolutional layers are as follows. The first convolutional layer has a kernel of size 8x8 and a stride of 4x4, and 16 feature maps. The second layer has a kernel of size 4x4 and a stride of 2x2, and 32 feature maps. The fully connected layer, in the FF A3C architecture in Figure 2a has 256 hidden units (and outputs visual features ft). The LSTM in the LSTM A3C architecture has 256 hidden units (and outputs LSTM hidden activations ht). The LSTMs in Figure 2c and 2d are fed extra inputs (past reward rt\u22121, previous action at expressed as a one-hot vector of dimension 8 and agent-relative lateral and rotational velocity vt encoded by a 6-dimensional vector), which are all concatenated to vector ft. The Nav A3C architectures (Figure 2c,d) have a first LSTM with 64 or 128 hiddens and a second LSTM with 256 hiddens. The depth predictor modules gd, g\u2032d and the loop closure detection module gl are all single-layer MLPs with 128 hidden units. The depth MLPs are followed by 64 independent 8-dimensional softmax outputs (one per depth pixel). The loop closure MLP is followed by a 2-dimensional softmax output. We illustrate on Figure 8 the architecture of the Nav A3C+D+L+Dr agent.\nDepth is taken as the Z-buffer from the Labyrinth environment (with values between 0 and 255), divided by 255 and taken to power 10 to spread the values in interval [0, 1]. We empirically decided to use the following quantization: {0, 0.05, 0.175, 0.3, 0.425, 0.55, 0.675, 0.8, 1} to ensure a uniform\n5Video of the Nav A3C*+D1L agent on the I-maze: https://youtu.be/PS4iJ7Hk_BU 6Video of the Nav A3C*+D1L agent on static maze 1: https://youtu.be/-HsjQoIou_c 7Video of the Nav A3C*+D1L agent on static maze 2: https://youtu.be/kH1AvRAYkbI 8Video of the Nav A3C*+D1L agent on random goal maze 1: https://youtu.be/5IBT2UADJY0 9Video of the Nav A3C*+D1L agent on random goal maze 2: https://youtu.be/e10mXgBG9yo\nbinning across 8 classes. The previous version of the agent had a single depth prediction MLP gd for regressing 8\u00d7 16 = 128 depth pixels from the convnet outputs ft. The parameters of each of the modules point to a subset of a common vector of parameters. We optimise these parameters using an asynchronous version of RMSProp (Tieleman & Hinton, 2012). (Nair et al., 2015) was a recent example of asynchronous and parallel gradient updates in deep reinforcement learning; in our case, we focus on the specific Asynchronous Advantage Actor Critic (A3C) reinforcement learning procedure in (Mnih et al., 2016).\nLearning follows closely the paradigm described in (Mnih et al., 2016). We use 16 workers and the same RMSProp algorithm without momentum or centering of the variance. Gradients are computed over non-overlaping chunks of the episode. The score for each point of a training curve is the average over all the episodes the model gets to finish in 5e4 environment steps.\nThe whole experiments are run for a maximum of 1e8 environment step. The agent has an action repeat of 4 as in (Mnih et al., 2016), which means that for 4 consecutive steps the agent will use the same action picked at the beginning of the series. For this reason through out the paper we actually report results in terms of agent perceived steps rather than environment steps. That is, the maximal number of agent perceived step that we do for any particular run is 2.5e7.\nIn our grid we sample hyper-parameters from categorical distributions:\n\u2022 Learning rate was sampled from [10\u22124, 5 \u00b7 10\u22124]. \u2022 Strength of the entropy regularization from [10\u22124, 10\u22123]. \u2022 Rewards were not scaled and not clipped in the new set of experiments. In our previous set\nof experiments, rewards were scaled by a factor from {0.3, 0.5} and clipped to 1 prior to back-propagation in the Advantage Actor-Critic algorithm. \u2022 Gradients are computed over non-overlaping chunks of 50 or 75 steps of the episode. In our\nprevious set of experiments, we used chunks of 100 steps.\nThe auxiliary tasks, when used, have hyperparameters sampled from:\n\u2022 Coefficient \u03b2d of the depth prediction loss from convnet features Ld sampled from {3.33, 10, 33}. \u2022 Coefficient \u03b2\u2032d of the depth prediction loss from LSTM hiddens Ld\u2032 sampled from {1, 3.33, 10}. \u2022 Coefficient \u03b2l of the loop closure prediction loss Ll sampled from {1, 3.33, 10}.\nLoop closure uses the following thresholds: maximum distance for position similarity \u03b71 = 1 square and minimum distance for removing trivial loop-closures \u03b72 = 2 squares.\nC ADDITIONAL RESULTS\nC.1 REWARD CLIPPING\nFigure 9 shows additional learning curves. In particular in the left plot we show that the baselines (A3C FF and A3C LSTM) as well as Nav A3C agent without auxiliary losses, perform worse without reward clipping than with reward clipping. It seems that removing reward clipping makes learning unstable in absence of auxiliary tasks. For this particular reason we chose to show the baselines with reward clipping in our main results.\nC.2 DEPTH PREDICTION AS REGRESSION OR CLASSIFICATION TASKS\nThe right subplot of Figure 9 compares having depth as an input versus as a target. Note that using RGBD inputs to the Nav A3C agent performs even worse than predicting depth as a regression task, and in general is worse than predicting depth as a classification task.\nC.3 NON-NAVIGATION TASKS IN 3D MAZE ENVIRONMENTS\nWe have evaluated the behaviour of the agents introduced in this paper, as well as agents with reward prediction, introduced in (Jaderberg et al., 2017) (Nav A3C*+R) and with a combination of reward prediction from the convnet and depth prediction from the policy LSTM (Nav A3C+RD2), on different 3D maze environments with non-navigation specific tasks. In the first environment, Seek-Avoid Arena, there are apples (yielding 1 point) and lemons (yielding -1 point) disposed in an arena, and the agents needs to pick all the apples before respawning; episodes last 20 seconds. The second environment, Stairway to Melon, is a thin square corridor; in one direction, there is a lemon followed by a stairway to a melon (10 points, resets the level) and in the other direction are 7 apples and a dead end, with the melon visible but not reachable. The agent spawns between the lemon and the apples with a random orientation. Both environments have been released in DeepMind Lab (Beattie et al., 2016). These environments do not require navigation skills such as shortest path planning, but a simple reward identification (lemon vs. apple or melon) and persistent exploration. As Figure 10 shows, there is no major difference between auxiliary tasks related to depth prediction or reward prediction. Depth prediction boosts the performance of the agent beyond that of the stacked LSTM architecture, hinting at a more general applicability of depth prediction beyond navigation tasks.\nC.4 SENSITIVITY TOWARDS HYPER-PARAMETER SAMPLING\nFor each of the experiments in this paper, 64 replicas were run with hyperparameters (learning rate, entropy cost) sampled from the same interval. Figure 11 shows that the Nav architectures with\nauxiliary tasks achieve higher results for a comparatively larger number of replicas, hinting at the fact that auxiliary tasks make learning more robust to the choice of hyperparameters.\nC.5 ASYMPTOTIC PERFORMANCE OF THE AGENTS\nFinally, we compared the asymptotic performance of the agents, both in terms of navigation (final rewards obtained at the end of the episode) and in terms of their representation in the policy LSTM. Rather than visualising the convolutional filters, we quantify the change in representation, with and\nwithout auxiliary task, in terms of position decoding, following the approach explained in Section 5.1. Specifically, we compare the baseline agent (LSTM A3C*) to a navigation agent with one auxiliary task (depth prediction), that gets about twice as many gradient updates for the same number of frames seen in the environment: once for the RL task and once for the auxiliary depth prediction task. As Table 3 shows, the performance of the baseline agent as well as the position decoding accuracy do significantly increase after twice the number of training steps (going from 57 points to 90 points, and from 33.4% to 66.5%, but do not reach the performance and position decoding accuracy of the Nav A3C+D2 agent after half the number of training frames. For this reason, we believe that the auxiliary task do more than simply accelerate training.\n", "cats": {"READ": 1.0, "DONTREAD": 0.0}, "reviews": [{"IS_META_REVIEW": true, "comments": "This paper shows that a deep RL approach augmented with auxiliary tasks improves performance on navigation in complex environments. Specifically, A3C is used for the RL problem, and the agent is simultaneously trained on an unsupervised depth prediction task and a self-supervised loop closure classification task. While the use of auxiliary tasks to improve training of models including RL agents is not new, the main contribution here is the use of tasks that encourage learning an intrinsic representation of space and movement that enables significant improvements on maze navigation tasks.\n\nThe paper is well written, experiments are convincing, and the value of the auxiliary tasks for the problem are clear. However, the contribution is relatively incremental given previous work on RL for navigation and on auxiliary tasks. The work could become of greater interest provided broader analysis and insights on either optimal combinations of tasks for visual navigation (e.g. the value of other visual / geometry-based tasks), or on auxiliary tasks with RL in general.  As it is, it is a useful demonstration of the benefit of geometry-based auxiliary tasks for navigation, but of relatively narrow interest."}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "The paper proposes an approach to navigating in complex environments using RL agents that have auxiliary tasks besides just the successful navigation itself (for instance, the task of predicting depth from images). The idea is a nice one, and the demonstration is fairly compelling. The one aspect that seems a bit unsatisfying is that the the approach does seem a bit ad-hoc, and could be made more formal, but presenting these results on a challenging task like this navigation problem is certainly sufficient for the paper to be worth accepting. The pros and cons are as follows:\n \n \n Pros:\n + Idea of formulating auxiliary tasks is a nice one and the precise form in which it is done here appears novel\n + Good results on a challenge task of maze navigation from visual data\n \n Cons:\n - Methodology does seem a bit ad-hoc, it would be nice to see if some of the auxiliary task mechanisms could be formalized beyond simple \"this is what worked for this domain\"", "OTHER_KEYS": "ICLR 2017 pcs"}, {"DATE": "13 Jan 2017", "TITLE": "Response to reviewers and revision summary", "IS_META_REVIEW": false, "comments": "We have addressed the points/suggestions raised through several additional experiments which have been added to the paper in the latest revision.\n\n1) Exploring the optimal combinations of auxiliary tasks: we now include an additional auxiliary task, reward prediction (shown to be effective across a range of tasks in Jaderberg et al. 2016, ICLR submission). Results show that depth prediction is superior to reward prediction in the navigational settings examined, with the combination of the two being no more effective (section 5.3, table 2).\n\n2) The focus of our paper on navigation and the depth prediction auxiliary task: we now present results showing that auxiliary depth prediction is beneficial in scenarios that have minimal navigational demands, suggesting its general effectiveness (appendix C.3; figure 10).\n\n3) Whether auxiliary tasks increase robustness to hyperparameters: we provide a more systematic analysis to show that this does seem to be the case (appendix C.4; figure 11).\n\n4) Whether auxiliary tasks simply accelerate training: we provide evidence that they do more that this through analyses of asymptotic performance (appendix C.5; table 3), effects on the representations learnt (i.e. indexed by position decoding), demonstration that depth prediction shows benefits compared to the reward prediction auxiliary task in the navigation domain. ", "OTHER_KEYS": "Piotr W Mirowski"}, {"TITLE": "well presented, convincing, but of limited novelty", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "This paper shows that a deep RL approach augmented with auxiliary tasks improves performance on navigation in complex environments. Specifically, A3C is used for the RL problem, and the agent is simultaneously trained on an unsupervised depth prediction task and a self-supervised loop closure classification task. While the use of auxiliary tasks to improve training of models including RL agents is not new, the main contribution here is the use of tasks that encourage learning an intrinsic representation of space and movement that enables significant improvements on maze navigation tasks.\n\nThe paper is well written, experiments are convincing, and the value of the auxiliary tasks for the problem are clear. However, the contribution is relatively incremental given previous work on RL for navigation and on auxiliary tasks. The work could become of greater interest provided broader analysis and insights on either optimal combinations of tasks for visual navigation (e.g. the value of other visual / geometry-based tasks), or on auxiliary tasks with RL in general.  As it is, it is a useful demonstration of the benefit of geometry-based auxiliary tasks for navigation, but of relatively narrow interest.", "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 5}, {"TITLE": "Depth is supervise learning", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "I do like the demonstration that including learning of auxiliary tasks does not interfere with the RL tasks but even helps. This is also not so surprising with deep networks. The deep structure of the model allows the model to learn first a good representation of the world on which it can base its solutions for specific goals. While even early representations do of course depend on the task performance itself, it is clear that there are common first stages in sensory representations like the need for edge detection etc. Thus, training by additional tasks will at least increase the effective training size. It is of course unclear how to adjust for this to make a fair comparison, but the paper could have included some more insights such as the change in representation with and without auxiliary training. \n\nI still strongly disagree with the implied definition of supervised or even self-supervised learning. The definition of unsupervised is learning without external labels. It does not matter if this comes from a human or for example from an expensive machine that is used to train a network so that a task can be solved later without this expensive machine. I would call EM a self-supervised method where labels are predicted from the model itself and used to bootstrap parameter learning. In this case you are using externally supplied labels, which is clearly a supervised learning task!\n", "IS_META_REVIEW": false, "RECOMMENDATION": 5, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "Review", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer4", "comments": "This relatively novel work proposes to augment current RL models by adding self-supervised tasks encouraging better internal representations. \nThe proposed tasks are depth prediction and loop closure detection. While these tasks assume a 3D environment as well some position information, such priors are well suited to a large variety of tasks pertaining to navigation and robotics.\n\nExtensive experiments suggest to incorporating such auxiliary tasks increase performance and to a large extent learning speed.\nAdditional analysis of value functions and internal representations suggest that some structure is being discovered by the model, which would not be without the auxiliary tasks.\n\n\nWhile specific to 3D-environment tasks, this work provides additional proof that using input data in addition to sparse external reward signals helps to boost learning speed as well as learning better internal representation. It is original, clearly presented, and strongly supported by empirical evidence.\n\nOne small downside of the experimental method (or maybe just the results shown) is that by picking top-5 runs, it is hard to judge whether such a model is better suited to the particular hyperparameter range that was chosen, or is simply more robust to these hyperparameter settings. Maybe an analysis of performance as a function of hyperparameters would help confirm the superiority of the approach to the baselines. My own suspicion is that adding auxiliary tasks would make the model robust to bad hyperparameters.\n\nAnother downside is that the authors dismiss navigation literature as \"not RL\". I sympathize with the limit on the number of things that can fit in a paper, but some experimental comparison with such literature may have proven insightful, if just in measuring the quality of the learned representations.\n", "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 3}, {"DATE": "04 Dec 2016", "TITLE": "other auxiliary tasks and gradient contribution", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1"}, {"DATE": "01 Dec 2016", "TITLE": "A few questions", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer4"}, {"DATE": "28 Nov 2016", "TITLE": "Unsupervised depth perception", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3"}, {"DATE": "26 Nov 2016", "TITLE": "Paper update", "IS_META_REVIEW": false, "comments": "We have just submitted an updated version of the paper, including additional references as well as new results on those agents that are enhanced with auxiliary tasks.\nSpecifically, we investigated:\n1) a new way of performing depth prediction, by formulating it as a classification task (over the quantized depth image) and compared it to depth regression.\n2) the use of depth prediction as an auxiliary task for the policy LSTM, instead of the convnet.\n3) the effect, during actor critic training, of reward clipping on the performance of the agent as well as on the stability of RL learning.", "OTHER_KEYS": "Piotr W Mirowski"}, {"DATE": "15 Nov 2016", "TITLE": "Prior Work", "IS_META_REVIEW": false, "comments": "I like the approach (and more thorough insights) into making the RL problem easier by providing additional, related SL tasks to learn; however I think that \"Recurrent Reinforcement Learning: A Hybrid Approach\" should be cited as previous work in this regard (on top of Lample & Chaplot).", "OTHER_KEYS": "Kai Arulkumaran"}, {"IS_META_REVIEW": true, "comments": "This paper shows that a deep RL approach augmented with auxiliary tasks improves performance on navigation in complex environments. Specifically, A3C is used for the RL problem, and the agent is simultaneously trained on an unsupervised depth prediction task and a self-supervised loop closure classification task. While the use of auxiliary tasks to improve training of models including RL agents is not new, the main contribution here is the use of tasks that encourage learning an intrinsic representation of space and movement that enables significant improvements on maze navigation tasks.\n\nThe paper is well written, experiments are convincing, and the value of the auxiliary tasks for the problem are clear. However, the contribution is relatively incremental given previous work on RL for navigation and on auxiliary tasks. The work could become of greater interest provided broader analysis and insights on either optimal combinations of tasks for visual navigation (e.g. the value of other visual / geometry-based tasks), or on auxiliary tasks with RL in general.  As it is, it is a useful demonstration of the benefit of geometry-based auxiliary tasks for navigation, but of relatively narrow interest."}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "The paper proposes an approach to navigating in complex environments using RL agents that have auxiliary tasks besides just the successful navigation itself (for instance, the task of predicting depth from images). The idea is a nice one, and the demonstration is fairly compelling. The one aspect that seems a bit unsatisfying is that the the approach does seem a bit ad-hoc, and could be made more formal, but presenting these results on a challenging task like this navigation problem is certainly sufficient for the paper to be worth accepting. The pros and cons are as follows:\n \n \n Pros:\n + Idea of formulating auxiliary tasks is a nice one and the precise form in which it is done here appears novel\n + Good results on a challenge task of maze navigation from visual data\n \n Cons:\n - Methodology does seem a bit ad-hoc, it would be nice to see if some of the auxiliary task mechanisms could be formalized beyond simple \"this is what worked for this domain\"", "OTHER_KEYS": "ICLR 2017 pcs"}, {"DATE": "13 Jan 2017", "TITLE": "Response to reviewers and revision summary", "IS_META_REVIEW": false, "comments": "We have addressed the points/suggestions raised through several additional experiments which have been added to the paper in the latest revision.\n\n1) Exploring the optimal combinations of auxiliary tasks: we now include an additional auxiliary task, reward prediction (shown to be effective across a range of tasks in Jaderberg et al. 2016, ICLR submission). Results show that depth prediction is superior to reward prediction in the navigational settings examined, with the combination of the two being no more effective (section 5.3, table 2).\n\n2) The focus of our paper on navigation and the depth prediction auxiliary task: we now present results showing that auxiliary depth prediction is beneficial in scenarios that have minimal navigational demands, suggesting its general effectiveness (appendix C.3; figure 10).\n\n3) Whether auxiliary tasks increase robustness to hyperparameters: we provide a more systematic analysis to show that this does seem to be the case (appendix C.4; figure 11).\n\n4) Whether auxiliary tasks simply accelerate training: we provide evidence that they do more that this through analyses of asymptotic performance (appendix C.5; table 3), effects on the representations learnt (i.e. indexed by position decoding), demonstration that depth prediction shows benefits compared to the reward prediction auxiliary task in the navigation domain. ", "OTHER_KEYS": "Piotr W Mirowski"}, {"TITLE": "well presented, convincing, but of limited novelty", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "This paper shows that a deep RL approach augmented with auxiliary tasks improves performance on navigation in complex environments. Specifically, A3C is used for the RL problem, and the agent is simultaneously trained on an unsupervised depth prediction task and a self-supervised loop closure classification task. While the use of auxiliary tasks to improve training of models including RL agents is not new, the main contribution here is the use of tasks that encourage learning an intrinsic representation of space and movement that enables significant improvements on maze navigation tasks.\n\nThe paper is well written, experiments are convincing, and the value of the auxiliary tasks for the problem are clear. However, the contribution is relatively incremental given previous work on RL for navigation and on auxiliary tasks. The work could become of greater interest provided broader analysis and insights on either optimal combinations of tasks for visual navigation (e.g. the value of other visual / geometry-based tasks), or on auxiliary tasks with RL in general.  As it is, it is a useful demonstration of the benefit of geometry-based auxiliary tasks for navigation, but of relatively narrow interest.", "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 5}, {"TITLE": "Depth is supervise learning", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "I do like the demonstration that including learning of auxiliary tasks does not interfere with the RL tasks but even helps. This is also not so surprising with deep networks. The deep structure of the model allows the model to learn first a good representation of the world on which it can base its solutions for specific goals. While even early representations do of course depend on the task performance itself, it is clear that there are common first stages in sensory representations like the need for edge detection etc. Thus, training by additional tasks will at least increase the effective training size. It is of course unclear how to adjust for this to make a fair comparison, but the paper could have included some more insights such as the change in representation with and without auxiliary training. \n\nI still strongly disagree with the implied definition of supervised or even self-supervised learning. The definition of unsupervised is learning without external labels. It does not matter if this comes from a human or for example from an expensive machine that is used to train a network so that a task can be solved later without this expensive machine. I would call EM a self-supervised method where labels are predicted from the model itself and used to bootstrap parameter learning. In this case you are using externally supplied labels, which is clearly a supervised learning task!\n", "IS_META_REVIEW": false, "RECOMMENDATION": 5, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "Review", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer4", "comments": "This relatively novel work proposes to augment current RL models by adding self-supervised tasks encouraging better internal representations. \nThe proposed tasks are depth prediction and loop closure detection. While these tasks assume a 3D environment as well some position information, such priors are well suited to a large variety of tasks pertaining to navigation and robotics.\n\nExtensive experiments suggest to incorporating such auxiliary tasks increase performance and to a large extent learning speed.\nAdditional analysis of value functions and internal representations suggest that some structure is being discovered by the model, which would not be without the auxiliary tasks.\n\n\nWhile specific to 3D-environment tasks, this work provides additional proof that using input data in addition to sparse external reward signals helps to boost learning speed as well as learning better internal representation. It is original, clearly presented, and strongly supported by empirical evidence.\n\nOne small downside of the experimental method (or maybe just the results shown) is that by picking top-5 runs, it is hard to judge whether such a model is better suited to the particular hyperparameter range that was chosen, or is simply more robust to these hyperparameter settings. Maybe an analysis of performance as a function of hyperparameters would help confirm the superiority of the approach to the baselines. My own suspicion is that adding auxiliary tasks would make the model robust to bad hyperparameters.\n\nAnother downside is that the authors dismiss navigation literature as \"not RL\". I sympathize with the limit on the number of things that can fit in a paper, but some experimental comparison with such literature may have proven insightful, if just in measuring the quality of the learned representations.\n", "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 3}, {"DATE": "04 Dec 2016", "TITLE": "other auxiliary tasks and gradient contribution", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1"}, {"DATE": "01 Dec 2016", "TITLE": "A few questions", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer4"}, {"DATE": "28 Nov 2016", "TITLE": "Unsupervised depth perception", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3"}, {"DATE": "26 Nov 2016", "TITLE": "Paper update", "IS_META_REVIEW": false, "comments": "We have just submitted an updated version of the paper, including additional references as well as new results on those agents that are enhanced with auxiliary tasks.\nSpecifically, we investigated:\n1) a new way of performing depth prediction, by formulating it as a classification task (over the quantized depth image) and compared it to depth regression.\n2) the use of depth prediction as an auxiliary task for the policy LSTM, instead of the convnet.\n3) the effect, during actor critic training, of reward clipping on the performance of the agent as well as on the stability of RL learning.", "OTHER_KEYS": "Piotr W Mirowski"}, {"DATE": "15 Nov 2016", "TITLE": "Prior Work", "IS_META_REVIEW": false, "comments": "I like the approach (and more thorough insights) into making the RL problem easier by providing additional, related SL tasks to learn; however I think that \"Recurrent Reinforcement Learning: A Hybrid Approach\" should be cited as previous work in this regard (on top of Lample & Chaplot).", "OTHER_KEYS": "Kai Arulkumaran"}]}
{"text": "TIONS WITH NEURAL SIMILARITY AND CONTEXT EN- CODERS\n1 INTRODUCTION\nMany dimensionality reduction or manifold learning algorithms optimize for retaining the pairwise similarities, distances, or local neighborhoods of data points. Classical scaling (Cox & Cox, 2000), kernel PCA (Sch\u00f6lkopf et al., 1998), isomap (Tenenbaum et al., 2000), and LLE (Roweis & Saul, 2000) achieve this by performing an eigendecomposition of some similarity matrix to obtain a low dimensional representation of the original data. However, this is computationally expensive if a lot of training examples are available. Additionally, out-of-sample representations can only be created when the similarities to the original training examples can be computed (Bengio et al., 2004).\nFor some methods such as t-SNE (van der Maaten & Hinton, 2008), great effort was put into extending the algorithm to work with large datasets (van der Maaten, 2013) or to provide an explicit mapping function which can be applied to new data points (van der Maaten, 2009). Current attempts at finding a more general solution to these issues are complex and require the development of specific cost functions and constraints when used in place of existing algorithms (Bunte et al., 2012), which limits their applicability to new objectives.\nIn this paper we introduce a new neural network architecture, that we will denote as similarity encoder (SimEc), which is able to learn representations that can retain arbitrary pairwise relations present in the input space, even those obtained from unknown similarity functions such as human ratings. A SimEc can learn a linear or non-linear mapping function to project new data points into a lower dimensional embedding space. Furthermore, it can take advantage of large datasets since the objective function is optimized iteratively using stochastic mini-batch gradient descent. We show on both image and text datasets that SimEcs can, on the one hand, recreate solutions found by traditional methods such as kPCA or isomap, and, on the other hand, obtain meaningful embeddings from similarities based on human labels.\nAdditionally, we propose the new context encoder (ConEc) model, a variation of similarity encoders for learning word embeddings, which extends word2vec (Mikolov et al., 2013b) by using the local context of words as input to the neural network to create representations for out-of-vocabulary words and to distinguish between multiple meanings of words. This is shown to be advantageous, for example, if the word embeddings are used as features in a named entity recognition task as demonstrated on the CoNLL 2003 challenge.\n2 SIMILARITY ENCODERS\nWe propose a novel dimensionality reduction framework termed similarity encoder (SimEc), which can be used to learn a linear or non-linear mapping function for computing low dimensional representations of data points such that the original pairwise similarities between the data points in the input space are preserved in the embedding space. For this, we borrow the \u201cbottleneck\u201d neural network (NN) architecture idea from autoencoders (Tishby et al., 2000; Hinton & Salakhutdinov, 2006). Autoencoders aim to transform the high dimensional data points into low dimensional embeddings such that most of the data\u2019s variance is retained. Their network architecture has two parts: The first part of the network maps the data points from the original feature space to the low dimensional embedding (at the bottleneck). The second part of the NN mirrors the first part and projects the embedding back to a high dimensional output. This output is then compared to the original input to compute the reconstruction error of the training samples, which is used in the backpropagation procedure to tune the network\u2019s parameters. After the training is complete, i.e. the low dimensional embeddings encode enough information about the original input samples to allow for their reconstruction, the second part of the network is discarded and only the first part is used to project data points into the low dimensional embedding space. Similarity encoders have a similar two fold architecture, where in the first part of the network, the data is mapped to a low dimensional embedding, and then in the second part (which is again only used during training), the embedding is transformed such that the error of the representation can be computed. However, since here the objective is to retain the (non-linear) pairwise similarities instead of the data\u2019s variance, the second part of the NN does not mirror the first like it does in the autoencoder architecture.\nThe similarity encoder architecture (Figure 1) uses as the first part of the network a flexible non-linear feed-forward neural network to map the high dimensional input data points xi \u2208 RD to a low dimensional embedding yi \u2208 Rd (at the bottleneck). As we make no assumptions on the range of values the embedding can take, the last layer of the first part of the NN (i.e. the one resulting in the embedding) is always linear. For example, with two additional non-linear hidden layers, the embedding would be computed as\nyi = \u03c31(\u03c30(xiW0)W1)W2,\nwhere \u03c30 and \u03c31 denote your choice of non-linear activation functions (e.g. tanh, sigmoid, or relu), but there is no non-linearity applied after multiplying with W2. The second part of the network then\nconsists of a single additional layer with the weight matrix W\u22121 \u2208 Rd\u00d7N to project the embedding to the output, the approximated similarities s\u2032 \u2208 RN :\ns\u2032 = \u03c3\u22121(yiW\u22121).\nThese approximated similarities are then compared to the target similarities (for one data point this is the corresponding row si \u2208 RN of the similarity matrix S \u2208 RN\u00d7N of the N training samples) and the computed error is used to tune the network\u2019s parameters with backpropagation.\nFor the model to learn most efficiently, the exact form of the cost function to optimize as well as the type of non-linearity \u03c3\u22121 applied when computing the network\u2019s output should be chosen with respect to the type of target similarities that the model is supposed to preserve. In the experimental section of the paper we are considering two application scenarios of SimEcs: a) to obtain the same low dimensional embedding as found by spectral methods such as kPCA, and b) to embed data points such that binary similarity relations obtained from human labels are preserved. In the first case (further discussed in the next section), we omit the non-linearity when computing the output of the network, i.e. s\u2032 = yiW\u22121, since the target similarities, computed by some kernel function, are not necessarily constrained to lie in a specific interval. As the cost function to minimize we choose the mean squared error between the output (approximated similarities) and the original (target) similarities. A regularization term is added to encourage the weights of the last layer (W\u22121) to be orthogonal.1 The model\u2019s objective function optimized during training is therefore:\nmin 1\nN N\u2211 i=1 \u2016si \u2212 s\u2032\u201622 + \u03bb 1 d2 \u2212 d \u2225\u2225W\u22121W>\u22121 \u2212 diag(W\u22121W>\u22121)\u2225\u22251\nwhere \u2016 \u00b7 \u2016p denotes the respective p-norms for vectors and matrices and \u03bb is a hyperparameter to control the strength of the regularization. In the second case, the target similarities are binary and it therefore makes sense to use a nonlinear activation function in the final layer when computing the output of the network to ensure the approximated similarities are between 0 and 1 as well:2\ns\u2032 = \u03c3\u22121(yiW\u22121) with \u03c3\u22121(z) = 1\n1 + e\u221210(z\u22120.5) .\nWhile the mean squared error between the target and approximated similarities would still be a natural choice of cost function to optimize, with the additional non-linearity in the output layer, learning might be slow due to small gradients and we therefore instead optimize the cross-entropy:\nmin \u2212 1 N\n\u2211 [si ln(s \u2032) + (1\u2212 si) ln(1\u2212 s\u2032)] .\nFor a different application scenario, yet another setup might lead to the best results. When using SimEcs in practice, we recommend to first try the first setup, i.e. keeping the output layer linear and minimizing the mean squared error, as this often already gives quite good results.\nAfter the training is completed, only the first part of the neural network, which maps the input to the embedding, is used to create the representations of new data points. Depending on the complexity of the feed-forward NN, the mapping function learned by similarity encoders can be linear or non-linear, and because of the iterative optimization using stochastic mini-batch gradient descent, large amounts of data can be utilized to learn optimal representations.3\n2.1 RELATION TO KERNEL PCA\nKernel PCA (kPCA) is a popular non-linear dimensionality reduction algorithm, which performs the eigendecomposition of a kernel matrix to obtain low dimensional representations of the data points\n1To get embeddings similar to those obtained by kPCA, orthogonal weights in the last layer of the NN help as they correspond to the orthogonal eigenvectors of the kernel matrix found by kPCA.\n2This scaled and shifted sigmoid function maps values between 0 and 1 almost linearly while thresholding values outside this interval.\n3To speed up the training procedure and limit memory requirements for large datasets, the columns of the similarity matrix can also be subsampled (yielding S \u2208 RN\u00d7n), i.e. the number of target similarities (and the dimensionality of the output layer) is n < N , however all N training examples can still be used as input to train the network.\n(Sch\u00f6lkopf et al., 1998). However, if the kernel matrix is very large this becomes computationally very expensive. Additionally, there are constraints on possible kernel functions (should be positive semi-definite) and new data points can only be embedded in the lower dimensional space if their kernel map (i.e. the similarities to the original training points) can be computed. As we show below, SimEc can optimize the same objective as kPCA but addresses these shortcomings.\nThe general idea is that both kPCA and SimEc embed the N data points in a feature space where the given target similarities can be approximated linearly (i.e. with the scalar product of the embedding vectors). When the error between the approximated (S\u2032) and the target similarities (S) is computed as the mean squared error, kPCA finds the optimal approximation by performing the eigendecomposition of the (centered) target similarity matrix, i.e.\nS\u2032 = Y Y >,\nwhere Y \u2208 RN\u00d7d is the low dimensional embedding of the data based on the eigenvectors belonging to the d largest eigenvalues of S.\nIn addition to the embedding itself, it is often desired to have a parametrized mapping function, which can be used to project new (out-of-sample) data points into the embedding space. If the target similarity matrix is the linear kernel, i.e. S = XX> where X \u2208 RN\u00d7D is the given input data, this can easily be accomplished with traditional PCA. Here, the covariance matrix of the centered input data, i.e. C = X>X is decomposed to obtain a matrix with parameters, W\u0303 \u2208 RD\u00d7d, based on the eigenvectors belonging to the d largest eigenvalues of the covariance matrix. Then the optimal embedding (i.e. the same solution obtained by linear kPCA) can be computed as\nY = XW\u0303.\nThis serves as a mapping function, with which new data points can be easily projected into the lower dimensional embedding space.\nWhen using a similarity encoder to embed data in a low dimensional space where the linear similarities are preserved, the SimEc\u2019s architecture would consist of a neural network with a single linear layer, i.e. the parameter matrix W0, to project the input data X to the embedding Y = XW0, and another matrix W\u22121 \u2208 Rd\u00d7N used to approximate the similarities as S\u2032 = YW\u22121. From these formulas one can immediately see the link between linear similarity encoders and PCA / linear kPCA: once the parameters of the neural network are tuned correctly, W0 would correspond to the mapping matrix W\u0303 found by PCA and W\u22121 could be interpreted as Y >, i.e. Y would be the same eigenvector based embedding as found with linear kPCA.\nFinding the corresponding function to map new data points into the embedding space is trivial for linear kPCA, but this is not the case for other kernel functions. While it is still possible to find the optimal embedding with kPCA for non-linear kernel functions, the mapping function remains unknown and new data points can only be projected into the embedding space if we can compute their kernel map, i.e. the similarities to the original training examples (Bengio et al., 2004). Some attempts were made to manually define an explicit mapping function to represent data points in the kernel feature space, however this only works for specific kernels and there exists no general solution (Rahimi & Recht, 2007). As neural networks are universal function approximators, with the right architecture similarity encoders could instead learn arbitrary mapping functions for unknown similarities to arrive at data driven kernel learning solutions.\n2.2 MODEL OVERVIEW\nThe properties of similarity encoders are summarized in the following. The objective of this dimensionality reduction approach is to retain pairwise similarities between data points in the embedding space. This is achieved by tuning the parameters of a neural network to obtain a linear or non-linear mapping (depending on the network\u2019s architecture) from the high dimensional input to the low dimensional embedding. Since the cost function is optimized using stochastic mini-batch gradient descent, we can take advantage of large datasets for training. The embedding for new test points can be easily computed with the explicit mapping function in the form of the tuned neural network. And since there is no need to compute the similarity of new test examples to the original training data for out-of-sample solutions (like with kPCA), the target similarities can be generated by an unknown process such as human similarity judgments.\n2.3 EXPERIMENTS\nIn the following experiments we demonstrate that similarity encoders can, on the one hand, reach the same solution as kPCA, and, on the other hand, generate meaningful embeddings from human labels. To illustrate that this is independent of the type of data, we present results obtained both on the well known MNIST handwritten digits dataset as well as the 20 newsgroups text corpus. Further details as well as the code to replicate these experiments and more is available online.4\nWe compare the embedding found with linear kPCA to that created with a linear similarity encoder (consisting of one linear layer mapping the input to the embedding and a second linear layer to project the embedding to the output, i.e. computing the approximated similarities). Additionally, we show that a non-linear SimEc can approximate the solution found with isomap (i.e. the eigendecomposition of the geodesic distance matrix). We found that for optimal results the kernel matrix used as the target similarity matrix for the SimEc should first be centered (as it is being done for kPCA as well (M\u00fcller et al., 2001)).\nIn a second step, we show that SimEcs can learn the mapping to a low dimensional embedding for arbitrary similarity functions and reliably create representations for new test samples without the need to compute their similarities to the original training examples, thereby going beyond the capabilities of kPCA. For both datasets we illustrate this by using the class labels assigned to the samples by human annotators to create the target similarity matrix for the training fold of the data, i.e. S is 1 for data points belonging to the same class and 0 everywhere else. We compare the solutions found by SimEc architectures with a varying number of additional non-linear hidden layers in the first part of the network (while keeping the embedding layer linear as before) to show how a more complex network improves the ability to map the data into an embedding space in which the class-based similarities are retained.\nMNIST The MNIST dataset contains 28\u00d7 28 pixel images depicting handwritten digits. For our experiments we randomly subsampled 10k images from all classes, of which 80% are assigned to the training fold and the remaining 20% to the test fold (in the following plots, data points belonging to the training set are displayed transparently while the test points are opaque). As shown in Figure 2, the embeddings of the MNIST dataset created with linear kPCA and a linear similarity encoder, which uses as target similarities the linear kernel matrix, are almost identical (up to a rotation). The same holds true for the isomap embedding, which is well approximated by a non-linear SimEc with two hidden layers using the geodesic distances between the data points as targets (Figure 8 in the Appendix). When optimizing SimEcs to retain the class-based similarities (Figure 3), additional\nnon-linear hidden layers in the feed-forward NN can improve the embedding by further separating data points belonging to different classes in tight clusters. As it can be seen, the test points (opaque) are nicely mapped into the same locations as the corresponding training points (transparent), i.e. the model learns to associate the input pixels with the class clusters only based on the imposed similarities between the training data points.\n4https://github.com/cod3licious/simec/examples_simec.ipynb\n20 newsgroups The 20 newsgroups dataset consists of around 18k newsgroup posts assigned to 20 different topics. We take a subset of seven categories and use the original train/test split (\u223c4.1k and \u223c2.7k samples respectively) and remove metadata such as headers to avoid overfitting.5 All text documents are transformed into 46k dimensional tf-idf feature vectors, which are used as input to the SimEc and to compute the linear kernel matrix of the training fold. The embedding created with linear kPCA is again well approximated by the solution found with a corresponding linear SimEc (Figure 9 in the Appendix). Additionally, this serves as an example where traditional PCA is not an option to obtain the corresponding mapping matrix for the linear kPCA solution, as due to the high dimensionality of the input data and comparatively low number of samples, the empirical covariance matrix would be poorly estimated and too large to decompose into eigenvalues and -vectors. With the objective to retain the class-based similarities, a SimEc with a non-linear hidden layer clusters documents by their topics (Figure 4).\n3 CONTEXT ENCODERS\nRepresentation learning is very prominent in the field of natural language processing (NLP). For example, word embeddings learned by neural network language models were shown to improve the performance when used as features for supervised learning tasks such as named entity recognition (NER) (Collobert et al., 2011; Turian et al., 2010). The popular word2vec model (Figure 5) learns meaningful word embeddings by considering only the words\u2019 local contexts and thanks to its shallow architecture it can be trained very efficiently on large corpora. However, an important limiting factor of current word embedding models is that they only learn the representations for words from a fixed vocabulary. This means, if in a task we encounter a new word which was not present in the texts used for training, we can not create an embedding for this word without repeating the time consuming\n5http://scikit-learn.org/stable/datasets/twenty_newsgroups.html\ntraining procedure of the model.6 Additionally, word2vec, like many other approaches, only learns a single representation for every word. However, it is often the case that a single word can have multiple meanings, e.g. \u201cWashington\u201d is both the name of a US state as well as a former president. It is only the local context in which these words appear that lets humans resolve this ambiguity and identify the proper sense of the word in question. While attempts were made to improve this, they lack flexibility as they require a clustering of word contexts beforehand (Huang et al., 2012), which still does not guarantee that all possible meanings of a word have been identified prior in the training documents. Other approaches require additional labels such part-of-speech tags (Trask et al., 2015) or other lexical resources like WordNet (Rothe & Sch\u00fctze, 2015) to create word embeddings which distinguish between the different senses of a word.\nAs a further contribution of this paper we provide a link between the successful word2vec natural language model and similarity encoders and thereby propose a new model we call context encoder (ConEc), which can efficiently learn word embeddings from huge amounts of training data and additionally make use of local contexts to create representations for out-of-vocabulary words and help distinguish between multiple meanings of words.\n6In practice these models are trained on such a large vocabulary that it is rare to encounter a word which does not have an embedding. However, there are still scenarios where this is the case, for example, it is unlikely that the term \u201cW10281545\u201d is encountered in a regular training corpus, but we might still want its embedding to represent a search query like \u201cwhirlpool W10281545 ice maker part\u201d.\nFormally, word embeddings are d -dimensional vector representations learned for all N words in the vocabulary. Word2vec is a shallow model with parameter matrices W0,W1 \u2208 RN\u00d7d, which are tuned iteratively by scanning huge amounts of texts sentence by sentence (see Figure 5). Based on some context words the algorithm tries to predict the target word between them. Mathematically this is realized by first computing the sum of the embeddings of the context words by selecting the appropriate rows from W0. This vector is then multiplied by several rows selected from W1: one of these rows corresponds to the target word, while the others correspond to k \u2018noise\u2019 words, selected at random (negative sampling). After applying a non-linear activation function, the backpropagation error is computed by comparing this output to a label vector t \u2208 Rk+1, which is 1 at the position of the target word and 0 for all k noise words. After the training of the model is complete, the word embedding for a target word is the corresponding row of W0.\nThe main principle utilized when learning word embeddings is that similar words appear in similar contexts (Harris, 1954; Melamud et al., 2015). Therefore, in theory one could compute the similarities between all words by checking how many context words any two words generally have in common (possibly weighted somehow to reduce the influence of frequent words such as \u2018the\u2019 and \u2018and\u2019). However, such a word similarity matrix would be very large, as typically the vocabulary for which word embeddings are learned comprises several 10, 000 words, making it computationally too expensive to be used with similarity encoders. But this matrix would also be quite sparse, because many words in fact do not occur in similar contexts and most words only have a handful of synonyms which could be used in their place. Therefore, we can view the negative sampling approach used for word2vec (Mikolov et al., 2013b) as an approximation of the words\u2019 context based similarities: while the similarity of a word to itself is 1, if for one word we select k random words out of the huge vocabulary, it is very unlikely that they are similar to the target word, i.e. we can approximate their similarities with 0. This is the main insight necessary for adapting similarity encoders to be used for learning (context sensitive) word embeddings.\nFigure 6 shows the architecture of the context encoder. For the training procedure we stick very closely to the optimization strategy used by word2vec: while parsing a document, we again select a target word and its context words. As input to the context encoder network, we use a vector xi of length N (i.e. the size of the vocabulary), which indicates the context words by non-zero values (either binary or e.g. giving lower weight to context words further away from the target word). This vector is then multiplied by a first matrix of weights W0 \u2208 RN\u00d7d yielding a low dimensional embedding yi, comparable to the summed context embedding created as a first step when training the word2vec model. This embedding is then multiplied by a second matrix W1 \u2208 Rd\u00d7N to yield the output. Instead of comparing this output vector to a whole row from a word similarity matrix (as we would with similarity encoders), only k + 1 entries are selected, namely those belonging to\nthe target word as well as k random and unrelated noise words. After applying a non-linearity we compare these entries s\u2032 \u2208 Rk+1 to the binary target vector exactly as in the word2vec model and use error backpropagation to tune the parameters.\nUp to now, there are no real differences between the word2vec model and our context encoders, we have merely provided an intuitive interpretation of the training procedure and objective. The main deviation from the word2vec model lies in the computation of the word embedding for a target word after the training is complete. In the case of word2vec, the word embedding is simply the row of the tuned W0 matrix. However, when considering the idea behind the optimization procedure, we instead propose to compute a target word\u2019s representation by multiplying W0 with the word\u2019s average context vector. This is closer to what is being done in the training procedure and additionally it enables us to compute the embeddings for out-of-vocabulary words (assuming at least most of such a new word\u2019s context words are in the vocabulary) as well as to place more emphasis on a word\u2019s local context (which helps to identify the proper meaning of the word (Melamud et al., 2015)) by creating a weighted sum between the word\u2019s average global and local context vectors used as input to the ConEc.\nWith this new perspective on the model and optimization procedure, another advancement is feasible. Since the context words are merely a sparse feature vector used as input to a neural network, there is no reason why this input vector should not contain other features about the target word as well. For example, the feature vector could be extended to contain information about the word\u2019s case, part-of-speech (POS) tag, or other relevant details. While this would increase the dimensionality of the first weight matrix W0 to include the additional features when mapping the input to the word\u2019s embedding, the training objective and therefore also W1 would remain unchanged. These additional features could be especially helpful if details about the words would otherwise get lost in preprocessing (e.g. by lowercasing) or to retain information about a word\u2019s position in the sentence, which is ignored in a BOW approach. These extended ConEcs are expected to create embeddings which distinguish even better between the words\u2019 different senses by taking into account, for example, if the word is used as a noun or verb in the current context, similar to the sense2vec algorithm (Trask et al., 2015). However, unlike sense2vec, not multiple embeddings per term are learned, instead the dimensionality of the input vector is increased to include the POS tag of the current word as a feature.\n3.1 EXPERIMENTS\nThe word embeddings learned with word2vec and context encoders are evaluated on a word analogy task (Mikolov et al., 2013a) as well as the CoNLL 2003 NER benchmark task (Tjong et al., 2003). The word2vec model used is a continuous BOW model trained with negative sampling as described above where k = 13, the embedding dimensionality d is 200 and we use a context window of 5. The word embeddings created by the context encoders are build directly on top of the word2vec model by multiplying the original embeddings (W0) with the respective context vectors. Code to replicate the experiments can be found online.7 The results of the analogy task can be found in the Appendix.8\nNamed Entity Recognition The main advantage of context encoders is that they can use local context to create out-of-vocabulary (OOV) embeddings and distinguish between the different senses of words. The effects of this are most prominent in a task such as named entity recognition (NER) where the local context of a word can make all the difference, e.g. to distinguish between the \u201cChicago Bears\u201d (an organization) and the city of Chicago (a location). To test this, we used the word embeddings as features in the CoNLL 2003 NER benchmark task (Tjong et al., 2003). The word2vec embeddings were trained on the documents used in the training part of the task.9 For the context encoders we experimented with different combinations of local and global context vectors. The global context vectors were computed on only the training documents as well, i.e. just as with\n7https://github.com/cod3licious/conec 8As it was recently demonstrated that a good performance on intrinsic evaluation tasks such as word similarity or analogy tasks does not necessarily transfer to extrinsic evaluation measures when using the word embeddings as features (Chiu et al., 2016; Linzen, 2016), we consider the performance on the NER challenge as more relevant.\n9Since this is a very small corpus, we trained word2vec for 25 iterations on these documents (afterwards the performance on the development split stopped improving significantly) while usually the model is trained in a single pass through a much larger corpus.\nthe word2vec model, when applied to the test documents there are some words which don\u2019t have a word embedding available as they did not occur in the training texts. The local context vectors on the other hand can be computed for all words occurring in the current document for which the model should identify the named entities. When combining these local context vectors with the global ones we always use the local context vector as is in case there is no global vector available and otherwise compute a weighted average between the two context vectors as wl \u00b7 CVlocal + (1\u2212 wl) \u00b7 CVglobal.10 The different word embeddings were used as features with a logistic regression classifier trained on the labels obtained from the training part of the task and the reported F1-scores were computed using the official evaluation script. Please note that we are using this task to show the potential of ConEc word embeddings as features in a real world task and to illustrate their advantages over the regular word2vec embeddings and did not optimize for competitive performance on this NER challenge.\nFigure 7 shows the results achieved with various word embeddings on the training, development and test part of the CoNLL task. As it can be seen there, taking into account the local context can yield large improvements, especially on the dev and test data. Context encoders using only the global context vectors already perform better than word2vec. When using the local context vectors only where the global ones are not available (wl = 0) we can see a jump in the development and test performance, while of course the training performance stays the same as here we have global context vectors for all words. The best performances on all folds are achieved when averaging the global and local context vectors with around wl = 0.4 before multiplying them with the word2vec embeddings. This clearly shows that using ConEcs with local context vectors can be very beneficial as they let us compute word embeddings for out-of-vocabulary words as well as help distinguish between multiple meanings of words.\n10The global context matrix is computed without taking the word itself into account (i.e. zero on the diagonal) to make the context vectors comparable to the local context vectors of OOV words where we can\u2019t count the target word either. Both global and local context vectors are normalized by their respective maximum values, then multiplied with the length normalized word2vec embeddings and again renormalized to have unit length.\n4 CONCLUSION\nRepresenting intrinsically complex data is an ubiquitous challenge in data analysis. While kernel methods and manifold learning have made very successful contributions, their ability to scale is somewhat limited. Neural autoencoders offer scalable nonlinear embeddings, but their objective is to minimize the reconstruction error of the input data which does not necessarily preserve important pairwise relations between data points. In this paper we have proposed SimEcs as a neural network framework which bridges this gap by optimizing the same objective as spectral methods, such as kPCA, for creating similarity preserving embeddings while retaining the favorable properties of autoencoders.\nSimilarity encoders are a novel method to learn similarity preserving embeddings and can be especially useful when it is computationally infeasible to perform the eigendecomposition of a kernel matrix, when the target similarities are obtained through an unknown process such as human similarity judgments, or when an explicit mapping function is required. To accomplish this, a feed-forward neural network is constructed to map the data into an embedding space where the original similarities can be approximated linearly.\nAs a second contribution we have defined context encoders, a practical extension of SimEcs, that can be readily used to enhance the word2vec model with further local context information and global word statistics. Most importantly, ConEcs allow to easily create word embeddings for out-of-vocabulary words on the spot and distinguish between different meanings of a word based its local context.\nFinally, we have demonstrated the usefulness of SimEcs and ConEcs for practical tasks such as the visualization of data from different domains and to create meaningful word embedding features for a NER task, going beyond the capabilities of traditional methods.\nFuture work will aim to further the theoretical understanding of SimEcs and ConEcs and explore other application scenarios where using this novel neural network architecture can be beneficial. As it is often the case with neural network models, determining the optimal architecture as well as other hyperparameter choices best suited for the task at hand can be difficult. While so far we mainly studied SimEcs based on fairly simple feed-forward networks, it appears promising to consider also deeper neural networks and possibly even more elaborate architectures, such as convolutional networks, for the initial mapping step to the embedding space, as in this manner hierarchical structures in complex data could be reflected. Note furthermore that prior knowledge as well as more general error functions could be employed to tailor the embedding to the desired application target(s).\nACKNOWLEDGMENTS\nWe would like to thank Antje Relitz, Christoph Hartmann, Ivana Bala\u017eevic\u0301, and other anonymous reviewers for their helpful comments on earlier versions of this manuscript. Additionally, Franziska Horn acknowledges funding from the Elsa-Neumann scholarship from the TU Berlin.\n", "cats": {"READ": 0.0, "DONTREAD": 1.0}, "reviews": [{"IS_META_REVIEW": true, "comments": "This paper presents a method for embedding data instances into a low-dimensional space that preserves some form of similarity.\n\nAlthough the paper presents this notion as new, basically every pre-trained embedding (be it auto-encoders or word2vec) has been doing the same: representing items in a low-dimensional space that inherently encodes their similarities. Even when looking at the specific case of word/context embeddings, the method is not novel either: this method is almost identical to one of the similarity functions presented in \"A Simple Word Embedding Model for Lexical Substitution\" (Melamud et al., 2015). The novelty claim must be more accurate and position itself with respect to existing work.\n\nIn addition, I think the evaluation could be done better. There are plenty of benchmarks for word embeddings in context, for example: \n*"}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "There is consensus among the reviewers that the novelty of the paper is limited, and that the experimental evaluation of the proposed method needs to be improved.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"TITLE": "Novelty claim is false, evaluation is partial", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "This paper presents a method for embedding data instances into a low-dimensional space that preserves some form of similarity.\n\nAlthough the paper presents this notion as new, basically every pre-trained embedding (be it auto-encoders or word2vec) has been doing the same: representing items in a low-dimensional space that inherently encodes their similarities. Even when looking at the specific case of word/context embeddings, the method is not novel either: this method is almost identical to one of the similarity functions presented in \"A Simple Word Embedding Model for Lexical Substitution\" (Melamud et al., 2015). The novelty claim must be more accurate and position itself with respect to existing work.\n\nIn addition, I think the evaluation could be done better. There are plenty of benchmarks for word embeddings in context, for example: \n* ", "IS_META_REVIEW": false, "RECOMMENDATION": 3, "DATE": "19 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "marginal novelty", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "this paper proposes to use feed-forward neural networks to learn similarity preserving embeddings. They also use the proposed idea to represent out-of-vocabulary words using the words in given context. \n\nFirst, considering the related work [1,2] the proposed approach brings marginal novelty. Especially\nContext Encoders is just a small improvement over word2vec. \n\nExperimental setup should provide more convincing results other than visualizations and non-standard benchmark for NER evaluation with word vectors [3].\n\n[1] ", "IS_META_REVIEW": false, "RECOMMENDATION": 2, "DATE": "17 Dec 2016", "REVIEWER_CONFIDENCE": 5}, {"TITLE": "Standard feed-forward neural net with unconvincing experimental results", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "This paper introduces a similarity encoder based on a standard feed-forward neural network with the aim of generating similarity-preserving embeddings. The approach is utilized to generate a simple extension of the CBOW word2vec model that transforms the learned embeddings by their average context vectors. Experiments are performed on an analogy task and named entity recognition.\n\nWhile this paper offers some reasonable intuitive arguments for why a feed-forward neural network can generate good similarity-preserving embeddings, the architecture and approach is far from novel. As far as I can tell, the model is nothing more than the most vanilla neural network trained with SGD on similarity signals.\n\nSlightly more original is the idea to use context embeddings to augment the expressive capacity of learned word representations. Of course, using explicit contextual information is not a new idea, especially for tasks like word sense disambiguation (see, e.g., 'Efficient Non-parametric Estimation of Multiple Embeddings per Word in Vector Space' by Neelakantan et al, which should also be cited), but the specific method used here is original, as far as I know.\n\nThe evaluation of the method is far from convincing. The corpora used to train the embeddings are far smaller than would ever be used in practice for unsupervised or semi-supervised embedding learning. The performance on the analogy task says little about the benefit of this method for larger corpora, and, as the authors mentioned in the comments, they expect \"the gain will be less significant, as the global context statistics brought in by the ConEc can also be picked up by word2vec with more training.\"\n\nThe argument can be made (and the authors do claim) that extrinsic evaluations are more important for real-world applications, so it is good to see experiments on NER. However, again the embeddings were trained on a very small corpus and I am not convinced that the observed benefit will persist when trained on larger corpora.\n\nOverall, I believe this paper offers little novelty and weak experimental evidence supporting its claims. I cannot recommend it for acceptance.", "IS_META_REVIEW": false, "RECOMMENDATION": 3, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"DATE": "03 Dec 2016", "TITLE": "Related literature on context embeddings", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2"}, {"DATE": "03 Dec 2016", "TITLE": "questions and suggestions", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3"}, {"DATE": "02 Dec 2016", "TITLE": "Analogy task", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1"}, {"IS_META_REVIEW": true, "comments": "This paper presents a method for embedding data instances into a low-dimensional space that preserves some form of similarity.\n\nAlthough the paper presents this notion as new, basically every pre-trained embedding (be it auto-encoders or word2vec) has been doing the same: representing items in a low-dimensional space that inherently encodes their similarities. Even when looking at the specific case of word/context embeddings, the method is not novel either: this method is almost identical to one of the similarity functions presented in \"A Simple Word Embedding Model for Lexical Substitution\" (Melamud et al., 2015). The novelty claim must be more accurate and position itself with respect to existing work.\n\nIn addition, I think the evaluation could be done better. There are plenty of benchmarks for word embeddings in context, for example: \n*"}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "There is consensus among the reviewers that the novelty of the paper is limited, and that the experimental evaluation of the proposed method needs to be improved.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"TITLE": "Novelty claim is false, evaluation is partial", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "This paper presents a method for embedding data instances into a low-dimensional space that preserves some form of similarity.\n\nAlthough the paper presents this notion as new, basically every pre-trained embedding (be it auto-encoders or word2vec) has been doing the same: representing items in a low-dimensional space that inherently encodes their similarities. Even when looking at the specific case of word/context embeddings, the method is not novel either: this method is almost identical to one of the similarity functions presented in \"A Simple Word Embedding Model for Lexical Substitution\" (Melamud et al., 2015). The novelty claim must be more accurate and position itself with respect to existing work.\n\nIn addition, I think the evaluation could be done better. There are plenty of benchmarks for word embeddings in context, for example: \n* ", "IS_META_REVIEW": false, "RECOMMENDATION": 3, "DATE": "19 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "marginal novelty", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "this paper proposes to use feed-forward neural networks to learn similarity preserving embeddings. They also use the proposed idea to represent out-of-vocabulary words using the words in given context. \n\nFirst, considering the related work [1,2] the proposed approach brings marginal novelty. Especially\nContext Encoders is just a small improvement over word2vec. \n\nExperimental setup should provide more convincing results other than visualizations and non-standard benchmark for NER evaluation with word vectors [3].\n\n[1] ", "IS_META_REVIEW": false, "RECOMMENDATION": 2, "DATE": "17 Dec 2016", "REVIEWER_CONFIDENCE": 5}, {"TITLE": "Standard feed-forward neural net with unconvincing experimental results", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "This paper introduces a similarity encoder based on a standard feed-forward neural network with the aim of generating similarity-preserving embeddings. The approach is utilized to generate a simple extension of the CBOW word2vec model that transforms the learned embeddings by their average context vectors. Experiments are performed on an analogy task and named entity recognition.\n\nWhile this paper offers some reasonable intuitive arguments for why a feed-forward neural network can generate good similarity-preserving embeddings, the architecture and approach is far from novel. As far as I can tell, the model is nothing more than the most vanilla neural network trained with SGD on similarity signals.\n\nSlightly more original is the idea to use context embeddings to augment the expressive capacity of learned word representations. Of course, using explicit contextual information is not a new idea, especially for tasks like word sense disambiguation (see, e.g., 'Efficient Non-parametric Estimation of Multiple Embeddings per Word in Vector Space' by Neelakantan et al, which should also be cited), but the specific method used here is original, as far as I know.\n\nThe evaluation of the method is far from convincing. The corpora used to train the embeddings are far smaller than would ever be used in practice for unsupervised or semi-supervised embedding learning. The performance on the analogy task says little about the benefit of this method for larger corpora, and, as the authors mentioned in the comments, they expect \"the gain will be less significant, as the global context statistics brought in by the ConEc can also be picked up by word2vec with more training.\"\n\nThe argument can be made (and the authors do claim) that extrinsic evaluations are more important for real-world applications, so it is good to see experiments on NER. However, again the embeddings were trained on a very small corpus and I am not convinced that the observed benefit will persist when trained on larger corpora.\n\nOverall, I believe this paper offers little novelty and weak experimental evidence supporting its claims. I cannot recommend it for acceptance.", "IS_META_REVIEW": false, "RECOMMENDATION": 3, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"DATE": "03 Dec 2016", "TITLE": "Related literature on context embeddings", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2"}, {"DATE": "03 Dec 2016", "TITLE": "questions and suggestions", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3"}, {"DATE": "02 Dec 2016", "TITLE": "Analogy task", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1"}]}
{"text": "1 INTRODUCTION\nThe machine learning community is well-practised at learning representations of data-points and sequences. A middle-ground between these two is representing, or summarizing, datasets - unordered collections of vectors, such as photos of a particular person, recordings of a given speaker or a document as a bag-of-words. Where these sets take the form of i.i.d samples from some distribution, such summaries are called statistics. We explore the idea of using neural networks to learn statistics and we refer to our approach as a neural statistician.\nThe key result of our approach is a statistic network that takes as input a set of vectors and outputs a vector of summary statistics specifying a generative model of that set - a mean and variance specifying a Gaussian distribution in a latent space we term the context. The advantages of our approach are that it is:\n\u2022 Unsupervised: It provides principled and unsupervised way to learn summary statistics as the output of a variational encoder of a generative model.\n\u2022 Data efficient: If one has a large number of small but related datasets, modelling the datasets jointly enables us to gain statistical strength.\n\u2022 Parameter Efficient: By using summary statistics instead of say categorical labellings of each dataset, we decouple the number of parameters of the model from the number of datasets.\n\u2022 Capable of few-shot learning: If the datasets correspond to examples from different classes, class embeddings (summary statistics associated with examples from a class), allow us to handle new classes at test time.\n2 PROBLEM STATEMENT\nWe are given datasets Di for i \u2208 I. Each dataset Di = {x1, . . . , xki} consists of a number of i.i.d samples from an associated distribution pi over Rn. The task can be split into learning and inference components. The learning component is to produce a generative model p\u0302i for each dataset Di. We assume there is a common underlying generative process p such that pi = p(\u00b7|ci) for ci \u2208 Rl drawn\nfrom p(c). We refer to c as the context. The inference component is to give an approximate posterior over the context q(c|D) for a given dataset produced by a statistic network.\n3 NEURAL STATISTICIAN\nIn order to exploit the assumption of a hierarchical generative process over datasets we will use a \u2018parameter-transfer approach\u2019 (see Pan & Yang, 2010) to extend the variational autoencoder model of Kingma & Welling (2013).\n3.1 VARIATIONAL AUTOENCODER\nThe variational autoencoder is a latent variable model p(x|z; \u03b8) (often called the decoder) with parameters \u03b8. For each observed x, a corresponding latent variable z is drawn from p(z) so that\np(x) = \u222b p(x|z; \u03b8)p(z) dz. (1)\nThe generative parameters \u03b8 are learned by introducing a recognition network (also called an encoder) q(z|x;\u03c6) with parameters \u03c6. The recognition network gives an approximate posterior over the latent variables that can then be used to give the standard variational lower bound (Saul & Jordan, 1996) on the single-datum log-likelihood. I.e. logP (x|\u03b8) \u2265 Lx, where\nLx = Eq(z|x,\u03c6) [log p(x|z; \u03b8)]\u2212DKL (q(z|x;\u03c6)\u2016p(z)) . (2)\nLikewise the full-data log likelihood is lower bounded by the sum of the Lx terms over the whole dataset. We can then optimize this lower bound with respect to \u03c6 and \u03b8 using the reparameterization trick introduced by Kingma & Welling (2013) and Rezende et al. (2014) to get a Monte-Carlo estimate of the gradient.\n3.2 BASIC MODEL\nWe extend the variational autoencoder to the model depicted on the left in Figure 1. This includes a latent variable c, the context, that varies between different datasets but is constant, a priori, for items within the same dataset. Now, the likelihood of the parameters \u03b8 for one single particular dataset D is given by\np(D) = \u222b p(c) [\u220f x\u2208D \u222b p(x|z; \u03b8)p(z|c; \u03b8) dz ] dc. (3)\nThe prior p(c) is chosen to be a spherical Gaussian with zero mean and unit variance. The conditional p(z|c; \u03b8) is Gaussian with diagonal covariance, where all the mean and variance parameters depend on c through a neural network. Similarly the observation model p(x|z; \u03b8) will be a simple likelihood function appropriate to the data modality with dependence on z parameterized by a neural network. For example, with real valued data, a diagonal Gaussian likelihood could be used where the mean and log variance of x are created from z via a neural network.\nWe use approximate inference networks q(z|x, c;\u03c6), q(c|D;\u03c6), with parameters collected into \u03c6, to once again enable the calculation and optimization of a variational lower bound on the loglikelihood. The single dataset log likelihood lower bound is given by\nLD = Eq(c|D;\u03c6) [\u2211 x\u2208d Eq(z|c,x;\u03c6) [log p(x|z; \u03b8)]\u2212DKL (q(z|c, x;\u03c6)\u2016p(z|c; \u03b8)) ] \u2212DKL (q(c|D;\u03c6)\u2016p(c)) . (4)\nAs with the generative distributions, the likelihood forms for q(z|x, c;\u03c6) and q(c|D;\u03c6) are diagonal Gaussian distributions, where all the mean and log variance parameters in each distribution are produced by a neural network taking the conditioning variables as inputs. Note that q(c|D;\u03c6) accepts as input a dataset D and we refer to this as the statistic network. We describe this in Subsection 3.4.\nThe full-data variational bound is given by summing the variational bound for each dataset in our collection of datasets. It is by learning the difference of the within-dataset and between-dataset distributions that we are able to discover an appropriate statistic network.\n3.3 FULL MODEL\nThe basic model works well for modelling simple datasets, but struggles when the datasets have complex internal structure. To increase the sophistication of the model we use multiple stochastic layers z1, . . . , zk and introduce skip-connections for both the inference and generative networks. The generative model is shown graphically in Figure 1 in the center. The probability of a dataset D is then given by\np(D) = \u222b p(c) \u220f x\u2208D \u222b p(x|c, z1:L; \u03b8)p(zL|c; \u03b8) L\u22121\u220f i=1 p(zi|zi+1, c; \u03b8) dz1:L dc (5)\nwhere the p(zi|zi+1, c, \u03b8) are again Gaussian distributions where the mean and log variance are given as the output of neural networks. The generative process for the full model is described in Algorithm 1.\nThe full approximate posterior factorizes analogously as\nq(c, z1:L|D;\u03c6) = q(c|D;\u03c6) \u220f x\u2208D q(zL|x, c;\u03c6) L\u22121\u220f i=1 q(zi|zi+1, x, c;\u03c6). (6)\nFor convenience we give the variational lower bound as sum of a three parts, a reconstruction term RD, a context divergence CD and a latent divergence LD:\nLD = RD + CD + LD with (7) RD = Eq(c|D;\u03c6) \u2211 x\u2208D Eq(z1:L|c,x;\u03c6) log p(x|z1:L, c; \u03b8) (8)\nCD = DKL (q(c|D;\u03c6)\u2016p(c)) (9)\nLD = Eq(c,z1:L|D;\u03c6) [\u2211 x\u2208D DKL (q(zL|c, x;\u03c6)\u2016p(zL|c; \u03b8))\n+ L\u22121\u2211 i=1 DKL (q(zi|zi+1, c, x;\u03c6)\u2016p(zi|zi+1, c; \u03b8))\n] . (10)\nThe skip-connections p(zi|zi+1, c; \u03b8) and q(zi|zi+1, x;\u03c6) allow the context to specify a more precise distribution for each latent variable by explaining-away more generic aspects of the dataset at each stochastic layer. This architecture was inspired by recent work on probabilistic ladder networks in Kaae S\u00f8nderby et al. (2016). Complementing these are the skip-connections from each latent variable to the observation p(x|z1:L, c; \u03b8), the intuition here is that each stochastic layer can focus on representing a certain level of abstraction, since its information does not need to be copied into the next layer, a similar approach was used in Maal\u00f8e et al. (2016).\nOnce again, note that we are maximizing the lower bound to the log likelihood over many datasets D: we want to maximize the expectation of LD over all datasets. We do this optimization using stochastic gradient descent. In contrast to a variational autoencoder where a minibatch would consist of a subsample of datapoints from the dataset, we use minibatches consisting of a subsample of datasets - tensors of shape (batch size, sample size, number of features).\n3.4 STATISTIC NETWORK\nIn addition to the standard inference networks we require a statistic network q(c|D;\u03c6) to give an approximate posterior over the context c given a datasetD = {x1, . . . , xk} . This inference network must capture the exchangeability of the data in D.\nWe use a feedforward neural network consisting of three main elements:\n\u2022 An instance encoder E that takes each individual datapoint xi to a vector ei = E(xi). \u2022 An exchangeable instance pooling layer that collapses the matrix (e1, . . . , ek) to a single\npre-statistic vector v. Examples include elementwise means, sums, products, geometric means and maximum. We use the sample mean for all experiments. \u2022 A final post-pooling network that takes v to a parameterization of a diagonal Gaussian.\nThe graphical model for this is given at the right of Figure 1.\nWe note that the humble sample mean already gives the statistic network a great deal of representational power due to the fact that the instance encoder can learn a representation where averaging makes sense. For example since the instance encoder can approximate a polynomial on a compact domain, and so can the post-pooling network, a statistic network can approximate any moment of a distribution.\n4 RELATED WORK\nDue to the general nature of the problem considered, our work touches on many different topics which we now attempt to summarize.\nTopic models and graphical models The form of the graphical model in Figure 1 on the left is equivalent to that of a standard topic model. In contrast to traditional topic models we do not use discrete latent variables, or restrict to discrete data. Work such as that by Ranganath et al. (2014) has extended topic models in various directions, but importantly we use flexible conditional distributions and dependency structures parameterized by deep neural networks. Recent work has explored neural networks for document models (see e.g. Miao et al., 2015) but has been limited to modelling datapoints with little internal structure. Along related lines are \u2018structured variational autoencoders\u2019 (see Johnson et al., 2016), where they treat the general problem of integrating graphical models with variational autoencoders.\nTransfer learning There is a considerable literature on transfer learning, for a survey see Pan & Yang (2010). There they discuss \u2018parameter-transfer\u2019 approaches whereby parameters or priors are shared across datasets, and our work fits into that paradigm. For examples see Lawrence & Platt (2004) where share they priors between Gaussian processes, and Evgeniou & Pontil (2004) where they take an SVM-like approach to share kernels.\nOne-shot Learning Learning quickly from small amounts of data is a topic of great interest. Lake et al. (2015) use Bayesian program induction for one-shot generation and classification, and Koch (2015) train a Siamese (Chopra et al. (2005)) convolutional network for one-shot image classification. We note the relation to the recent work (Rezende et al., 2016) in which the authors use a conditional recurrent variational autoencoder capable of one-shot generalization by taking as extra input a conditioning data point. The important differences here are that we jointly model datasets and datapoints and consider datasets of any size. Recent approaches to one-shot classification are matching networks (Vinyals et al., 2016b) (which was concurrent with the initial preprint of this work), and related previous work (Santoro et al., 2016). The former can be considered a kind of differentiable nearest neighbour classifier, and the latter augments their network with memory to store information about the classification problem. Both are trained end-to-end for the classification problem, whereas the present work is a general approach to learning representations of datasets. Probably the closest previous work is by Salakhutdinov et al. (2012) where the authors learn a topic\nmodel over the activations of a DBM for one-shot learning. Compared with their work we use modern architectures and easier to train VAEs, in particular we have fast and amortized feedforward inference for test (and training) datasets, avoiding the need for MCMC.\nMultiple-Instance Learning There is previous work on classifying sets in multiple-instance learning, for a useful survey see Cheplygina et al. (2015). Typical approaches involve adapting kernel based methods such as support measure machines (Muandet et al., 2012), support distribution machines (Po\u0301czos et al., 2012) and multiple-instance-kernels (Gartner et al., 2002). We do not consider applications to multiple-instance learning type problems here, but it may be fruitful to do so in the future.\nSet2Seq In very related work, Vinyals et al. (2016a) explore architectures for mapping sets to sequences. There they use an LSTM to repeatedly compute weighted-averages of the datapoints and use this to tackle problems such as sorting a list of numbers. The main difference between their work and ours is that they primarily consider supervised problems, whereas we present a general unsupervised method for learning representations of sets of i.i.d instances. In future work we may also explore recurrently computing statistics.\nABC There has also been work on learning summary statistics for Approximate Bayesian Computation by either learning to predict the parameters generating a sample as a supervised problem, or by using kernel embeddings as infinite dimensional summary statistics. See the work by Fukumizu et al. (2013) for an example of kernel-based approaches. More recently Jiang et al. (2015) used deep neural networks to predict the parameters generating the data. The crucial differences are that their problem is supervised, they do not leverage any exchangeability properties the data may have, nor can it deal with varying sample sizes.\n5 EXPERIMENTAL RESULTS\nGiven an input set x1, . . . xk we can use the statistic network to calculate an approximate posterior over contexts q(c|x1, . . . , xk;\u03c6). Under the generative model, each context c specifies a conditional model p(x|c; \u03b8). To get samples from the model corresponding to the most likely posterior value of c, we set c to the mean of the approximate posterior and then sample directly from the conditional distributions. This is described in Algorithm 2. We use this process in our experiments to show samples. In all experiments, we use the Adam optimization algorithm (Kingma & Ba, 2014) to optimize the parameters of the generative models and variational approximations. Batch normalization (Ioffe & Szegedy, 2015) is implemented for convolutional layers and we always use a batch size of 16. We primarily use the Theano (Theano Development Team, 2016) framework with the Lasagne (Dieleman et al., 2015) library, but the final experiments with face data were done using Tensorflow (Abadi et al., 2015). In all cases experiments were terminated after a given number of epochs when training appeared to have sufficiently converged (300 epochs for omniglot, youtube and spatial MNIST examples, and 50 epochs for the synthetic experiment).\n5.1 SIMPLE 1-D DISTRIBUTIONS\nIn our first experiment we wanted to know if the neural statistician will learn to cluster synthetic 1-D datasets by distribution family. We generated a collection of synthetic 1-D datasets each containing 200 samples. Datasets consist of samples from either an Exponential, Gaussian, Uniform or Laplacian distribution with equal probability. Means and variances are sampled from U [\u22121, 1] and U [0.5, 2] respectively. The training data contains 10K sets.\nThe architecture for this experiment contains a single stochastic layer with 32 units for z and 3 units for c, . The model p(x|z, c; \u03b8) and variational approximation q(z|x, c;\u03c6) are each a diagonal Gaussian distribution with all mean and log variance parameters given by a network composed of three dense layers with ReLU activations and 128 units. The statistic network determining the mean and log variance parameters of posterior over context variables is composed of three dense layers before and after pooling, each with 128 units with Rectified Linear Unit (ReLU) activations.\nFigure 2 shows 3-D scatter plots of the summary statistics learned. Notice that the different families of distribution cluster. It is interesting to observe that the Exponential cluster is differently orientated to the others, perhaps reflecting the fact that it is the only non-symmetric distribution. We also see that between the Gaussian and Laplacian clusters there is an area of ambiguity which is as one\nmight expect. We also see that within each cluster the mean and variance are mapped to orthogonal directions.\n5.2 SPATIAL MNIST\nBuilding on the previous experiments we investigate 2-D datasets that have complex structure, but the datapoints contain little information by themselves, making it a good test of the statistic network. We created a dataset called spatial MNIST. In spatial MNIST each image from MNIST (LeCun et al., 1998) is turned into a dataset by interpreting the normalized pixel intensities as a probability density and sampling coordinate values. An example is shown in Figure 3. This creates two-dimensional spatial datasets. We used a sample size of 50. Note that since the pixel coordinates are discrete, it is necessary to dequantize them by adding uniform noise u \u223c U [0, 1] to the coordinates if one models them as real numbers, else you can get arbitrarily high densities (see Theis et al. (2016) for a discussion of this point).\nThe generative architecture for this experiment contains 3 stochastic z layers, each with 2 units, and a single c layer with 64 units. The means and log variances of the Gaussian likelihood for p(x|z1:3, c; \u03b8), and each subnetwork for z in both the encoder and decoder contained 3 dense layers with 256 ReLU units each. The statistic network also contained 3 dense layers pre-pooling and 3 dense layers post pooling with 256 ReLU units.\nIn addition to being able to sample from the model conditioned on a set of inputs, we can also summarize a dataset by choosing a subset S \u2286 D to minimise the KL divergence of q(C|D;\u03c6) from q(C|S;\u03c6). We do this greedily by iteratively discarding points from the full sample. Pseudocode for this process is given in Algorithm 3. The results are shown in Figure 4. We see that the model is capable of handling complex arrangements of datapoints. We also see that it can select sensible subsets of a dataset as a summary.\n5.3 OMNIGLOT\nNext we work with the OMNIGLOT data (Lake et al., 2015). This contains 1628 classes of handwritten characters but with just 20 examples per class. This makes it an excellent test-bed for transfer / few-shot learning. We constructed datasets by splitting each class into datasets of size 5. We train\non datasets drawn from 1200 classes and reserve the remaining classes to test few-shot sampling and classification. We created new classes by rotating and reflecting characters. We resized the images to 28 \u00d7 28. We sampled a binarization of each image for each epoch. We also randomly applied the dilation operator from computer vision as further data augmentation since we observed that the stroke widths are quite uniform in the OMNIGLOT data, whereas there is substantial variation in MNIST, this augmentation improved the visual quality of the few-shot MNIST samples considerably and increased the few-shot classification accuracy by about 3 percent. Finally we used \u2018sample dropout\u2019 whereby a random subset of each dataset was removed from the pooling in the statistic network, and then included the number of samples remaining as an extra feature. This was beneficial since it reduced overfitting and also allowed the statistic network to learn to adjust the approximate posterior over c based on the number of samples.\nWe used a single stochastic layer with 16 units for z, and 512 units for c. We used a shared convolutional encoder between the inference and statistic networks and a deconvolutional decoder network. Full details of the networks are given in Appendix B.1. The decoder used a Bernoulli likelihood.\nIn Figure 5 we show two examples of few-shot learning by conditioning on samples of unseen characters from OMNIGLOT, and conditioning on samples of digits from MNIST. The samples are mostly of a high-quality, and this shows that the neural statistician can generalize even to new datasets.\nAs a further test we considered few-shot classification of both unseen OMNIGLOT characters and MNIST digits. Given a sets of labelled examples of each class D0, . . . , D9 (for MNIST say), we computed the approximate posteriors q(C|Di;\u03c6) using the statistic network. Then for each test image x we also computed the posterior q(C|x;\u03c6) and classified it according to the training dataset Di minimizing the KL divergence from the test context to the training context. This process is described in Algorithm 4. We tried this with either 1 or 5 labelled examples per class and either 5 or 20 classes. For each trial we randomly select K classes, randomly select training examples for each class, and test on the remaining examples. This process is repeated 100 times and the results averaged. The results are shown in Table 1. We compare to a number of results reported in Vinyals et al. (2016b) including Santoro et al. (2016) and Koch (2015). Overall we see that\nthe neural statistician model can be used as a strong classifier, particularly for the 5-way tasks, but performs worse than matching networks for the 20-way tasks. One important advantage that matching networks have is that, whilst each class is processed independently in our model, the representation in matching networks is conditioned on all of the classes in the few-shot problem. This means that it can exaggerate differences between similar classes, which are more likely to appear in a 20-way problem than a 5-way problem.\n5.4 YOUTUBE FACES\nFinally, we provide a proof of concept for generating faces of a particular person. We use the Youtube Faces Database from Wolf et al. (2011). It contains 3, 245 videos of 1, 595 different people. We use the aligned and cropped to face version, resized to 64 \u00d7 64. The validation and test sets contain 100 unique people each, and there is no overlap of persons between data splits. The sets were created by sampling frames randomly without replacement from each video, we use a set size of 5 frames. We resample the sets for the training data each epoch.\nOur architecture for this problem is based on one presented in Lamb et al. (2016). We used a single stochastic layer with 500 dimensional latent c and 16 dimensional z variable. The statistic network and the inference network q(z|x, c;\u03c6) share a common convolutional encoder, and the deocder uses deconvolutional layers. For full details see Appendix B.2. The likelihood function is a Gaussian, but where the variance parameters are shared across all datapoints, this was found to make training faster and more stable.\nThe results are shown in Figure 6. Whilst there is room for improvement, we see that it is possible to specify a complex distribution on-the-fly with a set of photos of a previously unseen person. The samples conditioned on an input set have a reasonable likeness of the input faces. We also show the ability of the model to generate new datasets and see that the samples have a consistent identity and varied poses.\n6 CONCLUSION\nWe have demonstrated a highly flexible model on a variety of tasks. Going forward our approach will naturally benefit from advances in generative models as we can simply upgrade our base generative model, and so future work will pursue this. Compared with some other approaches in the literature for few-shot learning, our requirement for supervision is weaker: we only ask at training time that we are given datasets, but we do not need labels for the datasets, nor even information on whether two datasets represent the same or different classes. It would be interesting then to explore application areas where only this weaker form of supervision is available. There are two important limitations to this work, firstly that the method is dataset hungry: it will likely not learn useful representations of datasets given only a small number of them. Secondly at test time the few-shot fit of the generative model will not be greatly improved by using larger datasets unless the model was also trained on similarly large datasets. The latter limitation seems like a promising future research direction - bridging the gap between fast adaptation and slow training.\nACKNOWLEDGMENTS\nThis work was supported in part by the EPSRC Centre for Doctoral Training in Data Science, funded by the UK Engineering and Physical Sciences Research Council (grant EP/L016427/1) and the University of Edinburgh.\nA APPENDIX A: PSEUDOCODE\nAlgorithm 1 Sampling a dataset of size k sample c \u223c p(c) for i = 1 to k do\nsample zi,L \u223c p(zL|c; \u03b8) for j = L\u2212 1 to 1 do\nsample zi,j \u223c p(zj |zi,j+1, c; \u03b8) end for sample xi \u223c p(x|zi,1, . . . , zi,L, c; \u03b8)\nend for\nAlgorithm 2 Sampling a dataset of size k conditioned on a dataset of size m \u00b5c, \u03c3 2 c \u2190 q(c|x1, . . . , xm;\u03c6) {Calculate approximate posterior over c using statistic network.}\nc\u2190 \u00b5c {Set c to be the mean of the approximate posterior.} for i = 1 to k do\nsample zi,L \u223c p(zL|c; \u03b8) for j = L\u2212 1 to 1 do\nsample zi,j \u223c p(zj |zi,j+1, c; \u03b8) end for sample xi \u223c p(x|zi,1, . . . , zi,L, c; \u03b8)\nend for\nAlgorithm 3 Selecting a representative sample of size k S \u2190 {x1, . . . , xm} I \u2190 {1, . . . ,m} SI = {xi \u2208 S : i \u2208 I} NSI \u2190 q(c|SI ;\u03c6) {Calculate approximate posterior over c using statistic network.} for i = 1 to k do t\u2190 argminj\u2208IDKL ( NS\u2016NSI\u2212j\n) I \u2190 I \u2212 t\nend for\nAlgorithm 4 K-way few-shot classification D0, . . . , DK \u2190 sets of labelled examples for each class x\u2190 datapoint to be classified Nx \u2190 q(c|x;\u03c6) {approximate posterior over c given query point} for i = 1 to K do Ni \u2190 q(c|Di;\u03c6)\nend for y\u0302 \u2190 argminiDKL (Ni\u2016Nx)\nB APPENDIX B: FURTHER EXPERIMENTAL DETAILS\nB.1 OMNIGLOT\nShared encoder x\u2192 h 2\u00d7 { conv2d 64 feature maps with 3\u00d7 3 kernels and ELU activations } conv2d 64 feature maps with 3\u00d7 3 kernels, stride 2 and ELU activations 2\u00d7 {conv2d 128 feature maps with 3\u00d7 3 kernels and ELU activations } conv2d 128 feature maps with 3\u00d7 3 kernels, stride 2 and ELU activations 2\u00d7 { conv2d 256 feature maps with 3\u00d7 3 kernels and ELU activations } conv2d 256 feature maps with 3\u00d7 3 kernels, stride 2 and ELU activations\nStatistic network q(c|D;\u03c6) : h1, . . . , hk \u2192 \u00b5c, \u03c32c fully-connected layer with 256 units and ELU activations sample-dropout and concatenation with number of samples average pooling within each dataset 2\u00d7 {fully-connected layer with 256 units and ELU activations } fully-connected linear layers to \u00b5c and log \u03c32c\nInference network q(z|x, c;\u03c6) : h, c\u2192 \u00b5z, \u03c32z concatenate c and h 3\u00d7 {fully-connected layer with 256 units and ELU activations } fully-connected linear layers to \u00b5z and log \u03c32z\nLatent decoder network p(z|c; \u03b8) : c\u2192 \u00b5z, \u03c32z 3\u00d7 {fully-connected layer with 256 units and ELU activations } fully-connected linear layers to \u00b5z and log \u03c32z\nObservation decoder network p(x|c, z; \u03b8) : c, z \u2192 \u00b5x concatenate z and c fully-connected linear layers with 4 \u00b7 4 \u00b7 256 units 2\u00d7 { conv2d 256 feature maps with 3\u00d7 3 kernels and ELU activations } deconv2d 256 feature maps with 2\u00d7 2 kernels, stride 2, ELU activations 2\u00d7 { conv2d 128 feature maps with 3\u00d7 3 kernels and ELU activations } deconv2d 128 feature maps with 2\u00d7 2 kernels, stride 2, ELU activations 2\u00d7 { conv2d 64 feature maps with 3\u00d7 3 kernels and ELU activations } deconv2d 64 feature maps with 2\u00d7 2 kernels, stride 2, ELU activations conv2d 1 feature map with 1\u00d7 1 kernels, sigmoid activations\nB.2 YOUTUBE FACES\nShared encoder x\u2192 h 2\u00d7 { conv2d 32 feature maps with 3\u00d7 3 kernels and ELU activations } conv2d 32 feature maps with 3\u00d7 3 kernels, stride 2 and ELU activations 2\u00d7 {conv2d 64 feature maps with 3\u00d7 3 kernels and ELU activations } conv2d 64 feature maps with 3\u00d7 3 kernels, stride 2 and ELU activations 2\u00d7 { conv2d 128 feature maps with 3\u00d7 3 kernels and ELU activations } conv2d 128 feature maps with 3\u00d7 3 kernels, stride 2 and ELU activations 2\u00d7 { conv2d 256 feature maps with 3\u00d7 3 kernels and ELU activations } conv2d 256 feature maps with 3\u00d7 3 kernels, stride 2 and ELU activations\nStatistic network q(c|D,\u03c6) : h1, . . . , hk \u2192 \u00b5c, \u03c32c fully-connected layer with 1000 units and ELU activations average pooling within each dataset fully-connected linear layers to \u00b5c and log \u03c32c\nInference network q(z|x, c, \u03c6) : h, c\u2192 \u00b5z, \u03c32z concatenate c and h fully-connected layer with 1000 units and ELU activations fully-connected linear layers to \u00b5z and log \u03c32z\nLatent decoder network p(z|c, ; \u03b8) : c\u2192 \u00b5z, \u03c32z fully-connected layer with 1000 units and ELU activations fully-connected linear layers to \u00b5z and log \u03c32z\nObservation decoder network p(x|c, z; \u03b8) : c, z \u2192 \u00b5x concatenate z and c fully-connected layer with 1000 units and ELU activations fully-connected linear layer with 8 \u00b7 8 \u00b7 256 units 2\u00d7 { conv2d 256 feature maps with 3\u00d7 3 kernels and ELU activations } deconv2d 256 feature maps with 2\u00d7 2 kernels, stride 2, ELU activations 2\u00d7 { conv2d 128 feature maps with 3\u00d7 3 kernels and ELU activations } deconv2d 128 feature maps with 2\u00d7 2 kernels, stride 2, ELU activations 2\u00d7 { conv2d 64 feature maps with 3\u00d7 3 kernels and ELU activations } deconv2d 64 feature maps with 2\u00d7 2 kernels, stride 2, ELU activations 2\u00d7 { conv2d 32 feature maps with 3\u00d7 3 kernels and ELU activations } deconv2d 32 feature maps with 2\u00d7 2 kernels, stride 2, ELU activations conv2d 3 feature maps with 1\u00d7 1 kernels, sigmoid activations\n", "cats": {"READ": 1.0, "DONTREAD": 0.0}, "reviews": [{"IS_META_REVIEW": true, "comments": "The authors introduce a variant of the variational autoencoder (VAE) that models dataset-level latent variables. The idea is clearly motivated and well described. In my mind the greatest contribution of this paper is the movement beyond the relatively simple graphical model structure of the traditional VAEs and the introduction of more interesting structures to the deep learning community. \n\nComments:\n\n- It's not clear to me why this should be called a \"statistician\". Learning an approximate posterior over summary statistics is not the only imaginable way to summarize a dataset with a neural network. One could consider a maximum likelihood approach, etc. In general it felt like the paper could be more clear, if it avoided coining new terms like \"statistic network\" and stuck to the more accurate \"approximate posterior\".\n\n- The experiments are nice, and I appreciate the response to my question regarding \"one shot generation\". I still think that language needs to be clarified, specifically at the end of page 6. My understanding of Figure 5 is the following: Take an input set, compute the approximate posterior over the context vector, then generate from the forward model given samples from the approximate posterior. I would like clarification on the following: \n\n(a) Are the data point dependent vectors z generated from the forward model or taken from the approximate posterior? \n\n(b) I agree that the samples are of high-quality, but that is not a quantified statement. The advantage of VAEs over GANs is that we have natural ways of computing log-probabilities. To that end, one \"proper\" way of computing the \"one shot generation\" performance is to report log p(x | c) (where c is sampled from the approximate posterior) or log p(x) for held-out datasets. I suspect that log probability performance of these networks relative to a vanilla VAE without the context latent variable will be impressive. I still don't see a reason not to include that."}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "This is an interesting paper that adds nicely to the literature on VAEs and one-shot generalisation. This will be of interest to the community and will contribute positively to the conference.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"TITLE": "Solid Contribution", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer4", "comments": "This paper proposes a hierarchical generative model where the lower level consists of points within datasets and the higher level models unordered sets of datasets.  The basic idea is to use a \"double\" variational bound where a higher level latent variable describes datasets and a lower level latent variable describes individual examples.  \n\nHierarchical modeling is an important and high impact problem, and I think that it's under-explored in the Deep Learning literature.  \n\nPros:\n  -The few-shot learning results look good, but I'm not an expert in this area.  \n  -The idea of using a \"double\" variational bound in a hierarchical generative model is well presented and seems widely applicable.  \n\nQuestions: \n  -When training the statistic network, are minibatches (i.e. subsets of the examples) used?  \n  -If not, does using minibatches actually give you an unbiased estimator of the full gradient (if you had used all examples)?  For example, what if the statistic network wants to pull out if *any* example from the dataset has a certain feature and treat that as the characterization.  This seems to fit the graphical model on the right side of figure 1.  If your statistic network is trained on minibatches, it won't be able to learn this characterization, because a given minibatch will be missing some of the examples from the dataset.  Using minibatches (as opposed to using all examples in the dataset) to train the statistic network seems like it would limit the expressive power of the model.  \n\nSuggestions: \n  -Hierarchical forecasting (electricity / sales) could be an interesting and practical use case for this type of model.  ", "IS_META_REVIEW": false, "RECOMMENDATION": 8, "DATE": "19 Dec 2016", "REVIEWER_CONFIDENCE": 2}, {"IMPACT": 3, "SUBSTANCE": 3, "RECOMMENDATION_UNOFFICIAL": 3, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "Sorry for the late review -- I've been having technical problems with OpenReview which prevented me from posting.\n\nThis paper presents a method for learning to predict things from sets of data points. The method is a hierarchical version of the VAE, where the top layer consists of an abstract context unit that summarizes a dataset. Experiments show that the method is able to \"learn to learn\" by acquiring the ability to learn distributions from small numbers of examples.\n\nOverall, this paper is a nice addition to the literature on one- or few-shot learning. The method is conceptually simple and elegant, and seems to perform well. Compared to other recent papers on one-shot learning, the proposed method is simpler, and is based on unsupervised representation learning. The paper is clearly written and a pleasure to read.\n\nThe name of the paper is overly grandiose relative to what was done; the proposed method doesn\u2019t seem to have much in common with a statistician, unless one means by that \"someone who thinks up statistics\". \n\nThe experiments are well chosen, and the few-shot learning results seem pretty solid given the simplicity of the method.\n\nThe spatial MNIST dataset is interesting and might make a good toy benchmark. The inputs in Figure 4 seem pretty dense, though; shouldn\u2019t the method be able to recognize the distribution with fewer samples?  (Nitpick: the red points in Figure 4 don\u2019t seem to correspond to meaningful points as was claimed in the text.) \n\nWill the authors release the code?\n", "ORIGINALITY": 2, "IS_ANNOTATED": true, "TITLE": "a nice addition to the one-/few-shot learning literature", "IS_META_REVIEW": false, "RECOMMENDATION": 8, "DATE": "19 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "TITLE": "Interesting paper that starts to expand the repertoire of variational autoencoders", "comments": "The authors introduce a variant of the variational autoencoder (VAE) that models dataset-level latent variables. The idea is clearly motivated and well described. In my mind the greatest contribution of this paper is the movement beyond the relatively simple graphical model structure of the traditional VAEs and the introduction of more interesting structures to the deep learning community. \n\nComments:\n\n- It's not clear to me why this should be called a \"statistician\". Learning an approximate posterior over summary statistics is not the only imaginable way to summarize a dataset with a neural network. One could consider a maximum likelihood approach, etc. In general it felt like the paper could be more clear, if it avoided coining new terms like \"statistic network\" and stuck to the more accurate \"approximate posterior\".\n\n- The experiments are nice, and I appreciate the response to my question regarding \"one shot generation\". I still think that language needs to be clarified, specifically at the end of page 6. My understanding of Figure 5 is the following: Take an input set, compute the approximate posterior over the context vector, then generate from the forward model given samples from the approximate posterior. I would like clarification on the following: \n\n(a) Are the data point dependent vectors z generated from the forward model or taken from the approximate posterior? \n\n(b) I agree that the samples are of high-quality, but that is not a quantified statement. The advantage of VAEs over GANs is that we have natural ways of computing log-probabilities. To that end, one \"proper\" way of computing the \"one shot generation\" performance is to report log p(x | c) (where c is sampled from the approximate posterior) or log p(x) for held-out datasets. I suspect that log probability performance of these networks relative to a vanilla VAE without the context latent variable will be impressive. I still don't see a reason not to include that.", "IS_ANNOTATED": true, "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "16 Dec 2016", "CLARITY": 3, "REVIEWER_CONFIDENCE": 4}, {"TITLE": "one-shot task for generative models", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "", "IS_ANNOTATED": true, "IS_META_REVIEW": false, "DATE": "05 Dec 2016", "CLARITY": 3}, {"IMPACT": 3, "SUBSTANCE": 3, "RECOMMENDATION_UNOFFICIAL": 3, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "", "ORIGINALITY": 2, "IS_ANNOTATED": true, "TITLE": "MNIST classification performance?", "IS_META_REVIEW": false, "DATE": "29 Nov 2016"}, {"IS_META_REVIEW": true, "comments": "The authors introduce a variant of the variational autoencoder (VAE) that models dataset-level latent variables. The idea is clearly motivated and well described. In my mind the greatest contribution of this paper is the movement beyond the relatively simple graphical model structure of the traditional VAEs and the introduction of more interesting structures to the deep learning community. \n\nComments:\n\n- It's not clear to me why this should be called a \"statistician\". Learning an approximate posterior over summary statistics is not the only imaginable way to summarize a dataset with a neural network. One could consider a maximum likelihood approach, etc. In general it felt like the paper could be more clear, if it avoided coining new terms like \"statistic network\" and stuck to the more accurate \"approximate posterior\".\n\n- The experiments are nice, and I appreciate the response to my question regarding \"one shot generation\". I still think that language needs to be clarified, specifically at the end of page 6. My understanding of Figure 5 is the following: Take an input set, compute the approximate posterior over the context vector, then generate from the forward model given samples from the approximate posterior. I would like clarification on the following: \n\n(a) Are the data point dependent vectors z generated from the forward model or taken from the approximate posterior? \n\n(b) I agree that the samples are of high-quality, but that is not a quantified statement. The advantage of VAEs over GANs is that we have natural ways of computing log-probabilities. To that end, one \"proper\" way of computing the \"one shot generation\" performance is to report log p(x | c) (where c is sampled from the approximate posterior) or log p(x) for held-out datasets. I suspect that log probability performance of these networks relative to a vanilla VAE without the context latent variable will be impressive. I still don't see a reason not to include that."}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "This is an interesting paper that adds nicely to the literature on VAEs and one-shot generalisation. This will be of interest to the community and will contribute positively to the conference.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"TITLE": "Solid Contribution", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer4", "comments": "This paper proposes a hierarchical generative model where the lower level consists of points within datasets and the higher level models unordered sets of datasets.  The basic idea is to use a \"double\" variational bound where a higher level latent variable describes datasets and a lower level latent variable describes individual examples.  \n\nHierarchical modeling is an important and high impact problem, and I think that it's under-explored in the Deep Learning literature.  \n\nPros:\n  -The few-shot learning results look good, but I'm not an expert in this area.  \n  -The idea of using a \"double\" variational bound in a hierarchical generative model is well presented and seems widely applicable.  \n\nQuestions: \n  -When training the statistic network, are minibatches (i.e. subsets of the examples) used?  \n  -If not, does using minibatches actually give you an unbiased estimator of the full gradient (if you had used all examples)?  For example, what if the statistic network wants to pull out if *any* example from the dataset has a certain feature and treat that as the characterization.  This seems to fit the graphical model on the right side of figure 1.  If your statistic network is trained on minibatches, it won't be able to learn this characterization, because a given minibatch will be missing some of the examples from the dataset.  Using minibatches (as opposed to using all examples in the dataset) to train the statistic network seems like it would limit the expressive power of the model.  \n\nSuggestions: \n  -Hierarchical forecasting (electricity / sales) could be an interesting and practical use case for this type of model.  ", "IS_META_REVIEW": false, "RECOMMENDATION": 8, "DATE": "19 Dec 2016", "REVIEWER_CONFIDENCE": 2}, {"IMPACT": 3, "SUBSTANCE": 3, "RECOMMENDATION_UNOFFICIAL": 3, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "Sorry for the late review -- I've been having technical problems with OpenReview which prevented me from posting.\n\nThis paper presents a method for learning to predict things from sets of data points. The method is a hierarchical version of the VAE, where the top layer consists of an abstract context unit that summarizes a dataset. Experiments show that the method is able to \"learn to learn\" by acquiring the ability to learn distributions from small numbers of examples.\n\nOverall, this paper is a nice addition to the literature on one- or few-shot learning. The method is conceptually simple and elegant, and seems to perform well. Compared to other recent papers on one-shot learning, the proposed method is simpler, and is based on unsupervised representation learning. The paper is clearly written and a pleasure to read.\n\nThe name of the paper is overly grandiose relative to what was done; the proposed method doesn\u2019t seem to have much in common with a statistician, unless one means by that \"someone who thinks up statistics\". \n\nThe experiments are well chosen, and the few-shot learning results seem pretty solid given the simplicity of the method.\n\nThe spatial MNIST dataset is interesting and might make a good toy benchmark. The inputs in Figure 4 seem pretty dense, though; shouldn\u2019t the method be able to recognize the distribution with fewer samples?  (Nitpick: the red points in Figure 4 don\u2019t seem to correspond to meaningful points as was claimed in the text.) \n\nWill the authors release the code?\n", "ORIGINALITY": 2, "IS_ANNOTATED": true, "TITLE": "a nice addition to the one-/few-shot learning literature", "IS_META_REVIEW": false, "RECOMMENDATION": 8, "DATE": "19 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "TITLE": "Interesting paper that starts to expand the repertoire of variational autoencoders", "comments": "The authors introduce a variant of the variational autoencoder (VAE) that models dataset-level latent variables. The idea is clearly motivated and well described. In my mind the greatest contribution of this paper is the movement beyond the relatively simple graphical model structure of the traditional VAEs and the introduction of more interesting structures to the deep learning community. \n\nComments:\n\n- It's not clear to me why this should be called a \"statistician\". Learning an approximate posterior over summary statistics is not the only imaginable way to summarize a dataset with a neural network. One could consider a maximum likelihood approach, etc. In general it felt like the paper could be more clear, if it avoided coining new terms like \"statistic network\" and stuck to the more accurate \"approximate posterior\".\n\n- The experiments are nice, and I appreciate the response to my question regarding \"one shot generation\". I still think that language needs to be clarified, specifically at the end of page 6. My understanding of Figure 5 is the following: Take an input set, compute the approximate posterior over the context vector, then generate from the forward model given samples from the approximate posterior. I would like clarification on the following: \n\n(a) Are the data point dependent vectors z generated from the forward model or taken from the approximate posterior? \n\n(b) I agree that the samples are of high-quality, but that is not a quantified statement. The advantage of VAEs over GANs is that we have natural ways of computing log-probabilities. To that end, one \"proper\" way of computing the \"one shot generation\" performance is to report log p(x | c) (where c is sampled from the approximate posterior) or log p(x) for held-out datasets. I suspect that log probability performance of these networks relative to a vanilla VAE without the context latent variable will be impressive. I still don't see a reason not to include that.", "IS_ANNOTATED": true, "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "16 Dec 2016", "CLARITY": 3, "REVIEWER_CONFIDENCE": 4}, {"TITLE": "one-shot task for generative models", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "", "IS_ANNOTATED": true, "IS_META_REVIEW": false, "DATE": "05 Dec 2016", "CLARITY": 3}, {"IMPACT": 3, "SUBSTANCE": 3, "RECOMMENDATION_UNOFFICIAL": 3, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "", "ORIGINALITY": 2, "IS_ANNOTATED": true, "TITLE": "MNIST classification performance?", "IS_META_REVIEW": false, "DATE": "29 Nov 2016"}]}
{"text": "AN ACTOR-CRITIC ALGORITHM FOR LEARNING RATE LEARNING\n1 INTRODUCTION\nWhile facing large scale of training data, stochastic learning such as stochastic gradient descent (SGD) is usually much faster than batch learning and often results in better models. An observation for SGD methods is that their performances are highly sensitive to the choice of learning rate LeCun et al. (2012). Clearly, setting a static learning rate for the whole training process is insufficient, since intuitively the learning rate should decrease when the model becomes more and more close to a (local) optimum as the training goes on over time Maclaurin et al. (2015). Although there are some empirical suggestions to guide how to adjust the learning rate over time in training, it is still a difficult task to find a good policy to adjust the learning rate, given that good policies are problem specific and depend on implementation details of a machine learning algorithm. One usually needs to try many times and adjust the learning rate manually to accumulate knowledge about the problem. However, human involvement often needs domain knowledge about the target problems, which is inefficient and difficult to scale up to different problems. Thus, a natural question arises: can we automatically adjust the learning rate? This is exactly the focus of this work and we aim to automatically learn the learning rates for SGD based machine learning (ML) algorithms without human-designed rules or hand-crafted features.\nBy examining the current practice of learning rate control/adjustment, we have two observations. First, learning rate control is a sequential decision process. At the beginning, we set an initial learning rate. Then at each step, we decide whether to change the learning rate and how to change it, based on the current model and loss, training data at hand, and maybe history of the training process. As suggested in Orr & Mu\u0308ller (2003), one well-principled method for estimating the ideal learning rate that is to decrease the learning rate when the weight vector oscillates, and increase it when the weight vector follows a relatively steady direction. Second, although at each step some immediate reward (e.g., the loss decrement) can be obtained by taking actions, we care more about the performance of the final model found by the ML algorithm. Consider two different learning rate\ncontrol policies: the first one leads to fast loss decrease at the beginning but gets saturated and stuck in a local minimum quickly, while the second one starts with slower loss decrease but results in much smaller final loss. Obviously, the second policy is better. That is, we prefer long-term rewards over short-term rewards.\nCombining the two observations, it is easy to see that the problem of finding a good policy to control/adjust learning rate falls into the scope of reinforcement learning (RL) Sutton & Barto (1998), if one is familiar with RL. Inspired by the recent success of RL for sequential decision problems, in this work, we leverage RL techniques and try to learn the learning rate for SGD based methods.\nWe propose an algorithm to learn the learning rate within the actor-critic framework Sutton (1984); Sutton et al. (1999); Barto et al. (1983); Silver et al. (2014) from RL. In particular, an actor network is trained to take an action that decides the learning rate for current step, and a critic network is trained to give feedbacks to the actor network about long-term performance and help the actor network to adjust itself so as to perform better in the future steps. The main contributions of this paper include:\n\u2022 We propose an actor-critic algorithm to automatically learn the learning rate for ML algorithms.\n\u2022 Long-term rewards are exploited by the critic network in our algorithm to choose a better learning rate at each step.\n\u2022 We propose to feed different training examples to the actor network and the critic network, which improve the generalization performance of the learnt ML model.\n\u2022 A series of experiments validate the effectiveness of our proposed algorithm for learning rate control.\n2 RELATED WORK\n2.1 IMPROVED GRADIENT METHODS\nOur focus is to improve gradient based ML algorithm through automatic learning of learning rate. Different approaches have been proposed to improve gradient methods, especially for deep neural networks.\nSince SGD solely rely on a given example (or a mini-batch of examples) to compare gradient, its model update at each step tends to be unstable and it takes many steps to converge. To solve this problem, momentum SGD Jacobs (1988) is proposed to accelerate SGD by using recent gradients. RMSprop Tieleman & Hinton (2012) utilizes the magnitude of recent gradients to normalize the gradients. It always keeps a moving average over the root mean squared gradients, by which it divides the current gradient. Adagrad Duchi et al. (2011) adapts component-wise learning rates, and performs larger updates for infrequent and smaller updates for frequent parameters. Adadelta Zeiler (2012) extends Adagrad by reducing its aggressive, monotonically decreasing learning rate. Instead of accumulating all past squared gradients, Adadelta restricts the window of accumulated past gradients to some fixed size. Adam Kingma & Ba (2014) computes component-wise learning rates using the estimates of first and second moments of the gradients, which combines the advantages of AdaGrad and RMSProp.\nSenior et al. (2013); Sutton (1992); Darken & Moody (1990) focus on predefining update rules to adjust learning rates during training. A limitation of these methods is that they have additional free parameters which need to be set manually. Another recent work Daniel et al. (2016) studies how to automatically select step sizes, but it still requires hand-tuned features. Schaul et al. (2013) proposes a method to choose good learning rate for SGD, which relies on the square norm of the expectation of the gradient, and the expectation of the square norm of the gradient. The method is much more constrained than ours and several assumption should be met.\n2.2 REINFORCEMENT LEARNING\nSince our proposed algorithm is based on RL techniques, here we give a very brief introduction to RL, which will ease the description of our algorithm in next section.\nReinforcement learning Sutton (1988) is concerned with how an agent acts in a stochastic environment by sequentially choosing actions over a sequence of time steps, in order to maximize a cumulative reward. In RL, a state st encodes the agents observation about the environment at a time step t, and a policy function \u03c0(st) determines how the agent behaves (e.g., which action to take) at state st. An action-value function (or, Q function) Q\u03c0(st, at) is usually used to denote the cumulative reward of taking action at at state st and then following policy \u03c0 afterwards.\nMany RL algorithms have been proposed Sutton & Barto (1998); Watkins & Dayan (1992), and many RL algorithms Sutton (1984); Sutton et al. (1999); Barto et al. (1983); Silver et al. (2014) can be described under the actor-critic framework. An actor-critic algorithm learns the policy function and the value function simultaneously and interactively. The policy structure is known as the actor, and is used to select actions; the estimated value function is known as the critic, and it criticizes the actions made by the actor.\nRecently, deep reinforcement learning, which uses deep neural networks to approximate/represent the policy function and/or the value function, have shown promise in various domains, including Atari games Mnih et al. (2015), Go Silver et al. (2016), machine translation Bahdanau et al. (2016), image recognition Xu et al. (2015), etc.\n3 METHOD\nIn this section, we present an actor-critic algorithm that can automate the learning rate control for SGD based machine learning algorithms.\nMany machine learning tasks need to train a model with parameters \u03c9 by minimizing a loss function f defined over a set X of training examples:\n\u03c9\u2217 = arg min \u03c9 f\u03c9(X). (1)\nA standard approach for the loss function minimization is gradient descent, which sequentially updates the parameters using gradients step by step:\n\u03c9t+1 = \u03c9t \u2212 at\u2207f t, (2) where at is the learning rate at step t, and \u2207f t is the local gradient of f at \u03c9t. Here one step can be the whole batch of all the training data, a mini batch of tens/hundreds of examples, or a random sample.\nIt is observed that the performance of SGD based methods is quite sensitive to the choice of at for non-convex loss function f . Unfortunately, f is usually non-convex with respect to the parameters\nw in many ML algorithms, especially for deep neural networks. We aim to learn a learning rate controller using RL techniques that can automatically control at.\nFigure 1 illustrates our automatic learning rate controller, which adopts the actor-critic framework in RL. The basic idea is that at each step, given the current model \u03c9t and training sample x, an actor network is used to take an action (the learning rate at, and it will be used to update the model \u03c9t), and a critic network is used to estimate the goodness of the action. The actor network will be updated using the estimated goodness of at, and the critic network will be updated by minimizing temporal difference (TD) Sutton & Barto (1990) error. We describe the details of our algorithm in the following subsections.\n3.1 ACTOR NETWORK\nThe actor network, which is called policy network in RL, plays the key role in our algorithm: it determines the learning rate control policy for the primary ML algorithm1 based on the current model, training data, and maybe historical information during the training process.\nNote that \u03c9t could be of huge dimensions, e.g., one widely used image recognition model VGGNet Simonyan & Zisserman (2014) has more than 140 million parameters. If the actor network takes all of those parameters as the inputs, its computational complexity would dominate the complexity of the primary algorithm, which is unfordable. Therefore, we propose to use a function \u03c7(\u00b7) to process and yield a compact vector st as the input of the actor network. Following the practice in RL, we call \u03c7(\u00b7) the state function, which takes \u03c9t and the training data x as inputs:\nst = \u03c7(\u03c9t, X). (3)\nThen the actor network \u03c0\u03b8(\u00b7) parameterized by \u03b8 yields an action at:\n\u03c0\u03b8(s t) = at, (4)\nwhere the action at \u2208 R is a continuous value. When at is determined, we update the model of the primary algorithm by Equation 2.\nNote that the actor network has its own parameters and we need to learn them to output a good action. To learn the actor network, we need to know how to evaluate the goodness of an actor network. The critic network exactly plays this role.\n3.2 CRITIC NETWORK\nRecall that our goal is to find a good policy for learning rate control to ensure that a good model can be learnt eventually by the primary ML algorithm. For this purpose, the actor network needs to output a good action at at state st so that finally a low training loss f(\u00b7) can be achieved. In RL, the Q function Q\u03c0(s, a) is often used to denote the long term reward of the state-action pair s, a while following the policy \u03c0 to take future actions. In our problem, Q\u03c0(st, at) indicates the accumulative decrement of training loss starting from step t. We define the immediate reward at step t as the one step loss decrement:\nrt = f t \u2212 f t+1. (5)\nThe accumulative value Rt\u03c0 of policy \u03c0 at step t is the total discounted reward from step t:\nRt\u03c0 = \u03a3 T k=t\u03b3 k\u2212tr(sk, ak),\nwhere \u03b3 \u2208 (0, 1] is the discount factor. Considering that both the states and actions are uncountable in our problem, the critic network uses a parametric function Q\u03d5(s, a) with parameters \u03d5 to approximate the Q value function Q\u03c0(s, a).\n1Here we have two learning algorithms. We call the one with learning rate to adjust as the primary ML algorithm, and the other one which optimizes the learning rate of the primary one as the secondary ML algorithm.\n3.3 TRAINING OF ACTOR AND CRITIC NETWORKS\nThe critic network has its own parameters \u03d5, which is updated at each step using TD learning. More precisely, the critic is trained by minimizing the square error between the estimation Q\u03d5(st, at) and the target yt:\nyt = rt + \u03b3Q\u03d5(s t+1, at+1). (6)\nThe TD error is defined as:\n\u03b4t = yt \u2212Q\u03d5(st, at) = rt + \u03b3Q\u03d5(s t+1, \u03c0\u03b8(s t+1))\u2212Q\u03d5(st, at)\n(7)\nThe weight update rule follows the on-policy deterministic actor-critic algorithm. The gradients of critic network are:\n\u2207\u03d5 = \u03b4t\u2207\u03d5Q\u03d5(st, at), (8)\nThe policy parameters \u03b8 of the actor network is updated by ensuring that it can output the action with the largest Q value at state st, i.e., a\u2217 = arg maxaQ\u03d5(st, a). Mathematically,\n\u2207\u03b8 = \u2207\u03b8\u03c0\u03b8(st+1)\u2207aQ\u03d5(st+1, at+1)|a=\u03c0\u03b8(s). (9)\nAlgorithm 1 Actor-Critic Algorithm for Learning Rate Learning Require: Training steps T ; training set X; loss function f ; state function \u03c7; discount factor: \u03b3 ; Ensure: Model parameters w, policy parameters \u03b8 of the actor network, and value parameters \u03d5 of\nthe critic network; 1: Initial parameters \u03c90, \u03b80, \u03d50; 2: for t = 0, ..., T do 3: Sample xi \u2208 X, i \u2208 1, ..., N . 4: Extract state vector: sti = \u03c7(\u03c9\nt, xi). 5: //Actor network selects an action. 6: Computes learning rate ati = \u03c0\u03b8(s t i). 7: //Update model parameters \u03c9. 8: Compute\u2207f t(xi). 9: Update \u03c9: \u03c9t+1 = \u03c9t \u2212 ati\u2207f t(xi).\n10: //Update critic network by minimizing square error between estimation and label. 11: rt = f t(xi)\u2212 f t+1(xi) 12: Extract state vector: st+1i = \u03c7(\u03c9 t+1, xi) 13: Compute Q\u03d5(st+1i , \u03c0\u03b8(s t+1 i )), Q\u03d5(s t i, a t i) 14: Compute \u03b4t according to Equation 7: \u03b4t = rt + \u03b3Q\u03d5(s t+1 i , \u03c0\u03b8(s t+1 i ))\u2212Q\u03d5(sti, ati) 15: Update \u03d5 using the following gradients according to Equation 8 : \u2207\u03d5 = \u03b4t\u2207\u03d5Q\u03d5(sti, ati) 16: // Update actor network 17: Sample xj \u2208 X, j \u2208 1, ..., N, j 6= i. 18: Extract state vector: st+1j = \u03c7(\u03c9\nt+1, xj). 19: Compute at+1j = \u03c0\u03b8(s t+1 j ). 20: Update \u03b8 from Equation 9: \u2207\u03b8 = \u2207\u03b8\u03c0\u03b8(st+1j )\u2207aQ\u03d5(s t+1 j , a t+1 j )|a=\u03c0\u03b8(s) 21: end for 22: return \u03c9, \u03b8, \u03d5;\n3.4 THE ALGORITHM\nThe overall algorithm is shown in Algorithm 1. In each step, we sample an example (Line 3), extract the current state vector (Line 4), compute the learning rate using the actor network (Line 6), update the model (Lines 8-9), compute TD error (Lines 11-14), update the critic network (Line 15), and sample another example (Line 17) to update the actor network (Line 18-20). We would like to make some discussions about the algorithm.\nFirst, in the current algorithm, for simplicity, we consider using only one example for model update. It is easy to generalize to a mini batch of random examples.\nSecond, one may notice that we use one example (e.g., xi) for model and the critic network update, but a different example (e.g., xj) for the actor network update. Doing so we can avoid that the algorithm will overfit on some (too) hard examples and can improve the generalization performance of the algorithm on the test set. Consider a hard example2 in a classification task. Since such an example is difficult to be classified correctly, intuitively its gradient will be large and the learning rate given by the actor network at this step will also be large. In other words, this hard example will greatly change the model, while itself is not a good representative of its category and the learning algorithm should not pay much attention to it. If we feed the same example to both the actor network and the critic network, both of them will encourage the model to change a lot to fit the example, consequently resulting in oscillation of the training, as shown in our experiments. By feeding different examples to the actor and critic networks, it is very likely the critic network will find that the gradient direction of the example fed into the actor network is inconsistent with its own training example and thus criticize the large learning rate suggested by the actor network. More precisely, the update of \u03c9 is based on xi and the learning rate suggested by the actor network, while the training target of the actor network is to maximize the output of the critic network on xj . If there is big gradient disagreement between xi and xj , the update of \u03c9, which is affected by actor\u2019s decision, would cause the critic\u2019s output on xj to be small. To compensate this effect, the actor network is forced to predict a small learning rate for a too hard xi in this situation.\n4 EXPERIMENTS\nWe conducted a set of experiments to test the performance of our learning rate learning algorithm and compared with several baseline methods. We report the experimental results in this section.\n4.1 EXPERIMENTAL SETUP\nWe tested our method on two widely used image classification datasets: MNIST LeCun et al. (1998) and CIFAR-10 Krizhevsky & Hinton (2009). Convolutional neural networks (CNNs) are the standard model for image classification tasks in recent years, and thus the primary ML algorithm adopted the CNN model in all our experiments.\nWe specified our actor-critic algorithm in experiments as follows. Given that stochastic mini-batch training is a common practice in deep learning, the actor-critic algorithm also operated on minibatches, i.e., each step is a mini batch in our experiments. We defined the state st = \u03c7(\u03c9t, Xi) as the average loss of learning model \u03c9t on the input min-batch Xi. We specified the actor network as a two-layer long short-term memory (LSTM) network with 20 units in each layer, considering that a good learning rate for step t depends on and correlates with the learning rates at previous steps while LSTM is well suited to model sequences with long-distance dependence. We used the absolute value activation function for the output layer of the LSTM to ensure a positive learning rate. The LSTM was unrolled for 20 steps during training. We specified the critic network as a simple neural network with one hidden layer and 10 hidden units. We use Adam with the default setting in TensorFlow optimizer toolbox Abadi et al. (2015) to train the actor and critic networks in all the experiments.\nWe compared our method with several mainstream SGD algorithms, including SGD, Adam Kingma & Ba (2014), Adagrad Duchi et al. (2011) and RMSprop Tieleman & Hinton (2012). For each of these algorithms and each dataset, we tried the following learning rates 10\u22124, 10\u22123, ..., 100. We report the best performance of these algorithms over those learning rates. If an algorithm needs some other parameters to set, such as decay coefficients for Adam, we used the default setting in TensorFlow optimizer toolbox. For each benchmark and our proposed method, five independent runs are averaged and reported in all of the following experiments.\n4.2 RESULTS ON MNIST\nMNIST is a dataset for handwritten digit classification task. Each example in the dataset is a 28\u00d728 black and white image containing a digit in {0, 1, \u00b7 \u00b7 \u00b7 , 9}. The CNN model used in the primary\n2For example, an example may has an incorrect label because of the limited quality of labelers.\nML algorithm is consist of two convolutional layers, each followed by a pooling layer, and finally a fully connected layer. The first convolutional layer filters each input image using 32 kernels of size 5 \u00d7 5. The max-pooling layer following the first convolutional layer is performed over 2 \u00d7 2 pixel windows, with stride 2. The second convolutional layer takes the outputs of the first max-pooling layer as inputs and filters them with 64 kernels of size 5 \u00d7 5. The max-pooling layer following the second convolutional layer is performed over 2 \u00d7 2 pixel windows, with stride 2. The outputs of second max pooling layer are fed to a fully connected layer with 512 neurons. Dropout was conducted on the fully connect layer with a dropout rate of 0.5. ReLU activation functions are used in the CNN model. There are 60,000 training images and 10,000 test images in this dataset. We scaled the pixel values to the [0,1] range before inputting to all the algorithms. Each mini batch contains 50 randomly sampled images.\nFigure 2 shows the results of our actor-critic algorithm for learning rate learning and the baseline methods, including the curves of training loss, test loss, and test accuracy. The final accuracies of these methods are summarized in Table 1. We have the following observations.\n\u2022 In terms of training loss, our algorithm has similar convergence speed to the baseline methods. One may expect that our algorithm should have significantly faster convergence speed considering that our algorithm learns both the learning rate and the CNN model while the baselines only learn the CNN model and choose the learning rates per some predefined rules. However, this is not correct. As discussed in Section 3.4, we carefully design the algorithm and feed different samples to the actor network and critic network. Doing so we can focus more on generalization performance than training loss: as shown in Figure 4, our algorithm achieves the best test accuracy.\n10-2 10-1 100 101\nEpoch\n0.010\n0.015\n0.020\n0.025\n0.030\n0.035\n0.040\n0.045\n0.050\n0.055\nLe a rn\nin g r\na te\nCIFAR-10\nFigure 5: The learning rate learned by actor network for CIFAR-10.\n\u2022 Our algorithm achieves the lowest error rate on MNIST. Although the improvement looks small, we would like to point out that given that the accuracy of CNN is already close to 100%, it is a very difficult task to further improve accuracy, not to mention that we only changed learning rate policy without changing the CNN model.\n4.3 RESULTS ON CIFAR-10\nCIFAR-10 is a dataset consisting of 60000 natural 32 \u00d7 32 RGB images in 10 classes: 50,000 imagesfor training and 10,000 for test. We used a CNN with 2 convolutional layers (each followed by max-pooling layer) and 2 fully connected layers for this task. There is a max pooling layer which performed over 2\u00d7 2 pixel windows, with stride 2 after each convolutional layer. All convolutional layers filter the input with 64 kernels of size 5\u00d7 5. The outputs of the second pooling layer are fed to a fully connected layer with 384 neurons. The last fully connected layer has 192 neurons. Before inputting an image to the CNN, we subtracted the per-pixel mean computed over the training set from each image.\nFigure 3 shows the results of all the algorithms on CIFAR-10, including the curves of training loss, the test loss and test accuracy. Table 2 shows the final test accuracy. We get similar observations as MNIST: our algorithm achieves similar convergence speed in terms of training loss and slightly better test accuracy than baselines. Figure 5 shows the learning rate learned by our method on CIFAR-10. To further understand the generalization performance of our algorithm, we ran all the\nalgorithms on two subsets of training data on CIFAR-10: one with only 20% training data The curves of training loss and test loss are shown in Figure 4. As can be seen from the figure, those baseline methods are easy to overfit and their test loss increases after 5000 steps (mini batches). In contrast, our algorithm is relatively robust and can prevent overfitting to some extent.\nAs we explained in Section 3.4, feeding different examples to the actor and critic networks is important to guarantee generalization ability. Here we conducted another experiment to verify our intuitive explanation. Figure 6 shows the results of two different implementations of our actor-critic algorithm on CIFAR-10. In the first implementation, we fed the sample examples to the two net-\nworks, i.e., xi = xj in the algorithm, and in the second implementation, the input xj of the critic network is different from the input xi of the actor network. It is easy to see from the figure that setting xi = xj tends to oscillate during training and leads to poor test performance. Thus, we need to feed different training data to the actor network and the critic network to ensure the performance of the algorithm.\n4.4 COMPARISON WITH OTHER ADAPTIVE LEARNING RATE METHOD\nWe also compare our method with \u201cvSGD\u201d from previous by work Schaul et al. (2013), which can automatically adjust learning rates to minimize the expected error. This method tries to compute learning rate at each update by optimizing the expected loss after the next update according to the square norm of the expectation of the gradient, and the expectation of the square norm of the gradient. Note that our method learns to predict a learning rate at each time step by utilizing the long term reward predicted by a critic network.\nFor a fair comparison, we followed the experiments settings of Schaul et al. (2013), which designed three different network architectures for MNIST task to measure the performance. The first one is denoted by \u2018M0\u2019 which is simple softmax regression (i.e. a network with no hidden layer). The second one (\u2018M1\u2019) is a fully connected multi-layer perceptron, with a single hidden layer. The third one (denoted \u2018M2\u2019) is a deep, fully connected multi-layer perceptron with two hidden layers. The vSGD has three variants in their paper. We referred to the results reported in their paper and compared our method with all of three variants of their algorithm (vSGD-l, vSGD-b, vSGD-g). The learning rates of SGD are decreased according to a human designed schedule, and the hyperparameters of SGD, ADAM, Adagrad, RMSprop are carefully determined by their lowest test error among a set of hyper-parameters. All hyper-parameters can be found in Schaul et al. (2013).\nThe experimental results are reported in Table 3. It shows that our proposed method performs better than vSGD and other baseline methods, and is stable across different network architectures.\n5 CONCLUSIONS AND FUTURE WORK\nIn this work, we have studied how to automatically learn learning rates for gradient based machine learning methods and proposed an actor-critic algorithm, inspired by the recent success of reinforcement learning. The experiments on two image classification datasets have shown that our method (1) has comparable convergence speed with expert-designed optimizer while achieving better test accuracy, and (2) can successfully adjust learning rate for different datasets and CNN model structures.\nFor the future work, we will explore the following directions. In this work, we have applied our algorithm to control the learning rates of SGD. We will apply to other variants of SGD methods. We have focused on learning a learning rate for all the model parameters. We will study how to learn an individual learning rate for each parameter. We have considered learning learning rates using RL techniques. We will consider learning other hyperparameters such as step-dependent dropout rates for deep neural networks.\nA APPENDIX\nA method of automatically controlling learning rate is proposed in the main body of the paper. The learning rate controller adjusts itself during training to control the learning rate. Here, we propose an improved version that can leverage experiences from several repeated training runs to learn a fixed learning rate controller. Empirically, this algorithm can achieve better performance than the previous one. Given that it requires more time for training the learning rate controller, this method is more suitable for training offline models.\nIn this algorithm, during every training run, we fix the actor network and compute the weighted sum of the gradients of its parameter \u03b8. The parameter is updated after each run (modified from Equation 9):\n\u2207\u03b8 = \u03a3Tt=1h(t)\u2207\u03b8\u03c0\u03b8(st+1)\u2207aQ\u03d5(st+1, at+1)|a=\u03c0\u03b8(s). (10)\nh(t) is weighted function which is used to amplify the feedback signal from the initial training stage. It is defined as h(t) = 1/t in our experiments. An error rate of 0.48% was achieved with 5 repeated training runs in MNIST experiment (the same setting as Table 1), and in CIFAR-10 experiment (the same setting as Table 2), 80.23% accuracy was achieved with 10 training runs. This method showed better performance in both experiments.\n", "cats": {"READ": 0.0, "DONTREAD": 1.0}, "reviews": [{"IS_META_REVIEW": true, "comments": "The paper proposes using an actor-critic RL algorithm for training learning rate controllers for supervised learning. The proposed method outperforms standard optimizers like SGD, ADAM and RMSprop in experiments conducted on MNIST and CIFAR 10.\n\nI have two main concerns. One is the lack of comparisons to similar recently proposed methods - \"Learning Step Size Controllers for Robust Neural Network Training\" by Daniel et al. and \"Learning to learn by gradient descent by gradient descent\" by Andrychowicz et al. The work of Daniel et al. is quite similar because it also proposes using a policy search RL method (REPS) and it is not clear what the downsides of their approach are. Their work does use more prior knowledge as the authors stated, but why is this a bad thing?\n\nMy second concern is with the experiments. Some of the numbers reported for the other methods are surprisingly low. For example, why is RMSprop so bad in Table 2 and Table 3? These results suggest that the methods are not being tuned properly, which reinforces the need for comparisons on standard architectures with previously reported results. For example, if the baselines used a better architecture like a ResNet or, for simplicty, Network in Network from this list:"}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "The authors use actor-critic reinforcement learning to adjust the step size of a supervised learning algorithm. There are no comparisons made to other, similar approaches, and the baselines are suspiciously weak, making the proposed method difficult to justify.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"TITLE": "Interesting application of actor-critic methods, but difficult to assess relative to other adaptive learning algorithms", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "The authors present a method for adaptively setting the step size for SGD by treating the learning rate as an action in an MDP whose reward is the change in loss function. The method is presented against popular adaptive first-order methods for training deep networks (Adagrad, Adam, RMSProp, etc). The results are interesting but difficult to assess in a true apples-to-apples manner. Some specific comments:\n\n-What is the computational overhead of the actor-critic algorithm relative to other algorithms? No plots with the wall-time of optimization are presented, even though the success of methods like Adagrad was due to their wall-time performance, not the number of iterations.\n-Why was only a single learning rate learned? To accurately compare against other popular first order methods, why not train a separate RL model for each parameter, similar to how popular first-order methods adaptively change the learning rate for each parameter.\n-Since learning is a non-stationary process, while RL algorithms assume a stationary environment, why should we expect an RL algorithm to work for learning a learning rate?\n-In figure 6, how does the proposed method compare to something like early stopping? It may be that the actor-critic method is overfitting less simply because it is worse at optimization.", "IS_META_REVIEW": false, "RECOMMENDATION": 5, "DATE": "20 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "No comparisons to recent alternatives", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "The paper proposes using an actor-critic RL algorithm for training learning rate controllers for supervised learning. The proposed method outperforms standard optimizers like SGD, ADAM and RMSprop in experiments conducted on MNIST and CIFAR 10.\n\nI have two main concerns. One is the lack of comparisons to similar recently proposed methods - \"Learning Step Size Controllers for Robust Neural Network Training\" by Daniel et al. and \"Learning to learn by gradient descent by gradient descent\" by Andrychowicz et al. The work of Daniel et al. is quite similar because it also proposes using a policy search RL method (REPS) and it is not clear what the downsides of their approach are. Their work does use more prior knowledge as the authors stated, but why is this a bad thing?\n\nMy second concern is with the experiments. Some of the numbers reported for the other methods are surprisingly low. For example, why is RMSprop so bad in Table 2 and Table 3? These results suggest that the methods are not being tuned properly, which reinforces the need for comparisons on standard architectures with previously reported results. For example, if the baselines used a better architecture like a ResNet or, for simplicty, Network in Network from this list:\n", "IS_META_REVIEW": false, "RECOMMENDATION": 4, "DATE": "15 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "Final Review", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "In the question response the authors mention and compare other works such as \"Learning to Learn by Gradient Descent by Gradient Descent\", but the goal of current work and that work is quite different. That work is a new form of optimization algorithm which is not the case here. And bayesian hyper-parameter optimization methods aim for multiple hyper-parameters but this work only tune one hyper-parameter.\nThe network architecture used for the experiments on CIFAR-10 is quite outdated and the performances are much poorer than any work that has published in last few years. So the comparison are not valid here, as if the paper claim the advantage of their method, they should use the state of the art network architecture and see if their claim still holds in that setting too.\nAs discussed before, the extra cost of hyper-parameter optimizers are only justified if the method could push the SOTA results in multiple modern datasets.\nIn summary, the general idea of having an actor-critic network as a meta-learner is an interesting idea. But the particular application proposed here does not seems to have any practical value and the reported results are very limited and it's hard to draw any conclusion about the effectiveness of the method. ", "IS_META_REVIEW": false, "RECOMMENDATION": 3, "DATE": "15 Dec 2016 (modified: 23 Jan 2017)", "REVIEWER_CONFIDENCE": 5}, {"DATE": "02 Dec 2016", "TITLE": "Pre review questions", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3"}, {"DATE": "01 Dec 2016", "TITLE": "The actor-crtic networks still need learning rate search", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1"}, {"IS_META_REVIEW": true, "comments": "The paper proposes using an actor-critic RL algorithm for training learning rate controllers for supervised learning. The proposed method outperforms standard optimizers like SGD, ADAM and RMSprop in experiments conducted on MNIST and CIFAR 10.\n\nI have two main concerns. One is the lack of comparisons to similar recently proposed methods - \"Learning Step Size Controllers for Robust Neural Network Training\" by Daniel et al. and \"Learning to learn by gradient descent by gradient descent\" by Andrychowicz et al. The work of Daniel et al. is quite similar because it also proposes using a policy search RL method (REPS) and it is not clear what the downsides of their approach are. Their work does use more prior knowledge as the authors stated, but why is this a bad thing?\n\nMy second concern is with the experiments. Some of the numbers reported for the other methods are surprisingly low. For example, why is RMSprop so bad in Table 2 and Table 3? These results suggest that the methods are not being tuned properly, which reinforces the need for comparisons on standard architectures with previously reported results. For example, if the baselines used a better architecture like a ResNet or, for simplicty, Network in Network from this list:"}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "The authors use actor-critic reinforcement learning to adjust the step size of a supervised learning algorithm. There are no comparisons made to other, similar approaches, and the baselines are suspiciously weak, making the proposed method difficult to justify.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"TITLE": "Interesting application of actor-critic methods, but difficult to assess relative to other adaptive learning algorithms", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "The authors present a method for adaptively setting the step size for SGD by treating the learning rate as an action in an MDP whose reward is the change in loss function. The method is presented against popular adaptive first-order methods for training deep networks (Adagrad, Adam, RMSProp, etc). The results are interesting but difficult to assess in a true apples-to-apples manner. Some specific comments:\n\n-What is the computational overhead of the actor-critic algorithm relative to other algorithms? No plots with the wall-time of optimization are presented, even though the success of methods like Adagrad was due to their wall-time performance, not the number of iterations.\n-Why was only a single learning rate learned? To accurately compare against other popular first order methods, why not train a separate RL model for each parameter, similar to how popular first-order methods adaptively change the learning rate for each parameter.\n-Since learning is a non-stationary process, while RL algorithms assume a stationary environment, why should we expect an RL algorithm to work for learning a learning rate?\n-In figure 6, how does the proposed method compare to something like early stopping? It may be that the actor-critic method is overfitting less simply because it is worse at optimization.", "IS_META_REVIEW": false, "RECOMMENDATION": 5, "DATE": "20 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "No comparisons to recent alternatives", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "The paper proposes using an actor-critic RL algorithm for training learning rate controllers for supervised learning. The proposed method outperforms standard optimizers like SGD, ADAM and RMSprop in experiments conducted on MNIST and CIFAR 10.\n\nI have two main concerns. One is the lack of comparisons to similar recently proposed methods - \"Learning Step Size Controllers for Robust Neural Network Training\" by Daniel et al. and \"Learning to learn by gradient descent by gradient descent\" by Andrychowicz et al. The work of Daniel et al. is quite similar because it also proposes using a policy search RL method (REPS) and it is not clear what the downsides of their approach are. Their work does use more prior knowledge as the authors stated, but why is this a bad thing?\n\nMy second concern is with the experiments. Some of the numbers reported for the other methods are surprisingly low. For example, why is RMSprop so bad in Table 2 and Table 3? These results suggest that the methods are not being tuned properly, which reinforces the need for comparisons on standard architectures with previously reported results. For example, if the baselines used a better architecture like a ResNet or, for simplicty, Network in Network from this list:\n", "IS_META_REVIEW": false, "RECOMMENDATION": 4, "DATE": "15 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "Final Review", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "In the question response the authors mention and compare other works such as \"Learning to Learn by Gradient Descent by Gradient Descent\", but the goal of current work and that work is quite different. That work is a new form of optimization algorithm which is not the case here. And bayesian hyper-parameter optimization methods aim for multiple hyper-parameters but this work only tune one hyper-parameter.\nThe network architecture used for the experiments on CIFAR-10 is quite outdated and the performances are much poorer than any work that has published in last few years. So the comparison are not valid here, as if the paper claim the advantage of their method, they should use the state of the art network architecture and see if their claim still holds in that setting too.\nAs discussed before, the extra cost of hyper-parameter optimizers are only justified if the method could push the SOTA results in multiple modern datasets.\nIn summary, the general idea of having an actor-critic network as a meta-learner is an interesting idea. But the particular application proposed here does not seems to have any practical value and the reported results are very limited and it's hard to draw any conclusion about the effectiveness of the method. ", "IS_META_REVIEW": false, "RECOMMENDATION": 3, "DATE": "15 Dec 2016 (modified: 23 Jan 2017)", "REVIEWER_CONFIDENCE": 5}, {"DATE": "02 Dec 2016", "TITLE": "Pre review questions", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3"}, {"DATE": "01 Dec 2016", "TITLE": "The actor-crtic networks still need learning rate search", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1"}]}
{"text": "1 INTRODUCTION\nRealistic simulated environments, where agents can be trained to learn a large repertoire of cognitive skills, are at the core of recent breakthroughs in AI (Bellemare et al., 2013; Mnih et al., 2015; Schulman et al., 2015a; Narasimhan et al., 2015; Mnih et al., 2016; Brockman et al., 2016; Oh et al., 2016). With richer realistic environments, the capabilities of our agents have increased and improved. Unfortunately, these advances have been accompanied by a substantial increase in the cost of simulation. In particular, every time an agent acts upon the environment, an expensive simulation step is conducted. Thus to reduce the cost of simulation, we need to reduce the number of simulation steps (i.e. samples of the environment). This need for sample efficiency is even more compelling when agents are deployed in the real world.\nExperience replay (Lin, 1992) has gained popularity in deep Q-learning (Mnih et al., 2015; Schaul et al., 2016; Wang et al., 2016; Narasimhan et al., 2015), where it is often motivated as a technique for reducing sample correlation. Replay is actually a valuable tool for improving sample efficiency and, as we will see in our experiments, state-of-the-art deep Q-learning methods (Schaul et al., 2016; Wang et al., 2016) have been up to this point the most sample efficient techniques on Atari by a significant margin. However, we need to do better than deep Q-learning, because it has two important limitations. First, the deterministic nature of the optimal policy limits its use in adversarial domains. Second, finding the greedy action with respect to the Q function is costly for large action spaces.\nPolicy gradient methods have been at the heart of significant advances in AI and robotics (Silver et al., 2014; Lillicrap et al., 2015; Silver et al., 2016; Levine et al., 2015; Mnih et al., 2016; Schulman et al., 2015a; Heess et al., 2015). Many of these methods are restricted to continuous domains or to very specific tasks such as playing Go. The existing variants applicable to both continuous and discrete domains, such as the on-policy asynchronous advantage actor critic (A3C) of Mnih et al. (2016), are sample inefficient.\nThe design of stable, sample efficient actor critic methods that apply to both continuous and discrete action spaces has been a long-standing hurdle of reinforcement learning (RL). We believe this paper\nis the first to address this challenge successfully at scale. More specifically, we introduce an actor critic with experience replay (ACER) that nearly matches the state-of-the-art performance of deep Q-networks with prioritized replay on Atari, and substantially outperforms A3C in terms of sample efficiency on both Atari and continuous control domains.\nACER capitalizes on recent advances in deep neural networks, variance reduction techniques, the off-policy Retrace algorithm (Munos et al., 2016) and parallel training of RL agents (Mnih et al., 2016). Yet, crucially, its success hinges on innovations advanced in this paper: truncated importance sampling with bias correction, stochastic dueling network architectures, and efficient trust region policy optimization.\nOn the theoretical front, the paper proves that the Retrace operator can be rewritten from our proposed truncated importance sampling with bias correction technique.\n2 BACKGROUND AND PROBLEM SETUP\nConsider an agent interacting with its environment over discrete time steps. At time step t, the agent observes the nx-dimensional state vector xt \u2208 X \u2286 Rnx , chooses an action at according to a policy \u03c0(a|xt) and observes a reward signal rt \u2208 R produced by the environment. We will consider discrete actions at \u2208 {1, 2, . . . , Na} in Sections 3 and 4, and continuous actions at \u2208 A \u2286 Rna in Section 5. The goal of the agent is to maximize the discounted return Rt = \u2211 i\u22650 \u03b3\nirt+i in expectation. The discount factor \u03b3 \u2208 [0, 1) trades-off the importance of immediate and future rewards. For an agent following policy \u03c0, we use the standard definitions of the state-action and state only value functions:\nQ\u03c0(xt, at) = Ext+1:\u221e,at+1:\u221e [Rt|xt, at] and V \u03c0(xt) = Eat [Q\u03c0(xt, at)|xt] . Here, the expectations are with respect to the observed environment states xt and the actions generated by the policy \u03c0, where xt+1:\u221e denotes a state trajectory starting at time t+ 1.\nWe also need to define the advantage function A\u03c0(xt, at) = Q\u03c0(xt, at)\u2212 V \u03c0(xt), which provides a relative measure of value of each action since Eat [A\u03c0(xt, at)] = 0.\nThe parameters \u03b8 of the differentiable policy \u03c0\u03b8(at|xt) can be updated using the discounted approximation to the policy gradient (Sutton et al., 2000), which borrowing notation from Schulman et al. (2015b), is defined as:\ng = Ex0:\u221e,a0:\u221e\n \u2211\nt\u22650\nA\u03c0(xt, at)\u2207\u03b8 log \u03c0\u03b8(at|xt)   . (1)\nFollowing Proposition 1 of Schulman et al. (2015b), we can replaceA\u03c0(xt, at) in the above expression with the state-action value Q\u03c0(xt, at), the discounted return Rt, or the temporal difference residual rt + \u03b3V\n\u03c0(xt+1) \u2212 V \u03c0(xt), without introducing bias. These choices will however have different variance. Moreover, in practice we will approximate these quantities with neural networks thus introducing additional approximation errors and biases. Typically, the policy gradient estimator using Rt will have higher variance and lower bias whereas the estimators using function approximation will have higher bias and lower variance. Combining Rt with the current value function approximation to minimize bias while maintaining bounded variance is one of the central design principles behind ACER.\nTo trade-off bias and variance, the asynchronous advantage actor critic (A3C) of Mnih et al. (2016) uses a single trajectory sample to obtain the following gradient approximation:\ng\u0302a3c = \u2211\nt\u22650\n(( k\u22121\u2211\ni=0\n\u03b3irt+i ) + \u03b3kV \u03c0\u03b8v (xt+k)\u2212 V \u03c0\u03b8v (xt) ) \u2207\u03b8 log \u03c0\u03b8(at|xt). (2)\nA3C combines both k-step returns and function approximation to trade-off variance and bias. We may think of V \u03c0\u03b8v (xt) as a policy gradient baseline used to reduce variance.\nIn the following section, we will introduce the discrete-action version of ACER. ACER may be understood as the off-policy counterpart of the A3C method of Mnih et al. (2016). As such, ACER builds on all the engineering innovations of A3C, including efficient parallel CPU computation.\nACER uses a single deep neural network to estimate the policy \u03c0\u03b8(at|xt) and the value function V \u03c0\u03b8v (xt). (For clarity and generality, we are using two different symbols to denote the parameters of the policy and value function, \u03b8 and \u03b8v , but most of these parameters are shared in the single neural network.) Our neural networks, though building on the networks used in A3C, will introduce several modifications and new modules.\n3 DISCRETE ACTOR CRITIC WITH EXPERIENCE REPLAY\nOff-policy learning with experience replay may appear to be an obvious strategy for improving the sample efficiency of actor-critics. However, controlling the variance and stability of off-policy estimators is notoriously hard. Importance sampling is one of the most popular approaches for offpolicy learning (Meuleau et al., 2000; Jie & Abbeel, 2010; Levine & Koltun, 2013). In our context, it proceeds as follows. Suppose we retrieve a trajectory {x0, a0, r0, \u00b5(\u00b7|x0), \u00b7 \u00b7 \u00b7 , xk, ak, rk, \u00b5(\u00b7|xk)}, where the actions have been sampled according to the behavior policy \u00b5, from our memory of experiences. Then, the importance weighted policy gradient is given by:\ng\u0302imp =\n( k\u220f\nt=0\n\u03c1t\n) k\u2211\nt=0\n( k\u2211\ni=0\n\u03b3irt+i ) \u2207\u03b8 log \u03c0\u03b8(at|xt), (3)\nwhere \u03c1t = \u03c0(at|xt) \u00b5(at|xt) denotes the importance weight. This estimator is unbiased, but it suffers from very high variance as it involves a product of many potentially unbounded importance weights. To prevent the product of importance weights from exploding, Wawrzyn\u0301ski (2009) truncates this product. Truncated importance sampling over entire trajectories, although bounded in variance, could suffer from significant bias.\nRecently, Degris et al. (2012) attacked this problem by using marginal value functions over the limiting distribution of the process to yield the following approximation of the gradient:\ngmarg = Ext\u223c\u03b2,at\u223c\u00b5 [\u03c1t\u2207\u03b8 log \u03c0\u03b8(at|xt)Q\u03c0(xt, at)] , (4) where Ext\u223c\u03b2,at\u223c\u00b5[\u00b7] is the expectation with respect to the limiting distribution \u03b2(x) = limt\u2192\u221e P (xt = x|x0, \u00b5) with behavior policy \u00b5. To keep the notation succinct, we will replace Ext\u223c\u03b2,at\u223c\u00b5[\u00b7] with Extat [\u00b7] and ensure we remind readers of this when necessary. Two important facts about equation (4) must be highlighted. First, note that it depends on Q\u03c0 and not on Q\u00b5, consequently we must be able to estimate Q\u03c0. Second, we no longer have a product of importance weights, but instead only need to estimate the marginal importance weight \u03c1t. Importance sampling in this lower dimensional space (over marginals as opposed to trajectories) is expected to exhibit lower variance.\nDegris et al. (2012) estimateQ\u03c0 in equation (4) using lambda returns: R\u03bbt = rt+(1\u2212\u03bb)\u03b3V (xt+1)+ \u03bb\u03b3\u03c1t+1R \u03bb t+1. This estimator requires that we know how to choose \u03bb ahead of time to trade off bias and variance. Moreover, when using small values of \u03bb to reduce variance, occasional large importance weights can still cause instability.\nIn the following subsection, we adopt the Retrace algorithm of Munos et al. (2016) to estimate Q\u03c0. Subsequently, we propose an importance weight truncation technique to improve the stability of the off-policy actor critic of Degris et al. (2012), and introduce a computationally efficient trust region scheme for policy optimization. The formulation of ACER for continuous action spaces will require further innovations that are advanced in Section 5.\n3.1 MULTI-STEP ESTIMATION OF THE STATE-ACTION VALUE FUNCTION\nIn this paper, we estimate Q\u03c0(xt, at) using Retrace (Munos et al., 2016). (We also experimented with the related tree backup method of Precup et al. (2000) but found Retrace to perform better in practice.) Given a trajectory generated under the behavior policy \u00b5, the Retrace estimator can be expressed recursively as follows1:\nQret(xt, at) = rt + \u03b3\u03c1\u0304t+1[Q ret(xt+1, at+1)\u2212Q(xt+1, at+1)] + \u03b3V (xt+1), (5)\n1For ease of presentation, we consider only \u03bb = 1 for Retrace.\nwhere \u03c1\u0304t is the truncated importance weight, \u03c1\u0304t = min {c, \u03c1t} with \u03c1t = \u03c0(at|xt)\u00b5(at|xt) , Q is the current value estimate of Q\u03c0, and V (x) = Ea\u223c\u03c0Q(x, a). Retrace is an off-policy, return-based algorithm which has low variance and is proven to converge (in the tabular case) to the value function of the target policy for any behavior policy, see Munos et al. (2016).\nThe recursive Retrace equation depends on the estimate Q. To compute it, in discrete action spaces, we adopt a convolutional neural network with \u201ctwo heads\u201d that outputs the estimate Q\u03b8v (xt, at), as well as the policy \u03c0\u03b8(at|xt). This neural representation is the same as in (Mnih et al., 2016), with the exception that we output the vector Q\u03b8v (xt, at) instead of the scalar V\u03b8v (xt). The estimate V\u03b8v (xt) can be easily derived by taking the expectation of Q\u03b8v under \u03c0\u03b8.\nTo approximate the policy gradient gmarg, ACER uses Qret to estimate Q\u03c0. As Retrace uses multistep returns, it can significantly reduce bias in the estimation of the policy gradient 2.\nTo learn the critic Q\u03b8v (xt, at), we again use Q ret(xt, at) as a target in a mean squared error loss and update its parameters \u03b8v with the following standard gradient:\n(Qret(xt, at)\u2212Q\u03b8v (xt, at))\u2207\u03b8vQ\u03b8v (xt, at)). (6) Because Retrace is return-based, it also enables faster learning of the critic. Thus the purpose of the multi-step estimator Qret in our setting is twofold: to reduce bias in the policy gradient, and to enable faster learning of the critic, hence further reducing bias.\n3.2 IMPORTANCE WEIGHT TRUNCATION WITH BIAS CORRECTION\nThe marginal importance weights in Equation (4) can become large, thus causing instability. To safe-guard against high variance, we propose to truncate the importance weights and introduce a correction term via the following decomposition of gmarg:\ngmarg =Extat [\u03c1t\u2207\u03b8log \u03c0\u03b8(at|xt)Q\u03c0(xt, at)]\n=Ext [ Eat[\u03c1\u0304t\u2207\u03b8log \u03c0\u03b8(at|xt)Q\u03c0(xt, at)]+E\na\u223c\u03c0 ([ \u03c1t(a)\u2212 c \u03c1t(a) ]\n+\n\u2207\u03b8log \u03c0\u03b8(a|xt)Q\u03c0(xt, a) )] ,(7)\nwhere \u03c1\u0304t = min {c, \u03c1t} with \u03c1t = \u03c0(at|xt)\u00b5(at|xt) as before. We have also introduced the notation \u03c1t(a) =\n\u03c0(a|xt) \u00b5(a|xt) , and [x]+ = x if x > 0 and it is zero otherwise. We remind readers that the above\nexpectations are with respect to the limiting state distribution under the behavior policy: xt \u223c \u03b2 and at \u223c \u00b5. The clipping of the importance weight in the first term of equation (7) ensures that the variance of the gradient estimate is bounded. The correction term (second term in equation (7)) ensures that our estimate is unbiased. Note that the correction term is only active for actions such that \u03c1t(a) > c. In particular, if we choose a large value for c, the correction term only comes into effect when the variance of the original off-policy estimator of equation (4) is very high. When this happens, our decomposition has the nice property that the truncated weight in the first term is at most c while the correction weight [ \u03c1t(a)\u2212c \u03c1t(a) ] + in the second term is at most 1.\nWe model Q\u03c0(xt, a) in the correction term with our neural network approximation Q\u03b8v (xt, at). This modification results in what we call the truncation with bias correction trick, in this case applied to the function \u2207\u03b8 log \u03c0\u03b8(at|xt)Q\u03c0(xt, at):\ng\u0302marg =Ext [ Eat [ \u03c1\u0304t\u2207\u03b8log \u03c0\u03b8(at|xt)Qret(xt, at) ] +E a\u223c\u03c0 ([ \u03c1t(a)\u2212 c \u03c1t(a) ]\n+\n\u2207\u03b8log \u03c0\u03b8(a|xt)Q\u03b8v (xt, a) )] .(8)\nEquation (8) involves an expectation over the stationary distribution of the Markov process. We can however approximate it by sampling trajectories {x0, a0, r0, \u00b5(\u00b7|x0), \u00b7 \u00b7 \u00b7 , xk, ak, rk, \u00b5(\u00b7|xk)}\n2An alternative to Retrace here is Q(\u03bb) with off-policy corrections (Harutyunyan et al., 2016) which we discuss in more detail in Appendix B.\ngenerated from the behavior policy \u00b5. Here the terms \u00b5(\u00b7|xt) are the policy vectors. Given these trajectories, we can compute the off-policy ACER gradient:\ng\u0302acert = \u03c1\u0304t\u2207\u03b8 log \u03c0\u03b8(at|xt)[Qret(xt, at)\u2212 V\u03b8v (xt)]\n+ E a\u223c\u03c0 ([ \u03c1t(a)\u2212 c \u03c1t(a) ]\n+\n\u2207\u03b8 log \u03c0\u03b8(a|xt)[Q\u03b8v (xt, a)\u2212 V\u03b8v (xt)] ) . (9)\nIn the above expression, we have subtracted the classical baseline V\u03b8v (xt) to reduce variance.\nIt is interesting to note that, when c = \u221e, (9) recovers (off-policy) policy gradient up to the use of Retrace. When c = 0, (9) recovers an actor critic update that depends entirely on Q estimates. In the continuous control domain, (9) also generalizes Stochastic Value Gradients if c = 0 and the reparametrization trick is used to estimate its second term (Heess et al., 2015).\n3.3 EFFICIENT TRUST REGION POLICY OPTIMIZATION\nThe policy updates of actor-critic methods do often exhibit high variance. Hence, to ensure stability, we must limit the per-step changes to the policy. Simply using smaller learning rates is insufficient as they cannot guard against the occasional large updates while maintaining a desired learning speed. Trust Region Policy Optimization (TRPO) (Schulman et al., 2015a) provides a more adequate solution.\nSchulman et al. (2015a) approximately limit the difference between the updated policy and the current policy to ensure safety. Despite the effectiveness of their TRPO method, it requires repeated computation of Fisher-vector products for each update. This can prove to be prohibitively expensive in large domains.\nIn this section we introduce a new trust region policy optimization method that scales well to large problems. Instead of constraining the updated policy to be close to the current policy (as in TRPO), we propose to maintain an average policy network that represents a running average of past policies and forces the updated policy to not deviate far from this average.\nWe decompose our policy network in two parts: a distribution f , and a deep neural network that generates the statistics \u03c6\u03b8(x) of this distribution. That is, given f , the policy is completely characterized by the network \u03c6\u03b8: \u03c0(\u00b7|x) = f(\u00b7|\u03c6\u03b8(x)). For example, in the discrete domain, we choose f to be the categorical distribution with a probability vector \u03c6\u03b8(x) as its statistics. The probability vector is of course parameterised by \u03b8.\nWe denote the average policy network as \u03c6\u03b8a and update its parameters \u03b8a \u201csoftly\u201d after each update to the policy parameter \u03b8: \u03b8a \u2190 \u03b1\u03b8a + (1\u2212 \u03b1)\u03b8. Consider, for example, the ACER policy gradient as defined in Equation (9), but with respect to \u03c6:\ng\u0302acert = \u03c1\u0304t\u2207\u03c6\u03b8(xt) log f(at|\u03c6\u03b8(x))[Qret(xt, at)\u2212 V\u03b8v (xt)]\n+ E a\u223c\u03c0 ([ \u03c1t(a)\u2212 c \u03c1t(a) ]\n+\n\u2207\u03c6\u03b8(xt) log f(at|\u03c6\u03b8(x))[Q\u03b8v (xt, a)\u2212 V\u03b8v (xt)] ) . (10)\nGiven the averaged policy network, our proposed trust region update involves two stages. In the first stage, we solve the following optimization problem with a linearized KL divergence constraint:\nminimize z\n1 2 \u2016g\u0302acert \u2212 z\u201622\nsubject to \u2207\u03c6\u03b8(xt)DKL [f(\u00b7|\u03c6\u03b8a(xt))\u2016f(\u00b7|\u03c6\u03b8(xt))] T z \u2264 \u03b4\n(11)\nSince the constraint is linear, the overall optimization problem reduces to a simple quadratic programming problem, the solution of which can be easily derived in closed form using the KKT conditions. Letting k = \u2207\u03c6\u03b8(xt)DKL [f(\u00b7|\u03c6\u03b8a(xt)\u2016f(\u00b7|\u03c6\u03b8(xt)], the solution is:\nz\u2217 = g\u0302acert \u2212max {\n0, kT g\u0302acert \u2212 \u03b4 \u2016k\u201622\n} k (12)\nThis transformation of the gradient has a very natural form. If the constraint is satisfied, there is no change to the gradient with respect to \u03c6\u03b8(xt). Otherwise, the update is scaled down in the direction\nof k, thus effectively lowering rate of change between the activations of the current policy and the average policy network.\nIn the second stage, we take advantage of back-propagation. Specifically, the updated gradient with respect to \u03c6\u03b8, that is z\u2217, is back-propagated through the network to compute the derivatives with respect to the parameters. The parameter updates for the policy network follow from the chain rule: \u2202\u03c6\u03b8(x) \u2202\u03b8 z \u2217.\nThe trust region step is carried out in the space of the statistics of the distribution f , and not in the space of the policy parameters. This is done deliberately so as to avoid an additional back-propagation step through the policy network.\nWe would like to remark that the algorithm advanced in this section can be thought of as a general strategy for modifying the backward messages in back-propagation so as to stabilize the activations.\nInstead of a trust region update, one could alternatively add an appropriately scaled KL cost to the objective function as proposed by Heess et al. (2015). This approach, however, is less robust to the choice of hyper-parameters in our experience.\nThe ACER algorithm results from a combination of the above ideas, with the precise pseudo-code appearing in Appendix A. A master algorithm (Algorithm 1) calls ACER on-policy to perform updates and propose trajectories. It then calls ACER off-policy component to conduct several replay steps. When on-policy, ACER effectively becomes a modified version of A3C where Q instead of V baselines are employed and trust region optimization is used.\n4 RESULTS ON ATARI\nWe use the Arcade Learning Environment of Bellemare et al. (2013) to conduct an extensive evaluation. We deploy one single algorithm and network architecture, with fixed hyper-parameters, to learn to play 57 Atari games given only raw pixel observations and game rewards. This task is highly demanding because of the diversity of games, and high-dimensional pixel-level observations.\nOur experimental setup uses 16 actor-learner threads running on a single machine with no GPUs. We adopt the same input pre-processing and network architecture as Mnih et al. (2015). Specifically, the network consists of a convolutional layer with 32 8\u00d7 8 filters with stride 4 followed by another convolutional layer with 64 4\u00d7 4 filters with stride 2, followed by a final convolutional layer with 64 3\u00d7 3 filters with stride 1, followed by a fully-connected layer of size 512. Each of the hidden layers is followed by a rectifier nonlinearity. The network outputs a softmax policy and Q values.\nWhen using replay, we add to each thread a replay memory that is up to 50 000 frames in size. The total amount of memory used across all threads is thus similar in size to that of DQN (Mnih et al., 2015). For all Atari experiments, we use a single learning rate adopted from an earlier implementation of A3C without further tuning. We do not anneal the learning rates over the course of training as in Mnih et al. (2016). We otherwise adopt the same optimization procedure as in Mnih et al. (2016). Specifically, we adopt entropy regularization with weight 0.001, discount the rewards with \u03b3 = 0.99, and perform updates every 20 steps (k = 20 in the notation of Section 2). In all our experiments with experience replay, we use importance weight truncation with c = 10. We consider training ACER both with and without trust region updating as described in Section 3.3. When trust region updating is used, we use \u03b4 = 1 and \u03b1 = 0.99 for all experiments.\nTo compare different agents, we adopt as our metric the median of the human normalized score over all 57 games. The normalization is calculated such that, for each game, human scores and random scores are evaluated to 1, and 0 respectively. The normalized score for a given game at time t is computed as the average normalized score over the past 1 million consecutive frames encountered until time t. For each agent, we plot its cumulative maximum median score over time. The result is summarized in Figure 1.\nThe four colors in Figure 1 correspond to four replay ratios (0, 1, 4 and 8) with a ratio of 4 meaning that we use the off-policy component of ACER 4 times after using the on-policy component (A3C). That is, a replay ratio of 0 means that we are using A3C. The solid and dashed lines represent ACER with and without trust region updating respectively. The gray and black curves are the original DQN (Mnih et al., 2015) and Prioritized Replay agent of Schaul et al. (2016) agents respectively.\nAs shown on the left panel of Figure 1, replay significantly increases data efficiency. We observe that when using the trust region optimizer, the average reward as a function of the number of environmental steps increases with the ratio of replay. This increase has diminishing returns, but with enough replay, ACER can match the performance of the best DQN agents. Moreover, it is clear that the off-policy actor critics (ACER) are much more sample efficient than their on-policy counterpart (A3C).\nThe right panel of Figure 1 shows that ACER agents perform similarly to A3C when measured by wall clock time. Thus, in this case, it is possible to achieve better data-efficiency without necessarily compromising on computation time. In particular, ACER with a replay ratio of 4 is an appealing alternative to either the prioritized DQN agent or A3C.\n5 CONTINUOUS ACTOR CRITIC WITH EXPERIENCE REPLAY\nRetrace requires estimates of both Q and V , but we cannot easily integrate over Q to derive V in continuous action spaces. In this section, we propose a solution to this problem in the form of a novel representation for RL, as well as modifications necessary for trust region updating.\n5.1 POLICY EVALUATION\nRetrace provides a target for learning Q\u03b8v , but not for learning V\u03b8v . We could use importance sampling to compute V\u03b8v given Q\u03b8v , but this estimator has high variance.\nWe propose a new architecture which we call Stochastic Dueling Networks (SDNs), inspired by the Dueling networks of Wang et al. (2016), which is designed to estimate both V \u03c0 and Q\u03c0 off-policy while maintaining consistency between the two estimates. At each time step, an SDN outputs a stochastic estimate Q\u0303\u03b8v of Q \u03c0 and a deterministic estimate V\u03b8v of V \u03c0 , such that\nQ\u0303\u03b8v (xt, at) \u223c V\u03b8v (xt) +A\u03b8v (xt, at)\u2212 1\nn\nn\u2211\ni=1\nA\u03b8v (xt, ui), and ui \u223c \u03c0\u03b8(\u00b7|xt) (13)\nwhere n is a parameter, see Figure 2. The two estimates are consistent in the sense that Ea\u223c\u03c0(\u00b7|xt) [ Eu1:n\u223c\u03c0(\u00b7|xt) ( Q\u0303\u03b8v (xt, a) )] = V\u03b8v (xt). Furthermore, we can learn about V \u03c0 by learn-\ning Q\u0303\u03b8v . To see this, assume we have learned Q \u03c0 perfectly such that Eu1:n\u223c\u03c0(\u00b7|xt) ( Q\u0303\u03b8v (xt, at) ) = Q\u03c0(xt, at), then V\u03b8v (xt) = Ea\u223c\u03c0(\u00b7|xt) [ Eu1:n\u223c\u03c0(\u00b7|xt) ( Q\u0303\u03b8v (xt, a) )] = Ea\u223c\u03c0(\u00b7|xt) [Q\u03c0(xt, a)] =\nV \u03c0(xt). Therefore, a target on Q\u0303\u03b8v (xt, at) also provides an error signal for updating V\u03b8v .\nIn addition to SDNs, however, we also construct the following novel target for estimating V \u03c0:\nV target(xt) = min { 1, \u03c0(at|xt) \u00b5(at|xt) }( Qret(xt, at)\u2212Q\u03b8v (xt, at) ) + V\u03b8v (xt). (14)\nThe above target is also derived via the truncation and bias correction trick; for more details, see Appendix D.\nFinally, when estimating Qret in continuous domains, we implement a slightly different formulation of the truncated importance weights \u03c1\u0304t = min { 1, ( \u03c0(at|xt) \u00b5(at|xt) ) 1 d } , where d is the dimensionality of\nthe action space. Although not essential, we have found this formulation to lead to faster learning.\n5.2 TRUST REGION UPDATING\nTo adopt the trust region updating scheme (Section 3.3) in the continuous control domain, one simply has to choose a distribution f and a gradient specification g\u0302acert suitable for continuous action spaces.\nFor the distribution f , we choose Gaussian distributions with fixed diagonal covariance and mean \u03c6\u03b8(x).\nTo derive g\u0302acert in continuous action spaces, consider the ACER policy gradient for the stochastic dueling network, but with respect to \u03c6:\ngacert = Ext [ Eat [ \u03c1\u0304t\u2207\u03c6\u03b8(xt) log f(at|\u03c6\u03b8(xt))(Qopc(xt, at)\u2212 V\u03b8v (xt)) ]\n+ E a\u223c\u03c0 ([ \u03c1t(a)\u2212 c \u03c1t(a) ]\n+\n(Q\u0303\u03b8v (xt, a)\u2212 V\u03b8v (xt))\u2207\u03c6\u03b8(xt) log f(a|\u03c6\u03b8(xt)) )] . (15)\nIn the above definition, we are using Qopc instead of Qret. Here, Qopc(xt, at) is the same as Retrace with the exception that the truncated importance ratio is replaced with 1 (Harutyunyan et al., 2016). Please refer to Appendix B an expanded discussion on this design choice. Given an observation xt, we can sample a\u2032t \u223c \u03c0\u03b8(\u00b7|xt) to obtain the following Monte Carlo approximation\ng\u0302acert = \u03c1\u0304t\u2207\u03c6\u03b8(xt) log f(at|\u03c6\u03b8(xt))(Qopc(xt, at)\u2212 V\u03b8v (xt))\n+\n[ \u03c1t(a \u2032 t)\u2212 c\n\u03c1t(a\u2032t)\n]\n+\n(Q\u0303\u03b8v (xt, a \u2032 t)\u2212 V\u03b8v (xt))\u2207\u03c6\u03b8(xt) log f(a\u2032t|\u03c6\u03b8(xt)). (16)\nGiven f and g\u0302acert , we apply the same steps as detailed in Section 3.3 to complete the update.\nThe precise pseudo-code of ACER algorithm for continuous spaces results is presented in Appendix A.\n6 RESULTS ON MUJOCO\nWe evaluate our algorithms on 6 continuous control tasks, all of which are simulated using the MuJoCo physics engine (Todorov et al., 2012). For descriptions of the tasks, please refer to Appendix E.1. Briefly, the tasks with action dimensionality in brackets are: cartpole (1D), reacher (3D), cheetah (6D), fish (5D), walker (6D) and humanoid (21D). These tasks are illustrated in Figure 3.\nTo benchmark ACER for continuous control, we compare it to its on-policy counterpart both with and without trust region updating. We refer to these two baselines as A3C and Trust-A3C. Additionally, we also compare to a baseline with replay where we truncate the importance weights over trajectories as in (Wawrzyn\u0301ski, 2009). For a detailed description of this baseline, please refer to Appendix E. Again, we run this baseline both with and without trust region updating, and refer to these choices as Trust-TIS and TIS respectively. Last but not least, we refer to our proposed approach with SDN and trust region updating as simply ACER. All five setups are implemented in the asynchronous A3C framework.\nAll the aforementioned setups share the same network architecture that computes the policy and state values. We maintain an additional small network that computes the stochastic A values in the case of ACER. We use n = 5 (using the notation in Equation (13)) in all SDNs. Instead of mixing on-policy and replay learning as done in the Atari domain, ACER for continuous actions is entirely off-policy, with experiences generated from the simulator (4 times on average). When using replay, we add to each thread a replay memory that is 5, 000 frames in size and perform updates every 50 steps (k = 50 in the notation of Section 2). The rate of the soft updating (\u03b1 as in Section 3.3) is set to 0.995 in all setups involving trust region updating. The truncation threshold c is set to 5 for ACER.\nWe use diagonal Gaussian policies with fixed diagonal covariances where the diagonal standard deviation is set to 0.3. For all setups, we sample the learning rates log-uniformly in the range [10\u22124, 10\u22123.3]. For setups involving trust region updating, we also sample \u03b4 uniformly in the range [0.1, 2]. With all setups, we use 30 sampled hyper-parameter settings.\nThe empirical results for all continuous control tasks are shown Figure 3, where we show the mean and standard deviation of the best 5 out of 30 hyper-parameter settings over which we searched 3. For sensitivity analyses with respect to the hyper-parameters, please refer to Figures 5 and 6 in the Appendix.\nIn continuous control, ACER outperforms the A3C and truncated importance sampling baselines by a very significant margin.\nHere, we also find that the proposed trust region optimization method can result in huge improvements over the baselines. The high-dimensional continuous action policies are much harder to optimize than the small discrete action policies in Atari, and hence we observe much higher gains for trust region optimization in the continuous control domains. In spite of the improvements brought in by trust region optimization, ACER still outperforms all other methods, specially in higher dimensions.\n6.1 ABLATIONS\nTo further tease apart the contributions of the different components of ACER, we conduct an ablation analysis where we individually remove Retrace / Q(\u03bb) off-policy correction, SDNs, trust region, and truncation with bias correction from the algorithm. As shown in Figure 4, Retrace and offpolicy correction, SDNs, and trust region are critical: removing any one of them leads to a clear deterioration of the performance. Truncation with bias correction did not alter the results in the Fish and Walker2d tasks. However, in Humanoid, where the dimensionality of the action space is much higher, including truncation and bias correction brings a significant boost which makes the originally kneeling humanoid stand. Presumably, the high dimensionality of the action space increases the variance of the importance weights which makes truncation with bias correction important. For more details on the experimental setup please see Appendix E.4.\n7 THEORETICAL ANALYSIS\nRetrace is a very recent development in reinforcement learning. In fact, this work is the first to consider Retrace in the policy gradients setting. For this reason, and given the core role that Retrace plays in ACER, it is valuable to shed more light on this technique. In this section, we will prove that Retrace can be interpreted as an application of the importance weight truncation and bias correction trick advanced in this paper.\nConsider the following equation:\nQ\u03c0(xt, at) = Ext+1at+1 [rt + \u03b3\u03c1t+1Q\u03c0(xt+1, at+1)] . (17)\nIf we apply the weight truncation and bias correction trick to the above equation we obtain\nQ\u03c0(xt, at) = Ext+1at+1 [ rt + \u03b3\u03c1\u0304t+1Q\n\u03c0(xt+1, at+1) + \u03b3 E a\u223c\u03c0 ([ \u03c1t+1(a)\u2212 c \u03c1t+1(a) ]\n+\nQ\u03c0(xt+1, a) )] .\n(18) By recursively expanding Q\u03c0 as in Equation (18), we can represent Q\u03c0(x, a) as:\nQ\u03c0(x, a) = E\u00b5\n \u2211\nt\u22650\n\u03b3t\n( t\u220f\ni=1\n\u03c1\u0304i )( rt + \u03b3 E\nb\u223c\u03c0 ([ \u03c1t+1(b)\u2212 c \u03c1t+1(b) ]\n+\nQ\u03c0(xt+1, b) ))  . (19)\nThe expectation E\u00b5 is taken over trajectories starting from x with actions generated with respect to \u00b5. When Q\u03c0 is not available, we can replace it with our current estimate Q to get a return-based\n3 For videos of the policies learned with ACER, please see: https://www.youtube.com/watch?v= NmbeQYoVv5g&list=PLkmHIkhlFjiTlvwxEnsJMs3v7seR5HSP-.\nesitmate of Q\u03c0 . This operation also defines an operator:\nBQ(x, a) = E\u00b5\n \u2211\nt\u22650\n\u03b3t\n( t\u220f\ni=1\n\u03c1\u0304i )( rt + \u03b3 E\nb\u223c\u03c0 ([ \u03c1t+1(b)\u2212 c \u03c1t+1(b) ]\n+\nQ(xt+1, b)\n))  . (20)\nIn the following proposition, we show that B is a contraction operator with a unique fixed point Q\u03c0 and that it is equivalent to the Retrace operator. Proposition 1. The operator B is a contraction operator such that \u2016BQ\u2212Q\u03c0\u2016\u221e \u2264 \u03b3\u2016Q\u2212Q\u03c0\u2016\u221e and B is equivalent to Retrace.\nThe above proposition not only shows an alternative way of arriving at the same operator, but also provides a different proof of contraction for Retrace. Please refer to Appendix C for the regularization conditions and proof of the above proposition.\nFinally, B, and therefore Retrace, generalizes both the Bellman operator T \u03c0 and importance sampling. Specifically, when c = 0, B = T \u03c0 and when c = \u221e, B recovers importance sampling; see Appendix C.\n8 CONCLUDING REMARKS\nWe have introduced a stable off-policy actor critic that scales to both continuous and discrete action spaces. This approach integrates several recent advances in RL in a principle manner. In addition, it integrates three innovations advanced in this paper: truncated importance sampling with bias correction, stochastic dueling networks and an efficient trust region policy optimization method.\nWe showed that the method not only matches the performance of the best known methods on Atari, but that it also outperforms popular techniques on several continuous control problems.\nThe efficient trust region optimization method advanced in this paper performs remarkably well in continuous domains. It could prove very useful in other deep learning domains, where it is hard to stabilize the training process.\nACKNOWLEDGMENTS\nWe are very thankful to Marc Bellemare, Jascha Sohl-Dickstein, and Se\u0301bastien Racaniere for proofreading and valuable suggestions.\nA ACER PSEUDO-CODE FOR DISCRETE ACTIONS\nAlgorithm 1 ACER for discrete actions (master algorithm) // Assume global shared parameter vectors \u03b8 and \u03b8v . // Assume ratio of replay r. repeat\nCall ACER on-policy, Algorithm 2. n\u2190 Possion(r) for i \u2208 {1, \u00b7 \u00b7 \u00b7 , n} do\nCall ACER off-policy, Algorithm 2. end for\nuntil Max iteration or time reached.\nAlgorithm 2 ACER for discrete actions Reset gradients d\u03b8 \u2190 0 and d\u03b8v \u2190 0. Initialize parameters \u03b8\u2032 \u2190 \u03b8 and \u03b8\u2032v \u2190 \u03b8v . if not On-Policy then\nSample the trajectory {x0, a0, r0, \u00b5(\u00b7|x0), \u00b7 \u00b7 \u00b7 , xk, ak, rk, \u00b5(\u00b7|xk)} from the replay memory. else\nGet state x0 end if for i \u2208 {0, \u00b7 \u00b7 \u00b7 , k} do\nCompute f(\u00b7|\u03c6\u03b8\u2032(xi)), Q\u03b8\u2032v (xi, \u00b7) and f(\u00b7|\u03c6\u03b8a(xi)). if On-Policy then\nPerform ai according to f(\u00b7|\u03c6\u03b8\u2032(xi)) Receive reward ri and new state xi+1 \u00b5(\u00b7|xi)\u2190 f(\u00b7|\u03c6\u03b8\u2032(xi))\nend if \u03c1\u0304i \u2190 min { 1,\nf(ai|\u03c6\u03b8\u2032 (xi)) \u00b5(ai|xi)\n} .\nend for\nQret \u2190 { 0 for terminal xk\u2211 aQ\u03b8\u2032v (xk, a)f(a|\u03c6\u03b8\u2032(xk)) otherwise for i \u2208 {k \u2212 1, \u00b7 \u00b7 \u00b7 , 0} do Qret \u2190 ri + \u03b3Qret Vi \u2190 \u2211 aQ\u03b8\u2032v (xi, a)f(a|\u03c6\u03b8\u2032(xi))\nComputing quantities needed for trust region updating:\ng \u2190 min {c, \u03c1i(ai)}\u2207\u03c6\u03b8\u2032 (xi) log f(ai|\u03c6\u03b8\u2032(xi))(Q ret \u2212 Vi)\n+ \u2211 a [ 1\u2212 c \u03c1i(a) ] + f(a|\u03c6\u03b8\u2032(xi))\u2207\u03c6\u03b8\u2032 (xi) log f(a|\u03c6\u03b8\u2032(xi))(Q\u03b8\u2032v (xi, ai)\u2212 Vi)\nk \u2190 \u2207\u03c6\u03b8\u2032 (xi)DKL [f(\u00b7|\u03c6\u03b8a(xi)\u2016f(\u00b7|\u03c6\u03b8\u2032(xi)]\nAccumulate gradients wrt \u03b8\u2032: d\u03b8\u2032 \u2190 d\u03b8\u2032 + \u2202\u03c6\u03b8\u2032 (xi) \u2202\u03b8\u2032\n( g \u2212max { 0, k\nT g\u2212\u03b4 \u2016k\u201622\n} k )\nAccumulate gradients wrt \u03b8\u2032v: d\u03b8v \u2190 d\u03b8v +\u2207\u03b8\u2032v (Q ret \u2212Q\u03b8\u2032v (xi, a)) 2 Update Retrace target: Qret \u2190 \u03c1\u0304i ( Qret \u2212Q\u03b8\u2032v (xi, ai) ) + Vi\nend for Perform asynchronous update of \u03b8 using d\u03b8 and of \u03b8v using d\u03b8v . Updating the average policy network: \u03b8a \u2190 \u03b1\u03b8a + (1\u2212 \u03b1)\u03b8\nB Q(\u03bb) WITH OFF-POLICY CORRECTIONS\nGiven a trajectory generated under the behavior policy \u00b5, the Q(\u03bb) with off-policy corrections estimator (Harutyunyan et al., 2016) can be expressed recursively as follows:\nQopc(xt, at) = rt + \u03b3[Q opc(xt+1, at+1)\u2212Q(xt+1, at+1)] + \u03b3V (xt+1). (21)\nNotice that Qopc(xt, at) is the same as Retrace with the exception that the truncated importance ratio is replaced with 1.\nAlgorithm 3 ACER for Continuous Actions Reset gradients d\u03b8 \u2190 0 and d\u03b8v \u2190 0. Initialize parameters \u03b8\u2032 \u2190 \u03b8 and \u03b8\u2032v \u2190 \u03b8v . Sample the trajectory {x0, a0, r0, \u00b5(\u00b7|x0), \u00b7 \u00b7 \u00b7 , xk, ak, rk, \u00b5(\u00b7|xk)} from the replay memory. for i \u2208 {0, \u00b7 \u00b7 \u00b7 , k} do\nCompute f(\u00b7|\u03c6\u03b8\u2032(xi)), V\u03b8\u2032v (xi), Q\u0303\u03b8\u2032v (xi, ai), and f(\u00b7|\u03c6\u03b8a(xi)). Sample a\u2032i \u223c f(\u00b7|\u03c6\u03b8\u2032(xi)) \u03c1i \u2190 f(ai|\u03c6\u03b8\u2032 (xi))\u00b5(ai|xi) and \u03c1 \u2032 i \u2190 f(a\u2032i|\u03c6\u03b8\u2032 (xi)) \u00b5(a\u2032i|xi)\nci \u2190 min { 1, (\u03c1i) 1 d } .\nend for\nQret \u2190 { 0 for terminal xk V\u03b8\u2032v (xk) otherwise Qopc \u2190 Qret for i \u2208 {k \u2212 1, \u00b7 \u00b7 \u00b7 , 0} do Qret \u2190 ri + \u03b3Qret Qopc \u2190 ri + \u03b3Qopc Computing quantities needed for trust region updating:\ng \u2190 min {c, \u03c1i}\u2207\u03c6\u03b8\u2032 (xi) log f(ai|\u03c6\u03b8\u2032(xi)) ( Qopc(xi, ai)\u2212 V\u03b8\u2032v (xi) ) + [ 1\u2212 c\n\u03c1\u2032i ] + (Q\u0303\u03b8\u2032v (xi, a \u2032 i)\u2212 V\u03b8\u2032v (xi))\u2207\u03c6\u03b8\u2032 (xi) log f(a \u2032 i|\u03c6\u03b8\u2032(xi))\nk \u2190 \u2207\u03c6\u03b8\u2032 (xi)DKL [f(\u00b7|\u03c6\u03b8a(xi)\u2016f(\u00b7|\u03c6\u03b8\u2032(xi)]\nAccumulate gradients wrt \u03b8: d\u03b8 \u2190 d\u03b8 + \u2202\u03c6\u03b8\u2032 (xi) \u2202\u03b8\u2032\n( g \u2212max { 0, k\nT g\u2212\u03b4 \u2016k\u201622\n} k )\nAccumulate gradients wrt \u03b8\u2032v: d\u03b8v \u2190 d\u03b8v + (Qret \u2212 Q\u0303\u03b8\u2032v (xi, ai))\u2207\u03b8\u2032v Q\u0303\u03b8\u2032v (xi, ai) d\u03b8v \u2190 d\u03b8v + min {1, \u03c1i} ( Qret(xt, ai)\u2212 Q\u0303\u03b8\u2032v (xt, ai) ) \u2207\u03b8\u2032vV\u03b8\u2032v (xi)\nUpdate Retrace target: Qret \u2190 ci ( Qret \u2212 Q\u0303\u03b8\u2032v (xi, ai) ) + V\u03b8\u2032v (xi)\nUpdate Retrace target: Qopc \u2190 ( Qopc \u2212 Q\u0303\u03b8\u2032v (xi, ai) ) + V\u03b8\u2032v (xi)\nend for Perform asynchronous update of \u03b8 using d\u03b8 and of \u03b8v using d\u03b8v . Updating the average policy network: \u03b8a \u2190 \u03b1\u03b8a + (1\u2212 \u03b1)\u03b8\nBecause of the lack of the truncated importance ratio, the operator defined by Qopc is only a contraction if the target and behavior policies are close to each other (Harutyunyan et al., 2016). Q(\u03bb) with off-policy corrections is therefore less stable compared to Retrace and unsafe for policy evaluation.\nQopc, however, could better utilize the returns as the traces are not cut by the truncated importance weights. As a result,Qopc could be used efficiently to estimateQ\u03c0 in policy gradient (e.g. in Equation (16)). In our continuous control experiments, we have found that Qopc leads to faster learning.\nC RETRACE AS TRUNCATED IMPORTANCE SAMPLING WITH BIAS CORRECTION\nFor the purpose of proving proposition 1, we assume our environment to be a Markov Decision Process (X ,A, \u03b3, P, r). We restrict X to be a finite state space. For notational simplicity, we also restrict A to be a finite action space. P : X ,A \u2192 X defines the state transition probabilities and r : X ,A \u2192 [\u2212RMAX, RMAX] defines a reward function. Finally, \u03b3 \u2208 [0, 1) is the discount factor.\nProof of proposition 1. First we show that B is a contraction operator. |BQ(x, a)\u2212Q\u03c0(x, a)|\n= \u2223\u2223\u2223\u2223\u2223\u2223 E\u00b5  \u2211\nt\u22650\n\u03b3t\n( t\u220f\ni=1\n\u03c1\u0304i )( \u03b3 E b\u223c\u03c0 ([ \u03c1t+1(b)\u2212 c \u03c1t+1(b) ]\n+ (Q(xt+1, b)\u2212Q\u03c0(xt+1, b)) ))  \u2223\u2223\u2223\u2223\u2223\u2223\n\u2264 E\u00b5\n \u2211\nt\u22650\n\u03b3t\n( t\u220f\ni=1\n\u03c1\u0304i )[ \u03b3 E b\u223c\u03c0 ([ \u03c1t+1(b)\u2212 c \u03c1t+1(b) ]\n+\n|Q(xt+1, b)\u2212Q\u03c0(xt+1, b)| )] \n\u2264 E\u00b5\n \u2211\nt\u22650\n\u03b3t\n( t\u220f\ni=1\n\u03c1\u0304i )( \u03b3(1\u2212 P\u0304t+1) sup\nb |Q(xt+1, b)\u2212Q\u03c0(xt+1, b)|\n)  (22)\nWhere P\u0304t+1 = 1\u2212 E b\u223c\u03c0 [[ \u03c1t+1(b)\u2212c \u03c1t+1(b) ] + ] = E b\u223c\u00b5 [\u03c1\u0304t+1(b)]. The last inequality in the above equation is due to Ho\u0308lder\u2019s inequality.\n(22) \u2264 sup x,b |Q(x, b)\u2212Q\u03c0(x, b)|E\u00b5\n \u2211\nt\u22650\n\u03b3t\n( t\u220f\ni=1\n\u03c1\u0304i ) ( \u03b3(1\u2212 P\u0304t+1) )  \n= sup x,b |Q(x, b)\u2212Q\u03c0(x, b)|E\u00b5\n \u03b3 \u2211\nt\u22650\n\u03b3t\n( t\u220f\ni=1\n\u03c1\u0304i\n) \u2212 \u2211\nt\u22650\n\u03b3t\n( t\u220f\ni=1\n\u03c1\u0304i ) ( \u03b3P\u0304t+1 )  \n= sup x,b |Q(x, b)\u2212Q\u03c0(x, b)|E\u00b5\n \u03b3 \u2211\nt\u22650\n\u03b3t\n( t\u220f\ni=1\n\u03c1\u0304i\n) \u2212 \u2211\nt\u22650\n\u03b3t+1\n( t+1\u220f\ni=1\n\u03c1\u0304i\n) \n= sup x,b |Q(x, b)\u2212Q\u03c0(x, b)| (\u03b3C \u2212 (C \u2212 1))\nwhereC = \u2211 t\u22650 \u03b3\nt (\u220ft\ni=1 \u03c1\u0304i ) . SinceC \u2265\u22110t=0 \u03b3t (\u220ft i=1 \u03c1\u0304i ) = 1, we have that \u03b3C\u2212(C\u22121) \u2264\n\u03b3. Therefore, we have shown that B is a contraction operator. Now we show that B is the same as Retrace. By apply the trunction and bias correction trick, we have\nE b\u223c\u03c0 [Q(xt+1, b)] = E b\u223c\u00b5 [\u03c1\u0304t+1(b)Q(xt+1, b)] + E b\u223c\u03c0 ([ \u03c1t+1(b)\u2212 c \u03c1t+1(b) ]\n+\nQ(xt+1, b)\n) . (23)\nBy adding and subtracting the two sides of Equation (23) inside the summand of Equation (20), we have BQ(x, a) = E\u00b5 [\u2211\nt\u22650\n\u03b3t ( t\u220f\ni=1\n\u03c1\u0304i )[ rt + \u03b3 E\nb\u223c\u03c0 ([ \u03c1t+1(b)\u2212 c \u03c1t+1(b) ]\n+\nQ(xt+1, b)\n) +\u03b3 E\nb\u223c\u03c0 [Q(xt+1, b)]\n\u2212\u03b3 E b\u223c\u00b5 [\u03c1\u0304t+1(b)Q(xt+1, b)]\u2212 \u03b3 E b\u223c\u03c0 ([ \u03c1t+1(b)\u2212 c \u03c1t+1(b) ]\n+\nQ(xt+1, b)\n)]]\n= E\u00b5\n \u2211\nt\u22650\n\u03b3t\n( t\u220f\ni=1\n\u03c1\u0304i )( rt + \u03b3 E\nb\u223c\u03c0 [Q(xt+1, b)]\u2212 \u03b3 E b\u223c\u00b5 [\u03c1\u0304t+1(b)Q(xt+1, b)]\n) \n= E\u00b5\n \u2211\nt\u22650\n\u03b3t\n( t\u220f\ni=1\n\u03c1\u0304i )( rt + \u03b3 E\nb\u223c\u03c0 [Q(xt+1, b)]\u2212 \u03b3\u03c1\u0304t+1Q(xt+1, at+1)\n) \n= E\u00b5\n \u2211\nt\u22650\n\u03b3t\n( t\u220f\ni=1\n\u03c1\u0304i )( rt + \u03b3 E\nb\u223c\u03c0 [Q(xt+1, b)]\u2212Q(xt, at)\n) +Q(x, a) = RQ(x, a)\nIn the remainder of this appendix, we show that B generalizes both the Bellman operator and importance sampling. First, we reproduce the definition of B:\nBQ(x, a) = E\u00b5\n \u2211\nt\u22650\n\u03b3t\n( t\u220f\ni=1\n\u03c1\u0304i )( rt + \u03b3 E\nb\u223c\u03c0 ([ \u03c1t+1(b)\u2212 c \u03c1t+1(b) ]\n+\nQ(xt+1, b)\n))  .\nWhen c = 0, we have that \u03c1\u0304i = 0 \u2200i. Therefore only the first summand of the sum remains:\nBQ(x, a) = E\u00b5 [ rt + \u03b3 E\nb\u223c\u03c0 (Q(xt+1, b))\n] .\nIn this case B = T . When c =\u221e, the compensation term disappears and \u03c1\u0304i = \u03c1i \u2200i:\nBQ(x, a) = E\u00b5\n \u2211\nt\u22650\n\u03b3t\n( t\u220f\ni=1\n\u03c1i )( rt + \u03b3 E\nb\u223c\u03c0 (0\u00d7Q(xt+1, b))\n)  = E\u00b5  \u2211\nt\u22650\n\u03b3t\n( t\u220f\ni=1\n\u03c1i ) rt   .\nIn this case B is the same operator defined by importance sampling.\nD DERIVATION OF V target\nBy using the truncation and bias correction trick, we can derive the following:\nV \u03c0(xt) = E a\u223c\u00b5\n[ min { 1, \u03c0(a|xt) \u00b5(a|xt) } Q\u03c0(xt, a) ] + E a\u223c\u03c0 ([ \u03c1t(a)\u2212 1 \u03c1t(a) ]\n+\nQ\u03c0(xt+1, a) ) .\nWe, however, cannot use the above equation as a target as we do not have access to Q\u03c0 . To derive a target, we can take a Monte Carlo approximation of the first expectation in the RHS of the above equation and replace the first occurrence of Q\u03c0 with Qret and the second with our current neural net approximation Q\u03b8v (xt, \u00b7):\nV targetpre (xt) := min { 1, \u03c0(at|xt) \u00b5(at|xt) } Qret(xt, at) + E a\u223c\u03c0 ([ \u03c1t(a)\u2212 1 \u03c1t(a) ]\n+\nQ\u03b8v (xt, a)\n) . (24)\nThrough the truncation and bias correction trick again, we have the following identity:\nE a\u223c\u03c0 [Q\u03b8v (xt, a)] = E a\u223c\u00b5\n[ min { 1, \u03c0(a|xt) \u00b5(a|xt) } Q\u03b8v (xt, a) ] + E a\u223c\u03c0 ([ \u03c1t(a)\u2212 1 \u03c1t(a) ]\n+\nQ\u03b8v (xt, a)\n) . (25)\nAdding and subtracting both sides of Equation (25) to the RHS of (24) while taking a Monte Carlo approximation, we arrive at V target(xt):\nV target(xt) := min { 1, \u03c0(at|xt) \u00b5(at|xt) }( Qret(xt, at)\u2212Q\u03b8v (xt, at) ) + V\u03b8v (xt).\nE CONTINUOUS CONTROL EXPERIMENTS\nE.1 DESCRIPTION OF THE CONTINUOUS CONTROL PROBLEMS\nOur continuous control tasks were simulated using the MuJoCo physics engine (Todorov et al. (2012)). For all experiments we considered an episodic setup with an episode length of T = 500 steps and a discount factor of 0.99.\nCartpole swingup This is an instance of the classic cart-pole swing-up task. It consists of a pole attached to a cart running on a finite track. The agent is required to balance the pole near the center of the track by applying a force to the cart only. An episode starts with the pole at a random angle and zero velocity. A reward zero is given except when the pole is approximately upright (within \u00b15 deg) and the track approximately in the center of the track (\u00b10.05) for a track length of 2.4. The observations include position and velocity of the cart, angle and angular velocity of the pole. a sine/cosine of the angle, the position of the tip of the pole, and Cartesian velocities of the pole. The dimension of the action space is 1.\nReacher3 The agent needs to control a planar 3-link robotic arm in order to minimize the distance between the end effector of the arm and a target. Both arm and target position are chosen randomly at the beginning of each episode. The reward is zero except when the tip of the arm is within 0.05 of the target, where it is one. The 8-dimensional observation consists of the angles and angular velocity of all joints as well as the displacement between target and the end effector of the arm. The 3-dimensional action are the torques applied to the joints.\nCheetah The Half-Cheetah (Wawrzyn\u0301ski (2009); Heess et al. (2015)) is a planar locomotion task where the agent is required to control a 9-DoF cheetah-like body (in the vertical plane) to move in the direction of the x-axis as quickly as possible. The reward is given by the velocity along the x-axis and a control cost: r = vx + 0.1\u2016a\u20162. The observation vector consists of the z-position of the torso and its x, z velocities as well as the joint angles and angular velocities. The action dimension is 6.\nFish The goal of this task is to control a 13-DoF fish-like body to swim to a random target in 3D space. The reward is given by the distance between the head of the fish and the target, a small penalty for the body not being upright, and a control cost. At the beginning of an episode the fish is initialized facing in a random direction relative to the target. The 24-dimensional observation is given by the displacement between the fish and the target projected onto the torso coordinate frame, the joint angles and velocities, the cosine of the angle between the z-axis of the torso and the world z-axis, and the velocities of the torso in the torso coordinate frame. The 5-dimensional actions control the position of the side fins and the tail.\nWalker The 9-DoF planar walker is inspired by (Schulman et al. (2015a)) and is required to move forward along the x-axis as quickly as possible without falling. The reward consists of the x-velocity of the torso, a quadratic control cost, and terms that penalize deviations of the torso from the preferred height and orientation (i.e. terms that encourage the walker to stay standing and upright). The 24-dimensional observation includes the torso height, velocities of all DoFs, as well as sines and cosines of all body orientations in the x-z plane. The 6-dimensional action controls the torques applied at the joints. Episodes are terminated early with a negative reward when the torso exceeds upper and lower limits on its height and orientation.\nHumanoid The humanoid is a 27 degrees-of-freedom body with 21 actuators (21 action dimensions). It is initialized lying on the ground in a random configuration and the task requires it to achieve a standing position. The reward function penalizes deviations from the height of the head when standing, and includes additional terms that encourage upright standing, as well as a quadratic action penalty. The 94 dimensional observation contains information about joint angles and velocities and several derived features reflecting the body\u2019s pose.\nE.2 UPDATE EQUATIONS OF THE BASELINE TIS\nThe baseline TIS follows the following update equations,\nupdates to the policy: min { 5, ( k\u22121\u220f\ni=0\n\u03c1t+i\n)}[ k\u22121\u2211\ni=0\n\u03b3irt+i + \u03b3 kV\u03b8v (xk+t)\u2212 V\u03b8v (xt) ] \u2207\u03b8 log \u03c0\u03b8(at|xt),\nupdates to the value: min { 5, ( k\u22121\u220f\ni=0\n\u03c1t+i\n)}[ k\u22121\u2211\ni=0\n\u03b3irt+i + \u03b3 kV\u03b8v (xk+t)\u2212 V\u03b8v (xt) ] \u2207\u03b8vV\u03b8v (xt).\nThe baseline Trust-TIS is appropriately modified according to the trust region update described in Section 3.3.\nE.3 SENSITIVITY ANALYSIS\nIn this section, we assess the sensitivity of ACER to hyper-parameters. In Figures 5 and 6, we show, for each game, the final performance of our ACER agent versus the choice of learning rates, and the trust region constraint \u03b4 respectively.\nNote, as we are doing random hyper-parameter search, each learning rate is associated with a random \u03b4 and vice versa. It is therefore difficult to tease out the effect of either hyper-parameter independently.\nWe observe, however, that ACER is not very sensitive to the hyper-parameters overall. In addition, smaller \u03b4\u2019s do not seem to adversely affect the final performance while larger \u03b4\u2019s do in domains of higher action dimensionality. Similarly, smaller learning rates perform well while bigger learning rates tend to hurt final performance in domains of higher action dimensionality.\nE.4 EXPERIMENTAL SETUP OF ABLATION ANALYSIS\nFor the ablation analysis, we use the same experimental setup as in the continuous control experiments while removing one component at a time.\nTo evaluate the effectiveness of Retrace/Q(\u03bb) with off-policy correction, we replace both with importance sampling based estimates (following Degris et al. (2012)) which can be expressed recursively: Rt = rt + \u03c1t+1Rt+1.\nTo evaluate the Stochastic Dueling Networks, we replace it with two separate networks: one computing the state values and the other Q values. Given Qret(xt, at), the naive way of estimating the state values is to use the following update rule:\n( \u03c1tQ ret(xt, at)\u2212 V\u03b8v (xt) ) \u2207\u03b8vV\u03b8v (xt).\nThe above update rule, however, suffers from high variance. We consider instead the following update rule:\n\u03c1t ( Qret(xt, at)\u2212 V\u03b8v (xt) ) \u2207\u03b8vV\u03b8v (xt)\nwhich has markedly lower variance. We update our Q estimates as before.\nTo evaluate the effects of the truncation and bias correction trick, we change our c parameter (see Equation (16)) to\u221e so as to use pure importance sampling.\n", "cats": {"READ": 1.0, "DONTREAD": 0.0}, "reviews": [{"DATE": "20 Apr 2017", "TITLE": "Equation 4", "IS_META_REVIEW": false, "comments": "First of all, thanks for this excellent work.\n\nMy question is about eq. 4. In Degris et al (2012) the policy gradient is computed as the expectation under the off-policy behavior of \\rho(s_t, a_t) \\psi(s_t, a_t) (R_t^\\lambda - V(s_t))\nWith \\rho(s_t,a_t) = \\pi(a_t | s_t) / \\mu(a_t | s_t) and \\psi(s_t, a_t) = \\grad_\\theta ( log \\pi (a_t | s_t) ) /  \\pi (a_t | s_t)\nThe last division by \\pi (a_t | s_t) is missing in equation (4).\n\nAm I mistaken or is the reference wrong?\nThanks for your time.", "OTHER_KEYS": "(anonymous)"}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "pros:\n - set of contributions leading to SOTA for sample complexity wrt Atari (discrete) and continuous domain problems\n - significant experimental analysis\n - long all-in-one paper\n \n cons:\n - builds on existing ideas, although ablation analysis shows each to be essential\n - long paper\n \nThe PCs believe this paper will be a good contribution to the conference track.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"DATE": "21 Jan 2017", "TITLE": "General Comments", "IS_META_REVIEW": false, "comments": "Dear reviewers, we would really appreciate it if you can take a look at the paper again in light of our replies, the updated paper, and the comments from Xi Chen. Thanks very much for your time!", "OTHER_KEYS": "ziyu wang"}, {"DATE": "21 Jan 2017", "TITLE": "Important contributions", "IS_META_REVIEW": false, "comments": "This submission has a couple important contributions and it'd be actually easy to split it into 2 strong papers.\n\nRoughly:\n1. Especially in deep rl, policy gradient methods have suffered from worse sample complexity compared to value-based methods like DQN. Learning a critic to improve sample efficiency for policy gradient methods is a straightforward idea but this is the first convincing demonstration (by carefully combing different elements like Retrace(\\lambda) and experience replay). This represents an important step towards making policy gradient methods more sample efficient and alone, I believe, merits acceptance. It's worth noting that there is another ICLR submission Q-Prop (", "OTHER_KEYS": "Xi Chen"}, {"DATE": "14 Jan 2017", "TITLE": "Ablations and why each ingredient is an important contribution on its own", "IS_META_REVIEW": false, "comments": "We thank the three reviewers. The one common concern is ablations. This paper proposes several new ideas, and then goes on to combine these ideas.  \n\nTo answer the reviewers concerns about ablations, we added a new figure (Figure 4). This is an extremely important figure and we urge the reviewers and readers to consult it as it should answer any concerns and highlight the value of the many contributions made in this paper. The figure shows that each ingredient (Retrace/Q-lambda with off-policy correction, stochastic dueling nets, and the NEW trust region method) on its own leads to a massive improvement. Likewise, truncation with bias correction plays an important role for large action spaces (control of humanoid). This figure indicates that this paper is not about making 4 small contributions and combining them. Rather it is about making 4 important contributions, which are all essential to obtain a stable, scalable, general, off-policy actor critic. Attaining this has been a holy grail, and this paper shows how to do it.\n\nGiven our good results, we could easily have written several papers; one for each contribution. Instead, we chose to do the honest thing and write a single solid 20-page paper aimed at truly building powerful deep RL agents for both continuous and discrete action spaces. The paper also presents novel theoretical results for RL, and a very comprehensive experimental study. \n\nWe did not want to claim state-of-the-art on Atari because this often depends on how one chooses to measure what should be state-of-the-art (eg sample complexity, highest median, highest mean, etc.). But clearly, in terms of median, ACER with 1 replay achieves a higher median score that any previously reported result. Note that this result is not just for a few games, but for the entire set of 57 games. The UNREAL agent submitted to this conference is the only method we know that achieves a higher median, but it does so by adding auxiliary tasks to A3C and massive hyper-parameter sweeps. We could also add auxiliary tasks to ACER and do hyper-parameter sweeps to further improve it, but this is left for future work as we wanted to focus on designing a powerful core RL agent.\n\nWe hope this reply and in particular the ablations clearly answer your concerns. With 3 6\u2019s this thorough paper will be rejected despite the several novel contributions it makes, new theoretical analysis, and excellent results on a comprehensive set of tasks. We hope you take the ablations and this reply into consideration to choose your final scores. \n", "OTHER_KEYS": "ziyu wang"}, {"TITLE": "No Title", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "This paper studies the off-policy learning of actor-critic with experience replay. This is an important and challenging problem in order to improve the sample efficiency of the reinforcement learning algorithms. The paper attacks the problem by introducing a new way to truncate importance weight, a modified trust region optimization, and by combining retrace method. The combination of the above techniques performs well on Atari and MuJoCo in terms of improving sample efficiency. My main comment is how does each of the technique contribute to the performance gain? If some experiments could be carried out to evaluate the separate gains from these tricks, it would be helpful. ", "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "25 Dec 2016 (modified: 21 Jan 2017)", "REVIEWER_CONFIDENCE": 3}, {"TITLE": "Review", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "This paper introduces an actor-critic deep RL approach with experience replay, which combines truncated importance sampling and trust region policy optimization. The paper also proposes a new method called stochastic duelling networks to estimate the critic for continuous action spaces. The method is applied to Atari games and continuous control problems, where it yields performance comparable to state-of-the-art methods.\n\nAs mentioned in the beginning of the paper, the main contributions of this work lies in combining 1) truncated importance sampling with retrace, 2) trust region policy optimization, and 3) stochastic duelling networks. These improvements work well and may be beneficial to future work in RL.\n\nHowever, each improvement appears to be quite incremental. Moreover, the ACER framework seems much more complex and fragile to implement compared to the standard deep q-learning with prioritized replay (which appears to perform just as well on Atari games). So for the Atari domain, I would still put my money on prioritized replay due to its simplicity. Thirdly, improving sample efficiency for deep RL is a laudable goal, but really this goal should be pursued in a problem setting where sample efficiency is important. Unfortunately, the paper only evaluates sample efficiency in the Atari and continuous control tasks domain; two domains where sample efficiency is not important. Thus, it is not clear that the proposed method ACER will generalize to problems where we really care about sample efficiency.\n\nSome technical aspects which need clarifications:\n- For Retrace, I assume that you compute recursively $Q^{ret}$ starting from the end of each trajectory? Please comment on this.\n- It's not clear to me how to derive eq. (7). Is an approximation (double tilde) sign missing?\n- In section 3.1 the paper argued that $Q^{ret}$ gives a lower-variance estimate of the action-value function. Then why not use it in eq. (8) for the bias correction term?\n- The paper states that it uses a replay memory of 50000 frames, so that across threads it is comparable in size to previous work. However, for each thread this is much smaller compared to earlier experiments on Atari games. For example, one million experience replay transitions were used in the paper \"Prioritized Experience Replay\" by Schaul et al. This may have a huge impact on performance of the models (both for ACER and for the competing models). In order to properly assess the improvements of ACER over previous work, the authors need to also experiment with larger experience replay memories.\n\n\nOther comments:\n- Please move Section 7 to the appendix.\n- \"Moreover, when using small values of lambda to reduce variance, occasional large importance weights can still cause instability\": I think what is meant is using *large* values of lambda.\n- Above eq. (6) mention that the squared error is used.\n- Missing a \"t\" subscript at the beginning of eq. (9)?\n- It was hard to understand the stochastic duelling networks. Please rephrase this part.\n- Please clarify this sentence \"To compare different agents, we adopt as our metric the median of the human normalized score over all 57 games.\"\n- Figure 2 (Bottom): Please add label to vertical axes.", "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 3}, {"TITLE": "Review", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "The paper looks at several innovations for deep RL, and evaluates their effect on solving games in the Atari domain.  The paper reads a bit like a laundry list of the researcher\u2019s latest tricks.  It is written clearly enough, but lacks a compelling message.  I expect the work will be interesting to people already implementing deep RL methods, but will probably not get much attention from the broader community.\n\nThe claims on p.1 suggest the approach is stable and sample efficience, and so I expected to see some theoretical analysis with respect to these properties. But this is an empirical claim; it would help to clarify that in the abstract.\n\nThe proposed innovations are based on sound methods.  It is particularly nice to see the same approach working for both discrete and continuous domains.\n\nThe paper has reasonably complete empirical results. It would be nice to see confidence intervals on more of the plots. Also, the results don\u2019t really tease apart the effect of each of the various innovations, so it\u2019s harder to understand the impact of each piece and to really get intuition, for example about why ACER outperforms A3C.  Also, it wasn\u2019t clear to me why you only get matching results on discrete tasks, but get state-of-the-art on continuous tasks.\n\nThe paper has good coverage of the related literature. It is nice to see this work draw more attention to Retrace, including the theoretical characterization in Sec.7.\n", "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "15 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"DATE": "14 Dec 2016", "TITLE": "clarification of Eq 3 ", "IS_META_REVIEW": false, "comments": "Should the inside summations of equation (3) should go from i = 0 to (k - t)?", "OTHER_KEYS": "(anonymous)"}, {"DATE": "03 Dec 2016", "TITLE": "Questions", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3"}, {"DATE": "01 Dec 2016", "TITLE": "Testing on Atari games and continuous control problems", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2"}, {"DATE": "07 Nov 2016", "TITLE": "ICLR Paper Format", "IS_META_REVIEW": false, "comments": "Dear Authors,\n\nPlease resubmit your paper in the ICLR 2017 format with the correct marging spacing for your submission to be considered. Thank you!", "OTHER_KEYS": "Tara N Sainath"}, {"DATE": "20 Apr 2017", "TITLE": "Equation 4", "IS_META_REVIEW": false, "comments": "First of all, thanks for this excellent work.\n\nMy question is about eq. 4. In Degris et al (2012) the policy gradient is computed as the expectation under the off-policy behavior of \\rho(s_t, a_t) \\psi(s_t, a_t) (R_t^\\lambda - V(s_t))\nWith \\rho(s_t,a_t) = \\pi(a_t | s_t) / \\mu(a_t | s_t) and \\psi(s_t, a_t) = \\grad_\\theta ( log \\pi (a_t | s_t) ) /  \\pi (a_t | s_t)\nThe last division by \\pi (a_t | s_t) is missing in equation (4).\n\nAm I mistaken or is the reference wrong?\nThanks for your time.", "OTHER_KEYS": "(anonymous)"}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "pros:\n - set of contributions leading to SOTA for sample complexity wrt Atari (discrete) and continuous domain problems\n - significant experimental analysis\n - long all-in-one paper\n \n cons:\n - builds on existing ideas, although ablation analysis shows each to be essential\n - long paper\n \nThe PCs believe this paper will be a good contribution to the conference track.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"DATE": "21 Jan 2017", "TITLE": "General Comments", "IS_META_REVIEW": false, "comments": "Dear reviewers, we would really appreciate it if you can take a look at the paper again in light of our replies, the updated paper, and the comments from Xi Chen. Thanks very much for your time!", "OTHER_KEYS": "ziyu wang"}, {"DATE": "21 Jan 2017", "TITLE": "Important contributions", "IS_META_REVIEW": false, "comments": "This submission has a couple important contributions and it'd be actually easy to split it into 2 strong papers.\n\nRoughly:\n1. Especially in deep rl, policy gradient methods have suffered from worse sample complexity compared to value-based methods like DQN. Learning a critic to improve sample efficiency for policy gradient methods is a straightforward idea but this is the first convincing demonstration (by carefully combing different elements like Retrace(\\lambda) and experience replay). This represents an important step towards making policy gradient methods more sample efficient and alone, I believe, merits acceptance. It's worth noting that there is another ICLR submission Q-Prop (", "OTHER_KEYS": "Xi Chen"}, {"DATE": "14 Jan 2017", "TITLE": "Ablations and why each ingredient is an important contribution on its own", "IS_META_REVIEW": false, "comments": "We thank the three reviewers. The one common concern is ablations. This paper proposes several new ideas, and then goes on to combine these ideas.  \n\nTo answer the reviewers concerns about ablations, we added a new figure (Figure 4). This is an extremely important figure and we urge the reviewers and readers to consult it as it should answer any concerns and highlight the value of the many contributions made in this paper. The figure shows that each ingredient (Retrace/Q-lambda with off-policy correction, stochastic dueling nets, and the NEW trust region method) on its own leads to a massive improvement. Likewise, truncation with bias correction plays an important role for large action spaces (control of humanoid). This figure indicates that this paper is not about making 4 small contributions and combining them. Rather it is about making 4 important contributions, which are all essential to obtain a stable, scalable, general, off-policy actor critic. Attaining this has been a holy grail, and this paper shows how to do it.\n\nGiven our good results, we could easily have written several papers; one for each contribution. Instead, we chose to do the honest thing and write a single solid 20-page paper aimed at truly building powerful deep RL agents for both continuous and discrete action spaces. The paper also presents novel theoretical results for RL, and a very comprehensive experimental study. \n\nWe did not want to claim state-of-the-art on Atari because this often depends on how one chooses to measure what should be state-of-the-art (eg sample complexity, highest median, highest mean, etc.). But clearly, in terms of median, ACER with 1 replay achieves a higher median score that any previously reported result. Note that this result is not just for a few games, but for the entire set of 57 games. The UNREAL agent submitted to this conference is the only method we know that achieves a higher median, but it does so by adding auxiliary tasks to A3C and massive hyper-parameter sweeps. We could also add auxiliary tasks to ACER and do hyper-parameter sweeps to further improve it, but this is left for future work as we wanted to focus on designing a powerful core RL agent.\n\nWe hope this reply and in particular the ablations clearly answer your concerns. With 3 6\u2019s this thorough paper will be rejected despite the several novel contributions it makes, new theoretical analysis, and excellent results on a comprehensive set of tasks. We hope you take the ablations and this reply into consideration to choose your final scores. \n", "OTHER_KEYS": "ziyu wang"}, {"TITLE": "No Title", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "This paper studies the off-policy learning of actor-critic with experience replay. This is an important and challenging problem in order to improve the sample efficiency of the reinforcement learning algorithms. The paper attacks the problem by introducing a new way to truncate importance weight, a modified trust region optimization, and by combining retrace method. The combination of the above techniques performs well on Atari and MuJoCo in terms of improving sample efficiency. My main comment is how does each of the technique contribute to the performance gain? If some experiments could be carried out to evaluate the separate gains from these tricks, it would be helpful. ", "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "25 Dec 2016 (modified: 21 Jan 2017)", "REVIEWER_CONFIDENCE": 3}, {"TITLE": "Review", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "This paper introduces an actor-critic deep RL approach with experience replay, which combines truncated importance sampling and trust region policy optimization. The paper also proposes a new method called stochastic duelling networks to estimate the critic for continuous action spaces. The method is applied to Atari games and continuous control problems, where it yields performance comparable to state-of-the-art methods.\n\nAs mentioned in the beginning of the paper, the main contributions of this work lies in combining 1) truncated importance sampling with retrace, 2) trust region policy optimization, and 3) stochastic duelling networks. These improvements work well and may be beneficial to future work in RL.\n\nHowever, each improvement appears to be quite incremental. Moreover, the ACER framework seems much more complex and fragile to implement compared to the standard deep q-learning with prioritized replay (which appears to perform just as well on Atari games). So for the Atari domain, I would still put my money on prioritized replay due to its simplicity. Thirdly, improving sample efficiency for deep RL is a laudable goal, but really this goal should be pursued in a problem setting where sample efficiency is important. Unfortunately, the paper only evaluates sample efficiency in the Atari and continuous control tasks domain; two domains where sample efficiency is not important. Thus, it is not clear that the proposed method ACER will generalize to problems where we really care about sample efficiency.\n\nSome technical aspects which need clarifications:\n- For Retrace, I assume that you compute recursively $Q^{ret}$ starting from the end of each trajectory? Please comment on this.\n- It's not clear to me how to derive eq. (7). Is an approximation (double tilde) sign missing?\n- In section 3.1 the paper argued that $Q^{ret}$ gives a lower-variance estimate of the action-value function. Then why not use it in eq. (8) for the bias correction term?\n- The paper states that it uses a replay memory of 50000 frames, so that across threads it is comparable in size to previous work. However, for each thread this is much smaller compared to earlier experiments on Atari games. For example, one million experience replay transitions were used in the paper \"Prioritized Experience Replay\" by Schaul et al. This may have a huge impact on performance of the models (both for ACER and for the competing models). In order to properly assess the improvements of ACER over previous work, the authors need to also experiment with larger experience replay memories.\n\n\nOther comments:\n- Please move Section 7 to the appendix.\n- \"Moreover, when using small values of lambda to reduce variance, occasional large importance weights can still cause instability\": I think what is meant is using *large* values of lambda.\n- Above eq. (6) mention that the squared error is used.\n- Missing a \"t\" subscript at the beginning of eq. (9)?\n- It was hard to understand the stochastic duelling networks. Please rephrase this part.\n- Please clarify this sentence \"To compare different agents, we adopt as our metric the median of the human normalized score over all 57 games.\"\n- Figure 2 (Bottom): Please add label to vertical axes.", "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 3}, {"TITLE": "Review", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "The paper looks at several innovations for deep RL, and evaluates their effect on solving games in the Atari domain.  The paper reads a bit like a laundry list of the researcher\u2019s latest tricks.  It is written clearly enough, but lacks a compelling message.  I expect the work will be interesting to people already implementing deep RL methods, but will probably not get much attention from the broader community.\n\nThe claims on p.1 suggest the approach is stable and sample efficience, and so I expected to see some theoretical analysis with respect to these properties. But this is an empirical claim; it would help to clarify that in the abstract.\n\nThe proposed innovations are based on sound methods.  It is particularly nice to see the same approach working for both discrete and continuous domains.\n\nThe paper has reasonably complete empirical results. It would be nice to see confidence intervals on more of the plots. Also, the results don\u2019t really tease apart the effect of each of the various innovations, so it\u2019s harder to understand the impact of each piece and to really get intuition, for example about why ACER outperforms A3C.  Also, it wasn\u2019t clear to me why you only get matching results on discrete tasks, but get state-of-the-art on continuous tasks.\n\nThe paper has good coverage of the related literature. It is nice to see this work draw more attention to Retrace, including the theoretical characterization in Sec.7.\n", "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "15 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"DATE": "14 Dec 2016", "TITLE": "clarification of Eq 3 ", "IS_META_REVIEW": false, "comments": "Should the inside summations of equation (3) should go from i = 0 to (k - t)?", "OTHER_KEYS": "(anonymous)"}, {"DATE": "03 Dec 2016", "TITLE": "Questions", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3"}, {"DATE": "01 Dec 2016", "TITLE": "Testing on Atari games and continuous control problems", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2"}, {"DATE": "07 Nov 2016", "TITLE": "ICLR Paper Format", "IS_META_REVIEW": false, "comments": "Dear Authors,\n\nPlease resubmit your paper in the ICLR 2017 format with the correct marging spacing for your submission to be considered. Thank you!", "OTHER_KEYS": "Tara N Sainath"}]}
{"text": "DEEP CONVOLUTIONAL NEURAL NETWORK DESIGN PATTERNS\n1 INTRODUCTION\nMany recent articles discuss new architectures for neural networking, especially regarding Residual Networks (He et al. (2015; 2016); Larsson et al. (2016); Zhang et al. (2016); Huang et al. (2016b)). Although the literature covers a wide range of network architectures, we take a high-level view of the architectures as the basis for discovering universal principles of the design of network architecture. We discuss 14 original design patterns that could benefit inexperienced practitioners who seek to incorporate deep learning in various new applications. This paper addresses the current lack of guidance on design, a deficiency that may prompt the novice to rely on standard architecture, e.g., Alexnet, regardless of the architecture\u2019s suitability to the application at hand.\nThis abundance of research is also an opportunity to determine which elements provide benefits in what specific contexts. We ask: Do universal principles of deep network design exist? Can these principles be mined from the collective knowledge on deep learning? Which architectural choices work best in any given context? Which architectures or parts of architectures seem elegant?\nDesign patterns were first described by Christopher Alexander (Alexander (1979)) in regards to the architectures of buildings and towns. Alexander wrote of a timeless quality in architecture that \u201clives\u201d and this quality is enabled by building based on universal principles. The basis of design patterns is that they resolve a conflict of forces in a given context and lead to an equilibrium analogous to the ecological balance in nature. Design patterns are both highly specific, making them clear to follow, and flexible so they can be adapted to different environments and situations. Inspired by Alexander\u2019s work, the \u201cgang of four\u201d (Gamma et al. (1995)) applied the concept of design patterns to the architecture of object-oriented software. This classic computer science book describes 23 patterns that resolve issues prevalent in software design, such as \u201crequirements always change\u201d. We were inspired by these previous works on architectures to articulate possible design patterns for convolutional neural network (CNN) architectures.\nDesign patterns provide universal guiding principles, and here we take the first steps to defining network design patterns. Overall, it is an enormous task to define design principles for all neural networks and all applications, so we limit this paper to CNNs and their canonical application of image classification. However, we recognize that architectures must depend upon the application by defining our first design pattern; Design Pattern 1: Architectural Structure follows the Application\n(we leave the details of this pattern to future work). In addition, these principles allowed us to discover some gaps in the existing research and to articulate novel networks (i.e, Fractal of FractalNets, Stagewise Boosting and Taylor Series networks) and units (i.e., freeze-drop-path). We hope the rules of thumb articulated here are valuable for both the experienced and novice practitioners and that our preliminary work serves as a stepping stone for others to discover and share additional deep learning design patterns.\n2 RELATED WORK\nTo the best of our knowledge, there has been little written recently to provide guidance and understanding on appropriate architectural choices1. The book \u201dNeural Networks: Tricks of the Trade\u201d (Orr & Mu\u0308ller, 2003) contains recommendations for network models but without reference to the vast amount of research in the past few years. Perhaps the closest to our work is Szegedy et al. (2015b) where those authors describe a few design principles based on their experiences.\nMuch research has studied neural network architectures, but we are unaware of a recent survey of the field. Unfortunately, we cannot do justice to this entire body of work, so we focus on recent innovations in convolutional neural networks architectures and, in particular, on Residual Networks (He et al., 2015) and its recent family of variants. We start with Network In Networks (Lin et al., 2013), which describes a hierarchical network with a small network design repeatedly embedded in the overall architecture. Szegedy et al. (2015a) incorporated this idea into their Inception architecture. Later, these authors proposed modifications to the original Inception design (Szegedy et al., 2015b). A similar concept was contained in the multi-scale convolution architecture (Liao & Carneiro, 2015). In the meantime, Batch Normalization (Ioffe & Szegedy, 2015) was presented as a unit within the network that makes training faster and easier.\nBefore the introduction of Residual Networks, a few papers suggested skip connections. Skip connections were proposed by Raiko et al. (2012). Highway Networks (Srivastava et al., 2015) use a gating mechanism to decide whether to combine the input with the layer\u2019s output and showed how these networks allowed the training of very deep networks. The DropIn technique (Smith et al., 2015; 2016) also trains very deep networks by allowing a layer\u2019s input to skip the layer. The concept of stochastic depth via a drop-path method was introduced by Huang et al. (2016b).\nResidual Networks were introduced by He et al. (2015), where the authors describe their network that won the 2015 ImageNet Challenge. They were able to extend the depth of a network from tens to hundreds of layers and in doing so, improve the network\u2019s performance. The authors followed up with another paper (He et al., 2016) where they investigate why identity mappings help and report results for a network with more than a thousand layers. The research community took notice of this architecture and many modifications to the original design were soon proposed.\nThe Inception-v4 paper (Szegedy et al., 2016) describes the impact of residual connections on their Inception architecture and compared these results with the results from an updated Inception design. The Resnet in Resnet paper (Targ et al., 2016) suggests a duel stream architecture. Veit et al. (2016) provided an understanding of Residual Networks as an ensemble of relatively shallow networks. These authors illustrated how these residual connections allow the input to follow an exponential number of paths through the architecture. At the same time, the FractalNet paper (Larsson et al., 2016) demonstrated training deep networks with a symmetrically repeating architectural pattern. As described later, we found the symmetry introduced in their paper intriguing. In a similar vein, Convolutional Neural Fabrics (Saxena & Verbeek, 2016) introduces a three dimensional network, where the usual depth through the network is the first dimension.\nWide Residual Networks (Zagoruyko & Komodakis, 2016) demonstrate that simultaneously increasing both depth and width leads to improved performance. In Swapout (Singh et al., 2016), each layer can be dropped, skipped, used normally, or combined with a residual. Deeply Fused Nets (Wang et al., 2016) proposes networks with multiple paths. In the Weighted Residual Networks paper (Shen & Zeng, 2016), the authors recommend a weighting factor for the output from the convolutional layers, which gradually introduces the trainable layers. Convolutional Residual Memory Networks (Moniz & Pal, 2016) proposes an architecture that combines a convolutional Residual Network with\n1After submission we became aware of an online book being written on deep learning design patterns at http://www.deeplearningpatterns.com\nan LSTM memory mechanism. For Residual of Residual Networks (Zhang et al., 2016), the authors propose adding a hierarchy of skip connections where the input can skip a layer, a module, or any number of modules. DenseNets (Huang et al., 2016a) introduces a network where each module is densely connected; that is, the output from a layer is input to all of the other layers in the module. In the Multi-Residual paper (Abdi & Nahavandi, 2016), the authors propose expanding a residual block width-wise to contain multiple convolutional paths. Our Appendix A describes the close relationship between many of these Residual Network variants.\n3 DESIGN PATTERNS\nWe reviewed the literature specifically to extract commonalities and reduce their designs down to fundamental elements that might be considered design patterns. It seemed clear to us that in reviewing the literature some design choices are elegant while others are less so. In particular, the patterns described in this paper are the following:\n1. Architectural Structure follows the Application\n2. Proliferate Paths\n3. Strive for Simplicity\n4. Increase Symmetry\n5. Pyramid Shape\n6. Over-train\n7. Cover the Problem Space\n8. Incremental Feature Construction\n9. Normalize Layer Inputs\n10. Input Transition\n11. Available Resources Guide Layer Widths\n12. Summation Joining\n13. Down-sampling Transition\n14. Maxout for Competition\n3.1 HIGH LEVEL ARCHITECTURE DESIGN\nSeveral researchers have pointed out that the winners of the ImageNet Challenge (Russakovsky et al., 2015) have successively used deeper networks (as seen in, Krizhevsky et al. (2012), Szegedy et al. (2015a), Simonyan & Zisserman (2014), He et al. (2015)). It is also apparent to us from the ImageNet Challenge that multiplying the number of paths through the network is a recent trend that is illustrated in the progression from Alexnet to Inception to ResNets. For example, Veit et al. (2016) show that ResNets can be considered to be an exponential ensemble of networks with different lengths. Design Pattern 2: Proliferate Paths is based on the idea that ResNets can be an exponential ensemble of networks with different lengths. One proliferates paths by including a multiplicity of branches in the architecture. Recent examples include FractalNet (Larsson et al. 2016), Xception (Chollet 2016), and Decision Forest Convolutional Networks (Ioannou et al. 2016).\nScientists have embraced simplicity/parsimony for centuries. Simplicity was exemplified in the paper \u201dStriving for Simplicity\u201d (Springenberg et al. 2014) by achieving state-of-the-art results with fewer types of units. Design Pattern 3: Strive for Simplicity suggests using fewer types of units and keeping the network as simple as possible. We also noted a special degree of elegance in the FractalNet (Larsson et al. 2016) design, which we attributed to the symmetry of its structure. Design Pattern 4: Increase Symmetry is derived from the fact that architectural symmetry is typically considered a sign of beauty and quality. In addition to its symmetry, FractalNets also adheres to the Proliferate Paths design pattern so we used it as the baseline of our experiments in Section 4.\nAn essential element of design patterns is the examination of trade-offs in an effort to understand the relevant forces. One fundamental trade-off is the maximization of representational power versus\nelimination of redundant and non-discriminating information. It is universal in all convolutional neural networks that the activations are downsampled and the number of channels increased from the input to the final layer, which is exemplified in Deep Pyramidal Residual Networks (Han et al. (2016)). Design Pattern 5: Pyramid Shape says there should be an overall smooth downsampling combined with an increase in the number of channels throughout the architecture.\nAnother trade-off in deep learning is training accuracy versus the ability of the network to generalize to non-seen cases. The ability to generalize is an important virtue of deep neural networks. Regularization is commonly used to improve generalization, which includes methods such as dropout (Srivastava et al. 2014a) and drop-path (Huang et al. 2016b). As noted by Srivastava et al. 2014b, dropout improves generalization by injecting noise in the architecture. We believe regularization techniques and prudent noise injection during training improves generalization (Srivastava et al. 2014b, Gulcehre et al. 2016). Design Pattern 6: Over-train includes any training method where the network is trained on a harder problem than necessary to improve generalization performance of inference. Design Pattern 7: Cover the Problem Space with the training data is another way to improve generalization (e.g., Ratner et al. 2016, Hu et al. 2016, Wong et al. 2016, Johnson-Roberson et al. 2016). Related to regularization methods, cover the problem space includes the use of noise (Rasmus et al. 2015, Krause et al. 2015, Pezeshki et al. 2015), synthetic data, and data augmentation, such as random cropping, flipping, and varying brightness, contrast, and the like.\n3.2 DETAILED ARCHITECTURE DESIGN\nA common thread throughout many of the more successful architectures is to make each layer\u2019s \u201cjob\u201d easier. Use of very deep networks is an example because any single layer only needs to incrementally modify the input, and this partially explains the success of Residual Networks, since in very deep networks, a layer\u2019s output is likely similar to the input; hence adding the input to the layer\u2019s output makes the layer\u2019s job incremental. Also, this concept is part of the motivation behind design pattern 2 but it extends beyond that. Design Pattern 8: Incremental Feature Construction recommends using short skip lengths in ResNets. A recent paper (Alain & Bengio (2016)) showed in an experiment that using an identity skip length of 64 in a network of depth 128 led to the first portion of the network not being trained.\nDesign Pattern 9: Normalize Layer Inputs is another way to make a layer\u2019s job easier. Normalization of layer inputs has been shown to improve training and accuracy but the underlying reasons are not clear (Ioffe & Szegedy 2015, Ba et al. 2016, Salimans & Kingma 2016). The Batch Normalization paper (Ioffe & Szegedy 2015) attributes the benefits to handling internal covariate shift, while the authors of streaming normalization (Liao et al. 2016) express that it might be otherwise. We feel that normalization puts all the layer\u2019s input samples on more equal footing (analogous to a units conversion scaling), which allows back-propagation to train more effectively.\nSome research, such as Wide ResNets (Zagoruyko & Komodakis 2016), has shown that increasing the number of channels improves performance but there are additional costs with extra channels. The input data for many of the benchmark datasets have 3 channels (i.e., RGB). Design Pattern 10: Input Transition is based on the common occurrence that the output from the first layer of a CNN significantly increases the number of channels from 3. A few examples of this increase in channels/outputs at the first layer for ImageNet are AlexNet (96 channels), Inception (32), VGG (224), and ResNets (64). Intuitively it makes sense to increase the number of channels from 3 in the first layer as it allows the input data to be examined many ways but it is not clear how many outputs are best. Here, the trade-off is that of cost versus accuracy. Costs include the number of parameters in the network, which directly affects the computational and storage costs of training and inference. Design Pattern 11: Available Resources Guide Layer Widths is based on balancing costs against an application\u2019s requirements. Choose the number of outputs of the first layer based on memory and computational resources and desired accuracy.\n3.2.1 JOINING BRANCHES: CONCATENATION, SUMMATION/MEAN, MAXOUT\nWhen there are multiple branches, three methods have been used to combine the outputs; concatenation, summation (or mean), or Maxout. It seems that different researchers have their favorites and there hasn\u2019t been any motivation for using one in preference to another. In this Section, we propose some rules for deciding how to combine branches.\nSummation is one of the most common ways of combining branches. Design Pattern 12: Summation Joining is where the joining is performed by summation/mean. Summation is the preferred joining mechanism for Residual Networks because it allows each branch to compute corrective terms (i.e., residuals) rather than the entire approximation. The difference between summation and mean (i.e., fractal-join) is best understood by considering drop-path (Huang et al. 2016b). In a Residual Network where the input skip connection is always present, summation causes the layers to learn the residual (the difference from the input). On the other hand, in networks with several branches, where any branch can be dropped (e.g., FractalNet (Larsson et al. (2016))), using the mean is preferable as it keeps the output smooth as branches are randomly dropped.\nSome researchers seem to prefer concatenation (e.g., Szegedy et al. (2015a)). Design Pattern 13: Down-sampling Transition recommends using concatenation joining for increasing the number of outputs when pooling. That is, when down-sampling by pooling or using a stride greater than 1, a good way to combine branches is to concatenate the output channels, hence smoothly accomplishing both joining and an increase in the number of channels that typically accompanies down-sampling.\nMaxout has been used for competition, as in locally competitive networks (Srivastava et al. 2014b) and competitive multi-scale networks Liao & Carneiro (2015). Design Pattern 14: Maxout for Competition is based on Maxout choosing only one of the activations, which is in contrast to summation or mean where the activations are \u201ccooperating\u201d; here, there is a \u201ccompetition\u201d with only one \u201cwinner\u201d. For example, when each branch is composed of different sized kernels, Maxout is useful for incorporating scale invariance in an analogous way to how max pooling enables translation invariance.\n4 EXPERIMENTS\n4.1 ARCHITECTURAL INNOVATIONS\nIn elucidating these fundamental design principles, we also discovered a few architectural innovations. In this section we will describe these innovations.\nFirst, we recommended combining summation/mean, concatenation, and maxout joining mechanisms with differing roles within a single architecture, rather than the typical situation where only one is used throughout. Next, Design Pattern 2: Proliferate Branches led us to modify the overall sequential pattern of modules in the FractalNet architecture. Instead of lining up the modules for maximum depth, we arranged the modules in a fractal pattern as shown in 1b, which we named a Fractal of FractalNet (FoF) network, where we exchange depth for a greater number of paths.\n4.1.1 FREEZE-DROP-PATH AND STAGEWISE BOOSTING NETWORKS (SBN)\nDrop-path was introduced by Huang et al. (2016b), which works by randomly removing branches during an iteration of training, as though that path doesn\u2019t exist in the network. Symmetry considerations led us to an opposite method that we named freeze-path. Instead of removing a branch from the network during training, we freeze the weights, as though the learning rate was set to zero. A similar idea has been proposed for recurrent neural networks (Krueger et al. 2016).\nThe potential usefulness of combining drop-path and freeze-path, which we named freeze-drop-path, is best explained in the non-stochastic case. Figure 1 shows an example of a fractal of FractalNet architecture. Let\u2019s say we start training only the leftmost branch in Figure 1b and use drop-path on all of the other branches. This branch should train quickly since it has only a relatively few parameters compared to the entire network. Next we freeze the weights in that branch and allow the next branch to the right to be active. If the leftmost branch is providing a good function approximation, the next branch works to produce a \u201csmall\u201d corrective term. Since the next branch contains more layers than the previous branch and the corrective term should be easier to approximate than the original function, the network should attain greater accuracy. One can continue this process from left to right to train the entire network. We used freeze-drop-path as the final/bottom join in the FoF architecture in Figure 1b and named this the Stagewise Boosting Networks (SBN) because they are analogous to stagewise boosting (Friedman et al. 2001). The idea of boosting neural networks is not new (Schwenk & Bengio 2000) but this architecture is new. In Appendix B we discuss the implementation we tested.\n4.1.2 TAYLOR SERIES NETWORKS (TSN)\nTaylor series expansions are classic and well known as a function approximation method, which is: f(x+ h) = f(x) + hf \u2032(x) + h2f \u2032\u2032(x)/2 + ... (1)\nSince neural networks are also function approximators, it is a short leap from FoFs and SBNs to consider the branches of that network as terms in a Taylor series expansion. Hence, the Taylor series implies squaring the second branch before the summation joining unit, analogous to the second order term in the expansion. Similarly, the third branch would be cubed. We call this \u201cTaylor Series Networks\u201d (TSN) and there is precedence for this idea in the literature with polynomial networks (Livni et al. 2014) and multiplication in networks (e.g. Lin et al. 2015. The implementation details of this TSN are discussed in the Appendix.\n4.2 RESULTS\nThe experiments in this section are primarily to empirically validate the architectural innovations described above but not to fully test them. We leave a more complete evaluation to future work.\nTable 1 and Figures 2 and 3 compare the final test accuracy results for CIFAR-10 and CIFAR-100 in a number of experiments. An accuracy value in Table 1 is computed as the mean of the last 6 test\naccuracies computed over the last 3,000 iterations (out of 100,000) of the training. The results from the original FractalNet (Larsson et al. 2016) are given in the first row of the table and we use this as our baseline. The first four rows of Table 1 and Figure 2 compare the test accuracy of the original FractalNet architectures to architectures with a few modifications advocated by design patterns. The first modification is to use concatenation instead of fractal-joins at all the downsampling locations in the networks. The results for both CIFAR-10 (2a) and CIFAR-100 (2b indicate that the results are equivalent when concatenation is used instead of fractal-joins at all the downsampling locations in the networks. The second experiment was to change the kernel sizes in the first module from 3x3 such that the left most column used a kernel size of 7x7, the second column 5x5, and the third used 3x3. The fractal-join for module one was replaced with Maxout. The results in Figure 2 are a bit worse, indicating that the more cooperative fractal-join (i.e., mean/summation) with 3x3 kernels has better performance than the competitive Maxout with multiple scales. Figure 2 also illustrates how an experiment replacing max pooling with average pooling throughout the architecture changes the training profile. For CIFAR-10, the training accuracy rises quickly, plateaus, then lags behind the original FractalNet but ends with a better final performance, which implies that average pooling might significantly reduce the length of the training (we plan to examine this in future work). This behavior provides some evidence that \u201ccooperative\u201d average pooling might be preferable to \u201ccompetitive\u201d max pooling.\nTable 1 and Figure 3 compare the test accuracy results for the architectural innovations described in Section 4.1. The FoF architecture ends with a similar final test accuracy as FractalNet but the SBN and TSN architectures (which use freeze-drop-path) lag behind when the learning rate is dropped. However, it is clear from both Figures 3a and 3b that the new architectures train more quickly than FractalNet. The FoF network is best as it trains more quickly than FractalNet and achieves similar accuracy. The use of freeze-drop-path in SBN and TSN is questionable since the final performance lags behind FractalNet, but we are leaving the exploration for more suitable applications of these new architectures for future work.\n5 CONCLUSION\nIn this paper we describe convolutional neural network architectural design patterns that we discovered by studying the plethora of new architectures in recent deep learning papers. We hope these design patterns will be useful to both experienced practitioners looking to push the state-of-the-art and novice practitioners looking to apply deep learning to new applications. There exists a large expanse of potential follow up work (some of which we have indicated here as future work). Our effort here is primarily focused on Residual Networks for classification but we hope this preliminary work will inspire others to follow up with new architectural design patterns for Recurrent Neural Networks, Deep Reinforcement Learning architectures, and beyond.\nACKNOWLEDGMENTS\nThe authors want to thank the numerous researchers upon whose work these design patterns are based and especially Larsson et al. 2016 for making their code publicly available. This work was supported by the US Naval Research Laboratory base program.\nA RELATIONSHIPS BETWEEN RESIDUAL ARCHITECTURES\nThe architectures mentioned in Section 2 commonly combine outputs from two or more layers using concatenation along the depth axis, element-wise summation, and element-wise average. We show here that the latter two are special cases of the former with weight-sharing enforced. Likewise, we show that skip connections can be considered as introducing additional layers into a network that share parameters with existing layers. In this way, any of the Residual Network variants can be reformulated into a standard form where many of the variants are equivalent.\nA filter has three dimensions: two spatial dimensions, along which convolution occurs, and a third dimension, depth. Each input channel corresponds to a different depth for each filter of a layer. As a result, a filter can be considered to consist of \u201cslices,\u201d each of which is convolved over one input channel. The results of these convolutions are then added together, along with a bias, to produce a single output channel. The output channels of multiple filters are concatenated to produce the output of a single layer. When the outputs of several layers are concatenated, the behavior is similar to that of a single layer. However, instead of each filter having the same spatial dimensions, stride, and padding, each filter may have a different structure. As far as the function within a network, though, the two cases are the same. In fact, a standard layer, one where all filters have the same shape, can be considered a special case of concatenating outputs of multiple layer types.\nIf summation is used instead of concatenation, the network can be considered to perform concatenation but enforce weight-sharing in the following layer. The results of first summing several channels element-wise and then convolving a filter slice over the output yields the same result as convolving the slice over the channels and then performing an element-wise summation afterwards. Therefore, enforcing weight-sharing such that the filter slices applied to the nth channel of all inputs share weight results in behavior identical to summation, but in a form similar to concatenation, which highlights the relationship between the two. When Batch Normalization (BN) (Ioffe & Szegedy 2015 is used, as is the current standard practice, performing an average is essentially identical to performing a summation, since BN scales the output. Therefore, scaling the input by a constant (i.e., averaging instead of a summation) is rendered irrelevant. The details of architecture-specific manipulations of summations and averages is described further in Section 3.2.1.\nDue to the ability to express depth-wise concatenation, element-wise sum, and element-wise mean as variants of each other, architectural features of recent works can be combined within a single network, regardless of choice of combining operation. However, this is not to say that concatenation has the most expressivity and is therefore strictly better than the others. Summation allows networks to divide up the network\u2019s task. Also, there is a trade-off between the number of parameters and the expressivity of a layer; summation uses weight-sharing to significantly reduce the number of parameters within a layer at the expense of some amount of expressivity.\nDifferent architectures can further be expressed in a similar fashion through changes in the connections themselves. A densely connected series of layers can be \u201cpruned\u201d to resemble any desired architecture with skip connections through zeroing specific filter slices. This operation removes the dependency of the output on a specific input channel; if this is done for all channels from a given layer, the connection between the two layers is severed. Likewise, densely connected layers can be turned into linearly connected layers while preserving the layer dependencies; a skip connection can be passed through the intermediate layers. A new filter can be introduced for each input channel passing through, where the filter performs the identity operation for the given input channel. All existing filters in the intermediate layers can have zeroed slices for this input so as to not introduce new dependencies. In this way, arbitrarily connected layers can be turned into a standard form.\nWe certainly do not recommend this representation for actual experimentation as it introduces fixed parameters. We merely describe it to illustrate the relationship between different architectures. This representation illustrates how skip connections effectively enforce specific weights in intermediate layers. Though this restriction reduces expressivity, the number of stored weights is reduced, the number of computations performed is decreased, and the network might be more easily trainable.\nB IMPLEMENTATION DETAILS\nOur implementations are in Caffe (Jia et al. 2014; downloaded October 9, 2016) using CUDA 8.0. These experiments were run on a 64 node cluster with 8 Nvidia Titan Black GPUs, 128 GB memory, and dual Intel Xenon E5-2620 v2 CPUs per node. We used the CIFAR10 and CIFAR-100 datasets (Krizhevsky & Hinton 2009 for our classification tests. These datasets consist of 60,000 32x32 colour images (50,000 for training and 10,000 for testing) in 10 or 100 classes, respectively. Our Caffe code and prototxt files are publicly available at https://github.com/iPhysicist/CNNDesignPatterns.\nB.1 ARCHITECTURES\nWe started with the FractalNet implementation 2 as our baseline and it is described in Larsson et al. 2016. We used the three column module as shown in Figure 1a. In some of our experiments, we replaced the fractal-join with concatenation at the downsampling locations. In other experiments, we modified the kernel sizes in module one and combined the branches with Maxout. A FractalNet module is shown in Figure 1a and the architecture consists of five sequential modules.\nOur fractal of FractalNet (FoF) architecture uses the same module but has an overall fractal design as in Figure 1b rather than the original sequential one. We limited our investigation to this one realization and left the study of other (possibly more complex) designs for future work. We followed the FractalNet implementation in regards to dropout where the dropout rate for a module were 0%, 10%, 20%, or 30%, depending on the depth of the module in the architecture. This choice for dropout rates were not found by experimentation and better values are possible. The local drop-path rate in the fractal-joins were fixed at 15%, which is identical to the FractalNet implementation.\nFreeze-drop-path introduces four new parameters. The first is whether the active branch is chosen stochastically or deterministically. If it is chosen stochastically, a random number is generated and the active branch is assigned based on which interval it falls in (intervals will be described shortly). If it is deterministically, a parameter is set by the user as to the number of iterations in one cycle through all the branches (we called this parameter num iter per cycle). In our Caffe implementation of the freeze-drop-path unit, the bottom input specified first is assigned as branch 1, the next is branch 2, then branch 3, etc. The next parameter indicates the proportion of iterations each branch should be active relative to all the other branches. The first type of interval uses the square of the branch number (i.e., 1, 4, 9, 16, ...) to assign the interval length for that branch to be active, which gives the more update iterations to the higher numbered branches. The next type gives the same amount of iterations to each branch. Our experiments showed that the first interval type works better (as we expected) and was used to obtained the results in Section 4.2. In addition, our experiments showed that the stochastic option works better than the deterministic option (unexpected) and was used for Section 4.2 results.\n2https://github.com/gustavla/fractalnet/tree/master/caffe\nThe Stagewise Boosting Network\u2019s (SBN) architecture is the same as the FoF architecture except that branches 2 and 3 are combined with a fractal-join and then combined with branch 1 in a freezedrop-path join. The reason for combining branches 2 and 3 came out of our first experiments; if branches 2 and 3 were separate, the performance deteriorated when branch 2 was frozen and branch 3 was active. In hindsight, this is due to the weights in the branch 2 path that are also in branch 3\u2019s path being modified by the training of branch 3. The Taylor series network has the same architecture as SBN with the addition of squaring the branch 2 and 3 combined activations before the freeze-drop-path join.\nFor all of our experiments, we trained for 400 epochs. Since the training used 8 GPUs and each GPU had a batchsize of 25, 400 epochs amounted to 100,000 iterations. We adopted the same learning rate as the FractalNet implementation, which started at 0.002 and dropped the learning rate by a factor of 10 at epochs 200, 300, and 350.\n", "cats": {"READ": 1.0, "DONTREAD": 0.0}, "reviews": [{"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "The authors agree with the reviewers that this manuscript is not yet ready.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"DATE": "27 Dec 2016", "TITLE": "Regarding the Reviewers' evaluation", "IS_META_REVIEW": false, "comments": "We wish to thank the esteemed Reviewers for their time and this valuable feedback to our paper.  We believe that the Reviewers are correct in their evaluations. Hence, our paper will require a significant rewrite and will not be ready for the ICLR conference paper deadline", "OTHER_KEYS": "Leslie N Smith"}, {"TITLE": "No Title", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "The paper formulates a number of rules for designing convolutional neural network architectures for image processing and computer vision problems. Essentially, it reads like a review paper about modern CNN architectures. It also proposes a few new architectural ideas inspired by these rules. These are experimentally evaluated on CIFAR-10 and CIFAR-100, but seem to achieve relatively poor performance on these datasets (Table 1), so their merit is unclear to me.\n\nI'm not sure if such a collection of rules extracted from prior work warrants publication as a research paper. It is not a bad idea to try and summarise some of these observations now that CNNs have been the model of choice for computer vision tasks for a few years, and such a summary could be useful for newcomers. However, a lot of it seems to boil down to common sense (e.g. #1, #3, #7, #11). The rest of it might be more suited for an \"introduction to training CNNs\" course / blog post. It also seems to be a bit skewed towards recent work that was fairly incremental (e.g. a lot of attention is given to the flurry of ResNet variants).\n\nThe paper states that \"it is universal in all convolutional neural networks that the activations are downsampled and the number of channels increased from the input to the final layer\", which is wrong. We already discussed this previously re: my question about design pattern 5, but I think the answer that was given (\"the nature of design patterns is that they only apply some of the time\") does not excuse making such sweeping claims. This should probably be removed.\n\n\"We feel that normalization puts all the layer's input samples on more equal footing, which allows backprop to train more effectively\" (section 3.2, 2nd paragraph) is very vague language that has many possible interpretations and should probably be clarified. It also seems odd to start this sentence with \"we feel\", as this doesn't seem like the kind of thing one should have an opinion about. Such claims should be corroborated by experiments and measurements. There are several other instances of this issue across the paper.\n\nThe connection between Taylor series and the proposed Taylor Series Networks seems very tenuous and I don't think the name is appropriate. The resulting function is not even a polynomial as all the terms represent different functions -- f(x) + g(x)**2 + h(x)**3 + ... is not a particularly interesting object, it is just a nonlinear function of x.\n\nOverall, the paper reads like a collection of thoughts and ideas that are not very well delineated, and the experimental results are unconvincing.", "IS_META_REVIEW": false, "RECOMMENDATION": 3, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "Review", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "The authors have grouped recent work in convolutional neural network design (specifically with respect to image classification) to identify core design principles guiding the field at large. The 14 principles they produce (along with associated references) include a number of useful and correct observations that would be an asset to anyone unfamiliar with the field. The authors explore a number of architectures on CIFAR-10 and CIFAR-100 guided by these principles.\n\nThe authors have collected a quality set of references on the subject and grouped them well which is valuable for young researchers. Clearly the authors explored a many of architectural changes as part of their experiments and publicly available code base is always nice.\n\nOverall the writing seems to jump around a bit and the motivations behind some design principles feel lost in the confusion. For example, \"Design Pattern 4: Increase Symmetry argues for architectural symmetry as a sign of beauty and quality\" is presented as one of 14 core design principles without any further justification. Similarly \"Design Pattern 6: Over-train includes any training method where the network is trained on a harder problem than necessary to improve generalization performance of inference\" is presented in the middle of a paragraph with no supporting references or further explanation.\n\nThe experimental portion of this paper feels scattered with many different approaches being presented based on subsets of the design principles. In general, these approaches either are minor modifications of existing networks (different FractalNet pooling strategies) or are novel architectures that do not perform well. The exception being the Fractal-of-Fractal network which achieves slightly improved accuracy but also introduces many more network parameters (increased capacity) over the original FractalNet.\n\n\nPreliminary rating:\nIt is a useful and perhaps noble task to collect and distill research from many sources to find patterns (and perhaps gaps) in the state of a field; however, some of the patterns presented do not seem well developed and include principles that are poorly explained. Furthermore, the innovative architectures motivated by the design principles either fall short or achieve slightly better accuracy by introducing many more parameters (Fractal-Of-Fractal networks). For a paper addressing the topic of higher level design trends, I would appreciate additional rigorous experimentation around each principle rather than novel architectures being presented.", "IS_META_REVIEW": false, "RECOMMENDATION": 3, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "interesting direction - but not really solid", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "The authors take on the task of figuring out a set of design patterns for current deep architectures - namely themes that are recurring in the literature.  If one may say so, a distributed representation of deep architectures. \n\nThere are two aspects of the paper that I particularly valued: firstly, the excellent review of recent works, which made me realize how many things I have been missing myself.  Secondly, the \"community service\" aspect of helping someone who starts figure out the \"coordinate system\" for deep architectures - this could potentially be more important than introducing yet-another trick of the trade, as most other submissions may do.\n\nHowever I think this work is still half-done, and even though working on this project is a great idea, the authors do not yet do it properly. \n\nFirstly, I am not too sure how the choice of these 14 patterns was made. Maxout for instance (pattern 14) is one of the many nonlinearities (PreLU, ReLU, ...) and I do not see how it stands on the same grounds as something as general as \"3 Strive for simplicity\".\n\nSimilarly some of the patterns are as vague as \"Increase symmetry\" and are backed up by statements such as \"we noted a special degree of elegance in the FractalNet\". I do not see how this leads to a design pattern that can be applied to a new architecture - or if it applies to anything other than the FractalNet. \n\nSome other patterns are phrased with weird names \"7 Cover the problem space\" - which I guess stands for dataset augmentation; or \"6 over-train\" which is not backed up by a single reference. Unless the authors relate it to regularization (text preceding \"overtrain\"), which then has no connection to the description of \"over-train\" provided by the authors (\"training a network on a harder problem to improve generalization\"). If \"harder problem\" means one where one adds an additional term (i.e. the regularizer), the authors are doing harm to the unexperienced reader, confusing \"regularization\" with something that sounds like \"overfitting\" (i.e. the exact opposite).\n\nFurthermore, the extensions proposed in Section 4 seem a bit off tune - in particular I could not figure out \n-how the Taylor Series networks stem from any of the design patterns proposed in the rest of the paper. \n-whether the text between 4.1 and 4.1.1 is another of the architecture innovations (and if yes, why it is not in the 4.1.2, or 4.1.0) \n-and, most importantly, how these design patterns would be deployed in practice to think of a new network. \n\nTo be more concrete, the authors mention that they propose the \"freeze-drop-path\" variant from \"symmetry considerations\" to \"drop-path\". \nIs this an application of the \"increase symmetry\" pattern? How would \"freeze-drop-path\" be more symmetric that \"drop-path\"?\n Can this be expressed concretely, or is it some intuitive guess? If the second, it is not really part of applying a pattern, in my understanding. If the first, this is missing. \n\n\nWhat I would have appreciated more (and would like to see in a revised version) would have been a table of \"design patterns\" on one axis, \"Deep network\" on another, and a breakdown of which network applies which design pattern. \n\nA big part of the previous work is also covered in cryptic language - some minimal explanation of what is taking place in the alternative works would be useful. \n\n\n  \n", "IS_META_REVIEW": false, "RECOMMENDATION": 4, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 3}, {"DATE": "03 Dec 2016", "TITLE": "Experimental Results", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3"}, {"DATE": "02 Dec 2016", "TITLE": "a few questions", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2"}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "The authors agree with the reviewers that this manuscript is not yet ready.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"DATE": "27 Dec 2016", "TITLE": "Regarding the Reviewers' evaluation", "IS_META_REVIEW": false, "comments": "We wish to thank the esteemed Reviewers for their time and this valuable feedback to our paper.  We believe that the Reviewers are correct in their evaluations. Hence, our paper will require a significant rewrite and will not be ready for the ICLR conference paper deadline", "OTHER_KEYS": "Leslie N Smith"}, {"TITLE": "No Title", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "The paper formulates a number of rules for designing convolutional neural network architectures for image processing and computer vision problems. Essentially, it reads like a review paper about modern CNN architectures. It also proposes a few new architectural ideas inspired by these rules. These are experimentally evaluated on CIFAR-10 and CIFAR-100, but seem to achieve relatively poor performance on these datasets (Table 1), so their merit is unclear to me.\n\nI'm not sure if such a collection of rules extracted from prior work warrants publication as a research paper. It is not a bad idea to try and summarise some of these observations now that CNNs have been the model of choice for computer vision tasks for a few years, and such a summary could be useful for newcomers. However, a lot of it seems to boil down to common sense (e.g. #1, #3, #7, #11). The rest of it might be more suited for an \"introduction to training CNNs\" course / blog post. It also seems to be a bit skewed towards recent work that was fairly incremental (e.g. a lot of attention is given to the flurry of ResNet variants).\n\nThe paper states that \"it is universal in all convolutional neural networks that the activations are downsampled and the number of channels increased from the input to the final layer\", which is wrong. We already discussed this previously re: my question about design pattern 5, but I think the answer that was given (\"the nature of design patterns is that they only apply some of the time\") does not excuse making such sweeping claims. This should probably be removed.\n\n\"We feel that normalization puts all the layer's input samples on more equal footing, which allows backprop to train more effectively\" (section 3.2, 2nd paragraph) is very vague language that has many possible interpretations and should probably be clarified. It also seems odd to start this sentence with \"we feel\", as this doesn't seem like the kind of thing one should have an opinion about. Such claims should be corroborated by experiments and measurements. There are several other instances of this issue across the paper.\n\nThe connection between Taylor series and the proposed Taylor Series Networks seems very tenuous and I don't think the name is appropriate. The resulting function is not even a polynomial as all the terms represent different functions -- f(x) + g(x)**2 + h(x)**3 + ... is not a particularly interesting object, it is just a nonlinear function of x.\n\nOverall, the paper reads like a collection of thoughts and ideas that are not very well delineated, and the experimental results are unconvincing.", "IS_META_REVIEW": false, "RECOMMENDATION": 3, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "Review", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "The authors have grouped recent work in convolutional neural network design (specifically with respect to image classification) to identify core design principles guiding the field at large. The 14 principles they produce (along with associated references) include a number of useful and correct observations that would be an asset to anyone unfamiliar with the field. The authors explore a number of architectures on CIFAR-10 and CIFAR-100 guided by these principles.\n\nThe authors have collected a quality set of references on the subject and grouped them well which is valuable for young researchers. Clearly the authors explored a many of architectural changes as part of their experiments and publicly available code base is always nice.\n\nOverall the writing seems to jump around a bit and the motivations behind some design principles feel lost in the confusion. For example, \"Design Pattern 4: Increase Symmetry argues for architectural symmetry as a sign of beauty and quality\" is presented as one of 14 core design principles without any further justification. Similarly \"Design Pattern 6: Over-train includes any training method where the network is trained on a harder problem than necessary to improve generalization performance of inference\" is presented in the middle of a paragraph with no supporting references or further explanation.\n\nThe experimental portion of this paper feels scattered with many different approaches being presented based on subsets of the design principles. In general, these approaches either are minor modifications of existing networks (different FractalNet pooling strategies) or are novel architectures that do not perform well. The exception being the Fractal-of-Fractal network which achieves slightly improved accuracy but also introduces many more network parameters (increased capacity) over the original FractalNet.\n\n\nPreliminary rating:\nIt is a useful and perhaps noble task to collect and distill research from many sources to find patterns (and perhaps gaps) in the state of a field; however, some of the patterns presented do not seem well developed and include principles that are poorly explained. Furthermore, the innovative architectures motivated by the design principles either fall short or achieve slightly better accuracy by introducing many more parameters (Fractal-Of-Fractal networks). For a paper addressing the topic of higher level design trends, I would appreciate additional rigorous experimentation around each principle rather than novel architectures being presented.", "IS_META_REVIEW": false, "RECOMMENDATION": 3, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "interesting direction - but not really solid", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "The authors take on the task of figuring out a set of design patterns for current deep architectures - namely themes that are recurring in the literature.  If one may say so, a distributed representation of deep architectures. \n\nThere are two aspects of the paper that I particularly valued: firstly, the excellent review of recent works, which made me realize how many things I have been missing myself.  Secondly, the \"community service\" aspect of helping someone who starts figure out the \"coordinate system\" for deep architectures - this could potentially be more important than introducing yet-another trick of the trade, as most other submissions may do.\n\nHowever I think this work is still half-done, and even though working on this project is a great idea, the authors do not yet do it properly. \n\nFirstly, I am not too sure how the choice of these 14 patterns was made. Maxout for instance (pattern 14) is one of the many nonlinearities (PreLU, ReLU, ...) and I do not see how it stands on the same grounds as something as general as \"3 Strive for simplicity\".\n\nSimilarly some of the patterns are as vague as \"Increase symmetry\" and are backed up by statements such as \"we noted a special degree of elegance in the FractalNet\". I do not see how this leads to a design pattern that can be applied to a new architecture - or if it applies to anything other than the FractalNet. \n\nSome other patterns are phrased with weird names \"7 Cover the problem space\" - which I guess stands for dataset augmentation; or \"6 over-train\" which is not backed up by a single reference. Unless the authors relate it to regularization (text preceding \"overtrain\"), which then has no connection to the description of \"over-train\" provided by the authors (\"training a network on a harder problem to improve generalization\"). If \"harder problem\" means one where one adds an additional term (i.e. the regularizer), the authors are doing harm to the unexperienced reader, confusing \"regularization\" with something that sounds like \"overfitting\" (i.e. the exact opposite).\n\nFurthermore, the extensions proposed in Section 4 seem a bit off tune - in particular I could not figure out \n-how the Taylor Series networks stem from any of the design patterns proposed in the rest of the paper. \n-whether the text between 4.1 and 4.1.1 is another of the architecture innovations (and if yes, why it is not in the 4.1.2, or 4.1.0) \n-and, most importantly, how these design patterns would be deployed in practice to think of a new network. \n\nTo be more concrete, the authors mention that they propose the \"freeze-drop-path\" variant from \"symmetry considerations\" to \"drop-path\". \nIs this an application of the \"increase symmetry\" pattern? How would \"freeze-drop-path\" be more symmetric that \"drop-path\"?\n Can this be expressed concretely, or is it some intuitive guess? If the second, it is not really part of applying a pattern, in my understanding. If the first, this is missing. \n\n\nWhat I would have appreciated more (and would like to see in a revised version) would have been a table of \"design patterns\" on one axis, \"Deep network\" on another, and a breakdown of which network applies which design pattern. \n\nA big part of the previous work is also covered in cryptic language - some minimal explanation of what is taking place in the alternative works would be useful. \n\n\n  \n", "IS_META_REVIEW": false, "RECOMMENDATION": 4, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 3}, {"DATE": "03 Dec 2016", "TITLE": "Experimental Results", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3"}, {"DATE": "02 Dec 2016", "TITLE": "a few questions", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2"}]}
{"text": "INCREMENTAL NETWORK QUANTIZATION: TOWARDS LOSSLESS CNNS WITH LOW-PRECISION WEIGHTS\n1 INTRODUCTION\nDeep convolutional neural networks (CNNs) have demonstrated record breaking results on a variety of computer vision tasks such as image classification (Krizhevsky et al., 2012; Simonyan & Zisserman, 2015), face recognition (Taigman et al., 2014; Sun et al., 2014), semantic segmentation (Long et al., 2015; Chen et al., 2015a) and object detection (Girshick, 2015; Ren et al., 2015). Regardless of the availability of significantly improved training resources such as abundant annotated data, powerful computational platforms and diverse training frameworks, the promising results of deep CNNs are mainly attributed to the large number of learnable parameters, ranging from tens of millions to even hundreds of millions. Recent progress further shows clear evidence that CNNs could easily enjoy the accuracy gain from the increased network depth and width (He et al., 2016; Szegedy et al., 2015; 2016). However, this in turn lays heavy burdens on the memory and other\n\u2217This work was done when Aojun Zhou was an intern at Intel Labs China, supervised by Anbang Yao who proposed the original idea and is responsible for correspondence. The first three authors contributed equally to the writing of the paper.\n1This notation applies to our method throughout the paper.\ncomputational resources. For instance, ResNet-152, a specific instance of the latest residual network architecture wining ImageNet classification challenge in 2015, has a model size of about 230 MB and needs to perform about 11.3 billion FLOPs to classify a 224\u00d7 224 image crop. Therefore, it is very challenging to deploy deep CNNs on the devices with limited computation and power budgets.\nSubstantial efforts have been made to the speed-up and compression on CNNs during training, feedforward test or both of them. Among existing methods, the category of network quantization methods attracts great attention from researches and developers. Some network quantization works try to compress pre-trained full-precision CNN models directly. Gong et al. (2014) address the storage problem of AlexNet (Krizhevsky et al., 2012) with vector quantization techniques. By replacing the weights in each of the three fully connected layers with respective floating-point centroid values obtained from the clustering, they can get over 20\u00d7 model compression at about 1% loss in top-5 recognition rate. HashedNet (Chen et al., 2015b) uses a hash function to randomly map pre-trained weights into hash buckets, and all the weights in the same hash bucket are constrained to share a single floating-point value. In HashedNet, only the fully connected layers of several shallow CNN models are considered. For better compression, Han et al. (2016) present deep compression method which combines the pruning (Han et al., 2015), vector quantization and Huffman coding, and reduce the model storage by 35\u00d7 on AlexNet and 49\u00d7 on VGG-16 (Simonyan & Zisserman, 2015). Vanhoucke et al. (2011) use an SSE 8-bit fixed-point implementation to improve the computation of neural networks on the modern Intel x86 CPUs in feed-forward test, yielding 3\u00d7 speed-up over an optimized floating-point baseline. Training CNNs by substituting the 32-bit floating-point representation with the 16-bit fixed-point representation has also been explored in Gupta et al. (2015). Other seminal works attempt to restrict CNNs into low-precision versions during training phase. Soudry et al. (2014) propose expectation backpropagation (EBP) to estimate the posterior distribution of deterministic network weights. With EBP, the network weights can be constrained to +1 and -1 during feed-forward test in a probabilistic way. BinaryConnect (Courbariaux et al., 2015) further extends the idea behind EBP to binarize network weights during training phase directly. It has two versions of network weights: floating-point and binary. The floating-point version is used as the reference for weight binarization. BinaryConnect achieves state-of-the-art accuracy using shallow CNNs for small datasets such as MNIST (LeCun et al., 1998) and CIFAR-10. Later on, a series of efforts have been invested to train CNNs with low-precision weights, low-precision activations and even low-precision gradients, including but not limited to BinaryNet (Courbariaux et al., 2016), XNOR-Net (Rastegari et al., 2016), ternary weight network (TWN) (Li & Liu, 2016), DoReFa-Net (Zhou et al., 2016) and quantized neural network (QNN) (Hubara et al., 2016).\nDespite these tremendous advances, CNN quantization still remains an open problem due to two critical issues which have not been well resolved yet, especially under scenarios of using low-precision weights for quantization. The first issue is the non-negligible accuracy loss for CNN quantization methods, and the other issue is the increased number of training iterations for ensuring convergence. In this paper, we attempt to address these two issues by presenting a novel incremental network quantization (INQ) method.\nIn our INQ, there is no assumption on the CNN architecture, and its basic goal is to efficiently convert any pre-trained full-precision (i.e., 32-bit floating-point) CNN model into a low-precision version whose weights are constrained to be either powers of two or zero. The advantage of such kind of low-precision models is that the original floating-point multiplication operations can be replaced by cheaper binary bit shift operations on dedicated hardware like FPGA. We noticed that most existing network quantization methods adopt a global strategy in which all the weights are simultaneously converted to low-precision ones (that are usually in the floating-point types). That is, they have not considered the different importance of network weights, leaving the room to retain network accuracy limited. In sharp contrast to existing methods, our INQ makes a very careful handling for the model accuracy drop from network quantization. To be more specific, it incorporates three interdependent operations: weight partition, group-wise quantization and re-training. Weight partition uses a pruning-inspired measure (Han et al., 2015; Guo et al., 2016) to divide the weights in each layer of a pre-trained full-precision CNN model into two disjoint groups which play complementary roles in our INQ. The weights in the first group are quantized to be either powers of two or zero by a variable-length encoding method, forming a low-precision base for the original model. The weights in the other group are re-trained while keeping the quantized weights fixed, compensating for the accuracy loss resulted from the quantization. Furthermore, these three operations are repeated on the\nlatest re-trained weight group in an iterative manner until all the weights are quantized, acting as an incremental network quantization and accuracy enhancement procedure (as illustrated in Figure 1).\nThe main insight of our INQ is that a compact combination of the proposed weight partition, groupwise quantization and re-training operations has the potential to get a lossless low-precision CNN model from any full-precision reference. We conduct extensive experiments on the ImageNet large scale classification task using almost all known deep CNN architectures to validate the effectiveness of our method. We show that: (1) For AlexNet, VGG-16, GoogleNet and ResNets with 5-bit quantization, INQ achieves improved accuracy in comparison with their respective full-precision baselines. The absolute top-1 accuracy gain ranges from 0.13% to 2.28%, and the absolute top-5 accuracy gain is in the range of 0.23% to 1.65%. (2) INQ has the property of easy convergence in training. In general, re-training with less than 8 epochs could consistently generate a lossless model with 5-bit weights in the experiments. (3) Taking ResNet-18 as an example, our quantized models with 4-bit, 3-bit and 2-bit ternary weights also have improved or very similar accuracy compared with its 32-bit floating-point baseline. (4) Taking AlexNet as an example, the combination of our network pruning and INQ outperforms deep compression method (Han et al., 2016) with significant margins.\n2 INCREMENTAL NETWORK QUANTIZATION\nIn this section, we clarify the insight of our INQ, describe its key components, and detail its implementation.\n2.1 WEIGHT QUANTIZATION WITH VARIABLE-LENGTH ENCODING\nSuppose a pre-trained full-precision (i.e., 32-bit floating-point) CNN model can be represented by {Wl : 1 \u2264 l \u2264 L}, where Wl denotes the weight set of the lth layer, and L denotes the number of learnable layers in the model. To simplify the explanation, we only consider convolutional layers and fully connected layers. For CNN models like AlexNet, VGG-16, GoogleNet and ResNets as tested in this paper, Wl can be a 4D tensor for the convolutional layer, or a 2D matrix for the fully connected layer. For simplicity, here the dimension difference is not considered in the expression. Given a pre-trained full-precision CNN model, the main goal of our INQ is to convert all 32-bit floating-point weights to be either powers of two or zero without loss of model accuracy. Besides, we also attempt to explore the limit of the expected bit-width under the premise of guaranteeing lossless network quantization. Here, we start with our basic network quantization method on how to\nconvert Wl to be a low-precision version W\u0302l, and each of its entries is chosen from\nPl = {\u00b12n1 , \u00b7 \u00b7 \u00b7 ,\u00b12n2 , 0}, (1)\nwhere n1 and n2 are two integer numbers, and they satisfy n2 \u2264 n1. Mathematically, n1 and n2 help to bound Pl in the sense that its non-zero elements are constrained to be in the range of either [\u22122n1 ,\u22122n2 ] or [2n2 , 2n1 ]. That is, network weights with absolute values smaller than 2n2 will be pruned away (i.e., set to zero) in the final low-precision model. Obviously, the problem is how to determine n1 and n2. In our INQ, the expected bit-width b for storing the indices in Pl is set beforehand, thus the only hyper-parameter shall be determined is n1 because n2 can be naturally computed once b and n1 are available. Here, n1 is calculated by using a tricky yet practically effective formula as\nn1 = floor(log2(4s/3)), (2)\nwhere floor(\u00b7) indicates the round down operation and s is calculated by using\ns = max(abs(Wl)), (3)\nwhere abs(\u00b7) is an element-wise operation and max(\u00b7) outputs the largest element of its input. In fact, Equation (2) helps to match the rounding power of 2 for s, and it could be easily implemented in practical programming. After n1 is obtained, n2 can be naturally determined as n2 = n1 + 1 \u2212 2(b\u22121)/2. For instance, if b = 3 and n1 = \u22121, it is easy to get n2 = \u22122. Once Pl is determined, we further use the ladder of powers to convert every entry of Wl into a low-precision one by using\nW\u0302l(i, j) = { \u03b2sgn(Wl(i, j)) if (\u03b1+ \u03b2)/2 \u2264 abs(Wl(i, j)) < 3\u03b2/2 0 otherwise,\n(4)\nwhere \u03b1 and \u03b2 are two adjacent elements in the sorted Pl, making the above equation as a numerical rounding to the quantum values. It should be emphasized that factor 4/3 in Equation (2) is set to make sure that all the elements in Pl correspond with the quantization rule defined in Equation (4). In other words, factor 4/3 in Equation (2) highly correlates with factor 3/2 in Equation (4).\nHere, an important thing we want to clarify is the definition of the expected bit-width b. Taking 5-bit quantization as an example, since zero value cannot be written as the power of two, we use 1 bit to represent zero value, and the remaining 4 bits to represent at most 16 different values for the powers of two. That is, the number of candidate quantum values is at most 2b\u22121 + 1, so our quantization method actually adopts a variable-length encoding scheme. It is clear that the quantization described above is performed in a linear scale. An alternative solution is to perform the quantization in the log scale. Although it may also be effective, it should be a little bit more difficult in implementation and may cause some extra computational overhead in comparison to our method.\n2.2 INCREMENTAL QUANTIZATION STRATEGY\nWe can naturally use the above described method to quantize any pre-trained full-precision CNN model. However, noticeable accuracy loss appeared in the experiments when using small bit-width values (e.g., 5-bit, 4-bit, 3-bit and 2-bit).\nIn the literature, there are many existing network quantization works such as HashedNet (Chen et al., 2015b), vector quantization (Gong et al., 2014), fixed-point representation (Vanhoucke et al., 2011; Gupta et al., 2015), BinaryConnect (Courbariaux et al., 2015), BinaryNet (Courbariaux et al., 2016), XNOR-Net (Rastegari et al., 2016), TWN (Li & Liu, 2016), DoReFa-Net (Zhou et al., 2016) and QNN (Hubara et al., 2016). Similar to our basic network quantization method, they also suffer from non-negligible accuracy loss on deep CNNs, especially when being applied on the ImageNet large scale classification dataset. For all these methods, a common fact is that they adopt a global strategy in which all the weights are simultaneously converted into low-precision ones, which in turn causes accuracy loss. Compared with the methods focusing on the pre-trained models, accuracy loss becomes worse for the methods such as XNOR-Net, TWN, DoReFa-Net and QNN which intend to train low-precision CNNs from scratch.\nRecall that our main goal is to achieve lossless low-precision quantization for any pre-trained fullprecision CNN model with no assumption on its architecture. To this end, our INQ makes a special\nhandling of the strategy for suppressing resulting quantization loss in model accuracy. We are partially inspired by the latest progress in network pruning (Han et al., 2015; Guo et al., 2016). In these methods, the accuracy loss from removing less important network weights of a pre-trained neural network model could be well compensated by following re-training steps. Therefore, we conjecture that the nature of changing network weight importance is critical to achieve lossless network quantization.\nBase on this assumption, we present INQ which incorporates three interdependent operations: weight partition, group-wise quantization and re-training. Weight partition is to divide the weights in each layer of a pre-trained full-precision CNN model into two disjoint groups which play complementary roles in our INQ. The weights in the first group are responsible for forming a low-precision base for the original model, thus they are quantized by using Equation (4). The weights in the second group adapt to compensate for the loss in model accuracy, thus they are the ones to be re-trained. Once the first run of the quantization and re-training operations is finished, all the three operations are further conducted on the second weight group in an iterative manner, until all the weights are converted to be either powers of two or zero, acting as an incremental network quantization and accuracy enhancement procedure. As a result, accuracy loss under low-precision CNN quantization can be well suppressed by our INQ. Illustrative results at iterative steps of our INQ are provided in Figure 2.\nFor the lth layer, weight partition can be defined as\nA (1) l \u222aA (2) l = {Wl(i, j)}, and A (1) l \u2229A (2) l = \u2205, (5)\nwhere A(1)l denotes the first weight group that needs to be quantized, and A2 denotes the other weight group that needs to be re-trained. We leave the strategies for group partition to be chosen in the experiment section. Here, we define a binary matrix Tl to help distinguish above two categories of weights. That is, Tl(i, j) = 0 means Wl(i, j) \u2208 A(1)l , and Tl(i, j) = 1 means Wl(i, j) \u2208 A(2)l .\n2.3 INCREMENTAL NETWORK QUANTIZATION ALGORITHM\nNow, we come to the training method. Taking the lth layer as an example, the basic optimization problem of making its weights to be either powers of two or zero can be expressed as\nmin Wl E(Wl) = L(Wl) + \u03bbR(Wl)\ns.t. Wl(i, j) \u2208 Pl, 1 \u2264 l \u2264 L, (6)\nwhere L(Wl) is the network loss, R(Wl) is the regularization term, \u03bb is a positive coefficient, and the constraint term indicates each weight entry Wl(i, j) should be chosen from the set Pl consisting of a fixed number of the values of powers of two plus zero. Direct solving above optimization problem in training from scratch is challenging since it is very easy to undergo convergence problem.\nBy performing weight partition and group-wise quantization operations beforehand, the optimization problem defined in (6) can be reshaped into a easier version. That is, we only need to optimize the following objective function\nmin Wl E(Wl) = L(Wl) + \u03bbR(Wl)\ns.t. Wl(i, j) \u2208 Pl, if Tl(i, j) = 0, 1 \u2264 l \u2264 L, (7)\nwhere Pl is determined at group-wise quantization operation, and the binary matrix Tl acts as a mask which is determined by weight partition operation. Since Pl and Tl are known, the optimization problem (7) can be solved using popular stochastic gradient decent (SGD) method. That is, in INQ, we can get the update scheme for the re-training as\nWl(i, j)\u2190Wl(i, j)\u2212 \u03b3 \u2202E\n\u2202(Wl(i, j)) Tl(i, j), (8)\nwhere \u03b3 is a positive learning rate. Note that the binary matrix Tl forces zero update to the weights that have been quantized. That is, only the weights still keep with floating-point values are updated, akin to the latest pruning methods (Han et al., 2015; Guo et al., 2016) in which only the weights that are not currently removed are re-trained to enhance network accuracy. The whole procedure of our INQ is summarized as Algorithm 1.\nWe would like to highlight that the merits of our INQ are in three aspects: (1) Weight partition introduces the importance-aware weight quantization. (2) Group-wise weight quantization introduces much less accuracy loss than simultaneously quantizing all the network weights, thus making retraining have larger room to recover model accuracy. (3) By integrating the operations of weight partition, group-wise quantization and re-training into a nested loop, our INQ has the potential to obtain lossless low-precision CNN model from the pre-trained full-precision reference.\nAlgorithm 1 Incremental network quantization for lossless CNNs with low-precision weights. Input: X: the training data, {Wl : 1 \u2264 l \u2264 L}: the pre-trained full-precision CNN model, {\u03c31, \u03c32, \u00b7 \u00b7 \u00b7 , \u03c3N}: the accumulated portions of weights quantized at iterative steps Output: {W\u0302l : 1 \u2264 l \u2264 L}: the final low-precision model with the weights constrained to be either powers of two or zero\n1: Initialize A(1)l \u2190 \u2205, A (2) l \u2190 {Wl(i, j)}, Tl \u2190 1, for 1 \u2264 l \u2264 L 2: for n = 1, 2, . . . , N do 3: Reset the base learning rate and the learning policy 4: According to \u03c3n, perform layer-wise weight partition and update A (1) l , A (2) l and Tl 5: Based on A(1)l , determine Pl layer-wisely 6: Quantize the weights in A(1)l by Equation (4) layer-wisely 7: Calculate feed-forward loss, and update weights in {A(2)l : 1 \u2264 l \u2264 L} by Equation (8) 8: end for\n3 EXPERIMENTAL RESULTS\nTo analyze the performance of our INQ, we perform extensive experiments on the ImageNet large scale classification task, which is known as the most challenging image classification benchmark so far. ImageNet dataset has about 1.2 million training images and 50 thousand validation images. Each image is annotated as one of 1000 object classes. We apply our INQ to AlexNet, VGG-16, GoogleNet, ResNet-18 and ResNet-50, covering almost all known deep CNN architectures. Using the center crops of validation images, we report the results with two standard measures: top-1 error rate and top-5 error rate. For fair comparison, all pre-trained full-precision (i.e., 32-bit floatingpoint) CNN models except ResNet-18 are taken from the Caffe model zoo2. Note that He et al. (2016) do not release their pre-trained ResNet-18 model to the public, so we use a publicly available re-implementation by Facebook3. Since our method is implemented with Caffe, we make use of an open source tool4 to convert the pre-trained ResNet-18 model from Torch to Caffe.\n3.1 RESULTS ON IMAGENET\nSetting expected bit-width to 5, the first set of experiments is performed to testify the efficacy of our INQ on different CNN architectures. Regarding weight partition, there are several candidate strategies as we tried in our previous work for efficient network pruning (Guo et al., 2016). In Guo et al. (2016), we found random partition and pruning-inspired partition are the two best choices compared with the others. Thus in this paper, we directly compare these two strategies for weight partition. In random strategy, the weights in each layer of any pre-trained full-precision deep CNN model are randomly split into two disjoint groups. In pruning-inspired strategy, the weights are divided into two disjoint groups by comparing their absolute values with layer-wise thresholds which are automatically determined by a given splitting ratio. Here we directly use pruning-inspired strategy and the experimental results in Section 3.2 will show why. After the re-training with no more than 8 epochs over each pre-trained full-precision model, we obtain the results as shown in Table 1. It can be concluded that the 5-bit CNN models generated by our INQ show consistently improved top-1 and top-5 recognition rates compared with respective full-precision references. Parameter settings are described below.\nAlexNet: AlexNet has 5 convolutional layers and 3 fully-connected layers. We set the accumulated portions of quantized weights at iterative steps as {0.3, 0.6, 0.8, 1}, the batch size as 256, the weight decay as 0.0005, and the momentum as 0.9.\nVGG-16: Compared with AlexNet, VGG-16 has 13 convolutional layers and more parameters. We set the accumulated portions of quantized weights at iterative steps as {0.5, 0.75, 0.875, 1}, the batch size as 32, the weight decay as 0.0005, and the momentum as 0.9.\n2https://github.com/BVLC/caffe/wiki/Model-Zoo 3https://github.com/facebook/fb.resnet.torch/tree/master/pretrained 4https://github.com/zhanghang1989/fb-caffe-exts\nGoogleNet: Compared with AlexNet and VGG-16, GoogleNet is more difficult to quantize due to a smaller number of parameters and the increased network width. We set the accumulated portions of quantized weights at iterative steps as {0.2, 0.4, 0.6, 0.8, 1}, the batch size as 80, the weight decay as 0.0002, and the momentum as 0.9.\nResNet-18: Different from above three networks, ResNets have batch normalization layers and relief the vanishing gradient problem by using shortcut connections. We first test the 18-layer version for exploratory purpose and test the 50-layer version later on. The network architectures of ResNet18 and ResNet-34 are very similar. The only difference is the number of filters in every convolutional layer. We set the accumulated portions of quantized weights at iterative steps as {0.5, 0.75, 0.875, 1}, the batch size as 80, the weight decay as 0.0005, and the momentum as 0.9. ResNet-50: Besides significantly increased network depth, ResNet-50 has a more complex network architecture in comparison to ResNet-18. However, regarding network architecture, ResNet-50 is very similar to ResNet-101 and ResNet-152. The only difference is the number of filters in every convolutional layer. We set the accumulated portions of quantized weights at iterative steps as {0.5, 0.75, 0.875, 1}, the batch size as 32, the weight decay as 0.0005, and the momentum as 0.9.\n3.2 ANALYSIS OF WEIGHT PARTITION STRATEGIES\nIn our INQ, the first operation is weight partition whose result will directly affect the following group-wise quantization and re-training operations. Therefore, the second set of experiments is conducted to analyze two candidate strategies for weight partition. As mentioned in the previous section, we use pruning-inspired strategy for weight partition. Unlike random strategy in which all the weights have equal probability to fall into the two disjoint groups, pruning-inspired strategy considers that the weights with larger absolute values are more important than the smaller ones to form a low-precision base for the original CNN model. We use ResNet-18 as a test case to compare the performance of these two strategies. In the experiments, the parameter settings are completely the same as described in Section 3.1. We set 4 epochs for weight re-training. Table 2 summarizes the results of our INQ with 5-bit quantization. It can be seen that our INQ achieves top-1 error rate of 32.11% and top-5 error rate of 11.73% by using random partition. Comparatively, pruning-inspired partition brings 1.09% and 0.83% decrease in top-1 and top-5 error rates, respectively. Apparently, pruning-inspired partition is better than random partition, and this is the reason why we use it in this paper. For future works, weight partition based on quantization error could also be an option worth exploring.\n3.3 THE TRADE-OFF BETWEEN EXPECTED BIT-WIDTH AND MODEL ACCURACY\nThe third set of experiments is performed to explore the limit of the expected bit-width under which our INQ can still achieve lossless network quantization. Similar to the second set of experiments, we also use ResNet-18 as a test case, and the parameter settings for the batch size, the weight decay and the momentum are completely the same. Finally, lower-precision models with 4-bit, 3-bit and even 2-bit ternary weights are generated for comparisons. As the expected bit-width goes down, the number of candidate quantum values will be decreased significantly, thus we shall increase the number of iterative steps accordingly for enhancing the accuracy of final low-precision model. Specifically, we set the accumulated portions of quantized weights at iterative steps as {0.3, 0.5, 0.8, 0.9, 0.95, 1}, {0.2, 0.4, 0.6, 0.7, 0.8, 0.9, 0.95, 1} and {0.2, 0.4, 0.6, 0.7, 0.8, 0.85, 0.9, 0.95, 0.975, 1} for 4-bit, 3-bit and 2-bit ternary models, respectively. The required number of epochs also increases when the expected bit-width goes down, and it reaches 30 when training our 2-bit ternary model. Although our 4-bit model shows slightly decreased accuracy when compared with the 5-bit model, its accuracy is still better than that of the pre-trained full-precision model. Comparatively, even when the expected bit-width goes down to 3, our low-precision model shows only 0.19% and\n0.33% losses in top-1 and top-5 recognition rates, respectively. As for our 2-bit ternary model, although it incurs 2.25% decrease in top-1 error rate and 1.56% decrease in top-5 error rate in comparison to the pre-trained full-precision reference, its accuracy is considerably better than stateof-the-art results reported for binary-weight network (BWN) (Rastegari et al., 2016) and ternary weight network (TWN) (Li & Liu, 2016). Detailed results are summarized in Table 3 and Table 4.\n3.4 LOW-BIT DEEP COMPRESSION\nIn the literature, recently proposed deep compression method (Han et al., 2016) reports so far best results on network compression without loss of model accuracy. Therefore, the last set of experiments is conducted to explore the potential of our INQ for much better deep compression. Note that Han et al. (2016) is a hybrid network compression solution combining three different techniques, namely network pruning (Han et al., 2015), vector quantization (Gong et al., 2014) and Huffman coding. Taking AlexNet as an example, network pruning gets 9\u00d7 compression, however this result is mainly obtained from the fully connected layers. Actually its compression performance on the convolutional layers is less than 3\u00d7 (as can be seen in the Table 4 of Han et al. (2016)). Besides, network pruning is realized by separately performing pruning and re-training in an iterative way, which is very time-consuming. It will cost at least several weeks for compressing AlexNet. We solved this problem by our dynamic network surgery (DNS) method (Guo et al., 2016) which achieves about 7\u00d7 speed-up in training and improves the performance of network pruning from 9\u00d7 to 17.7\u00d7. In Han et al. (2016), after network pruning, vector quantization further improves compression ratio from 9\u00d7 to 27\u00d7, and Huffman coding finally boosts compression ratio up to 35\u00d7. For fair comparison, we combine our proposed INQ and DNS, and compare the resulting method with Han et al. (2016). Detailed results are summarized in Table 5. When combing our proposed INQ and DNS, we achieve much better compression results compared with Han et al. (2016). Specifically, with 5-bit quantization, we can achieve 53\u00d7 compression with slightly larger gains both in top-5 and top-1 recognition rates, yielding 51.43%/96.30% absolute improvement in compression performance compared with full version/fair version (i.e., the combination of network pruning and vector quantization) of Han et al. (2016), respectively. Consistently better results have also obtained for our 4-bit and 3-bit models.\nBesides, we also perform a set of experiments on AlexNet to compare the performance of our INQ and vector quantization (Gong et al., 2014). For fair comparison, re-training is also used to enhance the performance of vector quantization, and we set the number of cluster centers for all of 5 convolutional layers and 3 fully connect layers to 32 (i.e., 5-bit quantization). In the experiment, vector quantization incurs over 3% loss in model accuracy. When we change the number of cluster centers for convolutional layers from 32 to 128, it gets an accuracy loss of 0.98%. This is consistent with the results reported in (Gong et al., 2014). Comparatively, vector quantization is mainly proposed\nto compress the parameters in the fully connected layers of a pre-trained full-precision CNN model, while our INQ addresses all network layers simultaneously and has no accuracy loss for 5-bit and 4-bit quantization. Therefore, it is evident that our INQ is much better than vector quantization. Last but not least, the final weights for vector quantization (Gong et al., 2014), network pruning (Han et al., 2015) and deep compression (Han et al., 2016) are still floating-point values, but the final weights for our INQ are in the form of either powers of two or zero. The direct advantage of our INQ is that the original floating-point multiplication operations can be replaced by cheaper binary bit shift operations on dedicated hardware like FPGA.\n4 CONCLUSIONS\nIn this paper, we present INQ, a new network quantization method, to address the problem of how to convert any pre-trained full-precision (i.e., 32-bit floating-point) CNN model into a lossless lowprecision version whose weights are constrained to be either powers of two or zero. Unlike existing methods which usually quantize all the network weights simultaneously, INQ is a more compact quantization framework. It incorporates three interdependent operations: weight partition, groupwise quantization and re-training. Weight partition splits the weights in each layer of a pre-trained full-precision CNN model into two disjoint groups which play complementary roles in INQ. The weights in the first group is directly quantized by a variable-length encoding method, forming a low-precision base for the original CNN model. The weights in the other group are re-trained while keeping all the quantized weights fixed, compensating for the accuracy loss from network quantization. More importantly, the operations of weight partition, group-wise quantization and re-training are repeated on the latest re-trained weight group in an iterative manner until all the weights are quantized, acting as an incremental network quantization and accuracy enhancement procedure. On the ImageNet large scale classification task, we conduct extensive experiments and show that our quantized CNN models with 5-bit, 4-bit, 3-bit and even 2-bit ternary weights have improved or at least comparable accuracy against their full-precision baselines, including AlexNet, VGG-16, GoogleNet and ResNets. As for future works, we plan to extend incremental idea behind INQ from low-precision weights to low-precision activations and low-precision gradients (we have actually already made some good progress on it, as shown in our supplementary materials). We will also investigate computation and power efficiency by implementing our low-precision CNN models on hardware platforms.\nA APPENDIX 1: STATISTICAL ANALYSIS OF THE QUANTIZED WEIGHTS\nTaking our 5-bit AlexNet model as an example, we analyze the distribution of the quantized weights. Detailed statistical results are summarized in Table 6. We can find: (1) in the 1st and 2nd convolutional layers, the values of {\u22122\u22126, \u22122\u22125, \u22122\u22124, 2\u22126, 2\u22125, 2\u22124} and {\u22122\u22128, \u22122\u22127, \u22122\u22126, \u22122\u22125, 0, 2\u22128, 2\u22127, 2\u22126, 2\u22125} occupy over 60% and 94% of all quantized weights, respectively; (2) the distributions of the quantized weights in the 3rd, 4th and 5th convolutional layers are similar to that of the 2nd convolutional layer, and more weights are quantized into zero in the 2nd, 3rd, 4th and 5th convolutional layers compared with the 1st convolutional layer; (3) in the 1st fully connected layer, the values of {\u22122\u221210, \u22122\u22129, \u22122\u22128, \u22122\u22127, 0, 2\u221210, 2\u22129, 2\u22128, 2\u22127} occupy about 98% of all quantized weights, and similar results can be seen for the 2nd fully connected layer; (4) generally, the distributions of the quantized weights in the convolutional layers are usually more scattered compared with the fully connected layers. This may be partially the reason why it is much easier to get good compression performance on fully connected layers in comparison to convolutional layers, when using methods such as network hashing (Chen et al., 2015b) and vector quantization (Gong et al., 2014); (5) for 5-bit AlexNet model, the required bit-width for each layer is actually 4 but not 5.\nB APPENDIX 2: LOSSLESS CNNS WITH LOW-PRECISION WEIGHTS AND LOW-PRECISION ACTIVATIONS\nRecently, we have made some good progress on developing our INQ for lossless CNNs with both low-precision weights and low-precision activations. According to the results summarized in Table 7, it can be seen that our VGG-16 model with 5-bit weights and 4-bit activations shows improved top-5 and top-1 recognition rates in comparison to the pre-trained reference with 32-bit floating-point weights and 32-bit floating-point activations. To the best of our knowledge, this should be the best results reported on VGG-16 architecture so far.\n", "cats": {"READ": 1.0, "DONTREAD": 0.0}, "reviews": [{"IS_META_REVIEW": true, "comments": "Nice idea but not complete, model size is not reduced by the large factors found in one of your references (Song 2016), where they go to 5 bits, but this is ontop of pruning which gives overall 49X reduction in model size of VGG (without loss of accuracy). You may achieve similar reductions with inclusion of pruning (or better since you go to 4 bits with no loss) but we should see this in the paper, so at the moment it is difficult to compare"}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "The paper presents a method for iterative quantization of neural networks weights to powers of 2. The technique is simple, but novel and effective, with thorough evaluation on a variety of ImageNet classification models.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"DATE": "14 Jan 2017 (modified: 16 Jan 2017)", "TITLE": "Paper update: all required result comparisons have been added!", "IS_META_REVIEW": false, "comments": "Thanks to all the reviewers for constructive suggestions and comments. We are really excited that the novelty of our paper has been well recognized.\n\nIn this updated version, we carefully considered all reviewers\u2019 suggestions to improve the paper. Generally, we performed four aspects of works: (1) the result comparison of pruning + quantization between our method and Han et al.\u2019s method [1] was incorporated into the paper (please see Section 3.4 for details); (2) the result comparison of weight quantization between our method and vector quantization [2] was also incorporated into the paper (please see Section 3.4 for details); (3) we tried our best to improve the clarifications of our encoding method for weight quantization, definition of bit-width, detailed experimental settings and so on, and several rounds of proof-reading and revising were also conducted; (4) more experimental results (including the statistical analyses on the distribution of weights after quantization and our latest progress on developing INQ for deep CNNs with low-precision weights and low-precision activations) that reviewers may be interested were added to the paper as the supplementary materials.\n \nMoreover, to make our work fully reproducible, the code (along with an instruction manual) will be released to public as we promised in the paper submission.\n\n(1) To reviewer 1:\n\nQuestion1: \u201cAlso, the description of the pruning-inspired partitioning strategy could be clarified somewhat... e.g., the chosen splitting ratio of 50% only seems to be referenced in a figure caption and not the main text.\u201d\n\nFollowing your suggestion, we added detailed parameter settings (such as splitting ratio and etc.) to the respective sets of experiments described in Section 3 accordingly.\n\nQuestion 2: \u201cThe paper could use another second pass for writing style and grammar.\u201d\n\nFollowing your suggestion, we tried our best to do a much better work on revising and proof-reading, with the helps from the native colleagues in USA.\n\n(2) To reviewer 2:\n\nThanks for your recognition of the novelty of our method. We believe that our responses posted on Dec. 16, 2016 should well address your concern on the result comparison of pruning + quantization between our method and Han et al.\u2019s method [1]. Furthermore, detailed result comparisons can be found in Section 3.4 of the paper. It can be clearly seen that our method outperforms Han et al.\u2019s method [1] with significant margins.\n\n(3) To reviewer 3:\n\nQuestion 1: \u201c1) It would be good to incorporate some of the answers into the paper, mainly the results with pruning + this method as that can be compared fairly to Han et al. and outperforms it.\u201d\n\nFollowing your suggestion, we incorporated related results into the paper (please see Section 3.4 for details).\n\nQuestion 2: \u201cIt would be good to better explain the encoding method (my question 4) as it is not that clear from the paper (e.g. made me make a mistake in question 5 for the computation of n2).\u201d\n\nFollowing your suggestion, we revised related parts, especially the clarification of our encoding method based on our previous responses to your questions accordingly (please see Section 2.1 for details).\n\nQuestion 3: \"The \"5 bits\" is misleading as in fact what is used is variable length encoding (which is on average close to 5 bits) where: - 0 is represented with 1 bit, e.g. 0 - other values are represented with 5 bits, where the first bit is needed to distinguish from 0, and the remaining 4 bits represent the 16 different values for the powers of 2.\u201d\n\nFollowing your suggestion, we made a clear clarification on the definition of bit-width accordingly.\n\nReferences:\nSong Han, Jeff Pool, John Tran, and William J. Dally. Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding. ICLR, 2016.\nYunchao Gong, Liu Liu, Ming Yang, and Lubomir Bourdev. Compressing deep convolutional networks using vector quantization. arXiv preprint arXiv:1412.6115v1, 2014.", "OTHER_KEYS": "Aojun Zhou"}, {"TITLE": "Great idea, very impressive results.", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "There is a great deal of ongoing interest in compressing neural network models. One line of work has focused on using low-precision representations of the model weights, even down to 1 or 2 bits. However, so far these approaches have been accompanied by a significant impact on accuracy. The paper proposes an iterative quantization scheme, in which the network weights are quantized in stages---the largest weights (in absolute value) are quantized and fixed, while unquantized weights can adapt to compensate for any resulting error. The experimental results show this is extremely effective, yielding models with 4 bit or 3 bit weights with essentially no reduction in accuracy. While at 2 bits the accuracy decreases slightly, the results are substantially better than those achieved with other quantization approaches.\n\nOverall this paper is clear, the technique is as far as I am aware novel, the experiments are thorough and the results are very compelling, so I recommend acceptance. The paper could use another second pass for writing style and grammar. Also, the description of the pruning-inspired partitioning strategy could be clarified somewhat... e.g., the chosen splitting ratio of 50% only seems to be referenced in a figure caption and not the main text.", "IS_META_REVIEW": false, "RECOMMENDATION": 8, "DATE": "19 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "Reasonable idea", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "The idea of this paper is reasonable - gradually go from original weights to compressed weights by compressing a part of them and fine-tuning the rest. Everything seems fine, results look good, and my questions have been addressed.\n\nTo improve the paper:\n\n1) It would be good to incorporate some of the answers into the paper, mainly the results with pruning + this method as that can be compared fairly to Han et al. and outperforms it.\n\n2) It would be good to better explain the encoding method (my question 4) as it is not that clear from the paper (e.g. made me make a mistake in question 5 for the computation of n2). The \"5 bits\" is misleading as in fact what is used is variable length encoding (which is on average close to 5 bits) where:\n- 0 is represented with 1 bit, e.g. 0\n- other values are represented with 5 bits, where the first bit is needed to distinguish from 0, and the remaining 4 bits represent the 16 different values for the powers of 2.\n", "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "Quantize a fully trained network with an iterative 3 step process of partition/hard quantize/retrain, repeated on the retrained partition until fully quantized. Achieves nice results on ImageNet tasks down to 4 bits, but is missing pruning steps which is needed for large competitive compression.", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "Nice idea but not complete, model size is not reduced by the large factors found in one of your references (Song 2016), where they go to 5 bits, but this is ontop of pruning which gives overall 49X reduction in model size of VGG (without loss of accuracy). You may achieve similar reductions with inclusion of pruning (or better since you go to 4 bits with no loss) but we should see this in the paper, so at the moment it is difficult to compare", "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "16 Dec 2016 (modified: 14 Jan 2017)", "REVIEWER_CONFIDENCE": 3}, {"DATE": "03 Dec 2016", "TITLE": "Questions", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1"}, {"DATE": "02 Dec 2016", "TITLE": "Various", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3"}, {"DATE": "26 Nov 2016 (modified: 26 Jan 2017)", "TITLE": "pre-review questions", "IS_META_REVIEW": false, "comments": "Dear Reviewers,\n\nPlease take a look through the paper and ask the authors to clarify any questions you might have. The deadline for this part of the review process is December 2, 2016.\n\nThanks!", "OTHER_KEYS": "(anonymous)"}, {"IS_META_REVIEW": true, "comments": "Nice idea but not complete, model size is not reduced by the large factors found in one of your references (Song 2016), where they go to 5 bits, but this is ontop of pruning which gives overall 49X reduction in model size of VGG (without loss of accuracy). You may achieve similar reductions with inclusion of pruning (or better since you go to 4 bits with no loss) but we should see this in the paper, so at the moment it is difficult to compare"}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "The paper presents a method for iterative quantization of neural networks weights to powers of 2. The technique is simple, but novel and effective, with thorough evaluation on a variety of ImageNet classification models.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"DATE": "14 Jan 2017 (modified: 16 Jan 2017)", "TITLE": "Paper update: all required result comparisons have been added!", "IS_META_REVIEW": false, "comments": "Thanks to all the reviewers for constructive suggestions and comments. We are really excited that the novelty of our paper has been well recognized.\n\nIn this updated version, we carefully considered all reviewers\u2019 suggestions to improve the paper. Generally, we performed four aspects of works: (1) the result comparison of pruning + quantization between our method and Han et al.\u2019s method [1] was incorporated into the paper (please see Section 3.4 for details); (2) the result comparison of weight quantization between our method and vector quantization [2] was also incorporated into the paper (please see Section 3.4 for details); (3) we tried our best to improve the clarifications of our encoding method for weight quantization, definition of bit-width, detailed experimental settings and so on, and several rounds of proof-reading and revising were also conducted; (4) more experimental results (including the statistical analyses on the distribution of weights after quantization and our latest progress on developing INQ for deep CNNs with low-precision weights and low-precision activations) that reviewers may be interested were added to the paper as the supplementary materials.\n \nMoreover, to make our work fully reproducible, the code (along with an instruction manual) will be released to public as we promised in the paper submission.\n\n(1) To reviewer 1:\n\nQuestion1: \u201cAlso, the description of the pruning-inspired partitioning strategy could be clarified somewhat... e.g., the chosen splitting ratio of 50% only seems to be referenced in a figure caption and not the main text.\u201d\n\nFollowing your suggestion, we added detailed parameter settings (such as splitting ratio and etc.) to the respective sets of experiments described in Section 3 accordingly.\n\nQuestion 2: \u201cThe paper could use another second pass for writing style and grammar.\u201d\n\nFollowing your suggestion, we tried our best to do a much better work on revising and proof-reading, with the helps from the native colleagues in USA.\n\n(2) To reviewer 2:\n\nThanks for your recognition of the novelty of our method. We believe that our responses posted on Dec. 16, 2016 should well address your concern on the result comparison of pruning + quantization between our method and Han et al.\u2019s method [1]. Furthermore, detailed result comparisons can be found in Section 3.4 of the paper. It can be clearly seen that our method outperforms Han et al.\u2019s method [1] with significant margins.\n\n(3) To reviewer 3:\n\nQuestion 1: \u201c1) It would be good to incorporate some of the answers into the paper, mainly the results with pruning + this method as that can be compared fairly to Han et al. and outperforms it.\u201d\n\nFollowing your suggestion, we incorporated related results into the paper (please see Section 3.4 for details).\n\nQuestion 2: \u201cIt would be good to better explain the encoding method (my question 4) as it is not that clear from the paper (e.g. made me make a mistake in question 5 for the computation of n2).\u201d\n\nFollowing your suggestion, we revised related parts, especially the clarification of our encoding method based on our previous responses to your questions accordingly (please see Section 2.1 for details).\n\nQuestion 3: \"The \"5 bits\" is misleading as in fact what is used is variable length encoding (which is on average close to 5 bits) where: - 0 is represented with 1 bit, e.g. 0 - other values are represented with 5 bits, where the first bit is needed to distinguish from 0, and the remaining 4 bits represent the 16 different values for the powers of 2.\u201d\n\nFollowing your suggestion, we made a clear clarification on the definition of bit-width accordingly.\n\nReferences:\nSong Han, Jeff Pool, John Tran, and William J. Dally. Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding. ICLR, 2016.\nYunchao Gong, Liu Liu, Ming Yang, and Lubomir Bourdev. Compressing deep convolutional networks using vector quantization. arXiv preprint arXiv:1412.6115v1, 2014.", "OTHER_KEYS": "Aojun Zhou"}, {"TITLE": "Great idea, very impressive results.", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "There is a great deal of ongoing interest in compressing neural network models. One line of work has focused on using low-precision representations of the model weights, even down to 1 or 2 bits. However, so far these approaches have been accompanied by a significant impact on accuracy. The paper proposes an iterative quantization scheme, in which the network weights are quantized in stages---the largest weights (in absolute value) are quantized and fixed, while unquantized weights can adapt to compensate for any resulting error. The experimental results show this is extremely effective, yielding models with 4 bit or 3 bit weights with essentially no reduction in accuracy. While at 2 bits the accuracy decreases slightly, the results are substantially better than those achieved with other quantization approaches.\n\nOverall this paper is clear, the technique is as far as I am aware novel, the experiments are thorough and the results are very compelling, so I recommend acceptance. The paper could use another second pass for writing style and grammar. Also, the description of the pruning-inspired partitioning strategy could be clarified somewhat... e.g., the chosen splitting ratio of 50% only seems to be referenced in a figure caption and not the main text.", "IS_META_REVIEW": false, "RECOMMENDATION": 8, "DATE": "19 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "Reasonable idea", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "The idea of this paper is reasonable - gradually go from original weights to compressed weights by compressing a part of them and fine-tuning the rest. Everything seems fine, results look good, and my questions have been addressed.\n\nTo improve the paper:\n\n1) It would be good to incorporate some of the answers into the paper, mainly the results with pruning + this method as that can be compared fairly to Han et al. and outperforms it.\n\n2) It would be good to better explain the encoding method (my question 4) as it is not that clear from the paper (e.g. made me make a mistake in question 5 for the computation of n2). The \"5 bits\" is misleading as in fact what is used is variable length encoding (which is on average close to 5 bits) where:\n- 0 is represented with 1 bit, e.g. 0\n- other values are represented with 5 bits, where the first bit is needed to distinguish from 0, and the remaining 4 bits represent the 16 different values for the powers of 2.\n", "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "Quantize a fully trained network with an iterative 3 step process of partition/hard quantize/retrain, repeated on the retrained partition until fully quantized. Achieves nice results on ImageNet tasks down to 4 bits, but is missing pruning steps which is needed for large competitive compression.", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "Nice idea but not complete, model size is not reduced by the large factors found in one of your references (Song 2016), where they go to 5 bits, but this is ontop of pruning which gives overall 49X reduction in model size of VGG (without loss of accuracy). You may achieve similar reductions with inclusion of pruning (or better since you go to 4 bits with no loss) but we should see this in the paper, so at the moment it is difficult to compare", "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "16 Dec 2016 (modified: 14 Jan 2017)", "REVIEWER_CONFIDENCE": 3}, {"DATE": "03 Dec 2016", "TITLE": "Questions", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1"}, {"DATE": "02 Dec 2016", "TITLE": "Various", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3"}, {"DATE": "26 Nov 2016 (modified: 26 Jan 2017)", "TITLE": "pre-review questions", "IS_META_REVIEW": false, "comments": "Dear Reviewers,\n\nPlease take a look through the paper and ask the authors to clarify any questions you might have. The deadline for this part of the review process is December 2, 2016.\n\nThanks!", "OTHER_KEYS": "(anonymous)"}]}
{"text": "Event Linking with Sentential Features from Convolutional Neural Networks\n1 000\n011\n012\n013\n014\n015\n016\n017\n018\n019\n020\n021\n022\n023\n024\n025\n026\n027\n028\n029\n030\n031\n032\n033\n034\n035\n036\n037\n038\n039\n040\n041\n042\n043\n044\n045\n046\n047\n048\n049\n061\n062\n063\n064\n065\n066\n067\n068\n069\n070\n071\n072\n073\n074\n075\n076\n077\n078\n079\n080\n081\n082\n083\n084\n085\n086\n087\n088\n089\n090\n091\n092\n093\n094\n095\n096\n097\n098\n099\n1 Introduction\nEvent extraction aims at detecting mentions of realworld events and their arguments in text documents of different domains, e.g., news articles. The subsequent task of event linking is concerned with resolving coreferences between recognized event mentions in a document, and is the focus of this paper.\nSeveral studies investigate event linking and related problems such as relation mentions spanning multiple sentences. Swampillai and Stevenson (2010) find that 28.5 % of binary relation mentions in the MUC 6 dataset are affected, as are 9.4 % of\nrelation mentions in the ACE corpus from 2003. Ji and Grishman (2011) estimate that 15 % of slot fills in the training data for the \u201cTAC 2010 KBP Slot Filling\u201d task require cross-sentential inference. To confirm these numbers, we analyzed the event annotation of the ACE 2005 corpus and found that approximately 23 % of the event mentions are incomplete on the argument level, with respect to the information in other mentions of the same event instance in the respective document. These numbers suggest that event linking is an important task.\nPrevious approaches for modeling event mentions in context of coreference resolution (Bejan and Harabagiu, 2010; Sangeetha and Arock, 2012; Liu et al., 2014) make either use of external feature sources with limited cross-domain availability like WordNet (Fellbaum, 1998) and FrameNet (Baker et al., 1998), or show low performance. At the same time, recent literature proposes a new kind of feature class for modeling events (and relations) in order to detect mentions and extract their arguments, i.e., sentential features from event-/relationmention representations that have been created by taking the full extent and surrounding sentence of a mention into account (Zeng et al., 2014; Nguyen and Grishman, 2015; Chen et al., 2015; dos Santos et al., 2015; Zeng et al., 2015). Their promising results motivate our work. We propose to use such features for event coreference resolution, hoping to thereby remove the need for extensive external semantic features while preserving the current stateof-the-art performance level.\nOur contributions in this paper are as follows: We design a system for event linking which in a first step models intra-sentential event mentions via the use of convolutional neural networks for the integration of sentential features. In the next step, our model learns to make coreference decisions for pairs of event mentions based on the previously generated representations. This approach does not\n2\n101\n102\n103\n104\n105\n106\n107\n108\n109\n110\n111\n112\n113\n114\n115\n116\n117\n118\n119\n120\n121\n122\n123\n124\n125\n126\n127\n128\n129\n130\n131\n132\n133\n134\n135\n136\n137\n138\n139\n140\n141\n142\n143\n144\n145\n146\n147\n148\n149\n150\n151\n152\n153\n154\n155\n156\n157\n158\n159\n160\n161\n162\n163\n164\n165\n166\n167\n168\n169\n170\n171\n172\n173\n174\n175\n176\n177\n178\n179\n180\n181\n182\n183\n184\n185\n186\n187\n188\n189\n190\n191\n192\n193\n194\n195\n196\n197\n198\n199\nrely on external semantic features, but rather employs a combination of local and sentential features to describe individual event mentions, and combines these intermediate event representations with standard pairwise features for the coreference decision. The model achieves state-of-the-art performance in our experiments on two datasets, one of which is publicly available. Furthermore, we present an analysis of the system errors to identify directions for further research.\n2 Problem definition\nWe follow the notion of events from the ACE 2005 dataset (LDC, 2005; Walker et al., 2006). Consider the following example:\nBritish bank Barclays had agreed to buy Spanish rival Banco Zaragozano for 1.14 billion euros. The combination of the banking operations of Barclays Spain and Zaragozano will bring together two complementary businesses and will happen this year, in contrast to Barclays\u2019 postponed merger with Lloyds.1\nProcessing these sentences in a prototypical, ACE-style information extraction (IE) pipeline would involve (a) the recognition of entity mentions. In the example, mentions of entities are underlined. Next, (b) words in the text are processed as to whether they elicit an event reference, i.e., event triggers are identified and their semantic type is classified. The above sentences contain three event mentions with type Business.MergeOrg, shown in boldface. The task of event extraction further requires that (c) participants of recognized events are determined among the entity mentions in the same sentence, i.e., an event\u2019s arguments are identified and their semantic role wrt. to the event is classified. The three recognized event mentions are:\nE1: buy(British bank Barclays, Spanish rival Banco Zaragozano, 1.14 billion euros) E2: combination(Barclays Spain, Zaragozano, this year) E3: merger(Barclays, Lloyds)\nOften, an IE system involves (d) a disambiguation step of the entity mentions against one another in the same document. This allows to identify that the three mentions of \u201cBarclays\u201d in the text as referring to the same real-world entity. The analogous task on the level of event mentions is called (e) event linking (or: event coreference resolution) and is the focus of this paper. Specifically, the task is\n1Based on an example in (Araki and Mitamura, 2015).\nto determine that E3 is a singleton reference in this example, while E1 and E2 are coreferential, with the consequence that a document-level event instance can be produced from E1 and E2, listing four arguments (two companies, buying price, and acquisition date).\n3 Model design\nThis section first motivates the design decisions of our model for event linking, before going into details about its two-step architecture.\nEvent features from literature So far, a wide range of features has been used for the representation of events and relations for extraction (Zhou et al., 2005; Mintz et al., 2009; Sun et al., 2011) and coreference resolution (Bejan and Harabagiu, 2010; Lee et al., 2012; Liu et al., 2014; Araki and Mitamura, 2015; Cybulska and Vossen, 2015) purposes. The following is an attempt to list the most common classes among them, along with examples:\n\u2022 lexical: surface string, lemma, word embeddings, context around trigger \u2022 syntactic: depth of trigger in parse tree, dependency arcs from/to trigger \u2022 discourse: distance between coreference candidates, absolute position in document \u2022 semantic (intrinsic): comparison of event arguments (entity fillers, present roles), event type of coreference candidates \u2022 semantic (external): coreference-candidates similarity in lexico-semantic resources (WordNet, FrameNet) and other datasets (VerbOcean corpus), enrichtment of arguments with alternative names from external sources (DBpedia, Geonames)\nWhile lexical, discourse, and intrinsic-semantic features are available in virtually all application scenarios of event extraction/linking, and even syntactic parsing is no longer considered an expensive feature source, semantic features from external knowledge sources pose a significant burden on the application of event processing systems, as these sources are created at high cost and come with limited domain coverage.\nFortunately, recent work has explored the use of a new feature class, sentential features, for tackling relation-/event-extraction related tasks with neural networks (Zeng et al., 2014; Nguyen and Grishman, 2015; Chen et al., 2015; dos Santos et al., 2015; Zeng et al., 2015). These approaches have shown that processing sentences with neural models yields representations suitable for IE, which motivates their use in our approach.\n3\n201\n202\n203\n204\n205\n206\n207\n208\n209\n210\n211\n212\n213\n214\n215\n216\n217\n218\n219\n220\n221\n222\n223\n224\n225\n226\n227\n228\n229\n230\n231\n232\n233\n234\n235\n236\n237\n238\n239\n240\n241\n242\n243\n244\n245\n246\n247\n248\n249\n250\n251\n252\n253\n254\n255\n256\n257\n258\n259\n260\n261\n262\n263\n264\n265\n266\n267\n268\n269\n270\n271\n272\n273\n274\n275\n276\n277\n278\n279\n280\n281\n282\n283\n284\n285\n286\n287\n288\n289\n290\n291\n292\n293\n294\n295\n296\n297\n298\n299\n... had agreed to buy Spanish rival ... -3 -2 -1 0 +1 +2\nto buy Spanish\nEmbedding look-up for words and positions Convolution layer Piecewise max-pooling Concat. Hidden layer\nGeneration of event-mention representation \u2026\nSentence-level feature generation\nEmbedding look-up for trigger & context\nTrigger-local feature generation\nModel part (a).\n\u25cf type compatibility \u25cf position in discourse \u25cf realis match \u25cf argument overlap\nDistributed similarity\nThe combination of the banking operations ...\n... had agreed to buy Spanish rival ...\n2x model part (a) Concat. Logistic regression\nCoreference scoring\nPairwise features\nTrigger-local & sentencelevel features\nModel part (b).\nFigure 1: The two parts of the model. The first part computes a representation for a single event mention. The second part is fed with two such event-mention representations plus a number of pairwise features for the input event-mention pair, and calculates a coreference score.\nData properties A preliminary analysis of one dataset used in our experiments (ACE++; see Section 5) further motivates the design of our model. We found that 50.97 % of coreferential event-mentions pairs share no arguments, either by mentioning distinct argument roles or because one/both mentions have no annotated arguments. Furthermore, 47.29 % of positive event-mention pairs have different trigger words. It is thus important to not solely rely on intrinsic event properties in order to model event mentions, but to additionally take the surrounding sentence\u2019s semantics into account. Another observation regards the distance of coreferential event mentions in a document. 55.42% are more than five sentences apart. This indicates that a locality-based heuristic would not perform well and also encourages the use of sentential features for making coreference decisions.\n3.1 Learning event representations\nThe architecture of the model (Figure 1) is split into two parts. The first one aims at adequately representing individual event mentions. As is common in literature, words of the whole sentence of an input event mention are represented as real-valued vectors viw of a fixed size dw, with i being a word\u2019s position in the sentence. These word embeddings\nare updated during model training and are stored in a matrix Ww \u2208 Rdw\u00d7|V |; |V | being the vocabulary size of the dataset.\nFurthermore, we take the relative position of tokens with respect to the mention into account, as suggested by (Collobert et al., 2011; Zeng et al., 2014). The rationale is that while the absolute position of learned features in a sentence might not be relevant for an event-related decision, the position of them wrt. the event mention is. Embeddings v(\u00b7)p of size dp for relative positions are generated in a way similar to word embeddings. Embeddings for words and positions are concatenated into vectors v (\u00b7) t of size dt = dw + dp. A sentence with s words is thus represented by a matrix of dimensions s\u00d7dt. This matrix serves as input to a convolution layer.\nIn order to compress the semantics of s words into a sentence-level feature vector with constant size, the convolution layer applies dc filters to each window of n consecutive words, calculating dc features for each n-gram of a sentence. For a single filter and particular window, this operation is defined as\nvic = relu(wc \u00b7 vi:i+n\u22121t + bc), (1)\nwhere wc \u2208 Rn\u2217dt is a filter, vi:i+n\u22121t is the flattened concatenation of vectors v(\u00b7)t for words at\n4\n301\n302\n303\n304\n305\n306\n307\n308\n309\n310\n311\n312\n313\n314\n315\n316\n317\n318\n319\n320\n321\n322\n323\n324\n325\n326\n327\n328\n329\n330\n331\n332\n333\n334\n335\n336\n337\n338\n339\n340\n341\n342\n343\n344\n345\n346\n347\n348\n349\n356\n357\n358\n359\n360\n361\n362\n363\n364\n365\n366\n367\n368\n369\n370\n371\n372\n373\n374\n375\n376\n377\n378\n379\n380\n381\n382\n383\n384\n385\n386\n387\n388\n389\n390\n391\n392\n393\n394\n395\n396\n397\n398\n399\npositions i through i+n\u22121, bc is a bias, and relu is the activation function of a rectified linear unit. In Figure 1, dc = 3 and n = 2.\nIn order to identify the most indicative features in the sentence and to introduce invariance for the absolute position of these, we feed the n-gram representations to a max-pooling layer, which identifies the maximum value for each filter. We treat n-grams on each side of the trigger word separately, which allows the model to handle multiple event mentions per sentence, similar in spirit to (Chen et al., 2015; Zeng et al., 2015). The pooling is defined as\nvj,km = max (v i c), (2)\nwhere 1 \u2264 j \u2264 dc designates a feature, k \u2208 {left, right} corresponds to a sentence part, and i runs through the convolution windows of k. The output of this step are sentential features vsent \u2208 R2\u2217dc of the input event mention.\nAdditionally, we provide the network with trigger-local, lexical-level features by concatenating vsent with the word embeddings v (\u00b7) w of the trigger word and its left and right neighbor, resulting in vsent+lex \u2208 R2\u2217dc+3\u2217dw . This encourages the model to take the lexical semantics of the trigger into account, as these can be a strong indicator for coreference. The result is processed by an additional hidden layer, generating the final event-mention representation ve with size de used for the subsequent event-linking decision:\nve = tanh(Wevsent+lex + be). (3)\n3.2 Learning coreference decisions\nThe second part of the model (Figure 1 (b)) processes the representations for two event mentions, and augments these with pairwise comparison features to determine the compatibility of the event mentions. The following features are used, in parentheses we give the feature value for the pair E1, E2 from the example in Section 1: \u2022 Coarse-grained and/or fine-grained event type agree-\nment (yes, yes) \u2022 Antecedent event is in first sentence (yes) \u2022 (Bagged) distance between event mentions in #sen-\ntences/#intermediate event mentions (1, 0) \u2022 Agreement in event modality (yes) \u2022 Overlap in arguments (two shared arguments)\nThe concatenation of these vectors (vsent+lex+pairw) is processed by a single-layer neural network which calculates a distributed similarity of size dsim for the two event mentions:\nvsim = square(Wsimvsent+lex+pairw + bsim). (4)\n1: procedure GENERATECLUSTERS(Pd, score): 2: Pd = {(mi,mj)}i,j 3: score : Pd 7\u2192 (0, 1) 4: Cd \u2190 {(mi,mj) \u2208 Pd : score(mi,mj) > 0.5} 5: while \u2203(mi,mk), (mk,mj) \u2208 Cd : (mi,mj) 6\u2208 Cd do 6: Cd \u2190 Cd \u222a {(mi,mj)} 7: return Cd\nFigure 3: Generation of event clusters Cd for a document d based on the coreference scores from the model. Pd is the set of all event-mention pairs from a document, as implemented in Figure 2.\nThe use of the square function as the network\u2019s non-linearity is backed by the intuition that for measuring similarity, an invariance under polarity changes is desirable. Having dsim similarity dimensions allows the model to learn multiple similarity facets in parallel; in our experiments, this setup outperformed model variants with different activation functions as well as a cosine-similarity based comparison.\nTo calculate the final output of the model, vsim is fed to a logistic regression classifier, whose output serves as the coreference score:\nscore = \u03c3(Woutvsim + bout) (5)\nWe train the model parameters\n\u03b8 = {Ww,Wp, {wc}, {bc},We, be,Wsim, bsim,Wout, bout} (6)\nby minimizing the logistic loss over shuffled minibatches with gradient descent using Adam (Kingma and Ba, 2014).\n3.3 Example generation and clustering\nWe investigated two alternatives for the generation of examples from documents with recognized event mentions. Figure 2 shows the strategy we found to perform best, which iterates over the event mentions of a document and pairs each mention (the \u201canaphors\u201d) with all preceding ones (the \u201cantecedent\u201d candidates). This strategy applies to both training and inference time. Soon et al. (2001) propose an alternative strategy, which during training\n5\n401\n402\n403\n404\n405\n406\n407\n408\n409\n410\n411\n412\n413\n414\n415\n416\n417\n418\n419\n420\n421\n422\n423\n424\n425\n426\n427\n428\n429\n430\n431\n432\n433\n434\n435\n436\n437\n438\n439\n440\n441\n442\n443\n444\n445\n446\n447\n448\n449\n459\n460\n461\n462\n463\n464\n465\n466\n467\n468\n469\n470\n471\n472\n473\n474\n475\n476\n477\n478\n479\n480\n481\n482\n483\n484\n485\n486\n487\n488\n489\n490\n491\n492\n493\n494\n495\n496\n497\n498\n499\nACE ACE++\n# documents 599 1950 # event instances 3617 7520 # event mentions 4728 9956\nTable 1: Dataset properties.\ncreates positive examples only for the closest actual antecedent of an anaphoric event mention with intermediate event mentions serving as negative antecedent candidates. In our experiments, this strategy performed worse than the less elaborate algorithm in Figure 2.\nThe pairwise coreference decisions of our model induce a clustering of a document\u2019s event mentions. In order to force the model to output a consistent view on a given document, a strategy for resolving conflicting decisions is needed. We followed the strategy detailed in Figure 3, which builds the transitive closure of all positive links. Additionally, we experimented with Ng and Gardent (2002)\u2019s \u201cBestLink\u201d strategy, which discards all but the highest-scoring antecedent of an anaphoric event mention. Liu et al. (2014) reported that for event linking, BestLink outperforms naive transitive closure, however, in our experiments (Section 5) we come to a different conclusion.\n4 Experimental setting, model training\nWe implemented our model using the TensorFlow framework (Abadi et al., 2015, v0.6), and chose the ACE 2005 dataset (Walker et al., 2006, later: ACE) as our main testbed. The annotation of this corpus focuses on the event types Conflict.Attack, Movement.Transport, and Life.Die reporting about terrorist attacks, movement of goods and people, and deaths of people; but also contains many more related event types as well as mentions of businessrelevant and judicial events. The corpus consists of merely 599 documents, which is why we create a second dataset that encompasses these documents and additionally contains 1351 more web documents annotated in an analogous fashion. We refer to this second dataset as ACE++. Both datasets are split 9:1 into a development (dev) and test partition; we further split dev 9:1 into a training (train) and validation (valid) partition.2 Table 1 lists statistics for the datasets.\nThere are a number of architectural alternatives 2The list of documents in ACEvalid/ACEtest is published\nhere: https://git.io/vwEEP.\nin the model as well as hyperparameters to optimize. Besides the size of intermediate representations in the model (dw, dp, dc, de, dsim), we experimented with different convolution window sizes n, activation functions for the similarity-function layer in model part (b), whether to use the dual pooling and final hidden layer in model part (a), whether to apply regularization with `2 penalties or Dropout, and parameters to Adam (\u03b7, \u03b21, \u03b22, ). We started our exploration of this space of possibilities from previously reported hyperparameter values (Zhang and Wallace, 2015; Chen et al., 2015) and followed a combined strategy of random sampling from the hyperparameter space (180 points) and line search. Optimization was done by training on ACE++train and evaluating on ACE ++ valid. The final settings we used for all following experiments are listed in Table 2. Ww is initialized with pre-trained embeddings of (Mikolov et al., 2013)3, all other model parameters are randomly initialized. Model training is run for 2000 epochs, after which the best model on the respective valid partition is selected.\n5 Evaluation\nThis section elaborates on the conducted experiments. First, we compare our approach to state-ofart systems on dataset ACE, after which we report experiments on ACE++, where we contrast variations of our model to gain insights about the impact of the utilized feature classes. We conclude this section with an error analysis.\n5.1 Comparison to state-of-the-art on ACE\nTable 3 depicts the performance of our model, trained on ACEtrain, on ACEtest, along with the performance of state-of-the-art systems from the literature. From the wide range of proposed metrics for the evaluation of coreference resolution, we believe\n3https://code.google.com/archive/p/ word2vec/\n6\n501\n502\n503\n504\n505\n506\n507\n508\n509\n510\n511\n512\n513\n514\n515\n516\n517\n518\n519\n520\n521\n522\n523\n524\n525\n526\n527\n528\n529\n530\n531\n532\n533\n534\n535\n536\n537\n538\n539\n540\n541\n542\n543\n544\n545\n546\n547\n548\n549\n550\n551\n552\n553\n554\n555\n556\n557\n558\n559\n560\n561\n562\n563\n564\n565\n566\n567\n568\n569\n570\n571\n572\n573\n574\n575\n576\n577\n578\n579\n580\n581\n582\n583\n584\n585\n586\n587\n588\n589\n590\n591\n592\n593\n594\n595\n596\n597\n598\n599\nBLANC B-CUBED MUC Positive links\n4 \u2217 (Precision / Recall / F1 score) in % This paper 71.80 75.16 73.31 90.52 86.12 88.26 61.54 45.16 52.09 47.89 56.20 51.71 (Liu et al., 2014) 70.88 70.01 70.43 89.90 88.86 89.38 53.42 48.75 50.98 55.86 40.52 46.97 (Bejan and Harabagiu, 2010) \u2014 \u2014 \u2014 83.4 84.2 83.8 \u2014 \u2014 \u2014 43.3 47.1 45.1 (Sangeetha and Arock, 2012) \u2014 \u2014 \u2014 \u2014 \u2014 87.7 \u2014 \u2014 \u2014 \u2014 \u2014 \u2014\nTable 3: Event-linking performance of our model & competitors on ACE. Best value per metric in bold.\nBLANC (Recasens and Hovy, 2011) has the highest validity, as it balances the impact of positive and negative event-mention links in a document. Negative links and consequently singleton event mentions are more common in this dataset (more than 90 % of links are negative). As Recasens and Hovy (2011) point out, the informativeness of metrics like MUC (Vilain et al., 1995), B-CUBED (Bagga and Baldwin, 1998), and the naive positivelink metric suffers from such imbalance. We still add these metrics for completeness, and because BLANC is not available for all systems.\nUnfortunately, there are two caveats to this comparison. Firstly, while a 9:1 train/test split is the commonly accepted way of using ACE, the exact documents in the partitions vary from system to system. Secondly, published methods follow different strategies regarding preprocessing components. While all systems in Table 3 use gold-annotated event-mention triggers, Bejan and Harabagiu (2010) and Liu et al. (2014) use a semantic-role labeler and other tools instead of gold-argument information. We argue that using gold-annotated event mentions is reasonable in order to mitigate error propagation along extraction pipeline and make performance values for the task at hand more informative.\nWe beat Liu et al. (2014)\u2019s system in terms of F1 score on BLANC, MUC, and positive-links, while their system performs better in terms of B-CUBED. Even when taking into account the caveats mentioned above, it seems justified to assess that our model performs in general on-par with their stateof-the-art system. Their approach involves randomforest classification with best-link clustering and propagation of attributes between event mentions, and is grounded on a manifold of external feature sources, i.e., it uses a \u201crich set of 105 semantic features\u201d. Thus, their approach is strongly tied to domains where these semantic features are available and is potentially hard to port to other text kinds. In contrast, our approach does not depend\nModel Dataset BLANC\n(P/R/F1 in %)\n1) Section 3 ACE 71.80 75.16 73.31 2) Sec. 3 + BestLink ACE 75.68 69.72 72.19\n3) Section 3 ACE++ 73.22 83.21 76.90 4) Sec. 3 + BestLink ACE++ 74.24 68.86 71.09\nTable 4: Impact of data amount and clustering.\non resources with restricted domain availability. Bejan and Harabagiu (2010) propose a nonparametric Bayesian model with standard lexicallevel features and WordNet-based similarity between event elements. We outperform their system in terms of B-CUBED and positive-links, which indicates that their system tends to over-merge event mentions, i.e., has a bias against singletons. They use a slightly bigger variant of ACE with 46 additional documents in their experiments.\nSangeetha and Arock (2012) hand-craft a similarity metric for event mentions based on the number of shared entities in the respective sentences, lexical terms, synsets in WordNet, which serves as input to a mincut-based cluster identification. Their system performs well in terms of B-cubed F1, however their paper provides few details about the exact experimental setup.\nAnother approach with results on ACE was presented by Chen et al. (2009), who employ a maximum-entropy classifier with agglomerative clustering and lexical, discourse, and semantic features, e.g., also a WordNet-based similarity measure. However, they report performance using a threshold optimized on the test set, thus we decided to not include the performance here.\n5.2 Further evaluation on ACE and ACE++\nWe now look at several aspects of the model performance to gain further insights about it\u2019s behavior.\nImpact of dataset size and clustering strategy Table 4 shows the impact of increasing the amount of training data (ACE \u2192 ACE++). This increase\n7\n601\n602\n603\n604\n605\n606\n607\n608\n609\n610\n611\n612\n613\n614\n615\n616\n617\n618\n619\n620\n621\n622\n623\n624\n625\n626\n627\n628\n629\n630\n631\n632\n633\n634\n635\n636\n637\n638\n639\n640\n641\n642\n643\n644\n645\n646\n647\n648\n649\n650\n651\n652\n653\n654\n655\n656\n657\n658\n659\n660\n661\n662\n663\n664\n665\n666\n667\n668\n669\n670\n671\n672\n673\n674\n675\n676\n677\n678\n679\n680\n681\n682\n683\n684\n685\n686\n687\n688\n689\n690\n691\n692\n693\n694\n695\n696\n697\n698\n699\nPw Loc Sen Dataset BLANC\n(P/R/F1 in %)\n1) X ACE++ 57.45 68.16 56.69 2) X X ACE++ 62.24 76.23 64.12 3) X X X ACE++ 73.22 83.21 76.90 4) X X ACE++ 82.60 70.71 74.97 5) X X ACE++ 59.67 66.25 61.28 6) X ACE++ 58.38 55.85 56.70\nTable 5: Impact of feature classes; Pw \u2261 pairwise features, Loc \u2261 trigger-local lexical features, Sen \u2261 sentential features.\nModel Dataset BLANC\n(P/R/F1 in %)\nSection 3 ACE++ 73.22 83.21 76.90 All singletons ACE++ 45.29 50.00 47.53 One instance ACE++ 4.71 50.00 8.60 Same type ACE++ 62.73 84.75 61.35\nTable 6: Event-linking performance of our model against naive baselines.\n(rows 1, 3) leads to a boost in recall, from 75.16% to 83.21%, at the cost of a small decrease in precision. This indicates that the model can generalize much better using this additional training data.\nLooking into the use of the alternative clustering strategy BestLink recommended by Liu et al. (2014), we can make the expected observation of a precision improvement (row 1 vs. 2; row 3 vs. 4), due to fewer positive links being used before the transitive-closure clustering takes place. This is however outweighed by a large decline in recall, resulting in a lower F1 score (73.31\u2192 72.19; 76.90 \u2192 71.09). The better performance of BestLink in Liu et al.\u2019s model suggests that our model already weeds out many low confidence links in the classification step, which makes a downstream filtering unnecessary in terms of precision, and even counter-productive in terms of recall.\nImpact of feature classes Table 5 shows our model\u2019s performance when particular feature classes are removed from the model (with retraining), with row 3 corresponding to the full model as described in Section 3. Unsurprisingly, classifying examples with just pairwise features (row 1) results in the worst performance, and adding first trigger-local lexical features (row 2), then sentential features (row 3) subsequently raises both precision and recall. Just using pairwise features and sentential ones (row 4), boosts precision,\nwhich is counter-intuitive at first, but may be explained by a different utilization of the sententialfeature part of the model during training. This part is then adapted to focus more on the trigger-word aspect, meaning the sentential features degrade to trigger-local features. While this allows to reach higher precision (recall that Section 3 finds that more than fifty percent of positive examples have trigger-word agreement), it substantially limits the model\u2019s ability to learn other coreference-relevant aspects of event-mention pairs, leading to low recall. Further considering rows 5 & 6, we can conclude that all feature classes indeed positively contribute to the overall model performance.\nBaselines The result of applying three naive baselines to ACE++ is shown in Table 6. The all singletons/one instance baselines predict every input link to be negative/positive, respectively. In particular the all-singletons baseline performs well, due to the large fraction of singleton event mentions in the dataset. The third baseline, same event, predicts a positive link whenever there is agreement on the event type, namely, it ignores the possibility that there could be multiple event mentions of the same type in a document which do not refer to the same real-world event, e.g., referring to different terrorist attacks. This baseline also performs quite well, in particular in terms of recall, but shows low precision.\nError analysis We manually investigated a sample of 100 false positives and 100 false negatives from ACE++ in order to get an understanding of system errors.\nIt turns out that a significant portion of the false negatives would involve the resolution of a pronoun to a previous event mention, a very hard and yet unsolved problem. Consider the following examples:\n\u2022 \u201cIt\u2019s crazy that we\u2019re bombing Iraq. It sickens me.\u201d \u2022 \u201cSome of the slogans sought to rebut war supporters\u2019\narguments that the protests are unpatriotic. [...] Nobody questions whether this is right or not.\nIn both examples, the event mentions (trigger words in bold font) are gold-annotated as coreferential, but our model failed to recognize this.\nAnother observation is that for 17 false negatives, we found analogous cases among the sampled false positives where annotators made a different annotation decision. Consider these examples:\n\u2022 The 1860 Presidential Election. [...] Lincoln won a plurality with about 40% of the vote.\n8\n701\n702\n703\n704\n705\n706\n707\n708\n709\n710\n711\n712\n713\n714\n715\n716\n717\n718\n719\n720\n721\n722\n723\n724\n725\n726\n727\n728\n729\n730\n731\n732\n733\n734\n735\n736\n737\n738\n739\n740\n741\n742\n743\n744\n745\n746\n747\n748\n749\n750\n751\n752\n753\n754\n755\n756\n757\n758\n759\n760\n761\n762\n763\n764\n765\n766\n767\n768\n769\n770\n771\n772\n773\n774\n775\n776\n777\n778\n779\n780\n781\n782\n783\n784\n785\n786\n787\n788\n789\n790\n791\n792\n793\n794\n795\n796\n797\n798\n799\n\u2022 She lost her seat in the 1997 election.\nEach bullet point has two event mentions (in bold font) taken from the same document and referring to the same event type, i.e., Personnel.Elect. While in the first example, the annotators identified the mentions as coreferential, the second pair of mentions is not annotated as such. Analogously, 22 out of the 100 analyzed false positives were cases where the misclassification of the system was plausible to a human rater. This exemplifies that this task has many boundary cases were a positive/negative decision is hard to make even for expert annotators, thus putting the overall performance of all models in Table 3 in perspective.\n6 Related work\nWe briefly point out other relevant approaches and efforts from the vast amount of literature.\nEvent coreference In addition to the competitors mentioned in Section 5, approaches for event linking were presented, e.g., by Chen and Ji (2009), who determine link scores with hand-crafted compatibility metrics for event mention pairs and a maximum-entropy model, and feed these to a spectral clustering algorithm. A variation of the eventcoreference resolution task extends the scope to cross-document relations. Cybulska and Vossen (2015) approach this task with various classification models and propose to use a type-specific granularity hierarchy for feature values. Lee et al. (2012) further extend the task definition by jointly resolving entity and event coreference, through several iterations of mention-cluster merge operations. Sachan et al. (2015) describe an active-learning based method for the same problem, where they derive a clustering of entities/events by incorporating bits of human judgment as constraints into the objective function. Araki and Mitamura (2015) simultaneously identify event triggers and disambiguate them wrt. one another with a structured-perceptron algorithm.\nResources Besides the ACE 2005 corpus, a number of other datasets with event-coreference annotation have been presented. Hovy et al. (2013) reports on the annotation process of two corpora from the domains of \u201cviolent events\u201d and biographic texts; to our knowledge neither of them is publicly available. OntoNotes (Weischedel et al., 2013) comprises different annotation layers including coreference (Pradhan et al., 2012), however intermin-\ngles entity and event coreference. A series of releases of the EventCorefBank corpus (Bejan and Harabagiu, 2010; Lee et al., 2012; Cybulska and Vossen, 2014) combine linking of event mentions within and across documents, for which Liu et al. (2014) report a lack of completeness on the withindocument aspect. The ProcessBank dataset (Berant et al., 2014) provides texts with event links from the difficult biological domain.\nOther A few approaches to the upstream task of event extraction, while not considering withindocument event linking, still utilize discourse-level information or even cross-document inference. For example, Liao and Grishman (2010) showed how the output of sentence-based classifiers can be filtered wrt. discourse-level consistency. Yao et al. (2010) resolved coreferences between events from different documents in order to make a global extraction decision, similar to (Ji and Grishman, 2008) and (Li et al., 2011).\nIn addition to convolutional neural networks, more types of neural architectures lend themselves to the generation of sentential features. Recently many recursive networks and recurrent ones have been proposed for the task of relation classification, with state-of-the-art results (Socher et al., 2012; Hashimoto et al., 2013; Ebrahimi and Dou, 2015; Xu et al., 2015; Li et al., 2015).\n7 Conclusion\nOur proposed model for the task of event linking achieves state-of-the-art results without relying on external feature sources. We have thus shown that low linking performance, coming from a lack of semantic knowledge about a domain, is evitable. In addition, our experiments give further empirical evidence for the usefulness of neural models for generating latent-feature representations for sentences.\nAs next steps, we plan to test the model on more datasets and task variations, i.e., in a crossdocument setting or for joint trigger identification and coreference resolution. Furthermore, the generation of sentential features from other types of neural networks seems promising. Regarding our medium-term research agenda, we would like to investigate if the model can benefit from more finegrained information about the discourse structure underlying a text. This could guide the model when encountering the problematic case of pronoun resolution, described in the error analysis.\n9\n801\n802\n803\n804\n805\n806\n807\n808\n809\n810\n811\n812\n813\n814\n815\n816\n817\n818\n819\n820\n821\n822\n823\n824\n825\n826\n827\n828\n829\n830\n831\n832\n833\n834\n835\n836\n837\n838\n839\n840\n841\n842\n843\n844\n845\n846\n847\n848\n849\n850\n851\n852\n853\n854\n855\n856\n857\n858\n859\n860\n861\n862\n863\n864\n865\n866\n867\n868\n869\n870\n871\n872\n873\n874\n875\n876\n877\n878\n879\n880\n881\n882\n883\n884\n885\n886\n887\n888\n889\n890\n891\n892\n893\n894\n895\n896\n897\n898\n899\n", "cats": {"READ": 0.0, "DONTREAD": 1.0}, "reviews": [{"comments": "This paper models event linking using CNNs. Given event mentions, the authors\ngenerate vector representations based on word embeddings passed through a CNN\nand followed by max-pooling. They also concatenate the resulting\nrepresentations with several word embeddings around the mention. Together with\ncertain pairwise features, they produce a vector of similarities using a\nsingle-layer neural network, and compute a coreference score. \nThe model is tested on an ACE dataset and an expanded version with performance\ncomparable to previous feature-rich systems.\nThe main contribution of the paper, in my opinion, is in developing a neural\napproach for entity linking that combines word embeddings with several\nlinguistic features. It is interesting to find out that just using the word\nembeddings is not sufficient for good performance. Fortunately, the linguistic\nfeatures used are limited and do not require manually-crafted external\nresources.  \n\nExperimental setting\n- It appears that gold trigger words are used rather than predicted ones. The\nauthors make an argument why this is reasonable, although I still would have\nliked to see performance with predicted triggers. This is especially\nproblematic as one of the competitor systems used predicted triggers, so the\ncomparison isn't fair. \n- The fact that different papers use different train/test splits is worrisome.\nI would encourage the authors to stick to previous splits as much as possible. \n\nUnclear points\n- The numbers indicating that cross-sentential information is needed are\nconvincing. However, the last statement in the second paragraph (lines 65-70)\nwas not clear to me.\n- Embeddings for positions are said to be generaties \"in a way similar to word\nembeddings\". How exactly? Are they randomly initialized? Are they lexicalized?\nIt is not clear to me why a relative position next to one word should have the\nsame embedding as a relative position next to a different word.\n- How exactly are left vs right neighbors used to create the representation\n(lines 307-311)? Does this only affect the max-pooling operation?\n- The word embeddings of one word before and one word after the trigger words\nare appended to it. This seems a bit arbitrary. Why one word before and after\nand not some other choice?  \n- It is not clear how the event-mention representation v_e (line 330) is used?\nIn the following sections only v_{sent+lex} appear to be used, not v_e.\n- How are pairwise features used in section 3.2? Most features are binary, so I\nassume they are encoded as a binary vector, but what about the distance feature\nfor example? And, are these kept fixed during training?\n\nOther issues and suggestions\n- Can the approach be applied to entity coreference resolution as well? This\nwould allow comparing with more previous work and popular datasets like\nOntoNotes. \n- The use of a square function as nonlinearity is interesting. Is it novel? Do\nyou think it has applicability in other tasks?\n- Datasets: one dataset is publicly available, but results are also presented\nwith ACE++, which is not. Do you have plans to release it? It would help other\nresearchers compare new methods. At least, it would have been good to see a\ncomparison to the feature-rich systems also on this dataset.\n- Results: some of the numbers reported in the results are quite close.\nSignificance testing would help substantiating the comparisons.\n- Related work: among the work on (entity) coreference resolution, one might\nmention the neural network approach by Wiseman et al. (2015)  \n\nMinor issues\n- line 143, \"that\" is redundant. \n- One of the baselines is referred to as \"same type\" in table 6, but \"same\nevent\" in the text (line 670).        \n\nRefs\n- Learning Anaphoricity and Antecedent Ranking Features for Coreference\nResolution. Sam Wiseman, Alexander M. Rush, Jason Weston, and Stuart M.\nShieber. ACL 2015.", "is_meta_review": null, "RECOMMENDATION": "4", "REPLICABILITY": "3", "PRESENTATION_FORMAT": "Poster", "CLARITY": "4", "MEANINGFUL_COMPARISON": "5", "SUBSTANCE": "4", "REVIEWER_CONFIDENCE": "4", "SOUNDNESS_CORRECTNESS": "4", "APPROPRIATENESS": "5", "IMPACT": "3", "ORIGINALITY": "3"}, {"comments": "This paper presents a model for the task of event entity linking, where they\npropose to use sentential features from CNNs in place of external knowledge\nsources which earlier methods have used. They train a two-part model: the first\npart learns an event mention representation, and the second part learns to\ncalculate a coreference score given two event entity mentions.\n\nThe paper is well-written, well-presented and is easy to follow. I rather like\nthe analysis done on the ACE corpus regarding the argument sharing between\nevent coreferences. Furthermore, the analysis on the size impact of the\ndataset is a great motivation for creating their ACE++ dataset. However, there\nare a few\nmajor issues that need to be addressed:\n\n- The authors fail to motivate and analyze the pros and cons of using CNN for\ngenerating mention representations. It is not discussed why they chose CNN and\nthere are no comparisons to the other models (e.g., straightforwardly an RNN).\nGiven that the improvement their model makes according various metrics against\nthe\nstate-of-the-art is only 2 or 3 points on F1 score, there needs to be more\nevidence that this architecture is indeed superior.\n\n- It is not clear what is novel about the idea of tackling event linking with\nsentential features, given that using CNN in this fashion for a classification\ntask is not new. The authors could explicitly point out and mainly compare to\nany existing continuous space methods for event linking. The choice of methods\nin Table 3 is not thorough enough.\n\n- There is no information regarding how the ACE++ dataset is collected. A major\nissue with the ACE dataset is its limited number of event types, making it too\nconstrained and biased. It is important to know what event types ACE++ covers.\nThis can also help support the claim in Section 5.1 that 'other approaches are\nstrongly tied to the domain where these semantic features are available\u00e2\u0080\u00a6our\napproach does not depend on resources with restricted\u00e2\u0080\u00a6', you need to show\nthat those earlier methods fail on some dataset that you succeed on. Also,\nfor enabling any meaningful comparison in future, the authors should think\nabout making this dataset publicly available.\n\nSome minor issues:\n- I would have liked to see the performance of your model without gold\nreferences in Table 3 as well.\n\n- It would be nice to explore how this model can or cannot be augmented with a\nvanilla coreference resolution system. For the specific example in line 687,\nthe off-the-shelf CoreNLP system readily links 'It' to 'bombing', which can be\nsomehow leveraged in an event entity linking baseline.\n\n- Given the relatively small size of the ACE dataset, I think having a\ncompelling model requires testing on the other available resources as well.\nThis further motivates working on entity and event coreference simultaneously.\nI also believe that testing on EventCorefBank in parallel with ACE is\nessential. \n\n- Table 5 shows that the pairwise features have been quite effective, which\nsignals that feature engineering may still be crucial for having a competitive\nmodel (at least on the scale of the ACE dataset). One would wonder which\nfeatures were the most effective, and why not report how the current set was\nchosen and what else was tried.", "is_meta_review": null, "RECOMMENDATION": "3", "REPLICABILITY": "3", "PRESENTATION_FORMAT": "Oral Presentation", "CLARITY": "5", "MEANINGFUL_COMPARISON": "3", "SUBSTANCE": "4", "REVIEWER_CONFIDENCE": "4", "SOUNDNESS_CORRECTNESS": "3", "APPROPRIATENESS": "5", "IMPACT": "2", "ORIGINALITY": "3"}]}
{"text": "Parsing for Universal Dependencies without training\n1 000\n011\n012\n013\n014\n015\n016\n017\n018\n019\n020\n021\n022\n023\n024\n025\n026\n027\n028\n029\n030\n031\n032\n033\n034\n035\n036\n037\n038\n039\n040\n041\n042\n043\n044\n045\n046\n047\n048\n049\n061\n062\n063\n064\n065\n066\n067\n068\n069\n070\n071\n072\n073\n074\n075\n076\n077\n078\n079\n080\n081\n082\n083\n084\n085\n086\n087\n088\n089\n090\n091\n092\n093\n094\n095\n096\n097\n098\n099\n1 Introduction\nGrammar induction and unsupervised dependency parsing are active fields of research in natural language processing (Klein and Manning, 2004; Gelling et al., 2012). However, many data-driven approaches struggle with learning relations that match the conventions of the test data, e.g., Klein and Manning reported the tendency of their DMV parser to make determiners the heads of German nouns. Even supervised transfer approaches (McDonald et al., 2011) suffer from target adaptation problems when facing word order differences.\nThe Universal Dependencies (UD) project (Nivre et al., 2015) offers a dependency formalism that aims at providing a consistent representation across languages, while enforcing a few hard constraints. The arrival of such treebanks, expanded and improved on a regular basis, provides a new milestone for cross-lingual dependency parsing research (McDonald et al., 2013). Furthermore, we expect that such a formalism lends itself more naturally to a simple and linguistically sound rulebased approach to cross-lingual parsing. In this paper we present such an approach.\nOur system is a dependency parser that requires no training, and relies solely on explicit part-ofspeech (POS) constraints that UD imposes. In par-\nticular, UD prescribes that trees are single-rooted, and that function words like adpositions, auxiliaries, and determiners are always dependents of content words, while other formalisms might treat them as heads (De Marneffe et al., 2014).\nContributions Our method goes beyond the existing work on rule-aided unsupervised dependency parsing by a) adapting dependency head rules to UD-compliant POS relations, b) incorporating the UD restriction of function words being leaves, c) using personalized PageRank to improve main predicate identification, and d) making it completely free of language-specific parameters by estimating adposition attachment direction directly on test data.\nWe evaluate our system on 32 languages1 in three setups, depending on the reliability of available POS tags, and compare to a multi-source delexicalized transfer system. In addition, we evaluate the systems\u2019 sensitivity to domain change for a subset of UD languages for which domain information was retrievable. The results expose a solid and competitive system for all UD languages. Our unsupervised parser compares favorably to delexicalized parsing, while being more robust to domain change across languages.\n2 Related work\nOver the recent years, cross-lingual linguistic structure prediction based on model transfer or projection of POS tags and dependency trees has become a relevant line of work (Das and Petrov, 2011; McDonald et al., 2011). These works mostly use supervised learning and different target language adaptation techniques.\n1Out of 33 languages in UD v1.2. We exclude Japanese because the treebank is distributed without word forms and hence we can not provide results on predicted POS.\n2\n101\n102\n103\n104\n105\n106\n107\n108\n109\n110\n111\n112\n113\n114\n115\n116\n117\n118\n119\n120\n121\n122\n123\n124\n125\n126\n127\n128\n129\n130\n131\n132\n133\n134\n135\n136\n137\n138\n139\n140\n141\n142\n143\n144\n145\n146\n147\n148\n149\n150\n151\n152\n153\n154\n155\n156\n157\n158\n159\n160\n161\n162\n163\n164\n165\n166\n167\n168\n169\n170\n171\n172\n173\n174\n175\n176\n177\n178\n179\n180\n181\n182\n183\n184\n185\n186\n187\n188\n189\n190\n191\n192\n193\n194\n195\n196\n197\n198\n199\nThe first group of approaches deals with annotation projection (Yarowsky et al., 2001), whereby parallel corpora are used to transfer annotations between resource-rich source languages and lowresource target languages. Projection relies on the availability and quality of parallel corpora, sourceside taggers and parsers, but also tokenizers, sentence aligners, and word aligners for sources and targets. Hwa et al. (2005) were the first to project syntactic dependencies, and Tiedemann (2014) improved on their projection algorithm. Current state of the art in cross-lingual dependency parsing involves leveraging parallel corpora (Ma and Xia, 2014; Rasooli and Collins, 2015).\nThe second group of approaches deals with transferring source parsing models to target languages. Zeman and Resnik (2008) were the first to introduce the idea of delexicalization: removing lexical features by training and cross-lingually applying parsers solely on POS sequences. S\u00f8gaard (2011) and McDonald et al. (2011) independently extend delexicalization to involve multiple sourceside parsers. This line of work depends on applying uniform POS and dependency representations (McDonald et al., 2013).\nBoth model transfer and annotation projection rely on a large number of presumptions to derive their competitive parsing models. By and large, these presumptions are unrealistic and exclusive to a group of very closely related, resource-rich Indo-European languages. Agic\u0301 et al. (2015) expose some of the biases in their proposal for realistic cross-lingual POS tagging, as they emphasize the lack of perfect sentence and word splitting for truly low-resource languages. Johannsen et al. (2016) introduce joint projection of POS and dependencies from multiple sources while sharing the outlook on bias removal in real-world multilingual processing.\nCross-lingual learning, realistic or not, depends entirely on the availability of data: for the sources, for the targets, or most often for both sets of languages. Moreover, it typically does not exploit the constraints placed on the linguistic structures through the formalism, and it does so by design. With the emergence of UD as the practical standard for cross-language annotation of POS and syntactic dependencies, we argue for an approach that takes a fresh angle on both aspects. Namely, we propose a parser that i) requires no training data, and in contrast ii) critically relies on exploit-\ning the UD constraints on building POS and dependency annotations.\nThese two characteristics make our parser unsupervised. Data-driven unsupervised dependency parsing is a well-established discipline (Klein and Manning, 2004; Spitkovsky et al., 2010a; Spitkovsky et al., 2010b). Still, the performance of unsupervised parsers falls far behind the approaches involving any sort of supervision.\nOur work builds on the research on rule-aided unsupervised dependency parsing (Gillenwater et al., 2010; Naseem et al., 2010; S\u00f8gaard, 2012a; S\u00f8gaard, 2012b). In particular, we make use of S\u00f8gaard\u2019s (2012b) PageRank method to rank words before decoding. Our system, however, has two key differences: i) the usage of PageRank personalization, and of ii) two-step decoding to treat content and function words differently according to the UD formalism. Through these differences, even without any training data, we parse nearly as well as a delexicalized transfer parser, and with increased stability to domain change.\n3 Method\nOur approach does not use any training or unlabeled data. We have used the English treebank during development to assess the contribution of individual head rules, and to tune PageRank parameters (Sec. 3.1) and function-word directionality (Sec. 3.2). Adposition direction is calculated on the fly on test data. In the following, we refer to our UD parser as UDP.\n3.1 PageRank setup\nOur system uses the PageRank (PR) algorithm (Page et al., 1999) to estimate the relevance of the content words of a sentence. PR gives higher rank to nodes with more incoming edges, as well as to nodes connected to those. Using PR to score word relevance requires an effective graphbuilding strategy. We have experimented with the strategies by S\u00f8gaard (2012b), but our system fares best strictly using the dependency rules in Table 1 to build the graph.\nWe build a multigraph of all words in the sentence covered by the head-dependent rules in Table 1, giving each word an incoming edge for each eligible dependent, i.e., ADV depends on ADJ and VERB. This strategy does not always yield connected graphs, and we use a teleport probability of 0.05 to ensure PR convergence. We chose this\n3\n201\n202\n203\n204\n205\n206\n207\n208\n209\n210\n211\n212\n213\n214\n215\n216\n217\n218\n219\n220\n221\n222\n223\n224\n225\n226\n227\n228\n229\n230\n231\n232\n233\n234\n235\n236\n237\n238\n239\n240\n241\n242\n243\n244\n245\n246\n247\n248\n249\n250\n251\n252\n253\n254\n255\n256\n257\n258\n259\n260\n261\n262\n263\n264\n265\n266\n267\n268\n269\n270\n271\n272\n273\n274\n275\n276\n277\n278\n279\n280\n281\n282\n283\n284\n285\n286\n287\n288\n289\n290\n291\n292\n293\n294\n295\n296\n297\n298\n299\nvalue incrementally in intervals of 0.01 during development until we found the smallest value that guaranteed PR convergence. A high teleport probability is undesirable, because the resulting stationary distribution can be almost uniform. We did not have to re-adjust this value when running on the actual test data.\nThe the main idea behind our personalized PR approach is the observation that ranking is only relevant for content words.2 PR can incorporate a priori knowledge of the relevance of nodes by means of personalization, namely giving more weight to certain nodes. Intuitively, the higher the rank of a word, the closer it should be to the root node, i.e., the main predicate of the sentence is the node that should have the highest PR, making it the dependent of the root node (Fig. 1, lines 4-5). We use PR personalization to give 5 times more weight (over an otherwise uniform distribution) to the node that is estimated to be main predicate, i.e., the first verb or the first content word if there are no verbs.\n3.2 Head direction\nHead direction is an important syntactic trait. Indeed, the UD feature inventory contains a trait to distinguish adposition between pre- and postpositions. Instead of relying on this feature from the treebanks, which is not always provided, we estimate the frequency of ADP-NOMINAL vs. NOMINAL-ADP bigrams.3 This estimation requires very few examples to converge (10-15 sentences), and we calculate it directly on test data.\nIf a language has more ADP-NOMINAL bigrams, we consider all its ADP to be prepositions (and thus dependent of elements at their right). Otherwise, we consider them to be postpositions.\nFor other function words, we have determined on the English dev data whether to make them strictly right- or left-attaching, or to allow either direction: AUX, DET, and SCONJ are right-attaching, while CONJ and PUNCT are leftattaching. There are no direction constraints for the rest.\n3.3 Decoding\nFig. 1 shows the tree-decoding algorithm. It has two blocks, namely a first block (3-11) where we assign the head of content words according to\n2ADJ, NOUN, PROPN, and VERB mark content words. 3NOMINAL= {NOUN, PROPN, PRON}\n1: H = \u2205; D = \u2205 2: C = \u3008c1, ...cm\u3009; F = \u3008f1, ...fm\u3009 3: for c \u2208 C do 4: if |H| = 0 then 5: h = root 6: else 7: h =argminj\u2208H {\u03b3(j, c) | \u03b4(j, c)\u2227\u03ba(j, c)} 8: end if 9: H = H \u222a {c} 10: D = D \u222a {(h, c)} 11: end for 12: for f \u2208 F do 13: h =argminj\u2208H {\u03b3(j, f) | \u03b4(j, f) \u2227 \u03ba(j, f)} 14: D = D \u222a {(h, f)} 15: end for 16: return D\nFigure 1: Two-step decoding algorithm for UDP.\nADJ \u2212\u2192 ADV VERB \u2212\u2192 ADV, AUX, NOUN, PROPN, PRON, SCONJ NOUN, PROPN \u2212\u2192 ADP, DET, NUM NOUN, PROPN \u2212\u2192 ADJ, NOUN, PROPN\nTable 1: UD dependency rules\ntheir PageRank and the constraints of the dependency rules, and a second block (12-15) where we assign the head of function words according to their proximity, direction of attachment, and dependency rules. The algorithm requires:\n1. The PR-sorted list of content words C. 2. The set of function words F . 3. A set H for the current possible heads, and\na set D for the dependencies assigned at each iteration, which we represent as headdependent tuples (h, d). 4. A symbol root for the root node. 5. A function \u03b3(n,m) that gives the linear dis-\ntance between two nodes. 6. A function \u03ba(h, d) that returns whether the\ndependency (h, d) has a valid attachment direction given the POS of the d (cf. Sec. 3.2). 7. A function \u03b4(h, d) that determines whether (h, d) is licensed by the rules in Table 1.\nThe head assignations in lines 7 and 13 read as follow: the head h of a word (either c or f ) is the closest element of the current list of heads (H) that has the right direction (\u03ba) and respects the POSdependency rules (\u03b4). These assignations have a back-off option to ensure the final D is a tree. If the conditions determined by \u03ba and \u03b4 are too strict,\n4\n301\n302\n303\n304\n305\n306\n307\n308\n309\n310\n311\n312\n313\n314\n315\n316\n317\n318\n319\n320\n321\n322\n323\n324\n325\n326\n327\n328\n329\n330\n331\n332\n333\n334\n335\n336\n337\n338\n339\n340\n341\n342\n343\n344\n345\n346\n347\n348\n349\n357\n358\n359\n360\n361\n362\n363\n364\n365\n366\n367\n368\n369\n378\n379\n380\n381\n382\n383\n384\n385\n386\n392\n393\n394\n395\n396\n397\n398\n399\ni.e. if the set of possible heads is empty, we drop the \u03b4 head-rule constraint and recalculate the closest possible head that respects the directionality imposed by \u03ba. If the set is empty again, we drop both constraints and assign the closest head.\nLines 4 and 5 enforce the single-root constraint. To enforce the leaf status of function nodes, the algorithm first attaches all content words (C), and then all function words (F ) in the second block where H is not updated, thereby ensuring leafness for all f \u2208 F . The order of head attachment is not monotonic wrt. PR between the first and second block, and can yield non-projectivities. Nevertheless, it still is a one-pass algorithm. Decoding runs in less than O(n2), namely O(n\u00d7 |C|). However, running PR incurs the main computation cost.\n4 Parser run example\nThis section exemplifies a full run of UDP for the example sentence \u201cThey also had a special connection to some extremists\u201d, an actual clause from the English test data.\n4.1 PageRank\nGiven an input sentence and its POS tags, we obtain rank of each word by building a graph using head rules and running PR on it. Table 2 provides the sentence, the POS of each word, the number of incoming edges for each word after building the graph with the head rules from Sec. 3.1, and the personalization vector for PR on this sentence. Note that all nodes have the same personalization weight, except the estimated main predicate, the verb \u201chad\u201d.\nWord: They also had a special connection to some extremists POS: PRON ADV VERB DET ADJ NOUN ADP DET NOUN\nPersonalization: 1 1 5 1 1 1 1 1 1 Incoming edges: 0 0 4 0 1 5 0 0 5\nTable 2: Words, POS, Personalization and incoming edges for the example sentence.\nTable 3 shows the directed multigraph used for PR in detail. We can see e.g. that the four incoming edges for the verb \u201ccome\u201d from the two nouns, plus from the adverb \u201calso\u201d and the pronoun \u201cThey\u201d.\nAfter running PR, we obtain the following ranking for content words: C = \u3008had,connection,extremists,special\u3009 Even though the verb has four incoming edges and the nouns have six each, the personalization makes the verb the highest-ranked word.\n4.2 Decoding\nOnce C is calculated, we can follow the algorithm in Fig. 1 to obtain a dependency parse. Table 4 shows a trace of the algorithm, given C and F : C = \u3008had,connection,extremists,special\u3009 F = {They, also, a, to, some}\nThe first four iterations calculate the head of content words following their PR, and the following iterations attach the function words in F .\nFinally, Fig. 2 shows the resulting dependency tree. Full lines are assigned in the first block (content dependents), dotted lines are assigned in the second block (function dependents). The edge labels indicate in which iteration the algorithm has assigned each dependency.\nNote that the algorithm is deterministic for a certain input POS sequence. Any 10-token sentence with the POS labels shown in Table 2 would yield the same dependency tree.4\n4The resulting trees always pass the validation script in\n5\n401\n402\n403\n404\n405\n406\n407\n408\n409\n410\n411\n412\n413\n414\n415\n416\n417\n418\n419\n420\n421\n422\n423\n424\n425\n426\n427\n428\n429\n430\n431\n432\n433\n434\n435\n436\n437\n438\n439\n440\n441\n442\n443\n444\n445\n446\n447\n448\n449\n450\n451\n452\n453\n454\n455\n456\n457\n458\n459\n460\n461\n462\n463\n464\n465\n466\n467\n468\n469\n470\n471\n472\n473\n474\n475\n476\n477\n478\n479\n480\n481\n482\n483\n484\n485\n486\n487\n488\n489\n490\n491\n492\n493\n494\n495\n496\n497\n498\n499\nLanguage BLG UDPG MSDG MSDP UDPP UDPN\nAncient Greek 42.2 L 43.4 48.6 46.5 41.6 27.0 Arabic 34.8 R 47.8 52.8 52.6 47.6 41.0 Basque 47.8 R 45.0 51.2 49.3 43.1 22.8\nBulgarian 54.9 R 70.5 78.7 76.6 68.1 27.1 Church Slavonic 53.8 L 59.2 61.8 59.8 59.2 35.2\nCroatian 41.6 L 56.7 69.1 65.6 54.5 25.2 Czech 46.5 R 61.0 69.5 67.6 59.3 25.3\nDanish 47.3 R 57.9 70.2 65.6 53.8 26.9 Dutch 36.1 L 49.5 57.0 59.2 50.0 24.1\nEnglish 46.2 R 53.0 62.1 59.9 51.4 27.9 Estonian 73.2 R 70.0 73.4 66.1 65.0 25.3 Finnish 43.8 R 45.1 52.9 50.4 43.1 21.6 French 47.1 R 64.5 72.7 70.6 62.1 36.3 German 48.2 R 60.6 66.9 62.5 57.0 24.2 Gothic 50.2 L 57.5 61.7 59.2 55.8 34.1 Greek 45.7 R 58.5 68.0 66.4 57.0 29.3\nHebrew 41.8 R 55.4 62.0 58.6 52.8 35.7 Hindi 43.9 R 46.3 34.6 34.5 45.7 27.0 Hungarian 53.1 R 56.7 58.4 56.8 54.8 22.7 Indonesian 44.6 L 60.6 63.6 61.0 58.4 35.3\nIrish 47.5 R 56.6 62.5 61.3 53.9 35.8 Italian 50.6 R 69.4 77.1 75.2 67.9 37.6 Latin 49.4 L 56.2 59.8 54.9 52.4 37.1 Norwegian 49.1 R 61.7 70.8 67.3 58.6 29.8 Persian 37.8 L 55.7 57.8 55.6 53.6 33.9 Polish 60.8 R 68.4 75.6 71.7 65.7 34.6 Portuguese 45.8 R 65.7 72.8 71.4 64.9 33.5 Romanian 52.7 R 63.7 69.2 64.0 58.9 32.1\nSlovene 50.6 R 63.6 74.7 71.0 56.0 24.3 Spanish 48.2 R 63.9 72.9 70.7 62.1 35.0 Swedish 52.4 R 62.8 72.2 67.2 58.5 25.3\nTamil 41.4 R 34.2 44.2 39.5 32.1 20.3\nAverage 47.8 57.5 63.9 61.2 55.3 29.9\nTable 5: UAS for baseline with gold POS (BLG) with direction (L/R) for backoff attachments, UDP with gold POS (UDPG) and predicted POS (UDPP ), PR with naive content-function POS (UDPN ), and multi-source delexicalized with gold and predicted POS (MSDG and MSDP , respectively). BL values higher than UDPG are underlined, and UDPG values higher than MSDG are in boldface.\n5 Experiments\nThis section describes the data and metrics used to assess the performance of UDP, as well as the systems we compare against. We evaluate on the test sections of the UD1.2 treebanks (Nivre et al., 2015) that contain word forms. If there is more than one treebank per language, we use the treebank that has the canonical language name (e.g. Finnish instead of Finnish-FTB).\nIt is common to evaluate unsupervised dependency parsing using alternative metrics like undirected attachment score or neutralized edge direction, or to evaluate short sentences separately (Schwartz et al., 2011; Gelling et al., 2012). In contrast, we use standard unlabeled attachment score (UAS) and evaluate on all sentences of the\ngithub.com/UniversalDependencies/tools\ncanonical UD test sets.\n5.1 Baseline\nWe compare our UDP system with the performance of a rule-based baseline that uses the head rules in Table 5. The baseline identifies the first verb (or first content word if there are no verbs) as the main predicate, and assigns heads to all words according to the rules in Table 1.\nWe have selected the set of head rules to maximize precision on the development set, and they do not provide full coverage. The system makes any word not covered by the rules (e.g., a word with a POS such as X or SYM) either dependent of their left or right neighbor, depending on the estimated runtime parameter.\nWe report the best head direction and its score for each language in Table 5. This baseline finds the head of each token based on its closest possible head, or on its immediate left or right neighbor if there is no head rule for the POS at hand, which means that this system does not necessarily yield well-formed tress. Each token receives a head, and while the structures are single-rooted, they are not necessarily connected.\nNote that we do not include results for the DMV model by Klein and Manning (2004), as it has been outperformed by a system similar to ours (S\u00f8gaard, 2012b). The usual adjacency baseline for unsupervised dependency parsing, where all words depend on their left or right neighbor, fares much worse than our baseline (20% UAS below on average) even when we make an oracle pick for the best per-language direction. Therefore we do not report those scores.\n5.2 Evaluation setup\nOur system relies solely on POS tags. To estimate the quality degradation of our system under non-gold POS scenarios, we evaluate UDP on two alternative scenarios. The first is predicted POS (UDPP ), where we tag the respective test set with TnT (Brants, 2000) trained on each language\u2019s training set. The second is a naive typeconstrained two-POS tag scenario (UDPN ), and approximates a lower bound. We give each word either CONTENT or FUNCTION tag, depending on the word\u2019s frequency. Words that belong to the 100 most frequent word types of the input test section receive the FUNCTION tag.\nFinally, we compare our system to a supervised cross-lingual system (MSD). It is a multi-\n6\n501\n502\n503\n504\n505\n506\n507\n508\n509\n510\n511\n512\n513\n514\n515\n516\n517\n518\n519\n520\n521\n522\n523\n524\n525\n526\n527\n528\n529\n530\n531\n532\n533\n534\n535\n536\n537\n538\n539\n540\n541\n542\n543\n544\n545\n546\n547\n548\n549\n550\n551\n552\n553\n554\n555\n556\n557\n558\n559\n560\n561\n562\n563\n564\n565\n566\n567\n568\n569\n570\n571\n572\n573\n574\n575\n576\n577\n578\n579\n580\n581\n582\n583\n584\n585\n586\n587\n588\n589\n590\n591\n592\n593\n594\n595\n596\n597\n598\n599\nsource delexicalized transfer parser, referred to as multi-dir in the original paper by McDonald et al. (2011). For this baseline we train TurboParser (Martins et al., 2013) on a delexicalized training set of 20k sentences, sampled uniformly from the UD training data excluding the target language. MSD is a competitive baseline in crosslingual transfer parsing work. This gives us an indication how our system compares to standard cross-lingual parsers.\n5.3 Results\nTable 5 shows that UDP is a competitive system; because UDPG is remarkably close to the supervised MSDG system, with an average difference of 6.4%, even outperforming MSDG on one language (Hindi).\nMore interestingly, on the evaluation scenario with predicted POS we observe that our system drops only marginally (2.2%) compared to MSD (2.7%). In the least robust rule-based setup, the error propagation rate from POS to dependency would be doubled, as either a wrongly tagged head or dependent would break the dependency rules. However, with an average POS accuracy by TnT of 94.1%, the error propagation is 0.37, i.e each POS error causes 0.37 additional dependency errors. In contrast, for the MSD system this error propagation is 0.46, thus higher.5\nFor the extreme POS scenario, content vs. function POS (CF), the drop in performance for UDP is however very large. But this might be a too crude evaluation setup. Nevertheless, UDP, the simple unsupervised system with PageRank, outperforms the adjacency baselines (BL) by 4% on average on the two type-based naive POS tag scenario. This difference indicates that even with very deficient POS tags, UDP can provide better structures.\n6 Discussion\nIn this section we provide a further error analysis of the UDP parser. We examine the contribution to the overal results of using PageRank to score content words, the behavior of the system across different parts of speech, and we assess the robustness of UDP when parsing text from different domains.\n5Err. prop. = (E(ParseP )\u2212E(ParseG))/E(POSP ), where E(x) = 1\u2212Accuracy(x).\n6.1 PageRank contribution\nThe performance of UDP depends on PageRank to score content words, and on two-step decoding to ensure the leaf status of function words. In this section we isolate the constribution of both parts. We do so by comparing the performance of BL, UDP, and UDPNoPR, a version of UDP where we disable PR and rank content words according to their reading order, i.e. the first word in the ranking is the first word to be read, regardless of the specific language\u2019s script direction\nThe baseline BL described in 5.1 already ensures function words are leaf nodes, because they have no listed dependent POS in the head rules. The task of the decoding steps is mainly to ensure the resulting structures are well-formed dependency trees.\nHowever, if we measure the difference between UDPNoPR and BL, we observe that UDPNoPR contributes with 4 UAS points on average over the baseline. Nevertheless, the baseline is oracleinformed about the language\u2019s best branching direction, a property that UDP does not have. Instead, the decoding step determines head direction as described in Section 3.2.\nComplementary, we can measure the contribution of PR by observing the difference between regular UDP and UDPNoPR. The latter scores on average 9 UAS points lower than UDP. These 9 points are strictly determined by the better attachment of content words.\n6.2 Breakdown by POS\nUD is a constantly-improving effort, and not all v1.2 treebanks have the same level of formalism compliance. Thus, the interpretation of, e.g., the AUX-VERB or DET-PRON distinctions might differ. However, we do not incorporate these differences in our analysis and consider all treebanks equally compliant.\nThe root accuracy scores oscillate around an average of 69%, with Arabic and Tamil (26%) and Estonian (93%) as outliers. Given the PR personalization (Sec. 3.1), UDP has a strong bias for chosing the first verb as main predicate. However, without personalization, performance drops 2% on average. This improvement is consistent even for verb-final languages like Hindi. Moreover, our personalization strategy makes PR converge a whole order of magnitude faster.\nThe bigram heuristic to determine adposition\n7\n601\n602\n603\n604\n605\n606\n607\n608\n609\n610\n611\n612\n613\n614\n615\n616\n617\n618\n619\n620\n621\n622\n623\n624\n625\n626\n627\n628\n629\n630\n631\n632\n633\n634\n635\n636\n637\n638\n639\n640\n641\n642\n643\n644\n645\n646\n647\n648\n649\n650\n651\n652\n653\n654\n655\n656\n657\n658\n659\n660\n661\n662\n663\n664\n665\n666\n667\n668\n669\n670\n671\n672\n673\n674\n675\n676\n677\n678\n679\n680\n681\n682\n683\n684\n685\n686\n687\n688\n689\n690\n691\n692\n693\n694\n695\n696\n697\n698\n699\nLanguage BLG MSDG UDPG MSDP UDPP\nBulgarian 50.1\u00b12.4 73.5 \u00b13.5 69.7\u00b11.8 71.3\u00b13.3 66.9\u00b13.2 Croatian+Serbian 42.1\u00b10.7 66.0\u00b13.0 57.8\u00b11.4 62.1\u00b13.0 54.4 \u00b12.0\nEnglish 42.2\u00b12.8 60.1\u00b16.2 53.9\u00b12.5 57.3\u00b14.3 52.0 \u00b13.3 Italian 50.3\u00b11.2 70.0\u00b15.4 70.1\u00b13.3 68.1\u00b16.0 68.7\u00b13.9\nAverage Std. 1.8 4.5 2.5 4.2 3.1\nTable 6: Average language-wise domain evaluation. We report average UAS and standard deviation per language. The bottom row provides the average standard deviation for each system.\ndirection succeeds at identifying the predominant pre- or postposition preference for all languages (average ADP UAS of 75%). The fixed direction for the other functional POS is largely effective, with few exceptions, e.g., DET is consistently right-attaching on all treebanks except Basque (average overall DET UAS of 84%, 32% for Basque). These alternations could also be estimated from the data in a manner similar to ADP. Our rules do not make nouns eligible heads for verbs. As a result, the system cannot infer relative clauses. We have excluded the NOUN \u2192 VERB head rule during development because it makes the hierarchical relation between verbs and nouns less conclusive.\nWe have not excluded punctuation from the evaluation. Indeed, the UAS for the PUNCT is low (an average of 21%, standard deviation of 9.6), even lower than the otherwise problematic CONJ. Even though conjunctions are pervasive and identifying their scope is one of the usual challenges for parsers, the average UAS for CONJ is much larger (an average of 38%, standard deviation of 13.5) than for PUNCT. Both POS show large standard deviations, which indicates great variability. This variability can be caused by linguistic properties of the languages or evaluation datasets, but also by differences in annotation convention.\n6.3 Cross-domain consistency\nModels with fewer parameters are less likely to be overfit for a certain dataset. In our case, a system with few, general rules is less likely to make attachment decisions that are very particular of a certain language or dataset. Plank and van Noord (2010) have shown that rule-based parsers can be more stable to domain shift. We explore if their finding holds for UDP as well, by testing on i) the UD development data as a readily available proxy for domain shift, and ii) manually curated domain splits of select UD test sets.\nLanguage Domain BLG MSDG UDPG MSDP UDPP Bulgarian bulletin 48.3 67.5 67.4 65.4 61.5 legal 47.9 76.9 69.2 73.0 68.6 literature 53.6 74.2 69.0 72.8 66.6 news 49.3 74.6 70.2 73.0 68.2 various 51.4 74.2 72.5 72.6 69.5\nCroatian news 41.2 62.4 57.9 61.8 52.2 wiki 41.9 64.8 55.8 58.2 56.3\nEnglish answers 44.1 61.6 55.9 59.5 53.7 email 42.8 58.8 52.1 57.1 56.3 newsgroup 41.7 55.5 49.7 52.9 51.1 reviews 47.4 66.8 54.9 63.9 52.2 weblog 43.3 51.6 50.9 49.8 53.8 magazine\u2020 41.4 60.9 55.6 58.4 53.3 bible\u2020 38.4 56.2 56.2 56.8 48.6 questions\u2020 38.7 69.7 55.6 60.5 47.2\nItalian europarl 50.8 64.1 70.6 62.7 69.7 legal 51.1 67.9 69.0 64.4 67.2 news 49.4 68.9 67.5 67.0 65.3 questions 48.7 80.0 77.0 79.1 76.1 various 49.7 67.8 69.0 65.3 67.6 wiki 51.8 71.2 68.1 70.3 66.6\nSerbian news 42.8 68.0 58.8 65.6 53.3 wiki 42.4 68.9 58.8 62.8 55.8\nTable 7: Evaluation across domains. UAS for baseline with gold POS (BLG), UDP with gold POS (UDPG) and predicted POS (UDPP ), and multi-source delexicalized with gold and predicted POS (MSDG and MSDP ). English datasets marked with \u2020 are in-house annotated. Lowest results per language underlined. Bold: UDP outperforms MSD.\nDevelopment sets We have used the English development data to choose which relations would be included as head rules in the final system (Cf. Table 1). It would be possible that some of the rules are indeed more befitting for the English data or for that particular section.\nHowever, if we regard the results for UDPG in Table 5, we can see that there are 24 languages (out of 32) for which the parser performs better than for English. This result indicates that the head rules are general enough to provide reasonable parses for languages other than the one chosen for development.\nIf we run UDPG on the development sections for the other languages, we find the results are very consistent. Any language scores on average \u00b11 UAS with regards to the test section. There is no clear tendency for either section being easier to parse with our system.\nCross-domain test sets To further assess the cross-domain robustness, we retrieved the domain (genre) splits6 from the test sections of the UD\n6The data splits are freely available at http://ANONYMIZED\n8\n701\n702\n703\n704\n705\n706\n707\n708\n709\n710\n711\n712\n713\n714\n715\n716\n717\n718\n719\n720\n721\n722\n723\n724\n725\n726\n727\n728\n729\n730\n731\n732\n733\n734\n735\n736\n737\n738\n739\n740\n741\n742\n743\n744\n745\n746\n747\n748\n749\n750\n751\n752\n753\n754\n755\n756\n757\n758\n759\n760\n761\n762\n763\n764\n765\n766\n767\n768\n769\n770\n771\n772\n773\n774\n775\n776\n777\n778\n779\n780\n781\n782\n783\n784\n785\n786\n787\n788\n789\n790\n791\n792\n793\n794\n795\n796\n797\n798\n799\ntreebanks where the domain information is available as sentence metadata: from Bulgarian, Croatian, and Italian. We also include a UD-compliant Serbian dataset which is not included in the UD release but which is based on the same parallel corpus as Croatian and has the same domain splits (Agic\u0301 and Ljubes\u030cic\u0301, 2015). When averaging we pool Croatian and Serbian together as they come from the same dataset. Also, we use a tagger trained on the Croatian UD training data for tagging Serbian.\nFor English, we have obtained the data splits in the test section matching the sentences from the original distribution of the English Web Treebank. In addition to these already available data sets, we have annotated three different datasets to asses domain variation more extensively, namely the first 50 verses of the King James Bible, 50 sentences from a magazine, and 75 sentences from the test split in QuestionBank (Judge et al., 2006). We include the third dataset to evaluate strictly on questions, which we could do already in Italian. While the answers domain in English is made up of text from the Yahoo! Answers forum, only one fourth of the sentences are questions. Note these three small datasets are not included in the results on the canonical test sections in Table 5.7\nTable 6 summarizes the per-language average score and standard deviation, as well as the macroaveraged standard deviation across languages. UDP has a much lower standard deviation across domains compared to MSD. This holds across languages. We attribute this higher stability to UDP being developed to satisfy a set of general properties of the UD syntactic formalism, instead of being a data-driven method more sensitive to sampling bias. This holds for both the gold-POS and predicted-POS setup. The differences in standard deviation are unsurprisingly smaller in the predicted POS setup. In general, the rule-based UPD is less sensitive to domain shifts than the datadriven MSD counterpart, confirming earlier findings (Plank and van Noord, 2010).\nTable 7 gives the detailed scores per language and domain. From the scores we can see that presidental bulletin, legal and weblogs are amongst the hardest domains to parse. However, the systems often do not agree on which domain is hardest, with the exception of Bulgarian\n7The three in-house annotated datasets are freely available at http://ANONYMIZED\nbulletin. More importantly, this might even change between gold and predicted POS, highlighting the importance of evaluating systems beyond gold POS. Interestingly, for the Italian data and some of the hardest domains UDP outperforms MSD, confirming that it is a robust baseline.\n7 Conclusion\nWe have presented UDP, an unsupervised dependency parser for Universal Dependencies that makes use of personalized PageRank and a small set of head-dependent rules. The parser requires no training data and estimates adpositon direction directly from test data. We achieve competitive performance on all but two UD languages, and even beat a multi-source delexicalized parser (MSD) on Hindi. We evaluated the parser on three POS setups and across domains. Our results show that UDP is less affected by deteriorating POS tags than MSD, and is more resilient to domain changes. Both the parser and the in-domain annotated test sets are freely available.8\nFurther work includes extending the parser to handle multiword expressions, coordination, and proper names. Moreover, our usage of PR could be expanded to directly score the potential dependency edges\u2014e.g., by means of edge reification\u2014 instead of words. Finally, we only considered unlabeled attachment, however, our system could easily be augmented with partial edge labeling.\n", "cats": {"READ": 0.0, "DONTREAD": 1.0}, "reviews": [{"comments": "This paper describes a new deterministic dependency parsing algorithm and\nanalyses its behaviour across a range of languages.\nThe core of the algorithm is a set of rules defining permitted dependencies\nbased on POS tags.\nThe algorithm starts by ranking words using a slightly biased PageRank over a\ngraph with edges defined by the permitted dependencies.\nStepping through the ranking, each word is linked to the closest word that will\nmaintain a tree and is permitted by the head rules and a directionality\nconstraint.\n\nOverall, the paper is interesting and clearly presented, though seems to differ\nonly slightly from Sogaard (2012), \"Unsupervised Dependency Parsing without\nTraining\".\nI have a few questions and suggestions:\n\nHead Rules (Table 1) - It would be good to have some analysis of these rules in\nrelation to the corpus.\nFor example, in section 3.1 the fact that they do not always lead to a\nconnected graph is mentioned, but not how frequently it occurs, or how large\nthe components typically are.\n\nI was surprised that head direction was chosen using the test data rather than\ntraining or development data.\nGiven how fast the decision converges (10-15 sentences), this is not a major\nissue, but a surprising choice.\n\nHow does tie-breaking for words with the same PageRank score work?\nDoes it impact performance significantly, or are ties rare enough that it\ndoesn't have an impact?\n\nThe various types of constraints (head rules, directionality, distance) will\nlead to upper bounds on possible performance of the system.\nIt would be informative to include oracle results for each constraint, to show\nhow much they hurt the maximum possible score.\nThat would be particularly helpful for guiding future work in terms of where to\ntry to modify this system.\n\nMinor:\n\n- 4.1, \"we obtain [the] rank\"\n\n- Table 5 and Table 7 have columns in different orders. I found the Table 7\narrangement clearer.\n\n- 6.1, \"isolate the [contribution] of both\"", "is_meta_review": null, "RECOMMENDATION": "3", "REPLICABILITY": "4", "PRESENTATION_FORMAT": "Poster", "CLARITY": "4", "MEANINGFUL_COMPARISON": "5", "SUBSTANCE": "3", "REVIEWER_CONFIDENCE": "4", "SOUNDNESS_CORRECTNESS": "4", "APPROPRIATENESS": "5", "IMPACT": "2", "ORIGINALITY": "2"}, {"comments": "The authors proposed an unsupervised algorithm for Universal Dependencies that\ndoes not require training. The tagging is based on PageRank for the words and a\nsmall amount of hard-coded rules.\nThe article is well written, very detailed and the intuition behind all prior\ninformation being added to the model is explained clearly.\nI think that the contribution is substantial to the field of unsupervised\nparsing, and the possibilities for future work presented by the authors give\nrise to additional research.", "is_meta_review": null, "RECOMMENDATION": "4", "REPLICABILITY": "5", "PRESENTATION_FORMAT": "Oral Presentation", "CLARITY": "5", "MEANINGFUL_COMPARISON": "5", "SUBSTANCE": "5", "REVIEWER_CONFIDENCE": "4", "SOUNDNESS_CORRECTNESS": "4", "APPROPRIATENESS": "5", "IMPACT": "4", "ORIGINALITY": "4"}, {"comments": "This paper presents a way to parse trees (namely the universal dependency\ntreebanks) by relying only on POS and by using a modified version of the\nPageRank to give more way to some meaningful words (as opposed to stop words).\n\nThis idea is interesting though very closed to what was done in S\u00c3\u00b8gaard\n(2012)'s paper. The personalization factor giving more weight to the main\npredicate is nice but it would have been better to take it to the next level.\nAs far as I can tell, the personalization is solely used for the main predicate\nand its weight of 5 seems arbitrary.\n\nRegarding the evaluation and the detailed analyses, some charts would have been\nbeneficial, because it is sometimes hard to get the gist out of the tables.\nFinally, it would have been interesting to get the scores of the POS tagging in\nthe prediction mode to be able to see if the degradation in parsing performance\nis heavily correlated to the degradation in tagging performance (which is what\nwe expect).\n\nAll in all, the paper is interesting but the increment over the work of\nS\u00c3\u00b8gaard (2012) is small.\n\nSmaller issues:\n-------------------\n\nl. 207 : The the main idea -> The main idea", "is_meta_review": null, "RECOMMENDATION": "3", "REPLICABILITY": "4", "PRESENTATION_FORMAT": "Poster", "CLARITY": "4", "MEANINGFUL_COMPARISON": "3", "SUBSTANCE": "3", "REVIEWER_CONFIDENCE": "4", "SOUNDNESS_CORRECTNESS": "4", "APPROPRIATENESS": "5", "IMPACT": "3", "ORIGINALITY": "2"}]}
